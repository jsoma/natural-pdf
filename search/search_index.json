{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Natural PDF","text":"<p>A friendly library for working with PDFs, built on top of pdfplumber.</p> <p>Natural PDF lets you find and extract content from PDFs using simple code that makes sense.</p> <ul> <li>Live demo here</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install natural_pdf\n# All the extras\npip install \"natural_pdf[all]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from natural_pdf import PDF\n\npdf = PDF('document.pdf')\npage = pdf.pages[0]\n\n# Find the title and get content below it\ntitle = page.find('text:contains(\"Summary\"):bold')\ncontent = title.below().extract_text()\n\n# Exclude everything above 'CONFIDENTIAL' and below last line on page\npage.add_exclusion(page.find('text:contains(\"CONFIDENTIAL\")').above())\npage.add_exclusion(page.find_all('line')[-1].below())\n\n# Get the clean text without header/footer\nclean_text = page.extract_text()\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<p>Here are a few highlights of what you can do:</p>"},{"location":"#find-elements-with-selectors","title":"Find Elements with Selectors","text":"<p>Use CSS-like selectors to find text, shapes, and more.</p> <pre><code># Find bold text containing \"Revenue\"\npage.find('text:contains(\"Revenue\"):bold').extract_text()\n\n# Find all large text\npage.find_all('text[size&gt;=12]').extract_text()\n</code></pre> <p>Learn more about selectors \u2192</p>"},{"location":"#navigate-spatially","title":"Navigate Spatially","text":"<p>Move around the page relative to elements, not just coordinates.</p> <pre><code># Extract text below a specific heading\nintro_text = page.find('text:contains(\"Introduction\")').below().extract_text()\n\n# Extract text from one heading to the next\nmethods_text = page.find('text:contains(\"Methods\")').below(\n    until='text:contains(\"Results\")'\n).extract_text()\n</code></pre> <p>Explore more navigation methods \u2192</p>"},{"location":"#extract-clean-text","title":"Extract Clean Text","text":"<p>Easily extract text content, automatically handling common page elements like headers and footers (if exclusions are set).</p> <pre><code># Extract all text from the page (respecting exclusions)\npage_text = page.extract_text()\n\n# Extract text from a specific region\nsome_region = page.find(...)\nregion_text = some_region.extract_text()\n</code></pre> <p>Learn about text extraction \u2192 Learn about exclusion zones \u2192</p>"},{"location":"#apply-ocr","title":"Apply OCR","text":"<p>Extract text from scanned documents using various OCR engines.</p> <pre><code># Apply OCR using the default engine\nocr_elements = page.apply_ocr()\n\n# Extract text (will use OCR results if available)\ntext = page.extract_text()\n</code></pre> <p>Explore OCR options \u2192</p>"},{"location":"#analyze-document-layout","title":"Analyze Document Layout","text":"<p>Use AI models to detect document structures like titles, paragraphs, and tables.</p> <pre><code># Detect document structure\npage.analyze_layout()\n\n# Highlight titles and tables\npage.find_all('region[type=title]').highlight(color=\"purple\")\npage.find_all('region[type=table]').highlight(color=\"blue\")\n\n# Extract data from the first table\ntable_data = page.find('region[type=table]').extract_table()\n</code></pre> <p>Learn about layout models \u2192 Working with tables? \u2192</p>"},{"location":"#document-question-answering","title":"Document Question Answering","text":"<p>Ask natural language questions directly to your documents.</p> <pre><code># Ask a question\nresult = pdf.ask(\"What was the company's revenue in 2022?\")\nif result.get(\"found\", False):\n    print(f\"Answer: {result['answer']}\")\n</code></pre> <p>Learn about Document QA \u2192</p>"},{"location":"#classify-pages-and-regions","title":"Classify Pages and Regions","text":"<p>Categorize pages or specific regions based on their content using text or vision models.</p> <p>Note: Requires <code>pip install \"natural-pdf[ai]\"</code></p> <pre><code># Classify a page based on text\nlabels = [\"invoice\", \"scientific article\", \"presentation\"]\npage.classify(labels, using=\"text\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n\n\n# Classify a page based on what it looks like\nlabels = [\"invoice\", \"scientific article\", \"presentation\"]\npage.classify(labels, using=\"vision\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n</code></pre>"},{"location":"#visualize-your-work","title":"Visualize Your Work","text":"<p>Debug and understand your extractions visually.</p> <pre><code># Highlight headings\npage.find_all('text[size&gt;=14]').show(color=\"red\", label=\"Headings\")\n\n# Launch the interactive viewer (Jupyter)\n# Requires: pip install natural-pdf[viewer]\npage.viewer()\n\n# Or save an image\n# page.save_image(\"highlighted.png\")\n</code></pre> <p>See more visualization options \u2192</p>"},{"location":"#documentation-topics","title":"Documentation Topics","text":"<p>Choose what you want to learn about:</p>"},{"location":"#task-based-guides","title":"Task-based Guides","text":"<ul> <li>Getting Started: Install the library and run your first extraction</li> <li>PDF Navigation: Open PDFs and work with pages</li> <li>Element Selection: Find text and other elements using selectors</li> <li>Text Extraction: Extract clean text from documents</li> <li>Regions: Work with specific areas of a page</li> <li>Visual Debugging: See what you're extracting</li> <li>OCR: Extract text from scanned documents</li> <li>Layout Analysis: Detect document structure</li> <li>Tables: Extract tabular data</li> <li>Document QA: Ask questions to your documents</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference: Complete library reference</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all the classes and methods in Natural PDF.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#pdf-class","title":"PDF Class","text":"<p>The main entry point for working with PDFs.</p> <pre><code>class PDF:\n    \"\"\"\n    The main entry point for working with PDFs.\n\n    Parameters:\n        path (str): Path to the PDF file.\n        password (str, optional): Password for encrypted PDFs. Default: None\n        reading_order (bool, optional): Sort elements in reading order. Default: True\n        keep_spaces (bool, optional): Keep spaces in word elements. Default: True\n        font_attrs (list, optional): Font attributes to use for text grouping. \n                                    Default: ['fontname', 'size']\n        ocr (bool/dict/str, optional): OCR configuration. Default: False\n        ocr_engine (str/Engine, optional): OCR engine to use. Default: \"easyocr\"\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>pages</code> Access pages in the document N/A (property) <code>PageCollection</code> <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>add_exclusion(func, label=None)</code> Add a document-wide exclusion zone <code>func</code>: Function taking a page and returning region<code>label</code>: Optional label for the exclusion <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections across all pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries ('start', 'end', 'both', 'none') <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None)</code> Ask a question about the document content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path <code>dict</code>: Result with answer and metadata"},{"location":"api/#page-class","title":"Page Class","text":"<p>Represents a single page in a PDF document.</p> <pre><code>class Page:\n    \"\"\"\n    Represents a single page in a PDF document.\n\n    Properties:\n        page_number (int): 1-indexed page number\n        page_index (int): 0-indexed page position\n        width (float): Page width in points\n        height (float): Page height in points\n        pdf (PDF): Parent PDF object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the page <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>create_region(x0, top, x1, bottom)</code> Create a region at specific coordinates <code>x0</code>: Left coordinate<code>top</code>: Top coordinate<code>x1</code>: Right coordinate<code>bottom</code>: Bottom coordinate <code>Region</code> <code>highlight(elements, color=None, label=None)</code> Highlight elements on the page <code>elements</code>: Elements to highlight<code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight <code>Page</code> (self) <code>highlight_all(include_types=None, include_text_styles=False, include_layout_regions=False)</code> Highlight all elements on the page <code>include_types</code>: Element types to include<code>include_text_styles</code>: Whether to include text styles<code>include_layout_regions</code>: Whether to include layout regions <code>Page</code> (self) <code>save_image(path, resolution=72, labels=True)</code> Save an image of the page with highlights <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>None</code> <code>to_image(resolution=72, labels=True)</code> Get a PIL Image of the page with highlights <code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>PIL.Image</code> <code>analyze_text_styles()</code> Group text by visual style properties None <code>dict</code>: Mapping of style name to elements <code>analyze_layout(engine=\"yolo\", confidence=0.2, existing=\"replace\")</code> Detect layout regions using ML models <code>model</code>: Model to use (\"yolo\", \"tatr\")<code>confidence</code>: Confidence threshold<code>existing</code>: How to handle existing regions <code>ElementCollection</code>: Detected regions <code>add_exclusion(region, label=None)</code> Add an exclusion zone to the page <code>region</code>: Region to exclude<code>label</code>: Optional label for the exclusion <code>Region</code>: The exclusion region <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections from the page <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the page content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>apply_ocr(languages=None, min_confidence=0.0, **kwargs)</code> Apply OCR to the page <code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold<code>**kwargs</code>: Additional OCR engine parameters <code>ElementCollection</code>: OCR text elements"},{"location":"api/#region-class","title":"Region Class","text":"<p>Represents a rectangular area on a page.</p> <pre><code>class Region:\n    \"\"\"\n    Represents a rectangular area on a page.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the region\n        height (float): Height of the region\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the region <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>expand(left=0, top=0, right=0, bottom=0, width_factor=1.0, height_factor=1.0)</code> Expand the region in specified directions <code>left/top/right/bottom</code>: Points to expand in each direction<code>width_factor/height_factor</code>: Scale width/height by this factor <code>Region</code>: Expanded region <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight the region <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Region attributes to display <code>Region</code> (self) <code>to_image(resolution=72, crop_only=False)</code> Get a PIL Image of just the region <code>resolution</code>: Image resolution in DPI<code>crop_only</code>: Whether to exclude border <code>PIL.Image</code> <code>save_image(path, resolution=72, crop_only=False)</code> Save an image of just the region <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>crop_only</code>: Whether to exclude border <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections within the region <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the region content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>extract_table(method=None, table_settings=None, use_ocr=False)</code> Extract table data from the region <code>method</code>: Extraction method (\"pdfplumber\", \"tatr\")<code>table_settings</code>: Custom settings for extraction<code>use_ocr</code>: Whether to use OCR text <code>list</code>: Table data as rows and columns <code>intersects(other)</code> Check if this region intersects with another <code>other</code>: Another region <code>bool</code>: True if regions intersect <code>contains(x, y)</code> Check if a point is within the region <code>x</code>: X coordinate<code>y</code>: Y coordinate <code>bool</code>: True if point is in region"},{"location":"api/#element-types","title":"Element Types","text":""},{"location":"api/#element-base-class","title":"Element Base Class","text":"<p>The base class for all PDF elements.</p> <pre><code>class Element:\n    \"\"\"\n    Base class for all PDF elements.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the element\n        height (float): Height of the element\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>above(height=None, full_width=True, until=None, include_until=True)</code> Create a region above the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>below(height=None, full_width=True, until=None, include_until=True)</code> Create a region below the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>select_until(selector, include_endpoint=True, full_width=True)</code> Create a region from this element to another <code>selector</code>: Selector for endpoint<code>include_endpoint</code>: Whether to include endpoint<code>full_width</code>: Whether to span page width <code>Region</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight this element <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Element attributes to display <code>Element</code> (self) <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from this element <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>next(selector=None, limit=None, apply_exclusions=True)</code> Get the next element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>prev(selector=None, limit=None, apply_exclusions=True)</code> Get the previous element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>nearest(selector, max_distance=None, apply_exclusions=True)</code> Get the nearest element matching selector <code>selector</code>: Selector for elements<code>max_distance</code>: Maximum distance in points<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code>"},{"location":"api/#textelement","title":"TextElement","text":"<p>Represents text elements in the PDF.</p> <pre><code>class TextElement(Element):\n    \"\"\"\n    Represents text elements in the PDF.\n\n    Additional Properties:\n        text (str): The text content\n        fontname (str): The font name\n        size (float): The font size\n        bold (bool): Whether the text is bold\n        italic (bool): Whether the text is italic\n        color (tuple): The text color as RGB tuple\n        confidence (float): OCR confidence (for OCR text)\n        source (str): 'pdf' or 'ocr'\n    \"\"\"\n</code></pre> <p>Main Properties</p> Property Type Description <code>text</code> <code>str</code> The text content <code>fontname</code> <code>str</code> The font name <code>size</code> <code>float</code> The font size <code>bold</code> <code>bool</code> Whether the text is bold <code>italic</code> <code>bool</code> Whether the text is italic <code>color</code> <code>tuple</code> The text color as RGB tuple <code>confidence</code> <code>float</code> OCR confidence (for OCR text) <code>source</code> <code>str</code> 'pdf' or 'ocr' <code>font_variant</code> <code>str</code> Font variant identifier (e.g., 'AAAAAB+') <p>Additional Methods</p> Method Description Parameters Returns <code>font_info()</code> Get detailed font information None <code>dict</code>: Font properties"},{"location":"api/#collections","title":"Collections","text":""},{"location":"api/#elementcollection","title":"ElementCollection","text":"<p>A collection of elements with batch operations.</p> <pre><code>class ElementCollection:\n    \"\"\"\n    A collection of elements with batch operations.\n\n    This class provides operations that can be applied to multiple elements at once.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all elements <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>filter(selector)</code> Filter elements by selector <code>selector</code>: CSS-like selector string <code>ElementCollection</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight all elements <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Attributes to display <code>ElementCollection</code> (self) <code>first</code> Get the first element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>last</code> Get the last element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>highest()</code> Get the highest element on the page None <code>Element</code> or <code>None</code> <code>lowest()</code> Get the lowest element on the page None <code>Element</code> or <code>None</code> <code>leftmost()</code> Get the leftmost element on the page None <code>Element</code> or <code>None</code> <code>rightmost()</code> Get the rightmost element on the page None <code>Element</code> or <code>None</code> <code>__len__()</code> Get the number of elements None <code>int</code> <code>__getitem__(index)</code> Get an element by index <code>index</code>: Index or slice <code>Element</code> or <code>ElementCollection</code>"},{"location":"api/#pagecollection","title":"PageCollection","text":"<p>A collection of pages with cross-page operations.</p> <pre><code>class PageCollection:\n    \"\"\"\n    A collection of pages with cross-page operations.\n\n    This class provides operations that can be applied across multiple pages.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start', new_section_on_page_break=False)</code> Get sections spanning multiple pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries<code>new_section_on_page_break</code>: Whether to start new sections at page breaks <code>list[Region]</code> <code>__len__()</code> Get the number of pages None <code>int</code> <code>__getitem__(index)</code> Get a page by index <code>index</code>: Index or slice <code>Page</code> or <code>PageCollection</code>"},{"location":"api/#ocr-classes","title":"OCR Classes","text":""},{"location":"api/#ocrengine","title":"OCREngine","text":"<p>Base class for OCR engines.</p> <pre><code>class OCREngine:\n    \"\"\"\n    Base class for OCR engines.\n\n    This class provides the interface for OCR engines.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>process_image(image, languages=None, min_confidence=0.0, **kwargs)</code> Process an image with OCR <code>image</code>: PIL Image<code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold <code>list</code>: OCR results"},{"location":"api/#easyocrengine","title":"EasyOCREngine","text":"<p>OCR engine using EasyOCR.</p> <pre><code>class EasyOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using EasyOCR.\n\n    Parameters:\n        model_dir (str, optional): Directory for models. Default: None\n    \"\"\"\n</code></pre>"},{"location":"api/#paddleocrengine","title":"PaddleOCREngine","text":"<p>OCR engine using PaddleOCR.</p> <pre><code>class PaddleOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using PaddleOCR.\n\n    Parameters:\n        use_angle_cls (bool, optional): Use text direction classification. Default: False\n        lang (str, optional): Language code. Default: \"en\"\n        det (bool, optional): Use text detection. Default: True\n        rec (bool, optional): Use text recognition. Default: True\n        cls (bool, optional): Use text direction classification. Default: False\n        det_model_dir (str, optional): Detection model directory. Default: None\n        rec_model_dir (str, optional): Recognition model directory. Default: None\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre>"},{"location":"api/#document-qa-classes","title":"Document QA Classes","text":""},{"location":"api/#documentqa","title":"DocumentQA","text":"<p>Class for document question answering.</p> <pre><code>class DocumentQA:\n    \"\"\"\n    Class for document question answering.\n\n    Parameters:\n        model (str, optional): Model name or path. Default: \"microsoft/layoutlmv3-base\"\n        device (str, optional): Device to use. Default: \"cpu\"\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>ask(question, image, word_boxes, min_confidence=0.0, max_answer_length=None, language=None)</code> Ask a question about a document <code>question</code>: Question to ask<code>image</code>: Document image<code>word_boxes</code>: Text positions<code>min_confidence</code>: Minimum confidence threshold<code>max_answer_length</code>: Maximum answer length<code>language</code>: Language code <code>dict</code>: Result with answer and metadata"},{"location":"api/#selector-syntax","title":"Selector Syntax","text":"<p>Natural PDF uses a CSS-like selector syntax to find elements in PDFs.</p>"},{"location":"api/#basic-selectors","title":"Basic Selectors","text":"Selector Description Example <code>element_type</code> Match elements of this type <code>text</code>, <code>rect</code>, <code>line</code> <code>[attribute=value]</code> Match elements with this attribute value <code>[fontname=Arial]</code>, <code>[size=12]</code> <code>[attribute&gt;=value]</code> Match elements with attribute &gt;= value <code>[size&gt;=12]</code> <code>[attribute&lt;=value]</code> Match elements with attribute &lt;= value <code>[size&lt;=10]</code> <code>[attribute~=value]</code> Match elements with attribute approximately equal <code>[color~=red]</code>, <code>[color~=(1,0,0)]</code> <code>[attribute*=value]</code> Match elements with attribute containing value <code>[fontname*=Arial]</code>"},{"location":"api/#pseudo-classes","title":"Pseudo-Classes","text":"Pseudo-Class Description Example <code>:contains(\"text\")</code> Match elements containing text <code>text:contains(\"Summary\")</code> <code>:starts-with(\"text\")</code> Match elements starting with text <code>text:starts-with(\"Summary\")</code> <code>:ends-with(\"text\")</code> Match elements ending with text <code>text:ends-with(\"2023\")</code> <code>:bold</code> Match bold text <code>text:bold</code> <code>:italic</code> Match italic text <code>text:italic</code>"},{"location":"api/#attribute-names","title":"Attribute Names","text":"Attribute Element Types Description <code>fontname</code> text Font name <code>size</code> text Font size <code>color</code> text, rect, line Color <code>width</code> rect, line Width <code>height</code> rect Height <code>confidence</code> text (OCR) OCR confidence score <code>source</code> text Source ('pdf' or 'ocr') <code>type</code> region Region type (e.g., 'table', 'title') <code>model</code> region Layout model that detected the region <code>font-variant</code> text Font variant identifier"},{"location":"api/#constants-and-configuration","title":"Constants and Configuration","text":""},{"location":"api/#color-names","title":"Color Names","text":"<p>Natural PDF supports color names in selectors.</p> Color Name RGB Value Example <code>red</code> (1, 0, 0) <code>[color~=red]</code> <code>green</code> (0, 1, 0) <code>[color~=green]</code> <code>blue</code> (0, 0, 1) <code>[color~=blue]</code> <code>black</code> (0, 0, 0) <code>[color~=black]</code> <code>white</code> (1, 1, 1) <code>[color~=white]</code>"},{"location":"api/#region-types","title":"Region Types","text":"<p>Layout analysis models detect the following region types:</p> Model Region Types YOLO <code>title</code>, <code>plain-text</code>, <code>table</code>, <code>figure</code>, <code>figure_caption</code>, <code>table_caption</code>, <code>table_footnote</code>, <code>isolate_formula</code>, <code>formula_caption</code>, <code>abandon</code> TATR <code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>"},{"location":"categorizing-documents/","title":"Categorizing Pages and Regions","text":"<p>Natural PDF allows you to automatically categorize pages or specific regions within a page using machine learning models. This is incredibly useful for filtering large collections of documents or understanding the structure and content of individual PDFs.</p>"},{"location":"categorizing-documents/#installation","title":"Installation","text":"<p>To use the classification features, you need to install the optional dependencies:</p> <pre><code>pip install \"natural-pdf[ai]\"\n</code></pre> <p>This installs necessary libraries like <code>torch</code>, <code>transformers</code>, and others.</p>"},{"location":"categorizing-documents/#core-concept-the-classify-method","title":"Core Concept: The <code>.classify()</code> Method","text":"<p>The primary way to perform categorization is using the <code>.classify()</code> method available on <code>Page</code> and <code>Region</code> objects.</p> <pre><code>from natural_pdf import PDF\n\n# Example: Classify a Page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\nlabels = [\"invoice\", \"letter\", \"report cover\", \"data table\"]\npage.classify(labels, using=\"text\")\n\n# Access the top result\nprint(f\"Top Category: {page.category}\")\nprint(f\"Confidence: {page.category_confidence:.3f}\")\n</code></pre> <p>Key Arguments:</p> <ul> <li><code>labels</code> (required): A list of strings representing the potential labels you want to classify the item into.</li> <li><code>using</code> (optional): Specifies which classification model or strategy to use. Defaults to <code>\"text\"</code>.<ul> <li><code>\"text\"</code>: Uses a text-based model (default: <code>facebook/bart-large-mnli</code>) suitable for classifying based on language content.</li> <li><code>\"vision\"</code>: Uses a vision-based model (default: <code>openai/clip-vit-base-patch32</code>) suitable for classifying based on visual layout and appearance.</li> <li>Specific Model ID: You can provide a Hugging Face model ID (e.g., <code>\"google/siglip-base-patch16-224\"</code>, <code>\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"</code>) compatible with zero-shot text or image classification. The library attempts to infer whether it's text or vision, but you might need <code>using</code>.</li> </ul> </li> <li><code>model</code> (optional): Explicitly model ID (HuggingFace repo name)</li> <li><code>min_confidence</code> (optional): A float between 0.0 and 1.0. Only labels with a confidence score greater than or equal to this threshold will be included in the results (default: 0.0).</li> </ul>"},{"location":"categorizing-documents/#text-vs-vision-classification","title":"Text vs. Vision Classification","text":"<p>Choosing the right model type depends on your goal:</p>"},{"location":"categorizing-documents/#text-classification-usingtext","title":"Text Classification (<code>using=\"text\"</code>)","text":"<ul> <li>How it works: Extracts the text from the page or region and analyzes the language content.</li> <li>Best for:<ul> <li>Topic Identification: Determining what a page or section is about (e.g., \"budget discussion,\" \"environmental impact,\" \"legal terms\").</li> <li>Content-Driven Document Types: Identifying document types primarily defined by their text (e.g., emails, meeting minutes, news articles, reports).</li> </ul> </li> <li>Data Journalism Example: You have thousands of pages of government reports. You can use text classification to find all pages discussing \"public health funding\" or classify paragraphs within environmental impact statements to find mentions of specific endangered species.</li> </ul> <pre><code># Find pages related to finance\nfinancial_labels = [\"budget\", \"revenue\", \"expenditure\", \"forecast\"]\npdf.classify_pages(financial_labels, using=\"text\")\nbudget_pages = [p for p in pdf.pages if p.category == \"budget\"]\n</code></pre>"},{"location":"categorizing-documents/#vision-classification-usingvision","title":"Vision Classification (<code>using=\"vision\"</code>)","text":"<ul> <li>How it works: Renders the page or region as an image and analyzes its visual layout, structure, and appearance.</li> <li>Best for:<ul> <li>Layout-Driven Document Types: Identifying documents recognizable by their structure (e.g., invoices, receipts, forms, presentation slides, title pages).</li> <li>Identifying Visual Elements: Distinguishing between pages dominated by text, tables, charts, or images.</li> </ul> </li> <li>Data Journalism Example: You have a scanned archive of campaign finance filings containing various document types. You can use vision classification to quickly isolate all the pages that look like donation receipts or expenditure forms, even if the OCR quality is poor.</li> </ul> <pre><code># Find pages that look like invoices or receipts\nvisual_labels = [\"invoice\", \"receipt\", \"letter\", \"form\"]\npage.classify(visual_labels, using=\"vision\")\nif page.category in [\"invoice\", \"receipt\"]:\n    print(f\"Page {page.number} looks like an invoice or receipt.\")\n</code></pre>"},{"location":"categorizing-documents/#classifying-specific-objects","title":"Classifying Specific Objects","text":""},{"location":"categorizing-documents/#pages-pageclassify","title":"Pages (<code>page.classify(...)</code>)","text":"<p>Classifying a whole page is useful for sorting documents or identifying the overall purpose of a page within a larger document.</p> <pre><code># Classify the first page\npage = pdf.pages[0]\npage_types = [\"cover page\", \"table of contents\", \"chapter start\", \"appendix\"]\npage.classify(page_types, using=\"vision\") # Vision often good for page structure\nprint(f\"Page 1 Type: {page.category}\")\n</code></pre>"},{"location":"categorizing-documents/#regions-regionclassify","title":"Regions (<code>region.classify(...)</code>)","text":"<p>Classifying a specific region allows for more granular analysis within a page. You might first detect regions using Layout Analysis and then classify those regions.</p> <pre><code># Assume layout analysis has run, find paragraphs\nparagraphs = page.find_all(\"region[type=paragraph]\")\nif paragraphs:\n    # Classify the topic of the first paragraph\n    topic_labels = [\"introduction\", \"methodology\", \"results\", \"conclusion\"]\n    # Use text model for topic\n    paragraphs[0].classify(topic_labels, using=\"text\")\n    print(f\"First paragraph category: {paragraphs[0].category}\")\n</code></pre>"},{"location":"categorizing-documents/#accessing-classification-results","title":"Accessing Classification Results","text":"<p>After running <code>.classify()</code>, you can access the results:</p> <ul> <li><code>page.category</code> or <code>region.category</code>: Returns the string label of the category with the highest confidence score from the last classification run. Returns <code>None</code> if no classification has been run or no category met the threshold.</li> <li><code>page.category_confidence</code> or <code>region.category_confidence</code>: Returns the float confidence score (0.0-1.0) for the top category. Returns <code>None</code> otherwise.</li> <li><code>page.classification_results</code> or <code>region.classification_results</code>: Returns the full result dictionary stored in the object's <code>.metadata['classification']</code>, containing the model used, engine type, labels provided, timestamp, and a list of all scores above the threshold sorted by confidence. Returns <code>None</code> if no classification has been run.</li> </ul> <pre><code>results = page.classify([\"invoice\", \"letter\"], using=\"text\", min_confidence=0.5)\n\nif page.category == \"invoice\":\n    print(f\"Found an invoice with confidence {page.category_confidence:.2f}\")\n\n# See all results above the threshold\n# print(page.classification_results['scores'])\n</code></pre>"},{"location":"categorizing-documents/#classifying-collections","title":"Classifying Collections","text":"<p>For batch processing, use the <code>.classify_all()</code> method on <code>PDFCollection</code> or <code>ElementCollection</code> objects. This displays a progress bar tracking individual items (pages or elements).</p>"},{"location":"categorizing-documents/#pdfcollection-collectionclassify_all","title":"PDFCollection (<code>collection.classify_all(...)</code>)","text":"<p>Classifies pages across all PDFs in the collection. Use <code>max_workers</code> for parallel processing across different PDF files.</p> <pre><code>collection = natural_pdf.PDFCollection.from_directory(\"./documents/\")\nlabels = [\"form\", \"datasheet\", \"image\", \"text document\"]\n\n# Classify all pages using vision model, processing 4 PDFs concurrently\ncollection.classify_all(labels, using=\"vision\", max_workers=4)\n\n# Filter PDFs containing forms\nform_pdfs = []\nfor pdf in collection:\n    if any(p.category == \"form\" for p in pdf.pages if p.category):\n        form_pdfs.append(pdf.path)\n    pdf.close() # Remember to close PDFs\n\nprint(f\"Found forms in: {form_pdfs}\")\n</code></pre>"},{"location":"categorizing-documents/#elementcollection-element_collectionclassify_all","title":"ElementCollection (<code>element_collection.classify_all(...)</code>)","text":"<p>Classifies all classifiable elements (currently <code>Page</code> and <code>Region</code>) within the collection.</p> <pre><code># Assume 'pdf' is loaded and 'layout_regions' is an ElementCollection of Regions\nlayout_regions = pdf.find_all(\"region\")\nregion_types = [\"paragraph\", \"list\", \"table\", \"figure\", \"caption\"]\n\n# Classify all detected regions based on vision\nlayout_regions.classify_all(region_types, model=\"vision\")\n\n# Count table regions using filter()\ntable_regions = layout_regions.filter(lambda region: region.category == \"table\")\nprint(f\"Found {len(table_regions)} regions classified as tables.\")\n</code></pre>"},{"location":"data-extraction/","title":"Structured Data Extraction","text":"<p>Extracting specific, structured information (like invoice numbers, dates, or addresses) from documents often requires more than simple text extraction. Natural PDF integrates with LLMs to pull out structured data.</p> <p>You need to install more than just the tiny baby default <code>natural_pdf</code> for this: <pre><code># Install the OpenAI (or compatible) client library\npip install openai\n\n# Or pull in the full AI stack (classification, QA, search, etc.)\npip install \"natural_pdf[ai]\"\n</code></pre></p>"},{"location":"data-extraction/#introduction","title":"Introduction","text":"<p>This feature allows you to define the exact data structure you want using a Pydantic model and then instruct an LLM to populate that structure based on the content of a PDF element (like a <code>Page</code> or <code>Region</code>).</p> <p>Not sure how to write a Pydantic schema? Just ask an LLM! \"Write me a Pydantic schema to pull out an invoice number (an integer), a company name (string) and a date (string).\" It'll go fine.</p>"},{"location":"data-extraction/#basic-extraction","title":"Basic Extraction","text":"<ol> <li>Define a Schema: Create a Pydantic model for your desired data.</li> <li>Extract: Use <code>.extract()</code> on a <code>PDF</code>, <code>Page</code>, or <code>Region</code> object.</li> <li>Access: Use <code>.extracted()</code> to retrieve the results.</li> </ol> <pre><code>from natural_pdf import PDF\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Initialize your LLM client\n# Anything OpenAI-compatible works!\nclient = OpenAI(\n    api_key=\"ANTHROPIC_API_KEY\",  # Your Anthropic API key\n    base_url=\"https://api.anthropic.com/v1/\"  # Anthropic's API endpoint\n)\n\n# Load the PDF\npdf = PDF(\"path/to/your/document.pdf\")\npage = pdf.pages[0]\n\n# Define your schema\nclass InvoiceInfo(BaseModel):\n    invoice_number: str = Field(description=\"The main invoice identifier\")\n    total_amount: float = Field(description=\"The final amount due\")\n    company_name: Optional[str] = Field(None, description=\"The name of the issuing company\")\n\n# Extract data\npage.extract(schema=InvoiceInfo, client=client) \n\n# Access the full result object\nfull_data = page.extracted() \nprint(full_data)\n\n# Access a single field\ninv_num = page.extracted('invoice_number')\nprint(f\"Invoice Number: {inv_num}\") \n</code></pre>"},{"location":"data-extraction/#keys-and-overwriting","title":"Keys and Overwriting","text":"<ul> <li>By default, results are stored under the key <code>\"default-structured\"</code> in the element's <code>.analyses</code> dictionary.</li> <li>Use the <code>analysis_key</code> parameter in <code>.extract()</code> to store results under a different name (e.g., <code>analysis_key=\"customer_details\"</code>).</li> <li>Attempting to extract using an existing <code>analysis_key</code> will raise an error unless <code>overwrite=True</code> is specified.</li> </ul> <pre><code># Extract using a specific key\npage.extract(InvoiceInfo, client=client, analysis_key=\"invoice_header\")\n\n# Access using the specific key\nheader_data = page.extracted(analysis_key=\"invoice_header\") \ncompany = page.extracted('company_name', analysis_key=\"invoice_header\")\n</code></pre>"},{"location":"data-extraction/#text-vs-vision","title":"Text vs vision","text":"<p>When sending a page (or a region or etc) to an LLM, you can choose either <code>using='text'</code> (default) or <code>using='vision'</code>.</p> <ul> <li><code>text</code> sends the text, somewhat respecting layout using <code>.extract_text(layout=True)</code></li> <li><code>vision</code> sends an image of the page with <code>.to_image(resolution=72)</code> (no highlights or labels)</li> </ul>"},{"location":"data-extraction/#batch-and-bulk-extraction","title":"Batch and bulk extraction","text":"<p>If you have a lot of pages or a lot of PDFs or a lot of anything, the <code>.extract()</code> and <code>.extracted()</code> methods work identically on most parts of a PDF - regions, pages, collections of pdfs, etc, allowing a lot of flexibility in what you analyze.</p> <pre><code># Assuming 'header_region' is a Region object you defined\nheader_region.extract(InvoiceInfo, client)\ncompany = header_region.extracted('company_name')\n</code></pre> <p>Furthermore, you can apply extraction to collections of elements (like <code>pdf.pages</code>, or the result of <code>pdf.find_all(...)</code>) using the <code>.apply()</code> method. This iterates through the collection and calls <code>.extract()</code> on each item.</p> <pre><code># Example: Extract InvoiceInfo from the first 5 pages\nresults = pdf.pages[:5].apply(\n    lambda page: page.extract(\n        client=client,\n        schema=InvoiceInfo, \n        client=client, \n        analysis_key=\"page_invoice_info\",\n    )\n)\n\n# Access results for the first page in the collection\npdf.pages[0].extracted('company_name', analysis_key=\"page_invoice_info\")\n</code></pre> <p>This provides a powerful way to turn unstructured PDF content into structured, usable data.</p>"},{"location":"describe/","title":"Describe Functionality","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.describe()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.describe() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Describe all elements on the page\npage.find_all('text').describe()\n</pre> # Describe all elements on the page page.find_all('text').describe() Out[2]: In\u00a0[3]: Copied! <pre># Describe all elements on the page\npage.find_all('rect').describe()\n</pre> # Describe all elements on the page page.find_all('rect').describe() Out[3]: In\u00a0[4]: Copied! <pre>page.find_all('text').inspect()\n</pre> page.find_all('text').inspect() Out[4]: In\u00a0[5]: Copied! <pre>page.find_all('line').inspect()\n</pre> page.find_all('line').inspect() Out[5]:"},{"location":"describe/#describe-functionality","title":"Describe Functionality\u00b6","text":"<p>The <code>describe()</code> and <code>inspect()</code> methods provide an easy way to understand the contents of your PDF elements without having to visualize them as images.</p>"},{"location":"describe/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Get a summary of an entire page:</p>"},{"location":"describe/#page-1-summary","title":"Page 1 Summary\u00b6","text":"<p>Elements:</p> <ul> <li>text: 44 elements</li> <li>line: 21 elements</li> <li>rect: 8 elements</li> </ul> <p>Text Analysis:</p> <ul> <li>typography:<ul> <li>fonts:<ul> <li>Helvetica: 44</li> </ul> </li> <li>sizes:<ul> <li>10.0pt: 40</li> <li>8.0pt: 3</li> <li>12.0pt: 1</li> </ul> </li> <li>styles: 9 bold</li> <li>colors:<ul> <li>black: 43</li> <li>other: 1</li> </ul> </li> </ul> </li> </ul>"},{"location":"describe/#element-collection-summaries","title":"Element collection summaries\u00b6","text":"<p>You can describe element collections on a page with <code>.describe()</code>.</p>"},{"location":"describe/#collection-summary-44-elements","title":"Collection Summary (44 elements)\u00b6","text":"<p>Typography:</p> <ul> <li>fonts:<ul> <li>Helvetica: 44</li> </ul> </li> <li>sizes:<ul> <li>10.0pt: 40</li> <li>8.0pt: 3</li> <li>12.0pt: 1</li> </ul> </li> <li>styles: 9 bold</li> <li>colors:<ul> <li>black: 43</li> <li>other: 1</li> </ul> </li> </ul>"},{"location":"describe/#collection-summary-8-elements","title":"Collection Summary (8 elements)\u00b6","text":"<p>Size Stats:</p> <ul> <li>width range: 8-180</li> <li>height range: 8-35</li> <li>avg area: 844 sq pts</li> </ul> <p>Styles:</p> <ul> <li>stroke widths:<ul> <li>0.5: 7</li> </ul> </li> <li>colors:<ul> <li>black: 8</li> </ul> </li> </ul>"},{"location":"describe/#inspecting-lists-of-elements","title":"Inspecting lists of elements\u00b6","text":"<p>For more detail, you can view specific details of element collections with <code>inspect()</code>.</p>"},{"location":"describe/#collection-inspection-44-elements","title":"Collection Inspection (44 elements)\u00b6","text":""},{"location":"describe/#word-elements","title":"Word Elements\u00b6","text":"text x0 top x1 bottom font_family size bold italic source confidence color Jungle Health and Safety Inspection Service 385 36 542 44 Helvetica 8 False False native 1.00 #000000 INS-UP70N51NCL41R 385 46 466 54 Helvetica 8 False False native 1.00 #ff0000 Site: 50 84 74 94 Helvetica 10 True False native 1.00 #000000 Durham\u2019s Meatpacking 74 84 182 94 Helvetica 10 False False native 1.00 #000000 Chicago, Ill. 182 84 235 94 Helvetica 10 False False native 1.00 #000000 Date: 50 104 81 114 Helvetica 10 True False native 1.00 #000000 February 3, 1905 81 104 157 114 Helvetica 10 False False native 1.00 #000000 Violation Count: 50 124 130 134 Helvetica 10 True False native 1.00 #000000 7 130 124 136 134 Helvetica 10 False False native 1.00 #000000 Summary: 50 144 102 154 Helvetica 10 True False native 1.00 #000000 Worst of any, however, were the fertilizer men, an... 102 144 506 154 Helvetica 10 False False native 1.00 #000000 These people could not be shown to the visitor - f... 50 160 512 170 Helvetica 10 False False native 1.00 #000000 visitor at a hundred yards, and as for the other m... 50 176 491 186 Helvetica 10 False False native 1.00 #000000 some of which there were open vats near the level ... 50 192 496 202 Helvetica 10 False False native 1.00 #000000 into the vats; and when they were fished out, ther... 50 208 465 218 Helvetica 10 False False native 1.00 #000000 exhibiting - sometimes they would be overlooked fo... 50 224 492 234 Helvetica 10 False False native 1.00 #000000 to the world as Durham\u2019s Pure Leaf Lard! 50 240 232 250 Helvetica 10 False False native 1.00 #000000 Violations 50 372 107 384 Helvetica 12 True False native 1.00 #000000 Statute 55 398 89 408 Helvetica 10 True False native 1.00 #000000 Description 105 398 160 408 Helvetica 10 True False native 1.00 #000000 Level 455 398 481 408 Helvetica 10 True False native 1.00 #000000 Repeat? 505 398 544 408 Helvetica 10 True False native 1.00 #000000 4.12.7 55 418 83 428 Helvetica 10 False False native 1.00 #000000 Unsanitary Working Conditions. 105 418 245 428 Helvetica 10 False False native 1.00 #000000 Critical 455 418 486 428 Helvetica 10 False False native 1.00 #000000 5.8.3 55 438 77 448 Helvetica 10 False False native 1.00 #000000 Inadequate Protective Equipment. 105 438 256 448 Helvetica 10 False False native 1.00 #000000 Serious 455 438 489 448 Helvetica 10 False False native 1.00 #000000 6.3.9 55 458 77 468 Helvetica 10 False False native 1.00 #000000 Ineffective Injury Prevention. 105 458 231 468 Helvetica 10 False False native 1.00 #000000 <p>Showing 30 of 44 elements (pass limit= to see more)</p>"},{"location":"describe/#collection-inspection-21-elements","title":"Collection Inspection (21 elements)\u00b6","text":""},{"location":"describe/#line-elements","title":"Line Elements\u00b6","text":"x0 top x1 bottom width is_horizontal is_vertical 50 352 550 352 2 True False 50 392 550 392 0 True False 50 392 50 552 0 False True 100 392 100 552 0 False True 450 392 450 552 0 False True 500 392 500 552 0 False True 550 392 550 552 0 False True 50 412 550 412 0 True False 520 418 528 426 0 False False 520 418 528 426 0 False False 50 432 550 432 0 True False 520 438 528 446 0 False False 520 438 528 446 0 False False 50 452 550 452 0 True False 50 472 550 472 0 True False 50 492 550 492 0 True False 50 512 550 512 0 True False 520 518 528 526 0 False False 520 518 528 526 0 False False 50 532 550 532 0 True False 50 552 550 552 0 True False"},{"location":"describe/","title":"Describe Functionality","text":"<p>The <code>describe()</code> and <code>inspect()</code> methods provide an easy way to understand the contents of your PDF elements without having to visualize them as images.</p>"},{"location":"describe/#basic-usage","title":"Basic Usage","text":"<p>Get a summary of an entire page:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.describe()\n</code></pre>"},{"location":"describe/#element-collection-summaries","title":"Element collection summaries","text":"<p>You can describe element collections on a page with <code>.describe()</code>.</p> <pre><code># Describe all elements on the page\npage.find_all('text').describe()\n</code></pre> <pre><code># Describe all elements on the page\npage.find_all('rect').describe()\n</code></pre>"},{"location":"describe/#inspecting-lists-of-elements","title":"Inspecting lists of elements","text":"<p>For more detail, you can view specific details of element collections with <code>inspect()</code>.</p> <pre><code>page.find_all('text').inspect()\n</code></pre> <pre><code>page.find_all('line').inspect()\n</code></pre>"},{"location":"document-qa/","title":"Document Question Answering","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n\n# Display the first page \npage = pdf.pages[0]\npage.show()\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")  # Display the first page  page = pdf.pages[0] page.show() Out[1]: In\u00a0[2]: Copied! <pre># Ask a question about the entire document\npage.ask(\"How many votes did Harris and Waltz get?\")\n</pre> # Ask a question about the entire document page.ask(\"How many votes did Harris and Waltz get?\") <pre>Device set to use mps:0\n</pre> Out[2]: <pre>{'answer': '148',\n 'confidence': 0.9995507001876831,\n 'start': 20,\n 'end': 20,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre>page.ask(\"Who got the most votes for Attorney General?\")\n</pre> page.ask(\"Who got the most votes for Attorney General?\") Out[3]: <pre>{'answer': 'DEM EUGENE DEPASQUALE',\n 'confidence': 0.9180722236633301,\n 'start': 63,\n 'end': 63,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre>page.ask(\"Who was the Republican candidate for Attorney General?\")\n</pre> page.ask(\"Who was the Republican candidate for Attorney General?\") Out[4]: <pre>{'answer': 'LIB ROBERT COWBURN',\n 'confidence': 0.21592436730861664,\n 'start': 67,\n 'end': 67,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[5]: Copied! <pre># Get a specific page\nregion = page.find('text:contains(\"Attorney General\")').below()\nregion.show()\n</pre> # Get a specific page region = page.find('text:contains(\"Attorney General\")').below() region.show() Out[5]: In\u00a0[6]: Copied! <pre>region.ask(\"How many write-in votes were cast?\")\n</pre> region.ask(\"How many write-in votes were cast?\") Out[6]: <pre>{'answer': '498',\n 'confidence': 0.9988918304443359,\n 'start': 17,\n 'end': 17,\n 'found': True,\n 'region': &lt;Region bbox=(0, 553.663, 612, 792)&gt;,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\n\nquestions = [\n    \"How many votes did Harris and Walz get?\",\n    \"How many votes did Trump get?\",\n    \"How many votes did Natural PDF get?\",\n    \"What was the date of this form?\"\n]\n\n# You can actually do this but with multiple questions\n# in the model itself buuuut Natural PDF can'd do it yet\nresults = [page.ask(q) for q in questions]\n\ndf = pd.json_normalize(results)\ndf.insert(0, 'question', questions)\ndf\n</pre> import pandas as pd  questions = [     \"How many votes did Harris and Walz get?\",     \"How many votes did Trump get?\",     \"How many votes did Natural PDF get?\",     \"What was the date of this form?\" ]  # You can actually do this but with multiple questions # in the model itself buuuut Natural PDF can'd do it yet results = [page.ask(q) for q in questions]  df = pd.json_normalize(results) df.insert(0, 'question', questions) df Out[7]: question answer confidence start end found page_num source_elements 0 How many votes did Harris and Walz get? 148 0.999671 20 20 True 0 [&lt;TextElement text='148' font='Helvetica' size... 1 How many votes did Trump get? 348 0.310203 22 22 True 0 [&lt;TextElement text='348' font='Helvetica' size... 2 How many votes did Natural PDF get? November 5, 2024 0.237136 3 3 True 0 [&lt;TextElement text='November 5...' font='Helve... 3 What was the date of this form? November 5, 2024 0.792696 3 3 True 0 [&lt;TextElement text='November 5...' font='Helve..."},{"location":"document-qa/#document-question-answering","title":"Document Question Answering\u00b6","text":"<p>Natural PDF includes document QA functionality that allows you to ask natural language questions about your PDFs and get relevant answers. This feature uses LayoutLM models to understand both the text content and the visual layout of your documents.</p>"},{"location":"document-qa/#setup","title":"Setup\u00b6","text":"<p>Let's start by loading a sample PDF to experiment with question answering.</p>"},{"location":"document-qa/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here's how to ask questions to a PDF page:</p>"},{"location":"document-qa/#asking-questions-to-part-of-a-page-questions","title":"Asking questions to part of a page questions\u00b6","text":"<p>You can also ask questions to a specific region of a page*:</p>"},{"location":"document-qa/#asking-multiple-questions","title":"Asking multiple questions\u00b6","text":""},{"location":"document-qa/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you've learned about document QA, explore:</p> <ul> <li>Element Selection: Find specific elements to focus your questions.</li> <li>Layout Analysis: Automatically detect document structure.</li> <li>Working with Regions: Define custom areas for targeted questioning.</li> <li>Text Extraction: Extract and preprocess text before QA.</li> </ul>"},{"location":"element-selection/","title":"Finding Elements with Selectors","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Find the first text element containing \"Summary\"\nsummary_text = page.find('text:contains(\"Summary\")')\nsummary_text\n</pre> # Find the first text element containing \"Summary\" summary_text = page.find('text:contains(\"Summary\")') summary_text Out[2]: <pre>&lt;TextElement text='Summary: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 144.07000000000005, 101.68, 154.07000000000005)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all text elements containing \"Inadequate\"\ncontains_inadequate = page.find_all('text:contains(\"Inadequate\")')\nlen(contains_inadequate)\n</pre> # Find all text elements containing \"Inadequate\" contains_inadequate = page.find_all('text:contains(\"Inadequate\")') len(contains_inadequate) Out[3]: <pre>2</pre> In\u00a0[4]: Copied! <pre>summary_text.highlight(label='summary')\ncontains_inadequate.highlight(label=\"inadequate\")\npage.to_image(width=700)\n</pre> summary_text.highlight(label='summary') contains_inadequate.highlight(label=\"inadequate\") page.to_image(width=700) Out[4]: In\u00a0[5]: Copied! <pre># Find all text elements\nall_text = page.find_all('text')\nlen(all_text)\n</pre> # Find all text elements all_text = page.find_all('text') len(all_text) Out[5]: <pre>44</pre> In\u00a0[6]: Copied! <pre># Find all rectangle elements\nall_rects = page.find_all('rect')\nlen(all_rects)\n</pre> # Find all rectangle elements all_rects = page.find_all('rect') len(all_rects) Out[6]: <pre>8</pre> In\u00a0[7]: Copied! <pre># Find all line elements\nall_lines = page.find_all('line')\nlen(all_lines)\n</pre> # Find all line elements all_lines = page.find_all('line') len(all_lines) Out[7]: <pre>21</pre> In\u00a0[8]: Copied! <pre>page.find_all('line').show()\n</pre> page.find_all('line').show() Out[8]: In\u00a0[9]: Copied! <pre># Find large text (size &gt;= 11 points)\npage.find_all('text[size&gt;=11]')\n</pre> # Find large text (size &gt;= 11 points) page.find_all('text[size&gt;=11]') Out[9]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[10]: Copied! <pre># Find text with 'Helvetica' in the font name\npage.find_all('text[fontname*=Helvetica]')\n</pre> # Find text with 'Helvetica' in the font name page.find_all('text[fontname*=Helvetica]') Out[10]: <pre>&lt;ElementCollection[TextElement](count=44)&gt;</pre> In\u00a0[11]: Copied! <pre># Find red text (using approximate color match)\n# This PDF has text with color (0.8, 0.0, 0.0)\nred_text = page.find_all('text[color~=red]')\n</pre> # Find red text (using approximate color match) # This PDF has text with color (0.8, 0.0, 0.0) red_text = page.find_all('text[color~=red]') In\u00a0[12]: Copied! <pre># Highlight the red text (ignoring existing highlights)\nred_text.show()\n</pre> # Highlight the red text (ignoring existing highlights) red_text.show() Out[12]: In\u00a0[13]: Copied! <pre># Find thick lines (width &gt;= 2)\npage.find_all('line[width&gt;=2]')\n</pre> # Find thick lines (width &gt;= 2) page.find_all('line[width&gt;=2]') Out[13]: <pre>&lt;ElementCollection[LineElement](count=1)&gt;</pre> In\u00a0[14]: Copied! <pre># Find bold text\npage.find_all('text:bold').show()\n</pre> # Find bold text page.find_all('text:bold').show() Out[14]: In\u00a0[15]: Copied! <pre># Combine attribute and pseudo-class: bold text size &gt;= 11\npage.find_all('text[size&gt;=11]:bold')\n</pre> # Combine attribute and pseudo-class: bold text size &gt;= 11 page.find_all('text[size&gt;=11]:bold') Out[15]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[16]: Copied! <pre># Find all text elements that are NOT bold\nnon_bold_text = page.find_all('text:not(:bold)')\n\n# Find all elements that are NOT regions of type 'table'\nnot_tables = page.find_all(':not(region[type=table])')\n\n# Find text elements that do not contain \"Total\" (case-insensitive)\nrelevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)\n\n# Find text elements that are not empty\nnon_empty_text = page.find_all('text:not(:empty)')\n</pre> # Find all text elements that are NOT bold non_bold_text = page.find_all('text:not(:bold)')  # Find all elements that are NOT regions of type 'table' not_tables = page.find_all(':not(region[type=table])')  # Find text elements that do not contain \"Total\" (case-insensitive) relevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)  # Find text elements that are not empty non_empty_text = page.find_all('text:not(:empty)') <p>Note: The selector inside <code>:not()</code> follows the same rules as regular selectors but currently does not support combinators (like <code>&gt;</code>, <code>+</code>, <code>~</code>, or descendant space) within <code>:not()</code>. You can nest basic type, attribute, and other pseudo-class selectors.</p> In\u00a0[17]: Copied! <pre># Find the thick horizontal line first\nref_line = page.find('line[width&gt;=2]')\n\n# Find text elements strictly above that line\ntext_above_line = page.find_all('text:above(\"line[width&gt;=2]\")')\ntext_above_line\n</pre> # Find the thick horizontal line first ref_line = page.find('line[width&gt;=2]')  # Find text elements strictly above that line text_above_line = page.find_all('text:above(\"line[width&gt;=2]\")') text_above_line Out[17]: <pre>&lt;ElementCollection[TextElement](count=17)&gt;</pre> In\u00a0[18]: Copied! <pre># Case-insensitive search for \"summary\"\npage.find_all('text:contains(\"summary\")', case=False)\n</pre> # Case-insensitive search for \"summary\" page.find_all('text:contains(\"summary\")', case=False) Out[18]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[19]: Copied! <pre># Regular expression search for the inspection ID (e.g., INS-XXX...)\n# The ID is in the red text we found earlier\npage.find_all('text:contains(\"INS-\\\\w+\")', regex=True)\n</pre> # Regular expression search for the inspection ID (e.g., INS-XXX...) # The ID is in the red text we found earlier page.find_all('text:contains(\"INS-\\\\w+\")', regex=True) Out[19]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[20]: Copied! <pre># Combine regex and case-insensitivity\npage.find_all('text:contains(\"jungle health\")', regex=True, case=False)\n</pre> # Combine regex and case-insensitivity page.find_all('text:contains(\"jungle health\")', regex=True, case=False) Out[20]: <pre>&lt;ElementCollection[TextElement](count=2)&gt;</pre> In\u00a0[21]: Copied! <pre># Get all headings (using a selector for large, bold text)\nheadings = page.find_all('text[size&gt;=11]:bold')\nheadings\n</pre> # Get all headings (using a selector for large, bold text) headings = page.find_all('text[size&gt;=11]:bold') headings Out[21]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[22]: Copied! <pre># Get the first and last heading in reading order\nfirst = headings.first\nlast = headings.last\n(first, last)\n</pre> # Get the first and last heading in reading order first = headings.first last = headings.last (first, last) Out[22]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[23]: Copied! <pre># Get the physically highest/lowest element in the collection\nhighest = headings.highest()\nlowest = headings.lowest()\n(highest, lowest)\n</pre> # Get the physically highest/lowest element in the collection highest = headings.highest() lowest = headings.lowest() (highest, lowest) Out[23]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[24]: Copied! <pre># Filter the collection further: headings containing \"Service\"\nservice_headings = headings.filter(lambda heading: 'Service' in heading.extract_text())\n</pre> # Filter the collection further: headings containing \"Service\" service_headings = headings.filter(lambda heading: 'Service' in heading.extract_text()) In\u00a0[25]: Copied! <pre># Extract text from all elements in the collection\nheadings.extract_text()\n</pre> # Extract text from all elements in the collection headings.extract_text() Out[25]: <pre>'Violations'</pre> <p>Remember: <code>.highest()</code>, <code>.lowest()</code>, <code>.leftmost()</code>, <code>.rightmost()</code> raise errors if the collection spans multiple pages.</p> In\u00a0[26]: Copied! <pre># Find text elements with a specific font variant prefix (if any exist)\n# This example PDF doesn't use variants, but the selector works like this:\npage.find_all('text[font-variant=AAAAAB]')\n</pre> # Find text elements with a specific font variant prefix (if any exist) # This example PDF doesn't use variants, but the selector works like this: page.find_all('text[font-variant=AAAAAB]') Out[26]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre> In\u00a0[27]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/geometry.pdf\")\npage = pdf.pages[0]\n\nrect = page.find('rect')\nrect.show(width=500)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/geometry.pdf\") page = pdf.pages[0]  rect = page.find('rect') rect.show(width=500) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[27]: <p>By default, being inside of something means being fully inside of the outer object.</p> In\u00a0[28]: Copied! <pre># rect.find_all('text', contains='all').show()\nrect.find_all('text').show()\n</pre> # rect.find_all('text', contains='all').show() rect.find_all('text').show() Out[28]: <p>If you're interested in any overlap, you can use <code>contains='any'</code>.</p> In\u00a0[29]: Copied! <pre>rect.find_all('text', contains='any').show()\n</pre> rect.find_all('text', contains='any').show() Out[29]: <p>For just the center being part of it, <code>contains='center'</code> will work for you.</p> In\u00a0[30]: Copied! <pre>rect.find_all('text', contains='center').show()\n</pre> rect.find_all('text', contains='center').show() Out[30]:"},{"location":"element-selection/#finding-elements-with-selectors","title":"Finding Elements with Selectors\u00b6","text":"<p>Natural PDF uses CSS-like selectors to find elements (text, lines, images, etc.) within a PDF page or document. This guide demonstrates how to use these selectors effectively.</p>"},{"location":"element-selection/#setup","title":"Setup\u00b6","text":"<p>Let's load a sample PDF to work with. We'll use <code>01-practice.pdf</code> which has various elements.</p>"},{"location":"element-selection/#basic-element-finding","title":"Basic Element Finding\u00b6","text":"<p>The core methods are <code>find()</code> (returns the first match) and <code>find_all()</code> (returns all matches as an <code>ElementCollection</code>).</p> <p>The basic selector structure is <code>element_type[attribute_filter]:pseudo_class</code>.</p>"},{"location":"element-selection/#finding-text-by-content","title":"Finding Text by Content\u00b6","text":""},{"location":"element-selection/#selecting-by-element-type","title":"Selecting by Element Type\u00b6","text":"<p>You can select specific types of elements found in PDFs.</p>"},{"location":"element-selection/#filtering-by-attributes","title":"Filtering by Attributes\u00b6","text":"<p>Use square brackets <code>[]</code> to filter elements by their properties (attributes).</p>"},{"location":"element-selection/#common-attributes-operators","title":"Common Attributes &amp; Operators\u00b6","text":"Attribute Example Usage Operators Notes <code>size</code> (text) <code>text[size&gt;=12]</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Font size in points <code>fontname</code> <code>text[fontname*=Bold]</code> <code>=</code>, <code>*=</code> <code>*=</code> for contains substring <code>color</code> (text) <code>text[color~=red]</code> <code>~=</code> Approx. match (name, rgb, hex) <code>width</code> (line) <code>line[width&gt;1]</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Line thickness <code>source</code> <code>text[source=ocr]</code> <code>=</code> <code>pdf</code>, <code>ocr</code>, <code>detected</code> <code>type</code> (region) <code>region[type=table]</code> <code>=</code> Layout analysis region type"},{"location":"element-selection/#using-pseudo-classes","title":"Using Pseudo-Classes\u00b6","text":"<p>Use colons <code>:</code> for special conditions (pseudo-classes).</p>"},{"location":"element-selection/#common-pseudo-classes","title":"Common Pseudo-Classes\u00b6","text":"Pseudo-Class Example Usage Notes <code>:contains('text')</code> <code>text:contains('Report')</code> Finds elements containing specific text <code>:bold</code> <code>text:bold</code> Finds text heuristically identified as bold <code>:italic</code> <code>text:italic</code> Finds text heuristically identified as italic <code>:below(selector)</code> <code>text:below('line[width&gt;=2]')</code> Finds elements physically below the reference element <code>:above(selector)</code> <code>text:above('text:contains(\"Summary\")')</code> Finds elements physically above the reference element <code>:left-of(selector)</code> <code>line:left-of('rect')</code> Finds elements physically left of the reference element <code>:right-of(selector)</code> <code>text:right-of('rect')</code> Finds elements physically right of the reference element <code>:near(selector)</code> <code>text:near('image')</code> Finds elements physically near the reference element <p>Note: Spatial pseudo-classes like <code>:below</code>, <code>:above</code> identify elements based on bounding box positions relative to the first element matched by the inner selector.</p>"},{"location":"element-selection/#negation-pseudo-class-not","title":"Negation Pseudo-class (<code>:not()</code>)\u00b6","text":"<p>You can exclude elements that match a certain selector using the <code>:not()</code> pseudo-class. It takes another simple selector as its argument.</p>"},{"location":"element-selection/#spatial-pseudo-classes-examples","title":"Spatial Pseudo-Classes Examples\u00b6","text":""},{"location":"element-selection/#advanced-text-searching-options","title":"Advanced Text Searching Options\u00b6","text":"<p>Pass options to <code>find()</code> or <code>find_all()</code> for more control over text matching.</p>"},{"location":"element-selection/#working-with-elementcollections","title":"Working with ElementCollections\u00b6","text":"<p><code>find_all()</code> returns an <code>ElementCollection</code>, which is like a list but with extra PDF-specific methods.</p>"},{"location":"element-selection/#font-variants","title":"Font Variants\u00b6","text":"<p>Sometimes PDFs use font variants (prefixes like <code>AAAAAB+</code>) which can be useful for selection.</p>"},{"location":"element-selection/#containment-geometry","title":"Containment geometry\u00b6","text":""},{"location":"element-selection/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you can find elements, explore:</p> <ul> <li>Text Extraction: Get text content from found elements.</li> <li>Spatial Navigation: Use found elements as anchors to navigate (<code>.above()</code>, <code>.below()</code>, etc.).</li> <li>Working with Regions: Define areas based on found elements.</li> <li>Visual Debugging: Techniques for highlighting and visualizing elements.</li> </ul>"},{"location":"finetuning/","title":"OCR Fine-tuning","text":"<p>While the built-in OCR engines (EasyOCR, PaddleOCR, Surya) offer good general performance, you might encounter situations where their accuracy isn't sufficient for your specific needs. This is often the case with:</p> <ul> <li>Unique Fonts: Documents using unusual or stylized fonts.</li> <li>Specific Languages: Languages or scripts not perfectly covered by the default models.</li> <li>Low Quality Scans: Noisy or degraded document images.</li> <li>Specialized Layouts: Text within complex tables, forms, or unusual arrangements.</li> </ul> <p>Fine-tuning allows you to adapt a pre-trained OCR recognition model to your specific data, significantly improving its accuracy on documents similar to those used for training.</p>"},{"location":"finetuning/#why-fine-tune","title":"Why Fine-tune?","text":"<ul> <li>Higher Accuracy: Achieve better text extraction results on your specific document types.</li> <li>Adaptability: Train the model to recognize domain-specific terms, symbols, or layouts.</li> <li>Reduced Errors: Minimize downstream errors in data extraction and processing pipelines.</li> </ul>"},{"location":"finetuning/#strategy-detect-llm-correct-export","title":"Strategy: Detect + LLM Correct + Export","text":"<p>Training an OCR model requires accurate ground truth: images of text snippets paired with their correct transcriptions. Manually creating this data is tedious. A powerful alternative leverages the strengths of different models:</p> <ol> <li>Detect Text Regions: Use a robust local OCR engine (like Surya or PaddleOCR) primarily for its detection capabilities (<code>detect_only=True</code>). This identifies the locations of text on the page, even if the initial recognition isn't perfect. You can combine this with layout analysis or region selections (<code>.region()</code>, <code>.below()</code>, <code>.add_exclusion()</code>) to focus on the specific areas you care about.</li> <li>Correct with LLM: For each detected text region, send the image snippet to a powerful Large Language Model (LLM) with multimodal capabilities (like GPT-4o, Claude 3.5 Sonnet/Haiku) using the <code>direct_ocr_llm</code> utility. The LLM performs high-accuracy OCR on the snippet, providing a \"ground truth\" transcription.</li> <li>Export for Fine-tuning: Use the <code>PaddleOCRRecognitionExporter</code> to package the original image snippets (from step 1) along with their corresponding LLM-generated text labels (from step 2) into the specific format required by PaddleOCR for fine-tuning its recognition model.</li> </ol> <p>This approach combines the efficient spatial detection of local models with the superior text recognition of large generative models to create a high-quality fine-tuning dataset with minimal manual effort.</p>"},{"location":"finetuning/#example-fine-tuning-for-greek-spreadsheet-text","title":"Example: Fine-tuning for Greek Spreadsheet Text","text":"<p>Let's walk through an example of preparing data to fine-tune PaddleOCR for text from a scanned Greek spreadsheet, adapting the process described above.</p> <pre><code># --- 1. Setup and Load PDF ---\nfrom natural_pdf import PDF\nfrom natural_pdf.ocr.utils import direct_ocr_llm\nfrom natural_pdf.exporters import PaddleOCRRecognitionExporter\nimport openai # Or your preferred LLM client library\nimport os\n\n# Ensure your LLM API key is set (using environment variables is recommended)\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" \n# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" \n\n# pdf_path = \"path/to/your/document.pdf\" \npdf_path = \"path/to/your/document.pdf\" \n# For demonstration we use a public sample PDF; replace with your own.\npdf_path = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\"\npdf = PDF(pdf_path)\n\n# --- 2. (Optional) Exclude Irrelevant Areas ---\n# If the document has consistent headers, footers, or margins you want to ignore\n# Use exclusions *before* detection\npdf.add_exclusion(lambda page: page.region(right=45)) # Exclude left margin/line numbers\npdf.add_exclusion(lambda page: page.region(left=500)) # Exclude right margin\n\n# --- 3. Detect Text Regions ---\n# Use a good detection engine. Surya is often robust for line detection.\n# We only want the bounding boxes, not the initial (potentially inaccurate) OCR text.\nprint(\"Detecting text regions...\")\n# Process only a subset of pages for demonstration if needed\nfor page in pdf.pages[:10]:\n    # Use a moderate resolution for detection; higher res used for LLM correction later\n    page.apply_ocr(engine='surya', resolution=120, detect_only=True) \nprint(f\"Detection complete for {num_pages_to_process} pages.\")\n\n# (Optional) Visualize detected boxes on a sample page\n# pdf.pages[9].find_all('text[source=ocr]').show() \n\n# --- 4. Correct with LLM ---\n# Configure your LLM client (example using OpenAI client, adaptable for others)\n# For Anthropic: client = openai.OpenAI(base_url=\"https://api.anthropic.com/v1/\", api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\nclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) \n\n# Craft a clear prompt for the LLM\n# Be as specific as possible! If it's in a specific language, what kinds\n# of characters, etc.\nprompt = \"\"\"OCR this image patch. Return only the exact text content visible in the image. \nPreserve original spelling, capitalization, punctuation, and symbols. \nDo not add any explanatory text, translations, comments, or quotation marks around the result.\nThe text is likely from a Greek document, potentially a spreadsheet, containing Modern Greek words or numbers.\"\"\"\n\n# Define the correction function using direct_ocr_llm\ndef correct_text_region(region):\n    # Use a high resolution for the LLM call for best accuracy\n    return direct_ocr_llm(\n        region, \n        client, \n        prompt=prompt, \n        resolution=300, \n        # model=\"claude-3-5-sonnet-20240620\" # Example Anthropic model\n        model=\"gpt-4o-mini\" # Example OpenAI model\n    )\n\n# Apply the correction function to the detected text regions\nprint(\"Applying LLM correction to detected regions...\")\nfor page in pdf.pages[:num_pages_to_process]:\n    # This finds elements added by apply_ocr and passes their regions to 'correct_text_region'\n    # The returned text from the LLM replaces the original OCR text for these elements\n    # The source attribute is updated (e.g., to 'ocr-llm-corrected')\n    page.correct_ocr(correct_text_region) \nprint(\"LLM correction complete.\")\n\n# --- 5. Export for PaddleOCR Fine-tuning ---\nprint(\"Configuring exporter...\")\nexporter = PaddleOCRRecognitionExporter(\n    # Select all of the non-blank OCR text\n    # Hopefully it's all been LLM-corrected! \n    selector=\"text[source^=ocr][text!='']\", \n    resolution=300,     # Resolution for the exported image crops\n    padding=2,          # Add slight padding around text boxes\n    split_ratio=0.9,    # 90% for training, 10% for validation\n    random_seed=42,     # For reproducible train/val split\n    include_guide=True  # Include the Colab fine-tuning notebook\n)\n\n# Define the output directory\noutput_directory = \"./my_paddleocr_finetune_data\"\nprint(f\"Exporting data to {output_directory}...\")\n\n# Run the export process\nexporter.export(pdf, output_directory)\n\nprint(\"Export complete.\")\nprint(f\"Dataset ready for fine-tuning in: {output_directory}\")\nprint(f\"Next step: Upload '{os.path.join(output_directory, 'fine_tune_paddleocr.ipynb')}' and the rest of the contents to Google Colab.\")\n\n# --- Cleanup ---\npdf.close() \n</code></pre>"},{"location":"finetuning/#running-the-fine-tuning","title":"Running the Fine-tuning","text":"<p>The <code>PaddleOCRRecognitionExporter</code> automatically includes a Jupyter Notebook (<code>fine_tune_paddleocr.ipynb</code>) in the output directory. This notebook is pre-configured to guide you through the fine-tuning process on Google Colab (which offers free GPU access):</p> <ol> <li>Upload: Upload the entire output directory (e.g., <code>my_paddleocr_finetune_data</code>) to your Google Drive or directly to your Colab instance.</li> <li>Open Notebook: Open the <code>fine_tune_paddleocr.ipynb</code> notebook in Google Colab.</li> <li>Set Runtime: Ensure the Colab runtime is set to use a GPU (Runtime -&gt; Change runtime type -&gt; GPU).</li> <li>Run Cells: Execute the cells in the notebook sequentially. It will:<ul> <li>Install necessary libraries (PaddlePaddle, PaddleOCR).</li> <li>Point the training configuration to your uploaded dataset (<code>images/</code>, <code>train.txt</code>, <code>val.txt</code>, <code>dict.txt</code>).</li> <li>Download a pre-trained PaddleOCR model (usually a multilingual one).</li> <li>Start the fine-tuning process using your data.</li> <li>Save the fine-tuned model checkpoints.</li> <li>Export the best model into an \"inference format\" suitable for use with <code>natural-pdf</code>.</li> </ul> </li> <li>Download Model: Download the resulting <code>inference_model</code> directory from Colab.</li> </ol>"},{"location":"finetuning/#using-the-fine-tuned-model","title":"Using the Fine-tuned Model","text":"<p>Once you have the <code>inference_model</code> directory, you can instruct <code>natural-pdf</code> to use it for OCR:</p> <pre><code>from natural_pdf import PDF\nfrom natural_pdf.ocr import PaddleOCROptions\n\n# Path to the directory you downloaded from Colab\nfinetuned_model_dir = \"/path/to/your/downloaded/inference_model\" \n\n# Specify the path in PaddleOCROptions\npaddle_opts = PaddleOCROptions(\n    rec_model_dir=finetuned_model_dir,\n    rec_char_dict_path=os.path.join(finetuned_model_dir, 'your_dict.txt') # Or wherever your dict is\n    use_gpu=True # If using GPU locally\n)\n\npdf = PDF(\"another-similar-document.pdf\")\npage = pdf.pages[0]\n\n# Apply OCR using your fine-tuned model\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# Extract text using the improved results\ntext = page.extract_text() \nprint(text)\n\npdf.close()\n</code></pre> <p>By following this process, you can significantly enhance OCR performance on your specific documents using the power of fine-tuning. </p>"},{"location":"installation/","title":"Getting Started with Natural PDF","text":"<p>Let's get Natural PDF installed and run your first extraction.</p>"},{"location":"installation/#installation","title":"Installation","text":"<p>The base installation includes the core library which will allow you to select, extract, and use spatial navigation.</p> <pre><code>pip install natural-pdf\n</code></pre> <p>But! If you want to recognize text, do page layout analysis, document q-and-a or other things, you can install optional dependencies.</p> <p>Natural PDF has modular dependencies for different features. Install them based on your needs:</p> <pre><code># Full ML / QA / semantic-search stack\npip install natural-pdf[ai]\n\n# Deskewing\npip install natural-pdf[deskew]\n\n# Semantic search\npip install natural-pdf[search]\n</code></pre> <p>Other OCR and layout analysis engines like <code>surya</code>, <code>easyocr</code>, <code>paddle</code>, <code>doctr</code>, and <code>docling</code> can be installed via <code>pip</code> as needed. The library will provide you with an error message and installation command if you try to use an engine that isn't installed.</p> <p>After the core install you have two ways to add optional engines:</p>"},{"location":"installation/#1-helper-cli-recommended","title":"1 \u2013 Helper CLI (recommended)","text":"<pre><code># list optional groups and their install-status\nnpdf list\n\n# everything for classification, QA, semantic search, etc.\nnpdf install ai\n\n# install PaddleOCR stack\nnpdf install paddle\n\n# install Surya OCR + YOLO layout detector\nnpdf install surya yolo\n</code></pre> <p>The CLI runs each wheel in its own resolver pass, so it avoids strict version pins like <code>paddleocr \u2192 paddlex==3.0.1</code> while still upgrading to <code>paddlex 3.0.2</code>.</p>"},{"location":"installation/#2-classic-extras-for-the-light-stuff","title":"2 \u2013 Classic extras (for the light stuff)","text":"<pre><code># Full AI/ML stack\npip install \"natural-pdf[ai]\"\n\n# Deskewing\npip install \"natural-pdf[deskew]\"\n\n# Semantic search service\npip install \"natural-pdf[search]\"\n</code></pre> <p>If you attempt to use an engine that is missing, the library will raise an error that tells you which <code>npdf install \u2026</code> command to run.</p>"},{"location":"installation/#your-first-pdf-extraction","title":"Your First PDF Extraction","text":"<p>Here's a quick example to make sure everything is working:</p> <pre><code>from natural_pdf import PDF\n\n# Open a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Extract all text\ntext = page.extract_text()\nprint(text)\n\n# Find something specific\ntitle = page.find('text:bold')\nprint(f\"Found title: {title.text}\")\n</code></pre>"},{"location":"installation/#whats-next","title":"What's Next?","text":"<p>Now that you have Natural PDF installed, you can:</p> <ul> <li>Learn to navigate PDFs</li> <li>Explore how to select elements</li> <li>See how to extract text</li> </ul>"},{"location":"interactive-widget/","title":"Interactive widget","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.viewer()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[1]:"},{"location":"interactive-widget/#interactive-widget","title":"Interactive widget\u00b6","text":"<p>This is the best possible way, in all of history, to explore a PDF.</p>"},{"location":"layout-analysis/","title":"Document Layout Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.to_image(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Analyze the layout using the default engine (YOLO)\n# This adds 'region' elements to the page\npage.analyze_layout()\n</pre> # Analyze the layout using the default engine (YOLO) # This adds 'region' elements to the page page.analyze_layout() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpy3rdeikf/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 699.0ms\n</pre> <pre>Speed: 5.5ms preprocess, 699.0ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[2]: <pre>&lt;ElementCollection[Region](count=7)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all detected regions\nregions = page.find_all('region')\nlen(regions) # Show how many regions were detected\n</pre> # Find all detected regions regions = page.find_all('region') len(regions) # Show how many regions were detected Out[3]: <pre>7</pre> In\u00a0[4]: Copied! <pre>first_region = regions[0]\nf\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\"\n</pre> first_region = regions[0] f\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\" Out[4]: <pre>\"First region: type='abandon', confidence=0.81\"</pre> In\u00a0[5]: Copied! <pre># Highlight all detected regions, colored by type\nregions.highlight(group_by='type')\npage.to_image(width=700)\n</pre> # Highlight all detected regions, colored by type regions.highlight(group_by='type') page.to_image(width=700) Out[5]: In\u00a0[6]: Copied! <pre># Find all detected titles\ntitles = page.find_all('region[type=title]')\ntitles\n</pre> # Find all detected titles titles = page.find_all('region[type=title]') titles Out[6]: <pre>&lt;ElementCollection[Region](count=1)&gt;</pre> In\u00a0[7]: Copied! <pre>titles.show()\n</pre> titles.show() Out[7]: In\u00a0[8]: Copied! <pre>page.find_all('region[type=table]').show()\n</pre> page.find_all('region[type=table]').show() Out[8]: In\u00a0[9]: Copied! <pre>page.find('region[type=table]').extract_text(layout=True)\n</pre> page.find('region[type=table]').extract_text(layout=True) Out[9]: <pre>'Statute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[10]: Copied! <pre>page.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"paddle\")\npage.find_all('region[model=paddle]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"paddle\") page.find_all('region[model=paddle]').highlight(group_by='region_type') page.to_image(width=700) <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>/Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> Out[10]: In\u00a0[11]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[11]: In\u00a0[12]: Copied! <pre># Analyze using Docling\n# https://docling-project.github.io/docling/\n\n# Docling has been weird, it's not included at the moment\n\n# page.clear_detected_layout_regions()\n# page.clear_highlights()\n\n# page.analyze_layout(engine=\"docling\")\n# page.find_all('region[model=docling]').highlight(group_by='region_type')\n# page.to_image(width=700)\n</pre> # Analyze using Docling # https://docling-project.github.io/docling/  # Docling has been weird, it's not included at the moment  # page.clear_detected_layout_regions() # page.clear_highlights()  # page.analyze_layout(engine=\"docling\") # page.find_all('region[model=docling]').highlight(group_by='region_type') # page.to_image(width=700) In\u00a0[13]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"surya\")\npage.find_all('region[model=surya]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"surya\") page.find_all('region[model=surya]').highlight(group_by='region_type') page.to_image(width=700) <pre>\rRecognizing layout:   0%|                                                 | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.62s/it]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.62s/it]</pre> <pre>\n</pre> <pre>\rRecognizing tables:   0%|                                                 | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing tables: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.73s/it]</pre> <pre>\rRecognizing tables: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.73s/it]</pre> <pre>\n</pre> Out[13]: <p>Note: Calling <code>analyze_layout</code> multiple times (even with the same engine) can add duplicate regions. You might want to use <code>page.clear_detected_layout_regions()</code> first, or filter by model using <code>region[model=yolo]</code>.</p> In\u00a0[14]: Copied! <pre># Re-run YOLO analysis (clearing previous results might be good practice)\npage.clear_detected_layout_regions()\npage.analyze_layout(engine=\"yolo\")\n\n# Find only high-confidence regions (e.g., &gt;= 0.8)\nhigh_conf_regions = page.find_all('region[confidence&gt;=0.8]')\nlen(high_conf_regions)\n</pre> # Re-run YOLO analysis (clearing previous results might be good practice) page.clear_detected_layout_regions() page.analyze_layout(engine=\"yolo\")  # Find only high-confidence regions (e.g., &gt;= 0.8) high_conf_regions = page.find_all('region[confidence&gt;=0.8]') len(high_conf_regions) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpetjodemb/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 780.2ms\n</pre> <pre>Speed: 4.5ms preprocess, 780.2ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre># Ensure TATR analysis has been run\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Ensure TATR analysis has been run page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[15]: In\u00a0[16]: Copied! <pre># Find different structural elements from TATR\ntables = page.find_all('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\n\nf\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\"\n</pre> # Find different structural elements from TATR tables = page.find_all('region[type=table][model=tatr]') rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]')  f\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\" Out[16]: <pre>'Found: 2 tables, 8 rows, 4 columns, 1 headers (from TATR)'</pre> In\u00a0[17]: Copied! <pre># Find the TATR table region again\ntatr_table = page.find('region[type=table][model=tatr]')\n\n# This extraction uses the detected rows/columns\ntatr_table.extract_table()\n</pre> # Find the TATR table region again tatr_table = page.find('region[type=table][model=tatr]')  # This extraction uses the detected rows/columns tatr_table.extract_table() Out[17]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>if you'd like the normal approach instead of the \"intelligent\" one, you can ask for pdfplumber.</p> In\u00a0[18]: Copied! <pre># This extraction uses the detected rows/columns\ntatr_table.extract_table(method='pdfplumber')\n</pre> # This extraction uses the detected rows/columns tatr_table.extract_table(method='pdfplumber') Out[18]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre>"},{"location":"layout-analysis/#document-layout-analysis","title":"Document Layout Analysis\u00b6","text":"<p>Natural PDF can automatically detect the structure of a document (titles, paragraphs, tables, figures) using layout analysis models. This guide shows how to use this feature.</p>"},{"location":"layout-analysis/#setup","title":"Setup\u00b6","text":"<p>We'll use a sample PDF that includes various layout elements.</p>"},{"location":"layout-analysis/#running-basic-layout-analysis","title":"Running Basic Layout Analysis\u00b6","text":"<p>Use the <code>analyze_layout()</code> method. By default, it uses the YOLO model.</p>"},{"location":"layout-analysis/#visualizing-detected-layout","title":"Visualizing Detected Layout\u00b6","text":"<p>Use <code>highlight()</code> or <code>show()</code> on the detected regions.</p>"},{"location":"layout-analysis/#finding-specific-region-types","title":"Finding Specific Region Types\u00b6","text":"<p>Use attribute selectors to find regions of a specific type.</p>"},{"location":"layout-analysis/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>Detected regions are like any other <code>Region</code> object. You can extract text, find elements within them, etc.</p>"},{"location":"layout-analysis/#using-different-layout-models","title":"Using Different Layout Models\u00b6","text":"<p>Natural PDF supports multiple engines (<code>yolo</code>, <code>paddle</code>, <code>tatr</code>). Specify the engine when calling <code>analyze_layout</code>.</p> <p>Note: Using different engines requires installing the corresponding extras (e.g., <code>natural-pdf[layout_paddle]</code>). <code>yolo</code> is the default.</p>"},{"location":"layout-analysis/#controlling-confidence-threshold","title":"Controlling Confidence Threshold\u00b6","text":"<p>Filter detections by their confidence score.</p>"},{"location":"layout-analysis/#table-structure-with-tatr","title":"Table Structure with TATR\u00b6","text":"<p>The TATR engine provides detailed table structure elements (<code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>). This is very useful for precise table extraction.</p>"},{"location":"layout-analysis/#enhanced-table-extraction-with-tatr","title":"Enhanced Table Extraction with TATR\u00b6","text":"<p>When a <code>region[type=table]</code> comes from the TATR model, <code>extract_table()</code> can use the underlying row/column structure for more robust extraction.</p>"},{"location":"layout-analysis/#using-gemini-for-layout-analysis-advanced","title":"Using Gemini for Layout Analysis (Advanced)\u00b6","text":"<p>Natural PDF supports layout analysis using Google's Gemini models via an OpenAI-compatible API. This is an advanced feature and requires you to provide your own OpenAI client, API key, and endpoint.</p> <p>Example usage:</p> <pre>from openai import OpenAI\nfrom natural_pdf import PDF\nfrom natural_pdf.analyzers.layout.layout_options import GeminiLayoutOptions\n\n# Create a compatible OpenAI client for Gemini\nclient = OpenAI(\n    api_key=\"YOUR_GOOGLE_API_KEY\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\noptions = GeminiLayoutOptions(\n    model_name=\"gemini-2.0-flash\",\n    client=client,\n    classes=[\"text\", \"title\"]\n)\n\npdf = PDF(\"your.pdf\")\npage = pdf.pages[0]\nregions = page.analyze_layout(engine=\"gemini\", options=options)\n</pre> <ul> <li>You must provide your own API key and endpoint for Gemini.</li> <li>The client must be compatible with the OpenAI API (see the <code>openai</code> Python package).</li> <li>This feature is intended for advanced users who need LLM-based layout analysis.</li> </ul>"},{"location":"layout-analysis/#next-steps","title":"Next Steps\u00b6","text":"<p>Layout analysis provides regions that you can use for:</p> <ul> <li>Table Extraction: Especially powerful with TATR regions.</li> <li>Text Extraction: Extract text only from specific region types (e.g., paragraphs).</li> <li>Document QA: Focus question answering on specific detected regions.</li> </ul>"},{"location":"loops-and-groups/","title":"Loops, groups and repetitive tasks","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Display the first page \npage = pdf.pages[0]\npage.to_image(width=500)\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Display the first page  page = pdf.pages[0] page.to_image(width=500) Out[1]: <p>We can find all of the book titles by finding (Removed: on the page...</p> In\u00a0[2]: Copied! <pre>page.find_all('text:contains(\"(Removed:\")').show()\n</pre> page.find_all('text:contains(\"(Removed:\")').show() Out[2]: <p>...but it's repeated on each following page, too!</p> In\u00a0[3]: Copied! <pre>pdf.pages[1].find_all('text:contains(\"(Removed:\")').show()\n</pre> pdf.pages[1].find_all('text:contains(\"(Removed:\")').show() Out[3]: <p>No problem, you can use <code>pdf.find_all</code> the same way to do with a single page - you just can't highlight them with <code>.show()</code> the same way.</p> In\u00a0[4]: Copied! <pre>pdf.find_all('text:contains(\"(Removed:\")')\n</pre> pdf.find_all('text:contains(\"(Removed:\")') Out[4]: <pre>&lt;ElementCollection[TextElement](count=37)&gt;</pre> <p>You can see there are 37 across the entire PDF.</p> In\u00a0[5]: Copied! <pre>titles = pdf.find_all('text:contains(\"(Removed:\")')\n\ntitles.extract_each_text()\n</pre> titles = pdf.find_all('text:contains(\"(Removed:\")')  titles.extract_each_text() Out[5]: <pre>['Tristan Strong punches a hole in the sky (Removed: 1)',\n 'Upside down in the middle of nowhere (Removed: 1)',\n 'Buddhism (Removed: 1)',\n 'Voodoo (Removed: 1)',\n 'The Abenaki (Removed: 1)',\n 'Afghanistan (Removed: 1)',\n 'Alexander the Great rocks the world (Removed: 1)',\n 'The Anasazi (Removed: 1)',\n 'And then what happened, Paul Revere? (Removed: 1)',\n 'The assassination of Martin Luther King Jr (Removed: 1)',\n 'Barbara Jordan. (Removed: 1)',\n 'Bedtime for Batman (Removed: 1)',\n 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskegee Airmen leader (Removed: 1)',\n 'Bigfoot Wallace (Removed: 1)',\n 'The blaze engulfs : January 1939 to December 1941 (Removed: 1)',\n 'The boys who challenged Hitler : Knud Pedersen and the Churchill Club (Removed: 1)',\n 'Brown v. Board of Education (Removed: 1)',\n 'The Cahuilla (Removed: 1)',\n 'Cambodia (Removed: 1)',\n 'Celebrate China (Removed: 1)',\n 'Cesar Chavez : a photo-illustrated biography (Removed: 1)',\n 'The Cherokee Indians (Removed: 1)',\n 'Children of the Philippines (Removed: 1)',\n 'The Chinook people (Removed: 1)',\n 'The Choctaw (Removed: 1)',\n 'Christopher Columbus (Removed: 1)',\n 'The Comanche Indians (Removed: 1)',\n 'Dare to dream : Coretta Scott King and the civil rights movement (Removed: 1)',\n 'A day in the life of a Native American (Removed: 1)',\n 'Dear Benjamin Banneker (Removed: 1)',\n 'Dolley Madison (Removed: 1)',\n 'Dreams from my father : a story of race and inheritance (Removed: 1)',\n 'Eleanor Roosevelt : a life of discovery (Removed: 1)',\n 'Elie Wiesel : bearing witness (Removed: 1)',\n 'Elizabeth Cady Stanton : a photo-illustrated biography (Removed: 1)',\n 'Family dinner (Removed: 1)',\n 'A firestorm unleashed : January 1942 - June 1943 (Removed: 1)']</pre> <p>You can also loop through them like a normal list...</p> In\u00a0[6]: Copied! <pre>for title in titles[:10]:\n    print(title.extract_text(), title.page.number)\n</pre> for title in titles[:10]:     print(title.extract_text(), title.page.number) <pre>Tristan Strong punches a hole in the sky (Removed: 1) 1\nUpside down in the middle of nowhere (Removed: 1) 1\nBuddhism (Removed: 1) 1\nVoodoo (Removed: 1) 1\nThe Abenaki (Removed: 1) 1\nAfghanistan (Removed: 1) 1\nAlexander the Great rocks the world (Removed: 1) 1\nThe Anasazi (Removed: 1) 2\nAnd then what happened, Paul Revere? (Removed: 1) 2\nThe assassination of Martin Luther King Jr (Removed: 1) 2\n</pre> <p>...but you can also use <code>.apply</code> for a little functional-programming flavor.</p> In\u00a0[7]: Copied! <pre>titles.apply(lambda title: {\n    'title': title.extract_text(),\n    'page': title.page.number\n})\n</pre> titles.apply(lambda title: {     'title': title.extract_text(),     'page': title.page.number }) Out[7]: <pre>[{'title': 'Tristan Strong punches a hole in the sky (Removed: 1)', 'page': 1},\n {'title': 'Upside down in the middle of nowhere (Removed: 1)', 'page': 1},\n {'title': 'Buddhism (Removed: 1)', 'page': 1},\n {'title': 'Voodoo (Removed: 1)', 'page': 1},\n {'title': 'The Abenaki (Removed: 1)', 'page': 1},\n {'title': 'Afghanistan (Removed: 1)', 'page': 1},\n {'title': 'Alexander the Great rocks the world (Removed: 1)', 'page': 1},\n {'title': 'The Anasazi (Removed: 1)', 'page': 2},\n {'title': 'And then what happened, Paul Revere? (Removed: 1)', 'page': 2},\n {'title': 'The assassination of Martin Luther King Jr (Removed: 1)',\n  'page': 2},\n {'title': 'Barbara Jordan. (Removed: 1)', 'page': 2},\n {'title': 'Bedtime for Batman (Removed: 1)', 'page': 2},\n {'title': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskegee Airmen leader (Removed: 1)',\n  'page': 2},\n {'title': 'Bigfoot Wallace (Removed: 1)', 'page': 2},\n {'title': 'The blaze engulfs : January 1939 to December 1941 (Removed: 1)',\n  'page': 2},\n {'title': 'The boys who challenged Hitler : Knud Pedersen and the Churchill Club (Removed: 1)',\n  'page': 3},\n {'title': 'Brown v. Board of Education (Removed: 1)', 'page': 3},\n {'title': 'The Cahuilla (Removed: 1)', 'page': 3},\n {'title': 'Cambodia (Removed: 1)', 'page': 3},\n {'title': 'Celebrate China (Removed: 1)', 'page': 3},\n {'title': 'Cesar Chavez : a photo-illustrated biography (Removed: 1)',\n  'page': 3},\n {'title': 'The Cherokee Indians (Removed: 1)', 'page': 3},\n {'title': 'Children of the Philippines (Removed: 1)', 'page': 4},\n {'title': 'The Chinook people (Removed: 1)', 'page': 4},\n {'title': 'The Choctaw (Removed: 1)', 'page': 4},\n {'title': 'Christopher Columbus (Removed: 1)', 'page': 4},\n {'title': 'The Comanche Indians (Removed: 1)', 'page': 4},\n {'title': 'Dare to dream : Coretta Scott King and the civil rights movement (Removed: 1)',\n  'page': 4},\n {'title': 'A day in the life of a Native American (Removed: 1)', 'page': 4},\n {'title': 'Dear Benjamin Banneker (Removed: 1)', 'page': 4},\n {'title': 'Dolley Madison (Removed: 1)', 'page': 5},\n {'title': 'Dreams from my father : a story of race and inheritance (Removed: 1)',\n  'page': 5},\n {'title': 'Eleanor Roosevelt : a life of discovery (Removed: 1)', 'page': 5},\n {'title': 'Elie Wiesel : bearing witness (Removed: 1)', 'page': 5},\n {'title': 'Elizabeth Cady Stanton : a photo-illustrated biography (Removed: 1)',\n  'page': 5},\n {'title': 'Family dinner (Removed: 1)', 'page': 5},\n {'title': 'A firestorm unleashed : January 1942 - June 1943 (Removed: 1)',\n  'page': 5}]</pre> <p>I think <code>.map</code> also works on that front?</p> In\u00a0[8]: Copied! <pre>elements = page.find_all('text:contains(\"Removed:\")')\nelements.show()\n</pre> elements = page.find_all('text:contains(\"Removed:\")') elements.show() Out[8]: <p>We can filter for the ones that don't say \"Copies Removed\"</p> In\u00a0[9]: Copied! <pre>titles = elements.filter(\n    lambda element: 'Copies Removed' not in element.extract_text()\n)\ntitles.show()\n</pre> titles = elements.filter(     lambda element: 'Copies Removed' not in element.extract_text() ) titles.show() Out[9]:"},{"location":"loops-and-groups/#loops-groups-and-repetitive-tasks","title":"Loops, groups and repetitive tasks\u00b6","text":"<p>Sometimes you need to do things again and again.</p>"},{"location":"loops-and-groups/#selecting-things","title":"Selecting things\u00b6","text":"<p>Let's say we have a lot of pages that all look like this:</p>"},{"location":"loops-and-groups/#extracting-data-from-elements","title":"Extracting data from elements\u00b6","text":"<p>If you just want the text, <code>.extract_text()</code> will smush it all together, but you can also get it as a list.</p>"},{"location":"loops-and-groups/#filtering","title":"Filtering\u00b6","text":"<p>You can also filter if you only want some of them. For example, maybe we weren't sure how to pick between the different Removed: text blocks.</p>"},{"location":"ocr/","title":"OCR Integration","text":"<p>Natural PDF includes OCR (Optical Character Recognition) to extract text from scanned documents or images embedded in PDFs.</p>"},{"location":"ocr/#ocr-engine-comparison","title":"OCR Engine Comparison","text":"<p>Natural PDF supports multiple OCR engines:</p> Feature EasyOCR PaddleOCR Surya OCR Gemini (Layout + potential OCR) Primary Strength Good general performance, simpler Excellent Asian language, speed High accuracy, multilingual lines Advanced layout analysis (via API) Speed Moderate Fast Moderate (GPU recommended) API Latency Memory Usage Higher Efficient Higher (GPU recommended) N/A (API) Paragraph Detect Yes (via option) No No (focuses on lines) Yes (Layout model) Handwritten Better support Limited Limited Potentially (API model dependent) Small Text Moderate Good Good Potentially (API model dependent) When to Use General documents, handwritten text Asian languages, speed-critical tasks Highest accuracy needed, line-level Complex layouts, API integration <p>If you try to use an engine that is not installed, the library will raise an error with a <code>pip install</code> command to install the required dependencies.</p>"},{"location":"ocr/#basic-ocr-usage","title":"Basic OCR Usage","text":"<p>Apply OCR directly to a page or region:</p> <pre><code>from natural_pdf import PDF\n\n# Assume 'page' is a Page object from a PDF\npage = pdf.pages[0]\n\n# Apply OCR using the default engine (or specify one)\nocr_elements = page.apply_ocr(languages=['en'])\n\n# Extract text (will use the results from apply_ocr if run previously)\ntext = page.extract_text()\nprint(text)\n</code></pre>"},{"location":"ocr/#configuring-ocr","title":"Configuring OCR","text":"<p>Specify the engine and basic options directly:</p>"},{"location":"ocr/#ocr-configuration","title":"OCR Configuration","text":"<pre><code># Use PaddleOCR for Chinese and English\nocr_elements = page.apply_ocr(engine='paddle', languages=['zh-cn', 'en'])\n\n# Use EasyOCR with a lower confidence threshold\nocr_elements = page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.3)\n</code></pre> <p>For advanced, engine-specific settings, use the Options classes:</p> <pre><code>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\nfrom natural_pdf.analyzers.layout import GeminiOptions # Note: Gemini is primarily layout\n\n# --- Configure PaddleOCR ---\npaddle_opts = PaddleOCROptions(\n    # Ugh check ocr_options.py! So many!\n)\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# --- Configure EasyOCR ---\neasy_opts = EasyOCROptions(\n    languages=['en', 'fr'],\n    gpu=True,            # Explicitly enable GPU if available\n    paragraph=True,      # Group results into paragraphs (if structure is clear)\n    detail=1,            # Ensure bounding boxes are returned (required)\n    text_threshold=0.6,  # Confidence threshold for text detection (adjust based on tuning table)\n    link_threshold=0.4,  # Standard EasyOCR param, uncomment if confirmed in wrapper\n    low_text=0.4,        # Standard EasyOCR param, uncomment if confirmed in wrapper\n    batch_size=8         # Processing batch size (adjust based on memory)\n    # See EasyOCROptions documentation or source code for all parameters\n )\nocr_elements = page.apply_ocr(engine='easyocr', options=easy_opts)\n\n# --- Configure Surya OCR ---\n# Surya focuses on line detection and recognition\nsurya_opts = SuryaOCROptions(\n    languages=['en', 'de'], # Specify languages for recognition\n    # device='cuda',       # Use GPU ('cuda') or CPU ('cpu') &lt;-- Set via env var TORCH_DEVICE\n    min_confidence=0.4   # Example: Adjust minimum confidence for results\n    # Core Surya options like device, batch size, and thresholds are typically\n    # set via environment variables (see note below).\n)\nocr_elements = page.apply_ocr(engine='surya', options=surya_opts)\n\n# --- Configure Gemini (as layout analyzer, can be used with OCR) ---\n# Gemini requires API key (GOOGLE_API_KEY environment variable)\n# Note: Gemini is used via apply_layout, but its options can influence OCR if used together\ngemini_opts = GeminiOptions(\n    prompt=\"Extract text content and identify document elements.\",\n    # model_name=\"gemini-1.5-flash-latest\" # Specify a model if needed\n    # See GeminiOptions documentation for more parameters\n)\n# Typically used like this (layout first, then potentially OCR on regions)\nlayout_elements = page.apply_layout(engine='gemini', options=gemini_opts)\n# If Gemini also performed OCR or you want to OCR layout regions:\n# ocr_elements = some_region.apply_ocr(...)\n\n# It can sometimes be used directly if the model supports it, but less common:\n# try:\n#     ocr_elements = page.apply_ocr(engine='gemini', options=gemini_opts)\n# except Exception as e:\n#     print(f\"Gemini might not be configured for direct OCR via apply_ocr: {e}\")\n</code></pre>"},{"location":"ocr/#applying-ocr-directly","title":"Applying OCR Directly","text":"<p>The <code>page.apply_ocr(...)</code> and <code>region.apply_ocr(...)</code> methods are the primary way to run OCR:</p> <pre><code># Apply OCR to a page and get the OCR elements\nocr_elements = page.apply_ocr(engine='easyocr')\nprint(f\"Found {len(ocr_elements)} text elements via OCR\")\n\n# Apply OCR to a specific region\ntitle = page.find('text:contains(\"Title\")')\ncontent_region = title.below(height=300)\nregion_ocr_elements = content_region.apply_ocr(engine='paddle', languages=['en'])\n\n# Note: Re-applying OCR to the same page or region will remove any\n# previously generated OCR elements for that area before adding the new ones.\n</code></pre>"},{"location":"ocr/#ocr-engines","title":"OCR Engines","text":"<p>Choose the engine best suited for your document and language requirements using the <code>engine</code> parameter in <code>apply_ocr</code>.</p>"},{"location":"ocr/#finding-and-working-with-ocr-text","title":"Finding and Working with OCR Text","text":"<p>After applying OCR, work with the text just like regular text:</p> <pre><code># Find all OCR text elements\nocr_text = page.find_all('text[source=ocr]')\n\n# Find high-confidence OCR text\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\n\n# Extract text only from OCR elements\nocr_text_content = page.find_all('text[source=ocr]').extract_text()\n\n# Filter OCR text by content\nnames = page.find_all('text[source=ocr]:contains(\"Smith\")', case=False)\n</code></pre>"},{"location":"ocr/#visualizing-ocr-results","title":"Visualizing OCR Results","text":"<p>See OCR results to help debug issues:</p> <pre><code># Apply OCR \nocr_elements = page.apply_ocr()\n\n# Highlight all OCR elements\nfor element in ocr_elements:\n    # Color based on confidence\n    if element.confidence &gt;= 0.8:\n        color = \"green\"  # High confidence\n    elif element.confidence &gt;= 0.5:\n        color = \"yellow\"  # Medium confidence\n    else:\n        color = \"red\"  # Low confidence\n\n    element.highlight(color=color, label=f\"OCR ({element.confidence:.2f})\")\n\n# Get the visualization as an image\nimage = page.to_image(labels=True)\n# Just return the image in a Jupyter cell\nimage\n\n# Highlight only high-confidence elements\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nhigh_conf.highlight(color=\"green\", label=\"High Confidence OCR\")\n</code></pre>"},{"location":"ocr/#detect-llm-ocr","title":"Detect + LLM OCR","text":"<p>Sometimes you have a difficult piece of content where you need to use a local model to identify the content, then send it off in pieces to be identified by the LLM. You can do this with Natural PDF!</p> <pre><code>from natural_pdf import PDF\nfrom natural_pdf.ocr.utils import direct_ocr_llm\nimport openai\n\npdf = PDF(\"needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Detect\npage.apply_ocr('paddle', resolution=120, detect_only=True)\n\n# Build the framework\nclient = openai.OpenAI(base_url=\"https://api.anthropic.com/v1/\",  api_key='sk-XXXXX')\nprompt = \"\"\"OCR this image. Return only the exact text from the image. Include misspellings,\npunctuation, etc. Do not surround it with quotation marks. Do not include translations or comments.\nThe text is from a Greek spreadsheet, so most likely content is Modern Greek or numeric.\"\"\"\n\n# This returns the cleaned-up text\ndef correct(region):\n    return direct_ocr_llm(region, client, prompt=prompt, resolution=300, model=\"claude-3-5-haiku-20241022\")\n\n# Run 'correct' on each text element\npage.correct_ocr(correct)\n\n# You're done!\n</code></pre>"},{"location":"ocr/#interactive-ocr-correction-debugging","title":"Interactive OCR Correction / Debugging","text":"<p>Natural PDF includes a utility to package a PDF and its detected elements, along with an interactive web application (SPA) for reviewing and correcting OCR results.</p> <ol> <li> <p>Package the data:     Use the <code>create_correction_task_package</code> function to create a zip file containing the necessary data for the SPA.</p> <pre><code>from natural_pdf.utils.packaging import create_correction_task_package\n\n# Assuming 'pdf' is your loaded PDF object after running apply_ocr or apply_layout\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</code></pre> </li> <li> <p>Run the SPA:     The correction SPA is bundled with the library. You need to run a simple web server from the directory containing the SPA's files. The location of these files might depend on your installation, but you can typically find them within the installed <code>natural_pdf</code> package directory under <code>templates/spa</code>.</p> <p>Example using Python's built-in server (run from your terminal):</p> <pre><code># Find the path to the installed natural_pdf package\n# (This command might vary depending on your environment)\nNATURAL_PDF_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/natural_pdf\n\n# Navigate to the SPA directory\ncd $NATURAL_PDF_PATH/templates/spa\n\n# Start the web server (e.g., on port 8000)\npython -m http.server 8000\n</code></pre> </li> <li> <p>Use the SPA:     Open your web browser to <code>http://localhost:8000</code>. The SPA should load, allowing you to drag and drop the <code>correction_package.zip</code> file you created into the application to view and edit the OCR results.</p> </li> </ol>"},{"location":"ocr/#next-steps","title":"Next Steps","text":"<p>With OCR capabilities, you can explore:</p> <ul> <li>Layout Analysis for automatically detecting document structure</li> <li>Document QA for asking questions about your documents</li> <li>Visual Debugging for visualizing OCR results</li> </ul>"},{"location":"pdf-navigation/","title":"PDF Navigation","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Open a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n</pre> from natural_pdf import PDF  # Open a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\") In\u00a0[2]: Copied! <pre># Get the total number of pages\nnum_pages = len(pdf)\nprint(f\"This PDF has {num_pages} pages\")\n\n# Get a specific page (0-indexed)\nfirst_page = pdf.pages[0]\nlast_page = pdf.pages[-1]\n\n# Iterate through the first 20 pages\nfor page in pdf.pages[:20]:\n    print(f\"Page {page.number} has {len(page.extract_text())} characters\")\n</pre> # Get the total number of pages num_pages = len(pdf) print(f\"This PDF has {num_pages} pages\")  # Get a specific page (0-indexed) first_page = pdf.pages[0] last_page = pdf.pages[-1]  # Iterate through the first 20 pages for page in pdf.pages[:20]:     print(f\"Page {page.number} has {len(page.extract_text())} characters\") <pre>This PDF has 153 pages\nPage 1 has 985 characters\nPage 2 has 778 characters\nPage 3 has 522 characters\nPage 4 has 984 characters\nPage 5 has 778 characters\nPage 6 has 523 characters\nPage 7 has 982 characters\nPage 8 has 772 characters\nPage 9 has 522 characters\nPage 10 has 1008 characters\nPage 11 has 796 characters\nPage 12 has 532 characters\nPage 13 has 986 characters\nPage 14 has 780 characters\nPage 15 has 523 characters\nPage 16 has 990 characters\nPage 17 has 782 characters\nPage 18 has 520 characters\nPage 19 has 1006 characters\nPage 20 has 795 characters\n</pre> In\u00a0[3]: Copied! <pre># Page dimensions in points (1/72 inch)\nprint(page.width, page.height)\n\n# Page number (1-indexed as shown in PDF viewers)\nprint(page.number)\n\n# Page index (0-indexed position in the PDF)\nprint(page.index)\n</pre> # Page dimensions in points (1/72 inch) print(page.width, page.height)  # Page number (1-indexed as shown in PDF viewers) print(page.number)  # Page index (0-indexed position in the PDF) print(page.index) <pre>612 792\n20\n19\n</pre> In\u00a0[4]: Copied! <pre># Extract text from all pages\nall_text = pdf.extract_text()\n\n# Find elements across all pages\nall_headings = pdf.find_all('text[size&gt;=14]:bold')\n\n# Add exclusion zones to all pages (like headers/footers)\npdf.add_exclusion(\n    lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n    label=\"header\"\n)\n</pre> # Extract text from all pages all_text = pdf.extract_text()  # Find elements across all pages all_headings = pdf.find_all('text[size&gt;=14]:bold')  # Add exclusion zones to all pages (like headers/footers) pdf.add_exclusion(     lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,     label=\"header\" ) Out[4]: <pre>&lt;natural_pdf.core.pdf.PDF at 0x109bee9b0&gt;</pre> In\u00a0[5]: Copied! <pre># Extract text from specific pages\ntext = pdf.pages[2:5].extract_text()\n\n# Find elements across specific pages\nelements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")')\n</pre> # Extract text from specific pages text = pdf.pages[2:5].extract_text()  # Find elements across specific pages elements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")') <pre>2025-05-06T15:29:28.620225Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,620] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> <pre>2025-05-06T15:29:28.631200Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,631] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> <pre>2025-05-06T15:29:28.640121Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,640] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> In\u00a0[6]: Copied! <pre># Get sections with headings as section starts\nsections = pdf.pages.get_sections(\n    start_elements='text[size&gt;=14]:bold',\n    new_section_on_page_break=False\n)\n</pre> # Get sections with headings as section starts sections = pdf.pages.get_sections(     start_elements='text[size&gt;=14]:bold',     new_section_on_page_break=False )"},{"location":"pdf-navigation/#pdf-navigation","title":"PDF Navigation\u00b6","text":"<p>This guide covers the basics of working with PDFs in Natural PDF - opening documents, accessing pages, and navigating through content.</p>"},{"location":"pdf-navigation/#opening-a-pdf","title":"Opening a PDF\u00b6","text":"<p>The main entry point to Natural PDF is the <code>PDF</code> class:</p>"},{"location":"pdf-navigation/#accessing-pages","title":"Accessing Pages\u00b6","text":"<p>Once you have a PDF object, you can access its pages:</p>"},{"location":"pdf-navigation/#page-properties","title":"Page Properties\u00b6","text":"<p>Each <code>Page</code> object has useful properties:</p>"},{"location":"pdf-navigation/#working-across-pages","title":"Working Across Pages\u00b6","text":"<p>Natural PDF makes it easy to work with content across multiple pages:</p>"},{"location":"pdf-navigation/#the-page-collection","title":"The Page Collection\u00b6","text":"<p>The <code>pdf.pages</code> object is a <code>PageCollection</code> that allows batch operations on pages:</p>"},{"location":"pdf-navigation/#document-sections-across-pages","title":"Document Sections Across Pages\u00b6","text":"<p>You can extract sections that span across multiple pages:</p>"},{"location":"pdf-navigation/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to navigate PDFs, you can:</p> <ul> <li>Find elements using selectors</li> <li>Extract text from your documents</li> <li>Work with specific regions</li> </ul>"},{"location":"reflowing-pages/","title":"Restructuring page content","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\nfrom natural_pdf.flows import Flow\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/multicolumn.pdf\")\npage = pdf.pages[0]\npage.to_image(width=500)\n</pre> from natural_pdf import PDF from natural_pdf.flows import Flow  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/multicolumn.pdf\") page = pdf.pages[0] page.to_image(width=500) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: <p>We can grab individual columns from it.</p> In\u00a0[2]: Copied! <pre>left = page.region(right=page.width/3)\nmid = page.region(left=page.width/3, right=page.width/3*2)\nright = page.region(left=page.width/3*2)\n\nmid.show(width=500)\n</pre> left = page.region(right=page.width/3) mid = page.region(left=page.width/3, right=page.width/3*2) right = page.region(left=page.width/3*2)  mid.show(width=500) Out[2]: In\u00a0[3]: Copied! <pre>stacked = [left, mid, right]\nflow = Flow(segments=stacked, arrangement=\"vertical\")\n</pre> stacked = [left, mid, right] flow = Flow(segments=stacked, arrangement=\"vertical\") <p>As a result, I can find text in the first column and ask it to grab what's \"below\" until it hits content in the second column.</p> In\u00a0[4]: Copied! <pre>region = (\n    flow\n    .find('text:contains(\"Table one\")')\n    .below(\n        until='text:contains(\"Table two\")',\n        include_endpoint=False\n    )\n)\nregion.show()\n</pre> region = (     flow     .find('text:contains(\"Table one\")')     .below(         until='text:contains(\"Table two\")',         include_endpoint=False     ) ) region.show() Out[4]: <p>While you can't easily extract tables yet, you can at least extract text!</p> In\u00a0[5]: Copied! <pre>print(region.extract_text())\n</pre> print(region.extract_text()) <pre>index number\n1 123\n2 456\n3 789\n4 1122\n5 1455\n6 1788\n7 2121\n8 2454\n9 2787\n10 3120\n11 3453\n12 3786\n13 4119\n14 4452\n15 4785\n16 5118\n17 5451\n18 5784\n19 6117\n20 6450\n21 6783\n22 7116\n23 7449\n24 7782\n25 8115\n26 8448\n27 8781\n28 9114\n29 9447\n30 9780\n31 10113\n32 10446\n33 10779\n34 11112\n35 11445\n36 11778\n37 12111\n38 12444\n39 12777\n</pre> In\u00a0[6]: Copied! <pre>(\n    flow\n    .find_all('text[size=12][width&gt;10]:bold')\n    .show()\n)\n</pre> (     flow     .find_all('text[size=12][width&gt;10]:bold')     .show() ) Out[6]: <p>...it's easy to extract each table that's betwen them.</p> In\u00a0[7]: Copied! <pre>regions = (\n    flow\n    .find_all('text[size=12][width&gt;10]:bold')\n    .below(\n        until='text[size=12][width&gt;10]:bold|text:contains(\"Here is a bit\")',\n        include_endpoint=False\n    )\n)\nregions.show()\n</pre> regions = (     flow     .find_all('text[size=12][width&gt;10]:bold')     .below(         until='text[size=12][width&gt;10]:bold|text:contains(\"Here is a bit\")',         include_endpoint=False     ) ) regions.show() Out[7]:"},{"location":"reflowing-pages/#restructuring-page-content","title":"Restructuring page content\u00b6","text":"<p>Flows are a way to restructure pages that are not in normal one-page reading order. This might be columnal data, tables than span pages, etc.</p>"},{"location":"reflowing-pages/#a-multi-column-pdf","title":"A multi-column PDF\u00b6","text":"<p>Here is a multi column PDF.</p>"},{"location":"reflowing-pages/#restructuring","title":"Restructuring\u00b6","text":"<p>We can use Flows to stack the three columns on top of each other.</p>"},{"location":"reflowing-pages/#find_all-and-reflows","title":"find_all and reflows\u00b6","text":"<p>Let's say we have a few headers...</p>"},{"location":"regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page page = pdf.pages[0]  # Display the page page.show(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Create a region by specifying (x0, top, x1, bottom) coordinates\n# Let's create a region in the middle of the page\nmid_region = page.create_region(\n    x0=100,         # Left edge\n    top=200,        # Top edge\n    x1=500,         # Right edge\n    bottom=400      # Bottom edge\n)\n\n# Highlight the region to see it\nmid_region.highlight(color=\"blue\").show()\n</pre> # Create a region by specifying (x0, top, x1, bottom) coordinates # Let's create a region in the middle of the page mid_region = page.create_region(     x0=100,         # Left edge     top=200,        # Top edge     x1=500,         # Right edge     bottom=400      # Bottom edge )  # Highlight the region to see it mid_region.highlight(color=\"blue\").show() Out[2]: In\u00a0[3]: Copied! <pre># Find a heading-like element\nheading = page.find('text[size&gt;=12]:bold')\n\n# Create a region below this heading element\nif heading:\n    region_below = heading.below()\n    \n    # Highlight the heading and the region below it\n    heading.highlight(color=\"red\")\n    region_below.highlight(color=\"blue\")\n    page.show()\n</pre> # Find a heading-like element heading = page.find('text[size&gt;=12]:bold')  # Create a region below this heading element if heading:     region_below = heading.below()          # Highlight the heading and the region below it     heading.highlight(color=\"red\")     region_below.highlight(color=\"blue\")     page.show() In\u00a0[4]: Copied! <pre># Create a region with height limit\nif heading:\n    # Only include 100px below the heading\n    small_region_below = heading.below(height=100)\n    \n    page.clear_highlights()\n    heading.highlight(color=\"red\")\n    small_region_below.highlight(color=\"green\")\n    page.show()\n</pre> # Create a region with height limit if heading:     # Only include 100px below the heading     small_region_below = heading.below(height=100)          page.clear_highlights()     heading.highlight(color=\"red\")     small_region_below.highlight(color=\"green\")     page.show() In\u00a0[5]: Copied! <pre># Find a line or other element to create a region above\nline = page.find('line')\nif line:\n    # Create a region above the line\n    region_above = line.above()\n    \n    page.clear_highlights()\n    line.highlight(color=\"black\")\n    region_above.highlight(color=\"purple\")\n    page.show()\n</pre> # Find a line or other element to create a region above line = page.find('line') if line:     # Create a region above the line     region_above = line.above()          page.clear_highlights()     line.highlight(color=\"black\")     region_above.highlight(color=\"purple\")     page.show() In\u00a0[6]: Copied! <pre># Find two elements to use as boundaries\nfirst_heading = page.find('text[size&gt;=11]:bold')\nnext_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None\n\nif first_heading and next_heading:\n    # Create a region from the first heading until the next heading\n    section = first_heading.below(until=next_heading, include_endpoint=False)\n    \n    # Highlight both elements and the region between them\n    page.clear_highlights()\n    first_heading.highlight(color=\"red\")\n    next_heading.highlight(color=\"red\")\n    section.highlight(color=\"yellow\")\n    page.show()\n</pre> # Find two elements to use as boundaries first_heading = page.find('text[size&gt;=11]:bold') next_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None  if first_heading and next_heading:     # Create a region from the first heading until the next heading     section = first_heading.below(until=next_heading, include_endpoint=False)          # Highlight both elements and the region between them     page.clear_highlights()     first_heading.highlight(color=\"red\")     next_heading.highlight(color=\"red\")     section.highlight(color=\"yellow\")     page.show() In\u00a0[7]: Copied! <pre># Find a region to work with (e.g., from a title to the next bold text)\ntitle = page.find('text:contains(\"Site\")')  # Adjust if needed\nif title:\n    # Create a region from title down to the next bold text\n    content_region = title.below(until='line:horizontal', include_endpoint=False)\n    \n    # Extract text from just this region\n    region_text = content_region.extract_text()\n    \n    # Show the region and the extracted text\n    page.clear_highlights()\n    content_region.highlight(color=\"green\")\n    page.show()\n    \n    # Displaying the text (first 300 chars if long)\n    print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text)\n</pre> # Find a region to work with (e.g., from a title to the next bold text) title = page.find('text:contains(\"Site\")')  # Adjust if needed if title:     # Create a region from title down to the next bold text     content_region = title.below(until='line:horizontal', include_endpoint=False)          # Extract text from just this region     region_text = content_region.extract_text()          # Show the region and the extracted text     page.clear_highlights()     content_region.highlight(color=\"green\")     page.show()          # Displaying the text (first 300 chars if long)     print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text) <pre>Date: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other me...\n</pre> In\u00a0[8]: Copied! <pre># Create a region in an interesting part of the page\ntest_region = page.create_region(\n    x0=page.width * 0.1, \n    top=page.height * 0.25, \n    x1=page.width * 0.9, \n    bottom=page.height * 0.75\n)\n\n# Find all text elements ONLY within this region\ntext_in_region = test_region.find_all('text')\n\n# Display result\npage.clear_highlights()\ntest_region.highlight(color=\"blue\")\ntext_in_region.highlight(color=\"red\")\npage.show()\n\nlen(text_in_region)  # Number of text elements found in region\n</pre> # Create a region in an interesting part of the page test_region = page.create_region(     x0=page.width * 0.1,      top=page.height * 0.25,      x1=page.width * 0.9,      bottom=page.height * 0.75 )  # Find all text elements ONLY within this region text_in_region = test_region.find_all('text')  # Display result page.clear_highlights() test_region.highlight(color=\"blue\") text_in_region.highlight(color=\"red\") page.show()  len(text_in_region)  # Number of text elements found in region Out[8]: <pre>17</pre> In\u00a0[9]: Copied! <pre># Find a specific region to capture\n# (Could be a table, figure, or any significant area)\nregion_for_image = page.create_region(\n    x0=100, \n    top=150,\n    x1=page.width - 100,\n    bottom=300\n)\n\n# Generate an image of just this region\nregion_for_image.to_image(crop_only=True)  # Shows just the region\n</pre> # Find a specific region to capture # (Could be a table, figure, or any significant area) region_for_image = page.create_region(     x0=100,      top=150,     x1=page.width - 100,     bottom=300 )  # Generate an image of just this region region_for_image.to_image(crop_only=True)  # Shows just the region Out[9]: In\u00a0[10]: Copied! <pre># Take an existing region and expand it\nregion_a = page.create_region(200, 200, 400, 400)\n\n# Expand by a certain number of points in each direction\nexpanded = region_a.expand(left=20, right=20, top=20, bottom=20)\n\n# Visualize original and expanded regions\npage.clear_highlights()\nregion_a.highlight(color=\"blue\", label=\"Original\")\nexpanded.highlight(color=\"red\", label=\"Expanded\")\npage.to_image()\n</pre> # Take an existing region and expand it region_a = page.create_region(200, 200, 400, 400)  # Expand by a certain number of points in each direction expanded = region_a.expand(left=20, right=20, top=20, bottom=20)  # Visualize original and expanded regions page.clear_highlights() region_a.highlight(color=\"blue\", label=\"Original\") expanded.highlight(color=\"red\", label=\"Expanded\") page.to_image() Out[10]: In\u00a0[11]: Copied! <pre># Create a region for the whole page\nfull_page_region = page.create_region(0, 0, page.width, page.height)\n\n# Extract text without exclusions as baseline\nfull_text = full_page_region.extract_text()\nprint(f\"Full page text length: {len(full_text)} characters\")\n</pre> # Create a region for the whole page full_page_region = page.create_region(0, 0, page.width, page.height)  # Extract text without exclusions as baseline full_text = full_page_region.extract_text() print(f\"Full page text length: {len(full_text)} characters\") <pre>Full page text length: 1255 characters\n</pre> In\u00a0[12]: Copied! <pre># Define an area we want to exclude (like a header)\n# Let's exclude the top 10% of the page\nheader_zone = page.create_region(0, 0, page.width, page.height * 0.1)\n\n# Add this as an exclusion for the page\npage.add_exclusion(header_zone)\n\n# Visualize the exclusion\npage.clear_highlights()\nheader_zone.highlight(color=\"red\", label=\"Excluded\")\npage.show()\n</pre> # Define an area we want to exclude (like a header) # Let's exclude the top 10% of the page header_zone = page.create_region(0, 0, page.width, page.height * 0.1)  # Add this as an exclusion for the page page.add_exclusion(header_zone)  # Visualize the exclusion page.clear_highlights() header_zone.highlight(color=\"red\", label=\"Excluded\") page.show() Out[12]: In\u00a0[13]: Copied! <pre># Now extract text again - the header should be excluded\ntext_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default\n\n# Compare text lengths\nprint(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\")\nprint(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\")\n</pre> # Now extract text again - the header should be excluded text_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default  # Compare text lengths print(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\") print(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\") <pre>Original text: 1255 chars\nText with exclusion: 1193 chars\nDifference: 62 chars excluded\n</pre> In\u00a0[14]: Copied! <pre># When done with this page, clear exclusions\npage.clear_exclusions()\n</pre> # When done with this page, clear exclusions page.clear_exclusions() Out[14]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[15]: Copied! <pre># Define a PDF-level exclusion for headers\n# This will exclude the top 30% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.3),\n    label=\"Header zone\"\n)\n\n# Define a PDF-level exclusion for footers\n# This will exclude the bottom 20% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),\n    label=\"Footer zone\"\n)\n\n# PDF-level exclusions are used whenever you extract text\n# Let's try on the first three pages\nfor page in pdf.pages[:3]:\n    text = page.extract_text()\n    text_original = page.extract_text(use_exclusions=False)\n    print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\")\n</pre> # Define a PDF-level exclusion for headers # This will exclude the top 30% of every page pdf.add_exclusion(     lambda p: p.create_region(0, 0, p.width, p.height * 0.3),     label=\"Header zone\" )  # Define a PDF-level exclusion for footers # This will exclude the bottom 20% of every page pdf.add_exclusion(     lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),     label=\"Footer zone\" )  # PDF-level exclusions are used whenever you extract text # Let's try on the first three pages for page in pdf.pages[:3]:     text = page.extract_text()     text_original = page.extract_text(use_exclusions=False)     print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\") <pre>Page 1 \u2013 Before: 456 After: 456\n</pre> In\u00a0[16]: Copied! <pre># Clear PDF-level exclusions when done\npdf.clear_exclusions()\nprint(\"Cleared all PDF-level exclusions\")\n</pre> # Clear PDF-level exclusions when done pdf.clear_exclusions() print(\"Cleared all PDF-level exclusions\") <pre>Cleared all PDF-level exclusions\n</pre> In\u00a0[17]: Copied! <pre># First, run layout analysis to detect regions\npage.analyze_layout()  # Uses 'yolo' engine by default\n\n# Find all detected regions\ndetected_regions = page.find_all('region')\nprint(f\"Found {len(detected_regions)} layout regions\")\n</pre> # First, run layout analysis to detect regions page.analyze_layout()  # Uses 'yolo' engine by default  # Find all detected regions detected_regions = page.find_all('region') print(f\"Found {len(detected_regions)} layout regions\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp3__v8qd3/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 2839.0ms\n</pre> <pre>Speed: 10.2ms preprocess, 2839.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Found 7 layout regions\n</pre> In\u00a0[18]: Copied! <pre># Highlight all detected regions by type\ndetected_regions.highlight(group_by='region_type').show()\n</pre> # Highlight all detected regions by type detected_regions.highlight(group_by='region_type').show() Out[18]: In\u00a0[19]: Copied! <pre># Extract text from a specific region type (e.g., title)\ntitle_regions = page.find_all('region[type=title]')\nif title_regions:\n    titles_text = title_regions.extract_text()\n    print(f\"Title text: {titles_text}\")\n</pre> # Extract text from a specific region type (e.g., title) title_regions = page.find_all('region[type=title]') if title_regions:     titles_text = title_regions.extract_text()     print(f\"Title text: {titles_text}\") <pre>Title text: \n</pre>"},{"location":"regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that define boundaries for operations like text extraction, element finding, or visualization. They're one of Natural PDF's most powerful features for working with specific parts of a document.</p>"},{"location":"regions/#setup","title":"Setup\u00b6","text":"<p>Let's set up a PDF to experiment with regions.</p>"},{"location":"regions/#creating-regions","title":"Creating Regions\u00b6","text":"<p>There are several ways to create regions in Natural PDF.</p>"},{"location":"regions/#using-create_region-with-coordinates","title":"Using <code>create_region()</code> with Coordinates\u00b6","text":"<p>This is the most direct method - provide the coordinates directly.</p>"},{"location":"regions/#using-element-methods-above-below-left-right","title":"Using Element Methods: <code>above()</code>, <code>below()</code>, <code>left()</code>, <code>right()</code>\u00b6","text":"<p>You can create regions relative to existing elements.</p>"},{"location":"regions/#creating-a-region-between-elements-with-until","title":"Creating a Region Between Elements with <code>until()</code>\u00b6","text":""},{"location":"regions/#using-regions","title":"Using Regions\u00b6","text":"<p>Once you have a region, here's what you can do with it.</p>"},{"location":"regions/#extract-text-from-a-region","title":"Extract Text from a Region\u00b6","text":""},{"location":"regions/#find-elements-within-a-region","title":"Find Elements Within a Region\u00b6","text":"<p>You can use a region as a \"filter\" to only find elements within its boundaries.</p>"},{"location":"regions/#generate-an-image-of-a-region","title":"Generate an Image of a Region\u00b6","text":""},{"location":"regions/#adjust-and-expand-regions","title":"Adjust and Expand Regions\u00b6","text":""},{"location":"regions/#using-exclusion-zones-with-regions","title":"Using Exclusion Zones with Regions\u00b6","text":"<p>Exclusion zones are regions that you want to ignore during operations like text extraction.</p>"},{"location":"regions/#document-level-exclusions","title":"Document-Level Exclusions\u00b6","text":"<p>PDF-level exclusions apply to all pages and use functions to adapt to each page.</p>"},{"location":"regions/#working-with-layout-analysis-regions","title":"Working with Layout Analysis Regions\u00b6","text":"<p>When you run layout analysis, the detected regions (tables, titles, etc.) are also Region objects.</p>"},{"location":"regions/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you understand regions, you can:</p> <ul> <li>Extract tables from table regions</li> <li>Ask questions about specific regions</li> <li>Exclude content from extraction</li> </ul>"},{"location":"tables/","title":"Table Extraction","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Extract the first table found on the page using pdfplumber\n# This works best for simple tables with clear lines\ntable_data = page.extract_table() # Returns a list of lists\ntable_data\n</pre> # Extract the first table found on the page using pdfplumber # This works best for simple tables with clear lines table_data = page.extract_table() # Returns a list of lists table_data Out[2]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>This might fail or give poor results if there are multiple tables or the table structure is complex.</p> In\u00a0[3]: Copied! <pre># Detect layout elements using YOLO (default)\npage.analyze_layout(engine='yolo')\n\n# Find regions detected as tables\ntable_regions_yolo = page.find_all('region[type=table][model=yolo]')\ntable_regions_yolo.show()\n</pre> # Detect layout elements using YOLO (default) page.analyze_layout(engine='yolo')  # Find regions detected as tables table_regions_yolo = page.find_all('region[type=table][model=yolo]') table_regions_yolo.show() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp8dlzmxv0/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1984.9ms\n</pre> <pre>Speed: 6.8ms preprocess, 1984.9ms inference, 8.1ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[3]: In\u00a0[4]: Copied! <pre>table_regions_yolo[0].extract_table()\n</pre> table_regions_yolo[0].extract_table() Out[4]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[5]: Copied! <pre>page.clear_detected_layout_regions() # Clear previous YOLO regions for clarity\npage.analyze_layout(engine='tatr')\n</pre> page.clear_detected_layout_regions() # Clear previous YOLO regions for clarity page.analyze_layout(engine='tatr') Out[5]: <pre>&lt;ElementCollection[Region](count=15)&gt;</pre> In\u00a0[6]: Copied! <pre># Find the main table region(s) detected by TATR\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.show()\n</pre> # Find the main table region(s) detected by TATR tatr_table = page.find('region[type=table][model=tatr]') tatr_table.show() Out[6]: In\u00a0[7]: Copied! <pre># Find rows, columns, headers detected by TATR\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\nf\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\"\n</pre> # Find rows, columns, headers detected by TATR rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]') f\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\" Out[7]: <pre>'TATR found: 8 rows, 4 columns, 1 headers'</pre> In\u00a0[8]: Copied! <pre>tatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.extract_table(method='tatr')\n</pre> tatr_table = page.find('region[type=table][model=tatr]') tatr_table.extract_table(method='tatr') Out[8]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[9]: Copied! <pre># Force using pdfplumber even on a TATR-detected region\n# (Might be useful for comparison or if TATR structure is flawed)\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.extract_table(method='pdfplumber')\n</pre> # Force using pdfplumber even on a TATR-detected region # (Might be useful for comparison or if TATR structure is flawed) tatr_table = page.find('region[type=table][model=tatr]') tatr_table.extract_table(method='pdfplumber') Out[9]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre> In\u00a0[10]: Copied! <pre># Example: Use text alignment for vertical lines, explicit lines for horizontal\n# See pdfplumber documentation for all settings\ntable_settings = {\n    \"vertical_strategy\": \"text\",\n    \"horizontal_strategy\": \"lines\",\n    \"intersection_x_tolerance\": 5, # Increase tolerance for intersections\n}\n\nresults = page.extract_table(\n    table_settings=table_settings\n)\n</pre> # Example: Use text alignment for vertical lines, explicit lines for horizontal # See pdfplumber documentation for all settings table_settings = {     \"vertical_strategy\": \"text\",     \"horizontal_strategy\": \"lines\",     \"intersection_x_tolerance\": 5, # Increase tolerance for intersections }  results = page.extract_table(     table_settings=table_settings ) In\u00a0[11]: Copied! <pre>import pandas as pd\n\npd.DataFrame(page.extract_table())\n</pre> import pandas as pd  pd.DataFrame(page.extract_table()) Out[11]: 0 1 2 3 0 Statute Description Level Repeat? 1 4.12.7 Unsanitary Working Conditions. Critical 2 5.8.3 Inadequate Protective Equipment. Serious 3 6.3.9 Ineffective Injury Prevention. Serious 4 7.1.5 Failure to Properly Store Hazardous Materials. Critical 5 8.9.2 Lack of Adequate Fire Safety Measures. Serious 6 9.6.4 Inadequate Ventilation Systems. Serious 7 10.2.7 Insufficient Employee Training for Safe Work P... Serious In\u00a0[12]: Copied! <pre># This doesn't work! I forget why, I should troubleshoot later.\n# tatr_table.cells\n</pre> # This doesn't work! I forget why, I should troubleshoot later. # tatr_table.cells"},{"location":"tables/#table-extraction","title":"Table Extraction\u00b6","text":"<p>Extracting tables from PDFs can range from straightforward to complex. Natural PDF provides several tools and methods to handle different scenarios, leveraging both rule-based (<code>pdfplumber</code>) and model-based (<code>TATR</code>) approaches.</p>"},{"location":"tables/#setup","title":"Setup\u00b6","text":"<p>Let's load a PDF containing tables.</p>"},{"location":"tables/#basic-table-extraction-no-detection","title":"Basic Table Extraction (No Detection)\u00b6","text":"<p>If you know a table exists, you can try <code>extract_table()</code> directly on the page or a region. This uses <code>pdfplumber</code> behind the scenes.</p>"},{"location":"tables/#layout-analysis-for-table-detection","title":"Layout Analysis for Table Detection\u00b6","text":"<p>A more robust approach can be to first detect the table boundaries using layout analysis.</p>"},{"location":"tables/#using-yolo-default","title":"Using YOLO (Default)\u00b6","text":"<p>The default YOLO model finds the overall bounding box of tables.</p>"},{"location":"tables/#using-tatr-table-transformer","title":"Using TATR (Table Transformer)\u00b6","text":"<p>The TATR model provides detailed table structure (rows, columns, headers).</p>"},{"location":"tables/#controlling-extraction-method-plumber-vs-tatr","title":"Controlling Extraction Method (<code>plumber</code> vs <code>tatr</code>)\u00b6","text":"<p>When you call <code>extract_table()</code> on a region:</p> <ul> <li>If the region was detected by YOLO (or not detected at all), it uses the <code>plumber</code> method.</li> <li>If the region was detected by TATR, it defaults to the <code>tatr</code> method, which uses the detected row/column structure.</li> </ul> <p>You can override this using the <code>method</code> argument.</p>"},{"location":"tables/#when-to-use-which-method","title":"When to Use Which Method?\u00b6","text":"<ul> <li><code>pdfplumber</code>: Good for simple tables with clear grid lines. Faster.</li> <li><code>tatr</code>: Better for tables without clear lines, complex cell merging, or irregular layouts. Leverages the model's understanding of rows and columns.</li> </ul>"},{"location":"tables/#customizing-pdfplumber-settings","title":"Customizing <code>pdfplumber</code> Settings\u00b6","text":"<p>If using the <code>pdfplumber</code> method (explicitly or implicitly), you can pass <code>pdfplumber</code> settings via <code>table_settings</code>.</p>"},{"location":"tables/#saving-extracted-tables","title":"Saving Extracted Tables\u00b6","text":"<p>You can easily save the extracted data (list of lists) to common formats.</p>"},{"location":"tables/#working-directly-with-tatr-cells","title":"Working Directly with TATR Cells\u00b6","text":"<p>The TATR engine implicitly creates cell regions at the intersection of detected rows and columns. You can access these for fine-grained control.</p>"},{"location":"tables/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Layout Analysis: Understand how table detection fits into overall document structure analysis.</li> <li>Working with Regions: Manually define table areas if detection fails.</li> </ul>"},{"location":"text-analysis/","title":"Text Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0] In\u00a0[2]: Copied! <pre># Find the first word element\nword = page.find('word') \n\nprint(f\"Text:\", word.text)\nprint(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name\nprint(f\"Size:\", word.size)\nprint(f\"Color:\", word.color) # Non-stroking color\nprint(f\"Is Bold:\", word.bold)\nprint(f\"Is Italic:\", word.italic)\n</pre> # Find the first word element word = page.find('word')   print(f\"Text:\", word.text) print(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name print(f\"Size:\", word.size) print(f\"Color:\", word.color) # Non-stroking color print(f\"Is Bold:\", word.bold) print(f\"Is Italic:\", word.italic) <pre>Text: Jungle Health and Safety Inspection Service\nFont Name: Helvetica\nSize: 8.0\nColor: (0, 0, 0)\nIs Bold: False\nIs Italic: False\n</pre> <ul> <li><code>fontname</code>: Often an internal reference (like 'F1', 'F2') or a basic name.</li> <li><code>size</code>: Font size in points.</li> <li><code>color</code>: The non-stroking color, typically a tuple representing RGB or Grayscale values (e.g., <code>(0.0, 0.0, 0.0)</code> for black).</li> <li><code>bold</code>, <code>italic</code>: Boolean flags indicating if the font style is bold or italic (heuristically determined based on font name conventions).</li> </ul> In\u00a0[3]: Copied! <pre># Find all bold text elements\nbold_text = page.find_all('text:bold')\n\n# Find all italic text elements\nitalic_text = page.find_all('text:italic')\n\n# Find text that is both bold and larger than 12pt\nbold_headings = page.find_all('text:bold[size&gt;=12]')\n\nprint(f\"Found {len(bold_text)} bold elements.\")\nprint(f\"Found {len(italic_text)} italic elements.\")\nprint(f\"Found {len(bold_headings)} bold headings.\")\n</pre> # Find all bold text elements bold_text = page.find_all('text:bold')  # Find all italic text elements italic_text = page.find_all('text:italic')  # Find text that is both bold and larger than 12pt bold_headings = page.find_all('text:bold[size&gt;=12]')  print(f\"Found {len(bold_text)} bold elements.\") print(f\"Found {len(italic_text)} italic elements.\") print(f\"Found {len(bold_headings)} bold headings.\") <pre>Found 9 bold elements.\nFound 0 italic elements.\nFound 1 bold headings.\n</pre> In\u00a0[4]: Copied! <pre>page.analyze_text_styles()\npage.text_style_labels\n</pre> page.analyze_text_styles() page.text_style_labels Out[4]: <pre>['10.0pt Bold Helvetica',\n '10.0pt Helvetica',\n '12.0pt Bold Helvetica',\n '8.0pt Helvetica']</pre> <p>One they're assigned, you can filter based on <code>style_label</code> instead of going bit-by-bit.</p> In\u00a0[5]: Copied! <pre>page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]')\n</pre> page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]') Out[5]: <pre>&lt;ElementCollection[TextElement](count=8)&gt;</pre> In\u00a0[6]: Copied! <pre>page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700)\n</pre> page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700) Out[6]: <p>This allows you to quickly see patterns in font usage across the page layout.</p> In\u00a0[7]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Select the first page page = pdf.pages[0] page.to_image(width=700) Out[7]: <p>Look!</p> In\u00a0[8]: Copied! <pre>page.find_all('text')[0].fontname\n</pre> page.find_all('text')[0].fontname Out[8]: <pre>'AAAAAB+font000000002a8d158a'</pre> <p>The part before the <code>+</code> is the variant \u2013 bold, italic, etc \u2013 while the part after it is the \"real\" font name.</p>"},{"location":"text-analysis/#text-analysis","title":"Text Analysis\u00b6","text":"<p>Analyzing the properties of text elements, such as their font, size, style, and color, can be crucial for understanding document structure and extracting specific information. Natural PDF provides tools to access and analyze these properties.</p>"},{"location":"text-analysis/#introduction","title":"Introduction\u00b6","text":"<p>Beyond just the sequence of characters, the style of text carries significant meaning. Headings are often larger and bolder, important terms might be italicized, and different sections might use distinct fonts. This page covers how to access and utilize this stylistic information.</p>"},{"location":"text-analysis/#accessing-font-information","title":"Accessing Font Information\u00b6","text":"<p>Every <code>TextElement</code> (representing characters or words) holds information about its font properties.</p>"},{"location":"text-analysis/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>You can directly select text based on its style using pseudo-classes in selectors:</p>"},{"location":"text-analysis/#analyzing-fonts-on-a-page","title":"Analyzing Fonts on a Page\u00b6","text":"<p>You can use <code>analyze_text_styles</code> to assign labels to text based on font sizes, bold/italic and font names.</p>"},{"location":"text-analysis/#visualizing-text-properties","title":"Visualizing Text Properties\u00b6","text":"<p>Use highlighting to visually inspect text properties. Grouping by attributes like <code>fontname</code> or <code>size</code> can be very insightful. In the example below we go right to grouping by the <code>style_label</code>, which combines font name, size and variant.</p>"},{"location":"text-analysis/#weird-font-names","title":"Weird font names\u00b6","text":"<p>Oftentimes font names aren't what you're used to \u2013 Arial, Helvetica, etc \u2013 the PDF has given them weird, weird names. Relax, it's okay, they're normal fonts.</p>"},{"location":"text-extraction/","title":"Text Extraction Guide","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page for initial examples\npage = pdf.pages[0]\n\n# Display the first page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page for initial examples page = pdf.pages[0]  # Display the first page page.show(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Extract all text from the first page\n# Displaying first 500 characters\nprint(page.extract_text()[:500])\n</pre> # Extract all text from the first page # Displaying first 500 characters print(page.extract_text()[:500]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the lev\n</pre> <p>You can also preserve layout with <code>layout=True</code>.</p> In\u00a0[3]: Copied! <pre># Extract text from the entire document (may take time)\n# Uncomment to run:\nprint(page.extract_text(layout=True)[:2000])\n</pre> # Extract text from the entire document (may take time) # Uncomment to run: print(page.extract_text(layout=True)[:2000]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[4]: Copied! <pre># Find a single element, e.g., a title containing \"Summary\"\n# Adjust selector as needed\ndate_element = page.find('text:contains(\"Site\")')\ndate_element # Display the found element object\n</pre> # Find a single element, e.g., a title containing \"Summary\" # Adjust selector as needed date_element = page.find('text:contains(\"Site\")') date_element # Display the found element object Out[4]: <pre>&lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;</pre> In\u00a0[5]: Copied! <pre>date_element.show()\n</pre> date_element.show() Out[5]: In\u00a0[6]: Copied! <pre>date_element.text\n</pre> date_element.text Out[6]: <pre>'Site: '</pre> In\u00a0[7]: Copied! <pre># Find multiple elements, e.g., bold headings (size &gt;= 8)\nheading_elements = page.find_all('text[size&gt;=8]:bold')\nheading_elements \n</pre> # Find multiple elements, e.g., bold headings (size &gt;= 8) heading_elements = page.find_all('text[size&gt;=8]:bold') heading_elements  Out[7]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[8]: Copied! <pre>page.find_all('text[size&gt;=8]:bold').show()\n</pre> page.find_all('text[size&gt;=8]:bold').show() Out[8]: In\u00a0[9]: Copied! <pre># Pull out all of their text (why? I don't know!)\nprint(heading_elements.extract_text())\n</pre> # Pull out all of their text (why? I don't know!) print(heading_elements.extract_text()) <pre>Site: Date:  Violation Count: Summary: ViolationsStatuteDescriptionLevelRepeat?\n</pre> In\u00a0[10]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"Hazardous Materials\")').text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"Hazardous Materials\")').text Out[10]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[11]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text Out[11]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[12]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\nregex = \"\\d+, \\d{4}\"\npage.find(f'text:contains(\"{regex}\")', regex=True)\n</pre> # Regular expression (e.g., \"YYYY Report\") regex = \"\\d+, \\d{4}\" page.find(f'text:contains(\"{regex}\")', regex=True) Out[12]: <pre>&lt;TextElement text='February 3...' font='Helvetica' size=10.0 bbox=(80.56, 104.07000000000005, 156.71000000000004, 114.07000000000005)&gt;</pre> In\u00a0[13]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\npage.find_all('text[fontname=\"Helvetica\"][size=10]')\n</pre> # Regular expression (e.g., \"YYYY Report\") page.find_all('text[fontname=\"Helvetica\"][size=10]') Out[13]: <pre>&lt;ElementCollection[TextElement](count=32)&gt;</pre> In\u00a0[14]: Copied! <pre># Region below an element (e.g., below \"Introduction\")\n# Adjust selector as needed\npage.find('text:contains(\"Summary\")').below(include_source=True).show()\n</pre> # Region below an element (e.g., below \"Introduction\") # Adjust selector as needed page.find('text:contains(\"Summary\")').below(include_source=True).show() Out[14]: In\u00a0[15]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_source=True)\n    .extract_text()\n    [:500]\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_source=True)     .extract_text()     [:500] ) Out[15]: <pre>'Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\ninto the vats; and when they were fished out, there was never enough of them left to b'</pre> In\u00a0[16]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_source=True, until='line:horizontal')\n    .show()\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_source=True, until='line:horizontal')     .show() ) Out[16]: In\u00a0[17]: Copied! <pre># Manually defined region via coordinates (x0, top, x1, bottom)\nmanual_region = page.create_region(30, 60, 600, 300)\nmanual_region.show()\n</pre> # Manually defined region via coordinates (x0, top, x1, bottom) manual_region = page.create_region(30, 60, 600, 300) manual_region.show() Out[17]: In\u00a0[18]: Copied! <pre># Extract text from the manual region\nmanual_region.extract_text()[:500]\n</pre> # Extract text from the manual region manual_region.extract_text()[:500] Out[18]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\nint'</pre> In\u00a0[19]: Copied! <pre>header_content = page.find('rect')\nfooter_content = page.find_all('line')[-1].below()\n\nheader_content.highlight()\nfooter_content.highlight()\npage.to_image()\n</pre> header_content = page.find('rect') footer_content = page.find_all('line')[-1].below()  header_content.highlight() footer_content.highlight() page.to_image() Out[19]: In\u00a0[20]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[20]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the lev'</pre> In\u00a0[21]: Copied! <pre>page.add_exclusion(header_content)\npage.add_exclusion(footer_content)\n</pre> page.add_exclusion(header_content) page.add_exclusion(footer_content) Out[21]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[22]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[22]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\nint'</pre> In\u00a0[23]: Copied! <pre>full_text_no_exclusions = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text()\nf\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\"\n</pre> full_text_no_exclusions = page.extract_text(use_exclusions=False) clean_text = page.extract_text() f\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\" Out[23]: <pre>'Original length: 1149, Excluded length: 1149'</pre> In\u00a0[24]: Copied! <pre>page.clear_exclusions()\n</pre> page.clear_exclusions() Out[24]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>Exclusions can also be defined globally at the PDF level using <code>pdf.add_exclusion()</code> with a function.</p> In\u00a0[25]: Copied! <pre>print(page.extract_text())\n</pre> print(page.extract_text()) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[26]: Copied! <pre>print(page.extract_text(use_exclusions=False, layout=True))\n</pre> print(page.extract_text(use_exclusions=False, layout=True)) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[27]: Copied! <pre># Find the first text element on the page\nfirst_text = page.find_all('text')[1]\nfirst_text # Display basic info\n</pre> # Find the first text element on the page first_text = page.find_all('text')[1] first_text # Display basic info Out[27]: <pre>&lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;</pre> In\u00a0[28]: Copied! <pre># Highlight the first text element\nfirst_text.show()\n</pre> # Highlight the first text element first_text.show() Out[28]: In\u00a0[29]: Copied! <pre># Get detailed font properties dictionary\nfirst_text.font_info()\n</pre> # Get detailed font properties dictionary first_text.font_info() Out[29]: <pre>{'text': 'INS-UP70N51NCL41R',\n 'fontname': 'Helvetica',\n 'font_family': 'Helvetica',\n 'font_variant': '',\n 'size': 8.0,\n 'bold': False,\n 'italic': False,\n 'color': (1, 0, 0)}</pre> In\u00a0[30]: Copied! <pre># Check specific style properties directly\nf\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\"\n</pre> # Check specific style properties directly f\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\" Out[30]: <pre>'Is Bold: False, Is Italic: False, Font: Helvetica, Size: 8.0'</pre> In\u00a0[31]: Copied! <pre># Find elements by font attributes (adjust selectors)\n# Example: Find Arial fonts\narial_text = page.find_all('text[fontname*=Helvetica]')\narial_text # Display list of found elements\n</pre> # Find elements by font attributes (adjust selectors) # Example: Find Arial fonts arial_text = page.find_all('text[fontname*=Helvetica]') arial_text # Display list of found elements Out[31]: <pre>&lt;ElementCollection[TextElement](count=44)&gt;</pre> In\u00a0[32]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nlarge_text = page.find_all('text[size&gt;=12]')\nlarge_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) large_text = page.find_all('text[size&gt;=12]') large_text Out[32]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[33]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nbold_text = page.find_all('text:bold')\nbold_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) bold_text = page.find_all('text:bold') bold_text Out[33]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[34]: Copied! <pre># Analyze styles on the page\n# This returns a dictionary mapping style names to ElementList objects\npage.analyze_text_styles()\npage.text_style_labels\n</pre> # Analyze styles on the page # This returns a dictionary mapping style names to ElementList objects page.analyze_text_styles() page.text_style_labels Out[34]: <pre>['10.0pt Bold Helvetica (medium)',\n '10.0pt Helvetica (medium)',\n '12.0pt Bold Helvetica (large)',\n '8.0pt Helvetica (small)']</pre> In\u00a0[35]: Copied! <pre>page.find_all('text').show(group_by='style_label')\n</pre> page.find_all('text').show(group_by='style_label') Out[35]: In\u00a0[36]: Copied! <pre>page.find_all('text[style_label=\"8.0pt Helvetica (small)\"]')\n</pre> page.find_all('text[style_label=\"8.0pt Helvetica (small)\"]') Out[36]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> In\u00a0[37]: Copied! <pre>page.find_all('text[fontname=\"Helvetica\"][size=8]')\n</pre> page.find_all('text[fontname=\"Helvetica\"][size=8]') Out[37]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> <p>Font variants (e.g., <code>AAAAAB+FontName</code>) are also accessible via the <code>font-variant</code> attribute selector: <code>page.find_all('text[font-variant=\"AAAAAB\"]')</code>.</p> In\u00a0[38]: Copied! <pre># Get first 5 text elements in reading order\nelements_in_order = page.find_all('text')\nelements_in_order[:5]\n</pre> # Get first 5 text elements in reading order elements_in_order = page.find_all('text') elements_in_order[:5] Out[38]: <pre>[&lt;TextElement text='Jungle Hea...' font='Helvetica' size=8.0 bbox=(385.0, 35.65599999999995, 541.9680000000001, 43.65599999999995)&gt;,\n &lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;,\n &lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;,\n &lt;TextElement text='Durham\u2019s M...' font='Helvetica' size=10.0 bbox=(74.45, 84.07000000000005, 182.26000000000002, 94.07000000000005)&gt;,\n &lt;TextElement text='Chicago, I...' font='Helvetica' size=10.0 bbox=(182.26000000000002, 84.07000000000005, 234.50000000000003, 94.07000000000005)&gt;]</pre> In\u00a0[39]: Copied! <pre># Text extracted via page.extract_text() respects this order automatically\n# (Result already shown in Basic Text Extraction section)\npage.extract_text()[:100]\n</pre> # Text extracted via page.extract_text() respects this order automatically # (Result already shown in Basic Text Extraction section) page.extract_text()[:100] Out[39]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Il'</pre> In\u00a0[40]: Copied! <pre>page.clear_highlights()\n\nstart = page.find('text:contains(\"Date\")')\nstart.highlight(label='Date label')\nstart.next().highlight(label='Maybe the date', color='green')\nstart.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')\n\npage.to_image()\n</pre> page.clear_highlights()  start = page.find('text:contains(\"Date\")') start.highlight(label='Date label') start.next().highlight(label='Maybe the date', color='green') start.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')  page.to_image() Out[40]:"},{"location":"text-extraction/#text-extraction-guide","title":"Text Extraction Guide\u00b6","text":"<p>This guide demonstrates various ways to extract text from PDFs using Natural PDF, from simple page dumps to targeted extraction based on elements, regions, and styles.</p>"},{"location":"text-extraction/#setup","title":"Setup\u00b6","text":"<p>First, let's import necessary libraries and load a sample PDF. We'll use <code>example.pdf</code> from the tutorials' <code>pdfs</code> directory. Adjust the path if your setup differs.</p>"},{"location":"text-extraction/#basic-text-extraction","title":"Basic Text Extraction\u00b6","text":"<p>Get all text from a page or the entire document.</p>"},{"location":"text-extraction/#extracting-text-from-specific-elements","title":"Extracting Text from Specific Elements\u00b6","text":"<p>Use selectors with <code>find()</code> or <code>find_all()</code> to target specific elements. Selectors like <code>:contains(\"Summary\")</code> are examples; adapt them to your PDF.</p>"},{"location":"text-extraction/#advanced-text-searches","title":"Advanced text searches\u00b6","text":""},{"location":"text-extraction/#regions","title":"Regions\u00b6","text":""},{"location":"text-extraction/#filtering-out-headers-and-footers","title":"Filtering Out Headers and Footers\u00b6","text":"<p>Use Exclusion Zones to remove unwanted content before extraction. Adjust selectors for typical header/footer content.</p>"},{"location":"text-extraction/#controlling-whitespace","title":"Controlling Whitespace\u00b6","text":"<p>Manage how spaces and blank lines are handled during extraction using <code>layout</code>.</p>"},{"location":"text-extraction/#font-information-access","title":"Font Information Access\u00b6","text":"<p>Inspect font details of text elements.</p>"},{"location":"text-extraction/#working-with-font-styles","title":"Working with Font Styles\u00b6","text":"<p>Analyze and group text elements by their computed font style, which combines attributes like font name, size, boldness, etc., into logical groups.</p>"},{"location":"text-extraction/#reading-order","title":"Reading Order\u00b6","text":"<p>Text extraction respects a pathetic attempt at natural reading order (top-to-bottom, left-to-right by default). <code>page.find_all('text')</code> returns elements already sorted this way.</p>"},{"location":"text-extraction/#element-navigation","title":"Element Navigation\u00b6","text":"<p>Move between elements sequentially based on reading order using <code>.next()</code> and <code>.previous()</code>.</p>"},{"location":"text-extraction/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to extract text, you might want to explore:</p> <ul> <li>Working with regions for more precise extraction</li> <li>OCR capabilities for scanned documents</li> <li>Document layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"},{"location":"tutorials/01-loading-and-extraction/","title":"Loading and Basic Text Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" <p>In this tutorial, we'll learn how to:</p> <ol> <li>Load a PDF document</li> <li>Extract text from pages</li> <li>Extract specific elements</li> </ol> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\nimport os\n\n# Load a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Basic info about the document\n{\n    \"Filename\": os.path.basename(pdf.path),\n    \"Pages\": len(pdf.pages),\n    \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),\n    \"Author\": pdf.metadata.get(\"Author\", \"N/A\")\n}\n</pre> from natural_pdf import PDF import os  # Load a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Basic info about the document {     \"Filename\": os.path.basename(pdf.path),     \"Pages\": len(pdf.pages),     \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),     \"Author\": pdf.metadata.get(\"Author\", \"N/A\") } Out[2]: <pre>{'Filename': '01-practice.pdf', 'Pages': 1, 'Title': 'N/A', 'Author': 'N/A'}</pre> In\u00a0[3]: Copied! <pre># Get the first page\npage = pdf.pages[0]\n\n# Extract text from the page\ntext = page.extract_text()\n\n# Show the first 200 characters of the text\nprint(text[:200])\n</pre> # Get the first page page = pdf.pages[0]  # Extract text from the page text = page.extract_text()  # Show the first 200 characters of the text print(text[:200]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men\n</pre> In\u00a0[4]: Copied! <pre># Find text elements containing specific words\nelements = page.find_all('text:contains(\"Inadequate\")')\n\n# Show these elements on the page\nelements.show()\n</pre> # Find text elements containing specific words elements = page.find_all('text:contains(\"Inadequate\")')  # Show these elements on the page elements.show() Out[4]: In\u00a0[5]: Copied! <pre># Analyze the page layout\npage.analyze_layout(engine='yolo')\n\n# Find and highlight all detected regions\npage.find_all('region').show(group_by='type')\n</pre> # Analyze the page layout page.analyze_layout(engine='yolo')  # Find and highlight all detected regions page.find_all('region').show(group_by='type') <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpzwc4lxwq/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1201.0ms\n</pre> <pre>Speed: 9.9ms preprocess, 1201.0ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[5]: In\u00a0[6]: Copied! <pre># Process all pages\nfor page in pdf.pages:\n    page_text = page.extract_text()\n    print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page\n</pre> # Process all pages for page in pdf.pages:     page_text = page.extract_text()     print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page <pre>Page 1 Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Il\n</pre> <p>This tutorial covered the basics of loading PDFs and extracting text. In the next tutorials, we'll explore more advanced features like searching for specific elements, extracting structured content, and working with tables.</p>"},{"location":"tutorials/01-loading-and-extraction/#loading-and-basic-text-extraction","title":"Loading and Basic Text Extraction\u00b6","text":""},{"location":"tutorials/01-loading-and-extraction/#loading-a-pdf","title":"Loading a PDF\u00b6","text":"<p>Let's start by loading a PDF file:</p>"},{"location":"tutorials/01-loading-and-extraction/#extracting-text","title":"Extracting Text\u00b6","text":"<p>Now that we have loaded the PDF, let's extract the text from the first page:</p>"},{"location":"tutorials/01-loading-and-extraction/#finding-and-extracting-specific-elements","title":"Finding and Extracting Specific Elements\u00b6","text":"<p>We can find specific elements using spatial queries and text content:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>We can analyze the layout of the page to identify different regions:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>You can also work with multiple pages:</p>"},{"location":"tutorials/02-finding-elements/","title":"Finding Specific Elements","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page (index 0)\npage = pdf.pages[0]\n\n# Find the text element containing \"Site:\"\n# The ':contains()' pseudo-class looks for text content.\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Find the text element containing \"Date:\"\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Visualize the found elements\nsite_label.highlight(color=\"red\", label=\"Site\")\ndate_label.highlight(color=\"blue\", label=\"Date\")\n\n# Access the text content directly\n{\n    \"Site Label\": site_label.text,\n    \"Date Label\": date_label.text\n}\n\n# Display page with both highlights\npage.to_image()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page (index 0) page = pdf.pages[0]  # Find the text element containing \"Site:\" # The ':contains()' pseudo-class looks for text content. site_label = page.find('text:contains(\"Site:\")')  # Find the text element containing \"Date:\" date_label = page.find('text:contains(\"Date:\")')  # Visualize the found elements site_label.highlight(color=\"red\", label=\"Site\") date_label.highlight(color=\"blue\", label=\"Date\")  # Access the text content directly {     \"Site Label\": site_label.text,     \"Date Label\": date_label.text }  # Display page with both highlights page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Find text elements that are red\nred_text = page.find('text[color~=red]')\nprint(f\"Found red text: {red_text.text}\")\nred_text.show()\n\n# Find elements with specific RGB colors\nblue_text = page.find('text[color=rgb(0,0,255)]')\n</pre> # Find text elements that are red red_text = page.find('text[color~=red]') print(f\"Found red text: {red_text.text}\") red_text.show()  # Find elements with specific RGB colors blue_text = page.find('text[color=rgb(0,0,255)]') <pre>Found red text: INS-UP70N51NCL41R\n</pre> In\u00a0[4]: Copied! <pre># Find horizontal lines\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Find thick lines (width &gt;= 2)\nthick_lines = page.find_all('line[width&gt;=2]')\n\n# Find rectangles\nrectangles = page.find_all('rect')\n\n# Visualize what we found\nhorizontal_lines.highlight(color=\"blue\", label=\"Horizontal\")\nthick_lines.highlight(color=\"red\", label=\"Thick\")\nrectangles.highlight(color=\"green\", label=\"Rectangles\")\n\n# Display page with all shapes highlighted\npage.to_image()\n</pre> # Find horizontal lines horizontal_lines = page.find_all('line:horizontal')  # Find thick lines (width &gt;= 2) thick_lines = page.find_all('line[width&gt;=2]')  # Find rectangles rectangles = page.find_all('rect')  # Visualize what we found horizontal_lines.highlight(color=\"blue\", label=\"Horizontal\") thick_lines.highlight(color=\"red\", label=\"Thick\") rectangles.highlight(color=\"green\", label=\"Rectangles\")  # Display page with all shapes highlighted page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Find text with specific font properties\nbold_text = page.find_all('text:bold')\nlarge_text = page.find_all('text[size&gt;=12]')\n\n# Find text with specific font names\nhelvetica_text = page.find_all('text[fontname=Helvetica]')\n</pre> # Find text with specific font properties bold_text = page.find_all('text:bold') large_text = page.find_all('text[size&gt;=12]')  # Find text with specific font names helvetica_text = page.find_all('text[fontname=Helvetica]') In\u00a0[6]: Copied! <pre># Find text above a specific element\nabove_text = page.find('line[width=2]').above().extract_text()\n\n# Find text below a specific element\nbelow_text = page.find('text:contains(\"Summary\")').below().extract_text()\n\n# Find text to the right of a specific element\nnearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text()\n</pre> # Find text above a specific element above_text = page.find('line[width=2]').above().extract_text()  # Find text below a specific element below_text = page.find('text:contains(\"Summary\")').below().extract_text()  # Find text to the right of a specific element nearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text() In\u00a0[7]: Copied! <pre># Find large, bold text that contains specific words\nimportant_text = page.find_all('text[size&gt;=12]:bold:contains(\"Critical\")')\n\n# Find red text inside a rectangle\nhighlighted_text = page.find('rect').find_all('text[color~=red]')\n</pre> # Find large, bold text that contains specific words important_text = page.find_all('text[size&gt;=12]:bold:contains(\"Critical\")')  # Find red text inside a rectangle highlighted_text = page.find('rect').find_all('text[color~=red]') <p>Handling Missing Elements</p> <pre><code>In these examples, we know certain elements exist in the PDF. In real-world scenarios, `page.find()` might not find a match and would return `None`. Production code should check for this:\n\n```py\nsite_label = page.find('text:contains(\"Site:\")')\nif site_label:\n    # Found it! Proceed...\n    print(site_label.extract_text())\nelse:\n    # Didn't find it, handle appropriately...\n    \"Warning: 'Site:' label not found.\"\n```</code></pre> <p>Visual Debugging</p> <pre><code>When working with complex selectors, it's helpful to visualize what you're finding:\n\n```py\nelements = page.find_all('text[color~=red]')\nelements.show()\n```</code></pre>"},{"location":"tutorials/02-finding-elements/#finding-specific-elements","title":"Finding Specific Elements\u00b6","text":"<p>Extracting all the text is useful, but often you need specific pieces of information. <code>natural-pdf</code> lets you find elements using selectors, similar to CSS.</p> <p>Let's find the \"Site\" and \"Date\" information from our <code>01-practice.pdf</code>:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-color","title":"Finding Elements by Color\u00b6","text":"<p>You can find elements based on their color:</p>"},{"location":"tutorials/02-finding-elements/#finding-lines-and-shapes","title":"Finding Lines and Shapes\u00b6","text":"<p>Find lines and rectangles based on their properties:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-font-properties","title":"Finding Elements by Font Properties\u00b6","text":""},{"location":"tutorials/02-finding-elements/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>You can find elements based on their position relative to other elements:</p>"},{"location":"tutorials/02-finding-elements/#combining-selectors","title":"Combining Selectors\u00b6","text":"<p>You can combine multiple conditions to find exactly what you need:</p>"},{"location":"tutorials/03-extracting-blocks/","title":"Extracting Text Blocks","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the starting element (\"Summary:\")\nstart_marker = page.find('text:contains(\"Summary:\")')\n\n# Select elements below the start_marker, stopping *before*\n# the thick horizontal line (a line with height &gt; 1).\nsummary_elements = start_marker.below(\n    include_source=True, # Include the \"Summary:\" text itself\n    until=\"line[height &gt; 1]\"\n)\n\n# Visualize the elements found in this block\nsummary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")\n\n# Extract and display the text from the collection of summary elements\nsummary_elements.extract_text()\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the starting element (\"Summary:\") start_marker = page.find('text:contains(\"Summary:\")')  # Select elements below the start_marker, stopping *before* # the thick horizontal line (a line with height &gt; 1). summary_elements = start_marker.below(     include_source=True, # Include the \"Summary:\" text itself     until=\"line[height &gt; 1]\" )  # Visualize the elements found in this block summary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")  # Extract and display the text from the collection of summary elements summary_elements.extract_text()  Out[2]: <pre>'Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\ninto the vats; and when they were fished out, there was never enough of them left to be worth\\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\\nto the world as Durham\u2019s Pure Leaf Lard!\\nViolations\\nStatute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[3]: Copied! <pre># Display the page image to see the visualization\npage.to_image()\n</pre> # Display the page image to see the visualization page.to_image() Out[3]: <p>This selects the elements using <code>.below(until=...)</code> and extracts their text. The second code block displays the page image with the visualized section.</p> <p>Selector Specificity</p> <pre><code>We used `line[height &gt; 1]` to find the thick horizontal line. You might need to adjust selectors based on the specific PDF structure. Inspecting element properties can help you find reliable start and end markers.</code></pre>"},{"location":"tutorials/03-extracting-blocks/#extracting-text-blocks","title":"Extracting Text Blocks\u00b6","text":"<p>Often, you need a specific section, like a paragraph between two headings. You can find a starting element and select everything below it until an ending element.</p> <p>Let's extract the \"Summary\" section from <code>01-practice.pdf</code>. It starts after \"Summary:\" and ends before the thick horizontal line.</p>"},{"location":"tutorials/04-table-extraction/","title":"Basic Table Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf  # core install already includes pdfplumber\n</pre> #%pip install natural-pdf  # core install already includes pdfplumber In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# For a single table, extract_table returns list-of-lists\ntable = page.extract_table(method=\"pdfplumber\")\ntable  # List-of-lists of cell text\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # For a single table, extract_table returns list-of-lists table = page.extract_table(method=\"pdfplumber\") table  # List-of-lists of cell text Out[2]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p><code>extract_table()</code> defaults to the plumber backend, so the explicit <code>method</code> is optional\u2014but it clarifies what's happening.</p> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Detect layout with Table Transformer\npage.analyze_layout(engine=\"tatr\")\n\n# Grab the first detected table region\ntable_region = page.find('region[type=table]')\n\ntable_region.show(label=\"TATR Table\", color=\"purple\")\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Detect layout with Table Transformer page.analyze_layout(engine=\"tatr\")  # Grab the first detected table region table_region = page.find('region[type=table]')  table_region.show(label=\"TATR Table\", color=\"purple\") Out[3]: In\u00a0[4]: Copied! <pre>tatr_rows = table_region.extract_table()  # Uses TATR backend implicitly\n</pre> tatr_rows = table_region.extract_table()  # Uses TATR backend implicitly In\u00a0[5]: Copied! <pre>page.clear_detected_layout_regions()\npage.analyze_layout(engine=\"paddle\", confidence=0.3)\n\npaddle_table = page.find('region[type=table]')\nif paddle_table:\n    paddle_table.show(color=\"green\", label=\"Paddle Table\")\n    paddle_rows = paddle_table.extract_table(method=\"pdfplumber\")  # fall back to ruling-line extraction inside the region\n</pre> page.clear_detected_layout_regions() page.analyze_layout(engine=\"paddle\", confidence=0.3)  paddle_table = page.find('region[type=table]') if paddle_table:     paddle_table.show(color=\"green\", label=\"Paddle Table\")     paddle_rows = paddle_table.extract_table(method=\"pdfplumber\")  # fall back to ruling-line extraction inside the region <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre>"},{"location":"tutorials/04-table-extraction/#basic-table-extraction","title":"Basic Table Extraction\u00b6","text":"<p>PDFs often contain tables, and <code>natural-pdf</code> provides methods to extract their data. The key is to first triangulate where your table is on the page, then use powerful extraction tools on that specific region.</p> <p>Let's extract the \"Violations\" table from our practice PDF.</p>"},{"location":"tutorials/04-table-extraction/#method-1-pdfplumber-default","title":"Method 1 \u2013 pdfplumber (default)\u00b6","text":""},{"location":"tutorials/04-table-extraction/#method-2-tatr-based-extraction","title":"Method 2 \u2013 TATR-based extraction\u00b6","text":"<p>When you do a TATR layout analysis, it detects tables, rows and cells with a LayoutLM model. Once a region has <code>source=\"detected\"</code> and <code>type=\"table\"</code>, calling <code>extract_table()</code> on that region uses the tatr backend automatically.</p>"},{"location":"tutorials/04-table-extraction/#method-3-paddleocr-layout","title":"Method 3 \u2013 PaddleOCR Layout\u00b6","text":"<p>You can also try PaddleOCR's layout detector to locate tables:</p>"},{"location":"tutorials/04-table-extraction/#choosing-the-right-backend","title":"Choosing the right backend\u00b6","text":"<ul> <li>plumber \u2013 fastest; needs rule lines or tidy whitespace.</li> <li>tatr \u2013 robust to missing lines; slower; requires AI extra.</li> <li>text \u2013 whitespace clustering; fallback when lines + models fail.</li> </ul> <p>You can call <code>page.extract_table(method=\"text\")</code> or on a <code>Region</code> as well.</p> <p>The general workflow is: try different layout analyzers to locate your table, then extract from the specific region. Keep trying options until one works for your particular PDF!</p> <p>For complex grids where even models struggle, see Tutorial 11 (enhanced table processing) for a lines-first workflow.</p>"},{"location":"tutorials/04-table-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Compare accuracy/time of the three methods on the sample PDF.</li> <li>Show how to call <code>page.extract_table(method=\"text\")</code> as a no-dependency fallback.</li> <li>Add snippet exporting <code>rows</code> to pandas DataFrame.</li> <li>Demonstrate cell post-processing (strip %, cast numbers).</li> </ul>"},{"location":"tutorials/05-excluding-content/","title":"Excluding Content (Headers/Footers)","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\n\n# Load the PDF\npdf = PDF(pdf_url)\npage = pdf.pages[0]\n\n# Let's see the bottom part of the text WITHOUT exclusions\n# It likely contains page numbers or other footer info.\nfull_text_unfiltered = page.extract_text()\n\n# Show the last 200 characters (likely containing footer text)\nfull_text_unfiltered[-200:]\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"  # Load the PDF pdf = PDF(pdf_url) page = pdf.pages[0]  # Let's see the bottom part of the text WITHOUT exclusions # It likely contains page numbers or other footer info. full_text_unfiltered = page.extract_text()  # Show the last 200 characters (likely containing footer text) full_text_unfiltered[-200:] Out[2]: <pre>' C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0\\nWrite-In Totals 0 0 0 0\\nPrecinct Summary - 11/06/2024 12:22 AM Page 1 of 387\\nReport generated with Electionware Copyright \u00a9 2007-2020'</pre> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\n\n# Define the exclusion region on every page using a lambda function\nfooter_height = 200\npdf.add_exclusion(\n    lambda page: page.region(top=page.height - footer_height),\n    label=\"Bottom 200pt Footer\"\n)\n\n# Now extract text from the first page again, exclusions are active by default\npage = pdf.pages[0]\n\n# Visualize the excluded area\nfooter_region_viz = page.region(top=page.height - footer_height)\nfooter_region_viz.show(label=\"Excluded Footer Area\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url)  # Define the exclusion region on every page using a lambda function footer_height = 200 pdf.add_exclusion(     lambda page: page.region(top=page.height - footer_height),     label=\"Bottom 200pt Footer\" )  # Now extract text from the first page again, exclusions are active by default page = pdf.pages[0]  # Visualize the excluded area footer_region_viz = page.region(top=page.height - footer_height) footer_region_viz.show(label=\"Excluded Footer Area\") page.to_image() Out[3]: In\u00a0[4]: Copied! <pre>filtered_text = page.extract_text() # use_exclusions=True is default\n\n# Show the last 200 chars with footer area excluded\nfiltered_text[-200:]\n</pre> filtered_text = page.extract_text() # use_exclusions=True is default  # Show the last 200 chars with footer area excluded filtered_text[-200:] Out[4]: <pre>'TOR\\nVote For 1\\nElection Provisional\\nTOTAL Mail Votes\\nDay Votes\\nDEM ROBERT P CASEY JR 99 70 29 0\\nREP DAVE MCCORMICK 79 69 10 0\\nLIB JOHN C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0'</pre> <p>This method is simple but might cut off content if the footer height varies or content extends lower on some pages.</p> In\u00a0[5]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\npage = pdf.pages[0] # Get page for finding elements\n\n# Find the last horizontal line on the first page\n# We'll use this logic to define our exclusion for all pages\nlast_line = page.find_all('line')[-1]\n\n# Define the exclusion function using a lambda\n# This finds the last line on *each* page and excludes below it\npdf.add_exclusion(\n    lambda p: p.find_all('line')[-1].below(),\n    label=\"Element-Based Footer\"\n)\n\n# Extract text again, with the element-based exclusion active\nfiltered_text_element = page.extract_text()\n\n# Show the last 200 chars with element-based footer exclusion\n\"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]\n\n# Visualize the element-based exclusion area\npage.clear_highlights()\n# Need to find the region again for visualization\nfooter_boundary = page.find_all('line')[-1]\nfooter_region_element = footer_boundary.below()\nfooter_region_element.show(label=\"Excluded Footer Area (Element)\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url) page = pdf.pages[0] # Get page for finding elements  # Find the last horizontal line on the first page # We'll use this logic to define our exclusion for all pages last_line = page.find_all('line')[-1]  # Define the exclusion function using a lambda # This finds the last line on *each* page and excludes below it pdf.add_exclusion(     lambda p: p.find_all('line')[-1].below(),     label=\"Element-Based Footer\" )  # Extract text again, with the element-based exclusion active filtered_text_element = page.extract_text()  # Show the last 200 chars with element-based footer exclusion \"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]  # Visualize the element-based exclusion area page.clear_highlights() # Need to find the region again for visualization footer_boundary = page.find_all('line')[-1] footer_region_element = footer_boundary.below() footer_region_element.show(label=\"Excluded Footer Area (Element)\") page.to_image() Out[5]: <p>This element-based approach is usually more reliable as it adapts to the content's position, but it depends on finding consistent boundary elements (like lines or specific text markers).</p>"},{"location":"tutorials/05-excluding-content/#excluding-content-headersfooters","title":"Excluding Content (Headers/Footers)\u00b6","text":"<p>Often, PDFs have repeating headers or footers on every page that you want to ignore when extracting the main content. <code>natural-pdf</code> allows you to define exclusion regions.</p> <p>We'll use a different PDF for this example, which has a distinct header and footer section: <code>0500000US42007.pdf</code>.</p>"},{"location":"tutorials/05-excluding-content/#approach-1-excluding-a-fixed-area","title":"Approach 1: Excluding a Fixed Area\u00b6","text":"<p>A simple way to exclude headers or footers is to define a fixed region based on page coordinates. Let's exclude the bottom 200 pixels of the page.</p>"},{"location":"tutorials/05-excluding-content/#approach-2-excluding-based-on-elements","title":"Approach 2: Excluding Based on Elements\u00b6","text":"<p>A more robust way is to find specific elements that reliably mark the start of the footer (or end of the header) and exclude everything below (or above) them. In <code>Examples.md</code>, the footer was defined as everything below the last horizontal line.</p>"},{"location":"tutorials/05-excluding-content/#todo","title":"TODO\u00b6","text":"<ul> <li>Show a text-based exclusion: <code>pdf.add_exclusion(lambda p: p.find('text:contains(\"Page \")').below())</code> for dynamic page numbers.</li> <li>Demonstrate stacking multiple exclusions (e.g., header + footer) and the order they are applied.</li> <li>Provide an example disabling exclusions temporarily with <code>extract_text(use_exclusions=False)</code>.</li> <li>Include a multi-page preview that outlines exclusions on every page.</li> </ul> <p>Applying Exclusions</p> <pre><code>*   `pdf.add_exclusion(func)` applies the exclusion function (which takes a page and returns a region) to *all* pages in the PDF.\n*   `page.add_exclusion(region)` adds an exclusion region only to that specific page.\n*   `extract_text(use_exclusions=False)` can be used to temporarily disable exclusions.</code></pre>"},{"location":"tutorials/06-document-qa/","title":"Document Question Answering (QA)","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"  # DocumentQA relies on torch + transformers\n</pre> #%pip install \"natural-pdf[ai]\"  # DocumentQA relies on torch + transformers In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Ask about the date\nquestion_1 = \"What is the inspection date?\"\nanswer_1 = page.ask(question_1)\n\n# The result dictionary always contains:\n#   answer      \u2013 extracted span (string, may be empty)\n#   confidence  \u2013 model score 0\u20131\n#   start / end \u2013 indices into page.words\n#   found       \u2013 False if confidence &lt; min_confidence\nanswer_1\n# \u279c {'answer': 'July 31, 2023', 'confidence': 0.82, 'start': 33, 'end': 36, 'found': True}\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Ask about the date question_1 = \"What is the inspection date?\" answer_1 = page.ask(question_1)  # The result dictionary always contains: #   answer      \u2013 extracted span (string, may be empty) #   confidence  \u2013 model score 0\u20131 #   start / end \u2013 indices into page.words #   found       \u2013 False if confidence &lt; min_confidence answer_1 # \u279c {'answer': 'July 31, 2023', 'confidence': 0.82, 'start': 33, 'end': 36, 'found': True} <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n</pre> Out[2]: <pre>{'answer': 'February 3, 1905',\n 'confidence': 0.9979940056800842,\n 'start': 6,\n 'end': 6,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre># Ask about the company name\nquestion_2 = \"What company was inspected?\"\nanswer_2 = page.ask(question_2)\n\n# Display the answer dictionary\nanswer_2\n</pre> # Ask about the company name question_2 = \"What company was inspected?\" answer_2 = page.ask(question_2)  # Display the answer dictionary answer_2 Out[3]: <pre>{'answer': 'Jungle Health and Safety Inspection Service',\n 'confidence': 0.9988948106765747,\n 'start': 0,\n 'end': 0,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre># Ask about specific content from the table\nquestion_3 = \"What is statute 5.8.3 about?\"\nanswer_3 = page.ask(question_3)\n\n# Display the answer\nanswer_3\n</pre> # Ask about specific content from the table question_3 = \"What is statute 5.8.3 about?\" answer_3 = page.ask(question_3)  # Display the answer answer_3 Out[4]: <pre>{'answer': 'Inadequate Protective Equipment.',\n 'confidence': 0.9997999668121338,\n 'start': 26,\n 'end': 26,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> <p>The results include the extracted <code>answer</code>, a <code>confidence</code> score (useful for filtering uncertain answers), the <code>page_num</code>, and the <code>source_elements</code>.</p> In\u00a0[5]: Copied! <pre>from natural_pdf.elements.collections import ElementCollection\n\npage.clear_highlights()\n\nif answer_1[\"found\"]:\n    words = ElementCollection(page.words[answer_1[\"start\"] : answer_1[\"end\"] + 1])\n    words.show(color=\"yellow\", label=question_1)\n\npage.to_image()\n</pre> from natural_pdf.elements.collections import ElementCollection  page.clear_highlights()  if answer_1[\"found\"]:     words = ElementCollection(page.words[answer_1[\"start\"] : answer_1[\"end\"] + 1])     words.show(color=\"yellow\", label=question_1)  page.to_image() Out[5]: In\u00a0[6]: Copied! <pre>from natural_pdf import PDF\nimport pandas as pd\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# List of questions to ask\nquestions = [\n    \"What is the inspection date?\",\n    \"What company was inspected?\",\n    \"What is statute 5.8.3 about?\",\n    \"How many violations were there in total?\" # This might be less reliable\n]\n\n# Collect answers for each question\nresults = []\nfor q in questions:\n    ans = page.ask(q, min_confidence=0.2)\n    ans[\"question\"] = q\n    results.append(ans)\n\ncols = [\"question\", \"answer\", \"confidence\", \"found\"]\nqa_df = pd.DataFrame(results)[cols]\nqa_df\n</pre> from natural_pdf import PDF import pandas as pd  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # List of questions to ask questions = [     \"What is the inspection date?\",     \"What company was inspected?\",     \"What is statute 5.8.3 about?\",     \"How many violations were there in total?\" # This might be less reliable ]  # Collect answers for each question results = [] for q in questions:     ans = page.ask(q, min_confidence=0.2)     ans[\"question\"] = q     results.append(ans)  cols = [\"question\", \"answer\", \"confidence\", \"found\"] qa_df = pd.DataFrame(results)[cols] qa_df Out[6]: question answer confidence found 0 What is the inspection date? February 3, 1905 0.997994 True 1 What company was inspected? Jungle Health and Safety Inspection Service 0.998895 True 2 What is statute 5.8.3 about? Inadequate Protective Equipment. 0.999800 True 3 How many violations were there in total? 4.12.7 0.662557 True <p>This shows how you can iterate through questions, collect the answer dictionaries, and then create a structured DataFrame, making it easy to review questions, answers, and their confidence levels together.</p>"},{"location":"tutorials/06-document-qa/#document-question-answering-qa","title":"Document Question Answering (QA)\u00b6","text":"<p>Sometimes, instead of searching for specific text patterns, you just want to ask the document a question directly. <code>natural-pdf</code> includes an extractive Question Answering feature.</p> <p>\"Extractive\" means it finds the literal answer text within the document, rather than generating a new answer or summarizing.</p> <p>Let's ask our <code>01-practice.pdf</code> a few questions.</p>"},{"location":"tutorials/06-document-qa/#visualising-where-the-answer-came-from","title":"Visualising Where the Answer Came From\u00b6","text":""},{"location":"tutorials/06-document-qa/#collecting-results-into-a-dataframe","title":"Collecting Results into a DataFrame\u00b6","text":"<p>If you're asking multiple questions, it's often useful to collect the results into a pandas DataFrame for easier analysis.</p>"},{"location":"tutorials/06-document-qa/#todo","title":"TODO\u00b6","text":"<ul> <li>Demonstrate passing <code>model=\"impira/layoutlm-document-qa\"</code> to switch models.</li> <li>Show multi-page QA: iterate over <code>pdf.pages</code> and add <code>page</code> column to the results.</li> <li>Add batch helper (<code>pdf.ask_many(questions)</code>) once implemented.</li> </ul>"},{"location":"tutorials/06-document-qa/#wish-list","title":"Wish List\u00b6","text":"<ul> <li>Support for highlighting answer automatically via a <code>show_answer()</code> helper.</li> <li>Option to return bounding box coordinates directly (<code>bbox</code>) in the answer dict.</li> <li>Add <code>ElementCollection.to_dataframe()</code> for one-call DataFrame creation.</li> </ul> <p>QA Model and Limitations</p> <pre><code>*   The QA system relies on underlying transformer models. Performance and confidence scores vary.\n*   It works best for questions where the answer is explicitly stated. It cannot synthesize information or perform calculations (e.g., counting items might fail or return text containing a number rather than the count itself).\n*   You can potentially specify different QA models via the `model=` argument in `page.ask()` if others are configured.</code></pre>"},{"location":"tutorials/07-layout-analysis/","title":"Layout Analysis","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Analyze the layout using the default model\n# This adds 'detected' Region objects to the page\n# It returns an ElementCollection of the detected regions\npage.analyze_layout()\ndetected_regions = page.find_all('region[source=\"detected\"]')\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Analyze the layout using the default model # This adds 'detected' Region objects to the page # It returns an ElementCollection of the detected regions page.analyze_layout() detected_regions = page.find_all('region[source=\"detected\"]') <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmppotkvntf/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1227.3ms\n</pre> <pre>Speed: 5.5ms preprocess, 1227.3ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[3]: Copied! <pre># Visualize all detected regions, using default colors based on type\ndetected_regions.show(group_by='type', include_attrs=['confidence'])\n</pre> # Visualize all detected regions, using default colors based on type detected_regions.show(group_by='type', include_attrs=['confidence']) Out[3]: In\u00a0[4]: Copied! <pre># Find and visualize only the detected table region(s)\ntables = page.find_all('region[type=table]')\ntables.show(color='lightgreen', label='Detected Table')\n</pre> # Find and visualize only the detected table region(s) tables = page.find_all('region[type=table]') tables.show(color='lightgreen', label='Detected Table') Out[4]: In\u00a0[5]: Copied! <pre># Extract text specifically from the detected table region\ntable_region = tables.first # Assuming only one table was detected\n# Extract text preserving layout\ntable_text_layout = table_region.extract_text(layout=True)\ntable_text_layout\n</pre> # Extract text specifically from the detected table region table_region = tables.first # Assuming only one table was detected # Extract text preserving layout table_text_layout = table_region.extract_text(layout=True) table_text_layout Out[5]: <pre>'Statute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[6]: Copied! <pre># Layout-detected regions can also be used for table extraction\n# This can be more robust than the basic page.extract_tables()\n# especially for tables without clear lines.\ntable_data = table_region.extract_table()\ntable_data\n</pre> # Layout-detected regions can also be used for table extraction # This can be more robust than the basic page.extract_tables() # especially for tables without clear lines. table_data = table_region.extract_table() table_data Out[6]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[7]: Copied! <pre># Re-run layout with PaddleOCR detector\npage.clear_detected_layout_regions()\n\npaddle_regions = page.analyze_layout(engine=\"paddle\", confidence=0.3)\n#paddle_regions.show(group_by=\"type\")\n\n# Only keep detections the model tagged as \"table\" or \"figure\"\ntables_and_figs = paddle_regions.filter(lambda r: r.normalized_type in {\"table\", \"figure\"})\n#tables_and_figs.show(label_format=\"{normalized_type} ({confidence:.2f})\")\n</pre> # Re-run layout with PaddleOCR detector page.clear_detected_layout_regions()  paddle_regions = page.analyze_layout(engine=\"paddle\", confidence=0.3) #paddle_regions.show(group_by=\"type\")  # Only keep detections the model tagged as \"table\" or \"figure\" tables_and_figs = paddle_regions.filter(lambda r: r.normalized_type in {\"table\", \"figure\"}) #tables_and_figs.show(label_format=\"{normalized_type} ({confidence:.2f})\") <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <p>The helper accepts these common kwargs (see <code>LayoutOptions</code> subclasses for full list):</p> <ul> <li><code>confidence</code> \u2013 minimum score for retaining a prediction.</li> <li><code>classes</code> / <code>exclude_classes</code> \u2013 whitelist or blacklist region types.</li> <li><code>device</code> \u2013 \"cuda\" or \"cpu\"; defaults to GPU if available.</li> </ul> <p>Each engine also exposes its own options class (e.g., <code>YOLOLayoutOptions</code>) for fine control over NMS thresholds, model sizes, etc. Pass an instance via the <code>options=</code> param.</p> <p>Layout analysis provides structured <code>Region</code> objects. You can filter these regions by their predicted <code>type</code> and then perform actions like visualization or extracting text/tables specifically from those regions.</p>"},{"location":"tutorials/07-layout-analysis/#layout-analysis","title":"Layout Analysis\u00b6","text":"<p>Beyond simple text and lines, <code>natural-pdf</code> can use layout analysis models (like YOLO or DETR) to identify semantic regions within a page, such as paragraphs, tables, figures, headers, etc. This provides a higher-level understanding of the document structure.</p>"},{"location":"tutorials/07-layout-analysis/#available-layout-engines","title":"Available Layout Engines\u00b6","text":"<ul> <li>yolo \u2013 YOLOv5 model trained on DocLayNet; fast and good at classic page objects (paragraph, table, figure, heading).  Install via <code>npdf install yolo</code>.</li> <li>tatr \u2013 Microsoft's Table Transformer (LayoutLM) specialised in tables; already included in the ai extra.</li> <li>paddle \u2013 PaddleOCR`s layout detector; lightweight and CPU-friendly.</li> <li>surya \u2013 Surya Layout Parser (DETR backbone) tuned for invoices and forms.</li> <li>docling \u2013 YOLOX model published by DocLING researchers; performs well on historical documents.</li> <li>gemini \u2013 Calls Google's Vision Gemini API (experimental, requires <code>OPENAI_API_KEY</code>).</li> </ul> <p><code>page.analyze_layout()</code> defaults to the first available engine (search order <code>yolo \u2192 paddle \u2192 tatr</code>), but you can pick one explicitly with <code>engine=\"...\"</code>.</p> <p>Let's analyze the layout of our <code>01-practice.pdf</code>.</p>"},{"location":"tutorials/07-layout-analysis/#switching-engines-and-tuning-thresholds","title":"Switching Engines and Tuning Thresholds\u00b6","text":""},{"location":"tutorials/07-layout-analysis/#todo","title":"TODO\u00b6","text":"<ul> <li>Add a speed/accuracy comparison snippet looping over all installed engines.</li> <li>Demonstrate multi-page batch: <code>pdf.pages[::2].analyze_layout(engine=\"yolo\")</code>.</li> <li>Show <code>page.get_sections(start_elements=page.find_all('region[type=heading]'))</code> to split by detected headings.</li> <li>Include an example of exporting regions to COCO JSON for custom model fine-tuning.</li> <li>Document how to override the model path via <code>model_name</code> and how to plug a remote inference client (<code>client=</code>).</li> </ul>"},{"location":"tutorials/07-layout-analysis/#wish-list-future-enhancements","title":"Wish List (Future Enhancements)\u00b6","text":"<ul> <li>Confidence palette \u2013 Allow <code>show(color_by=\"confidence\")</code> to auto-map scores to a red\u2013green gradient.</li> <li><code>ElementCollection.to_json()</code> \u2013 one-liner export of detected regions (and optionally <code>to_dataframe()</code>).</li> <li>Model cache override \u2013 honor an env variable like <code>NATPDF_MODEL_DIR</code> so enterprises can redirect weight downloads.</li> <li>Remote inference support \u2013 make the <code>client=</code> hook forward images to a custom REST or gRPC service.</li> </ul>"},{"location":"tutorials/07-working-with-regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Create a region in the top portion of the page\ntop_region = page.create_region(\n    50,          # x0 (left)\n    100,          # y0 (top)\n    page.width - 50,  # x1 (right)\n    200          # y1 (bottom)\n)\n\n# Visualize the region\ntop_region.show(color=\"blue\")\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Create a region in the top portion of the page top_region = page.create_region(     50,          # x0 (left)     100,          # y0 (top)     page.width - 50,  # x1 (right)     200          # y1 (bottom) )  # Visualize the region top_region.show(color=\"blue\") Out[2]: In\u00a0[3]: Copied! <pre># Extract text from this region\ntop_region.extract_text()\n</pre> # Extract text from this region top_region.extract_text() Out[3]: <pre>'Date: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'</pre> In\u00a0[4]: Copied! <pre># Find an element to create regions around\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Create regions relative to this element\nbelow_title = title.below(height=100)\nright_of_title = title.right(width=200) \nabove_title = title.above(height=50)\n\n# Visualize these regions\npage.clear_highlights()\nbelow_title.highlight(color=\"green\", label=\"Below\")\nright_of_title.highlight(color=\"red\", label=\"Right\")\nabove_title.highlight(color=\"orange\", label=\"Above\")\n\npage.to_image()\n</pre> # Find an element to create regions around title = page.find('text:contains(\"Jungle Health\")')  # Create regions relative to this element below_title = title.below(height=100) right_of_title = title.right(width=200)  above_title = title.above(height=50)  # Visualize these regions page.clear_highlights() below_title.highlight(color=\"green\", label=\"Below\") right_of_title.highlight(color=\"red\", label=\"Right\") above_title.highlight(color=\"orange\", label=\"Above\")  page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Extract text from the region below the title\nbelow_title.extract_text()\n</pre> # Extract text from the region below the title below_title.extract_text() Out[5]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[6]: Copied! <pre># Create a region for a specific document section\nform_region = page.create_region(50, 100, page.width - 50, 300)\n\n# Find elements only within this region\nlabels = form_region.find_all('text:contains(\":\")') \n\n# Visualize the region and the elements found\nform_region.show(\n    color=(0, 0, 1, 0.2),\n    label=\"Form Region\"\n)\nlabels.show(color=\"purple\", label=\"Labels\")\n</pre> # Create a region for a specific document section form_region = page.create_region(50, 100, page.width - 50, 300)  # Find elements only within this region labels = form_region.find_all('text:contains(\":\")')   # Visualize the region and the elements found form_region.show(     color=(0, 0, 1, 0.2),     label=\"Form Region\" ) labels.show(color=\"purple\", label=\"Labels\") Out[6]: In\u00a0[7]: Copied! <pre># Find an element to work with\nelement = page.find('text:contains(\"Summary:\")')\n\n# Create a tight region around the element\ntight_region = element.expand(0, 0, 0, 0)\n\n# Expand it to include surrounding content\nexpanded_region = tight_region.expand(\n    left=10,       # Expand 10 points to the left\n    right=200,     # Expand 200 points to the right\n    top=5,  # Expand 5 points above\n    bottom=100  # Expand 100 points below\n)\n\n# Visualize both regions\npage.clear_highlights()\ntight_region.highlight(color=\"red\", label=\"Original\")\nexpanded_region.highlight(color=\"blue\", label=\"Expanded\")\n\npage.to_image()\n</pre> # Find an element to work with element = page.find('text:contains(\"Summary:\")')  # Create a tight region around the element tight_region = element.expand(0, 0, 0, 0)  # Expand it to include surrounding content expanded_region = tight_region.expand(     left=10,       # Expand 10 points to the left     right=200,     # Expand 200 points to the right     top=5,  # Expand 5 points above     bottom=100  # Expand 100 points below )  # Visualize both regions page.clear_highlights() tight_region.highlight(color=\"red\", label=\"Original\") expanded_region.highlight(color=\"blue\", label=\"Expanded\")  page.to_image() Out[7]: In\u00a0[8]: Copied! <pre># Find two elements to serve as boundaries\nstart_elem = page.find('text:contains(\"Summary:\")')\nend_elem = page.find('text:contains(\"Violations\")')\n\n# Create a region from start to end element\nbounded_region = start_elem.until(end_elem)\n\n# Visualize the bounded region\nbounded_region.show(color=\"green\", label=\"Bounded Region\")\n\n# Extract text from this bounded region\nbounded_region.extract_text()[:200] + \"...\"\n</pre> # Find two elements to serve as boundaries start_elem = page.find('text:contains(\"Summary:\")') end_elem = page.find('text:contains(\"Violations\")')  # Create a region from start to end element bounded_region = start_elem.until(end_elem)  # Visualize the bounded region bounded_region.show(color=\"green\", label=\"Bounded Region\")  # Extract text from this bounded region bounded_region.extract_text()[:200] + \"...\" Out[8]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men...'</pre> In\u00a0[9]: Copied! <pre># Define multiple regions to extract different parts of the document\nheader_region = page.create_region(0, 0, page.width, 100)\nmain_region = page.create_region(100, 100, page.width - 100, page.height - 150)\nfooter_region = page.create_region(0, page.height - 50, page.width, page.height)\n\n# Visualize all regions\nheader_region.show(color=\"blue\", label=\"Header\")\nmain_region.show(color=\"green\", label=\"Main Content\")\nfooter_region.show(color=\"red\", label=\"Footer\")\n\n# Extract content from each region\ndocument_parts = {\n    \"header\": header_region.extract_text(),\n    \"main\": main_region.extract_text()[:100] + \"...\",\n    \"footer\": footer_region.extract_text()\n}\n\n# Show what we extracted\ndocument_parts\n</pre> # Define multiple regions to extract different parts of the document header_region = page.create_region(0, 0, page.width, 100) main_region = page.create_region(100, 100, page.width - 100, page.height - 150) footer_region = page.create_region(0, page.height - 50, page.width, page.height)  # Visualize all regions header_region.show(color=\"blue\", label=\"Header\") main_region.show(color=\"green\", label=\"Main Content\") footer_region.show(color=\"red\", label=\"Footer\")  # Extract content from each region document_parts = {     \"header\": header_region.extract_text(),     \"main\": main_region.extract_text()[:100] + \"...\",     \"footer\": footer_region.extract_text() }  # Show what we extracted document_parts Out[9]: <pre>{'header': 'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.',\n 'main': 'ruary 3, 1905\\nCount: 7\\nWorst of any, however, were the fertilizer men, and those who served in the c...',\n 'footer': 'Jungle Health and Safety Inspection Service'}</pre> In\u00a0[10]: Copied! <pre># Find a region of interest\ntable_header = page.find('text:contains(\"Statute\")')\ntable_region = table_header.below(height=100)\n\n# Visualize the region\ntable_region.show(color=\"purple\", label=\"Table Region\")\n\n# Create an image of just this region\ntable_region.to_image(resolution=150)\n</pre> # Find a region of interest table_header = page.find('text:contains(\"Statute\")') table_region = table_header.below(height=100)  # Visualize the region table_region.show(color=\"purple\", label=\"Table Region\")  # Create an image of just this region table_region.to_image(resolution=150) Out[10]: <p>Regions allow you to precisely target specific parts of a document for extraction and analysis. They're essential for handling complex document layouts and isolating the exact content you need.</p>"},{"location":"tutorials/07-working-with-regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that let you focus on specific parts of a document. They're perfect for extracting text from defined areas, finding elements within certain boundaries, and working with document sections.</p>"},{"location":"tutorials/07-working-with-regions/#creating-regions-from-elements","title":"Creating Regions from Elements\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#finding-elements-within-regions","title":"Finding Elements Within Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#expanding-and-adjusting-regions","title":"Expanding and Adjusting Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-bounded-regions","title":"Creating Bounded Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#working-with-multiple-regions","title":"Working with Multiple Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-an-image-of-a-region","title":"Creating an Image of a Region\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/","title":"Spatial Navigation","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the title of the document\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Visualize our starting point\ntitle.show(color=\"red\", label=\"Document Title\")\n\n# Display the title text\ntitle.text\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the title of the document title = page.find('text:contains(\"Jungle Health\")')  # Visualize our starting point title.show(color=\"red\", label=\"Document Title\")  # Display the title text title.text Out[2]: <pre>'Jungle Health and Safety Inspection Service'</pre> In\u00a0[3]: Copied! <pre># Create a region below the title\nregion_below = title.below(height=100)\n\n# Visualize the region\nregion_below.show(color=\"blue\", label=\"Below Title\")\n\n# Find and extract text from this region\ntext_below = region_below.extract_text()\ntext_below\n</pre> # Create a region below the title region_below = title.below(height=100)  # Visualize the region region_below.show(color=\"blue\", label=\"Below Title\")  # Find and extract text from this region text_below = region_below.extract_text() text_below Out[3]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[4]: Copied! <pre># Find two labels to serve as boundaries\nsite_label = page.find('text:contains(\"Site:\")')\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Get the region between these labels\nbetween_region = site_label.below(\n    include_source=True,     # Include starting element\n    until='text:contains(\"Date:\")',  # Stop at this element\n    include_endpoint=False    # Don't include ending element\n)\n\n# Visualize the region between labels\nbetween_region.show(color=\"green\", label=\"Between\")\n\n# Extract text from this bounded area\nbetween_region.extract_text()\n</pre> # Find two labels to serve as boundaries site_label = page.find('text:contains(\"Site:\")') date_label = page.find('text:contains(\"Date:\")')  # Get the region between these labels between_region = site_label.below(     include_source=True,     # Include starting element     until='text:contains(\"Date:\")',  # Stop at this element     include_endpoint=False    # Don't include ending element )  # Visualize the region between labels between_region.show(color=\"green\", label=\"Between\")  # Extract text from this bounded area between_region.extract_text() Out[4]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.'</pre> In\u00a0[5]: Copied! <pre># Find a field label\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Get the content to the right (the field value)\nvalue_region = site_label.right(width=200)\n\n# Visualize the label and value regions\nsite_label.show(color=\"red\", label=\"Label\")\nvalue_region.show(color=\"blue\", label=\"Value\")\n\n# Extract just the value text\nvalue_region.extract_text()\n</pre> # Find a field label site_label = page.find('text:contains(\"Site:\")')  # Get the content to the right (the field value) value_region = site_label.right(width=200)  # Visualize the label and value regions site_label.show(color=\"red\", label=\"Label\") value_region.show(color=\"blue\", label=\"Value\")  # Extract just the value text value_region.extract_text() Out[5]: <pre>'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt'</pre> In\u00a0[6]: Copied! <pre># Start with a label element\nlabel = page.find('text:contains(\"Site:\")')\n\n# Find the next and previous elements in reading order\nnext_elem = label.next()\nprev_elem = label.prev()\n\n# Visualize all three elements\nlabel.show(color=\"red\", label=\"Current\")\nnext_elem.show(color=\"green\", label=\"Next\") if next_elem else None\nprev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None\n\n# Show the text of adjacent elements\n{\n    \"current\": label.text,\n    \"next\": next_elem.text if next_elem else \"None\",\n    \"previous\": prev_elem.text if prev_elem else \"None\"\n}\n</pre> # Start with a label element label = page.find('text:contains(\"Site:\")')  # Find the next and previous elements in reading order next_elem = label.next() prev_elem = label.prev()  # Visualize all three elements label.show(color=\"red\", label=\"Current\") next_elem.show(color=\"green\", label=\"Next\") if next_elem else None prev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None  # Show the text of adjacent elements {     \"current\": label.text,     \"next\": next_elem.text if next_elem else \"None\",     \"previous\": prev_elem.text if prev_elem else \"None\" } Out[6]: <pre>{'current': 'Site: ', 'next': 'i', 'previous': 'S'}</pre> In\u00a0[7]: Copied! <pre># Find a section label\nsummary = page.find('text:contains(\"Summary:\")')\n\n# Find the next bold text element\nnext_bold = summary.next('text:bold', limit=20)\n\n# Find the nearest line element\nnearest_line = summary.nearest('line')\n\n# Visualize what we found\nsummary.show(color=\"red\", label=\"Summary\")\nnext_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None\nnearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None\n\n# Show the content we found\n{\n    \"summary\": summary.text,\n    \"next_bold\": next_bold.text if next_bold else \"None found\",\n    \"nearest_line\": nearest_line if nearest_line else \"None found\"\n}\n</pre> # Find a section label summary = page.find('text:contains(\"Summary:\")')  # Find the next bold text element next_bold = summary.next('text:bold', limit=20)  # Find the nearest line element nearest_line = summary.nearest('line')  # Visualize what we found summary.show(color=\"red\", label=\"Summary\") next_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None nearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None  # Show the content we found {     \"summary\": summary.text,     \"next_bold\": next_bold.text if next_bold else \"None found\",     \"nearest_line\": nearest_line if nearest_line else \"None found\" } Out[7]: <pre>{'summary': 'Summary: ',\n 'next_bold': 'u',\n 'nearest_line': &lt;LineElement type=horizontal width=2.0 bbox=(50.0, 352.0, 550.0, 352.0)&gt;}</pre> In\u00a0[8]: Copied! <pre># Find a table heading\ntable_heading = page.find('text:contains(\"Statute\")')\ntable_heading.show(color=\"purple\", label=\"Table Header\")\n\n# Extract table rows using spatial navigation\nrows = []\ncurrent = table_heading\n\n# Get the next 4 rows\nfor i in range(4):\n    # Find the next row below the current one\n    next_row = current.below(height=15)\n    \n    if next_row:\n        rows.append(next_row)\n        current = next_row  # Move to the next row\n    else:\n        break\n\n# Visualize all found rows\npage.clear_highlights()\nfor i, row in enumerate(rows):\n    row.highlight(label=f\"Row {i+1}\", use_color_cycling=True)\n</pre> # Find a table heading table_heading = page.find('text:contains(\"Statute\")') table_heading.show(color=\"purple\", label=\"Table Header\")  # Extract table rows using spatial navigation rows = [] current = table_heading  # Get the next 4 rows for i in range(4):     # Find the next row below the current one     next_row = current.below(height=15)          if next_row:         rows.append(next_row)         current = next_row  # Move to the next row     else:         break  # Visualize all found rows page.clear_highlights() for i, row in enumerate(rows):     row.highlight(label=f\"Row {i+1}\", use_color_cycling=True) In\u00a0[9]: Copied! <pre># Extract text from each row\n[row.extract_text() for row in rows]\n</pre> # Extract text from each row [row.extract_text() for row in rows] Out[9]: <pre>['4.12.7 Unsanitary Working Conditions. Critical',\n '4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious',\n '5.8.3 Inadequate Protective Equipment. Serious',\n '6.3.9 Ineffective Injury Prevention. Serious']</pre> In\u00a0[10]: Copied! <pre># Find all potential field labels (text with a colon)\nlabels = page.find_all('text:contains(\":\")') \n\n# Visualize the labels\nlabels.show(color=\"blue\", label=\"Labels\")\n\n# Extract key-value pairs\nfield_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    key = label.text.strip().rstrip(':')\n    \n    # Skip if not a proper label\n    if not key:\n        continue\n    \n    # Get the value to the right\n    value = label.right(width=200).extract_text().strip()\n    \n    # Add to our collection\n    field_data[key] = value\n\n# Show the extracted data\nfield_data\n</pre> # Find all potential field labels (text with a colon) labels = page.find_all('text:contains(\":\")')   # Visualize the labels labels.show(color=\"blue\", label=\"Labels\")  # Extract key-value pairs field_data = {}  for label in labels:     # Clean up the label text     key = label.text.strip().rstrip(':')          # Skip if not a proper label     if not key:         continue          # Get the value to the right     value = label.right(width=200).extract_text().strip()          # Add to our collection     field_data[key] = value  # Show the extracted data field_data Out[10]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> <p>Spatial navigation mimics how humans read documents, letting you navigate content based on physical relationships between elements. It's especially useful for extracting structured data from forms, tables, and formatted documents.</p> In\u00a0[11]: Copied! <pre># Step 1 \u2013 find the heading text\nheading = page.find('text:contains(\"Summary:\")')\n\n# Step 2 \u2013 get the first bold word after that heading (skip up to 30 elements)\nvalue_label = heading.next('text:bold', limit=30)\n\n# Step 3 \u2013 grab the value region to the right of that bold word\nvalue_region = value_label.right(until='line')  # Extend until the boundary line\n\nvalue_region.show(color=\"orange\", label=\"Summary Value\")\nvalue_region.extract_text()\n</pre> # Step 1 \u2013 find the heading text heading = page.find('text:contains(\"Summary:\")')  # Step 2 \u2013 get the first bold word after that heading (skip up to 30 elements) value_label = heading.next('text:bold', limit=30)  # Step 3 \u2013 grab the value region to the right of that bold word value_region = value_label.right(until='line')  # Extend until the boundary line  value_region.show(color=\"orange\", label=\"Summary Value\") value_region.extract_text() Out[11]: <pre>'e: Durha\\nte: Febr\\nolation C\\nmmary:\\nese peop\\nitor at a h\\nme of wh\\no the vats\\nhibiting -\\nthe world\\nolation\\ntatute\\n12.7\\n8.3\\n3.9\\n1.5\\n9.2\\n6.4\\n0.2.7'</pre> In\u00a0[12]: Copied! <pre>invoice_total = (\n    page.find('text:startswith(\"Date:\")')\n        .right(width=500, height='element')            # Move right to get the amount region\n        .find('text')                # Narrow to text elements only\n)\n</pre> invoice_total = (     page.find('text:startswith(\"Date:\")')         .right(width=500, height='element')            # Move right to get the amount region         .find('text')                # Narrow to text elements only ) <p>Because each call returns an element, you never lose the spatial context \u2013 you can always add another <code>.below()</code> or <code>.nearest()</code> later.</p>"},{"location":"tutorials/08-spatial-navigation/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>Spatial navigation lets you work with PDF content based on the physical layout of elements on the page. It's perfect for finding elements relative to each other and extracting information in context.</p>"},{"location":"tutorials/08-spatial-navigation/#finding-elements-above-and-below","title":"Finding Elements Above and Below\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-content-between-elements","title":"Finding Content Between Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#navigating-left-and-right","title":"Navigating Left and Right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-adjacent-elements","title":"Finding Adjacent Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#combining-with-element-selectors","title":"Combining with Element Selectors\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-table-rows-with-spatial-navigation","title":"Extracting Table Rows with Spatial Navigation\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-key-value-pairs","title":"Extracting Key-Value Pairs\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#todo","title":"TODO\u00b6","text":"<ul> <li>Add examples for navigating across multiple pages using <code>pdf.pages</code> slicing and <code>below(..., until=...)</code> that spans pages.</li> <li>Show how to chain selectors, e.g., <code>page.find('text:bold').below().right()</code> for complex paths.</li> <li>Include a sidebar on performance when many spatial calls are chained and how to cache intermediate regions.</li> <li>Add examples using <code>.until()</code> for one-liner \"from here until X\" extractions.</li> <li>Show using <code>width=\"element\"</code> vs <code>\"full\"</code> in <code>.below()</code> and <code>.above()</code> to restrict horizontal span.</li> <li>Demonstrate attribute selectors (e.g., <code>line[width&gt;2]</code>) and <code>:not()</code> pseudo-class for exclusion in spatial chains.</li> <li>Briefly introduce <code>.expand()</code> for fine-tuning region size after spatial selection.</li> </ul>"},{"location":"tutorials/08-spatial-navigation/#chaining-spatial-calls","title":"Chaining Spatial Calls\u00b6","text":"<p>Spatial helpers like <code>.below()</code>, <code>.right()</code>, <code>.nearest()</code> and friends return Element or Region objects, so you can keep chaining operations just like you would with jQuery or BeautifulSoup.</p> <ol> <li>Start with a selector (string or Element).</li> <li>Apply a spatial function.</li> <li>Optionally, add another selector to narrow the result.</li> <li>Repeat!</li> </ol>"},{"location":"tutorials/08-spatial-navigation/#example-1-heading-next-bold-word-value-to-its-right","title":"Example 1 \u2013 Heading \u2192 next bold word \u2192 value to its right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#example-2-find-a-label-anywhere-on-the-document-and-walk-to-its-value-in-one-chain","title":"Example 2 \u2013 Find a label anywhere on the document and walk to its value in one chain\u00b6","text":""},{"location":"tutorials/09-section-extraction/","title":"Section Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF using the relative path\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\n# Identify horizontal rules that look like section dividers\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Visualize the potential section boundaries (single element type \u279c use .show())\nhorizontal_lines.show(color=\"red\", label=\"Section Boundaries\")\npage.to_image()\n</pre> from natural_pdf import PDF  # Load the PDF using the relative path pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  # Identify horizontal rules that look like section dividers horizontal_lines = page.find_all('line:horizontal')  # Visualize the potential section boundaries (single element type \u279c use .show()) horizontal_lines.show(color=\"red\", label=\"Section Boundaries\") page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Count what we found\nlen(horizontal_lines)\n</pre> # Count what we found len(horizontal_lines) Out[3]: <pre>9</pre> In\u00a0[4]: Copied! <pre># Extract sections based on horizontal lines\n# Each section starts at a horizontal line and ends at the next one\nbook_sections = page.get_sections(\n    start_elements=horizontal_lines,\n    boundary_inclusion='start'  # Include the boundary in the section\n)\n\n# Visualize each section\npage.clear_highlights()\nfor section in book_sections:\n    section.show()\npage.to_image()\n</pre> # Extract sections based on horizontal lines # Each section starts at a horizontal line and ends at the next one book_sections = page.get_sections(     start_elements=horizontal_lines,     boundary_inclusion='start'  # Include the boundary in the section )  # Visualize each section page.clear_highlights() for section in book_sections:     section.show() page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Display section count and preview the first section\n{\n    \"total_sections\": len(book_sections),\n    \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\"\n}\n</pre> # Display section count and preview the first section {     \"total_sections\": len(book_sections),     \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\" } Out[5]: <pre>{'total_sections': 9,\n 'first_section_text': '6/12/2023 - Copies Removed: 2\\nTristan Strong punches a hole in the sky (Removed: 1)\\nAuthor: Mbalia, ...'}</pre> In\u00a0[6]: Copied! <pre># Extract and display content from the first few book entries\nbook_entries = []\n\nfor i, section in enumerate(book_sections[:5]):\n    # Extract the section text\n    text = section.extract_text().strip()\n    \n    # Try to parse book information\n    title = \"\"\n    author = \"\"\n    isbn = \"\"\n    \n    # Extract title (typically the first line)\n    title_match = section.find('text:contains(\"Title:\")')\n    if title_match:\n        title_value = title_match.right(width=400).extract_text()\n        title = title_value.strip()\n    \n    # Extract author\n    author_match = section.find('text:contains(\"Author:\")')\n    if author_match:\n        author_value = author_match.right(width=400).extract_text()\n        author = author_value.strip()\n    \n    # Extract ISBN\n    isbn_match = section.find('text:contains(\"ISBN:\")')\n    if isbn_match:\n        isbn_value = isbn_match.right(width=400).extract_text()\n        isbn = isbn_value.strip()\n    \n    # Add to our collection\n    book_entries.append({\n        \"number\": i + 1,\n        \"title\": title,\n        \"author\": author,\n        \"isbn\": isbn,\n        \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text\n    })\n\n# Display the structured book entries\nimport pandas as pd\npd.DataFrame(book_entries)\n</pre> # Extract and display content from the first few book entries book_entries = []  for i, section in enumerate(book_sections[:5]):     # Extract the section text     text = section.extract_text().strip()          # Try to parse book information     title = \"\"     author = \"\"     isbn = \"\"          # Extract title (typically the first line)     title_match = section.find('text:contains(\"Title:\")')     if title_match:         title_value = title_match.right(width=400).extract_text()         title = title_value.strip()          # Extract author     author_match = section.find('text:contains(\"Author:\")')     if author_match:         author_value = author_match.right(width=400).extract_text()         author = author_value.strip()          # Extract ISBN     isbn_match = section.find('text:contains(\"ISBN:\")')     if isbn_match:         isbn_value = isbn_match.right(width=400).extract_text()         isbn = isbn_value.strip()          # Add to our collection     book_entries.append({         \"number\": i + 1,         \"title\": title,         \"author\": author,         \"isbn\": isbn,         \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text     })  # Display the structured book entries import pandas as pd pd.DataFrame(book_entries) Out[6]: number title author isbn preview 0 1 Log Atlanta Public S\\n023\\nemoved: 2\\na hole i... Atlanta Public Schools\\nPublished: 2019\\nAcqui... 6/12/2023 - Copies Removed: 2\\nTristan Strong ... 1 2 6/7/2023 - Copies Removed: 2 2 3 Atlanta Public School\\nved: 2\\nin the sky (Rem... Atlanta Public Schools\\n93-2 Published: 2019\\n... Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Ba... 3 4 Atlanta Public Schools\\nd: 2\\nn the sky (Remov... Atlanta Public Schools\\nPublished: 2019\\nAcqui... Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book ... 4 5 6/6/2023 - Copies Removed: 130 In\u00a0[7]: Copied! <pre>page.viewer()\n</pre> page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[7]: <pre>InteractiveViewerWidget()</pre> In\u00a0[8]: Copied! <pre># Find title elements with specific selectors\ntitle_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\ntitle_elements.show()\n</pre> # Find title elements with specific selectors title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]') title_elements.show() Out[8]: In\u00a0[9]: Copied! <pre># Extract sections starting from each title\n# This now directly returns an ElementCollection\ntitle_sections = page.get_sections(\n    start_elements=title_elements,\n    boundary_inclusion='start'\n)\n\n# Show the title-based sections\npage.clear_highlights()\ntitle_sections.show()\npage.to_image()\n</pre> # Extract sections starting from each title # This now directly returns an ElementCollection title_sections = page.get_sections(     start_elements=title_elements,     boundary_inclusion='start' )  # Show the title-based sections page.clear_highlights() title_sections.show() page.to_image() Out[9]: In\u00a0[10]: Copied! <pre># Count the sections found\nlen(title_sections)\n</pre> # Count the sections found len(title_sections) Out[10]: <pre>7</pre> In\u00a0[11]: Copied! <pre># Use horizontal line elements as section dividers\ndividers = page.find_all('line:horizontal')\n\n# Compare the different boundary inclusion options\ninclusion_options = {\n    'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),\n    'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),\n    'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),\n    'both': page.get_sections(start_elements=dividers, boundary_inclusion='both')\n}\n\n# Count sections with each option\nsection_counts = {option: len(sections) for option, sections in inclusion_options.items()}\nsection_counts\n</pre> # Use horizontal line elements as section dividers dividers = page.find_all('line:horizontal')  # Compare the different boundary inclusion options inclusion_options = {     'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),     'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),     'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),     'both': page.get_sections(start_elements=dividers, boundary_inclusion='both') }  # Count sections with each option section_counts = {option: len(sections) for option, sections in inclusion_options.items()} section_counts Out[11]: <pre>{'none': 9, 'start': 9, 'end': 9, 'both': 9}</pre> In\u00a0[12]: Copied! <pre># Define specific start and end points - let's extract just one book entry\n# We'll look for the first and second horizontal lines\npage.clear_highlights()\n\nstart_point = title_elements[0]\nend_point = title_elements[1]\n\n# Extract the section between these points\nsingle_book_entry = page.get_sections(\n    start_elements=[start_point],\n    end_elements=[end_point],\n    boundary_inclusion='start'  # Include the start but not the end\n)\n    \n# Visualize the custom section\nsingle_book_entry.show(color=\"green\", label=\"Single Book Entry\")\n    \nprint(single_book_entry[0].extract_text())\n\npage.to_image()\n</pre> # Define specific start and end points - let's extract just one book entry # We'll look for the first and second horizontal lines page.clear_highlights()  start_point = title_elements[0] end_point = title_elements[1]  # Extract the section between these points single_book_entry = page.get_sections(     start_elements=[start_point],     end_elements=[end_point],     boundary_inclusion='start'  # Include the start but not the end )      # Visualize the custom section single_book_entry.show(color=\"green\", label=\"Single Book Entry\")      print(single_book_entry[0].extract_text())  page.to_image() <pre>Tristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-2 Published: 2019\nSite Barcode Price Acquired Removed By\nJoseph Humphries 32441014018707 6/11/2021 113396-42441\nElementary School\nWas Available -- Weeded\nUpside down in the middle of nowhere (Removed: 1)\n</pre> Out[12]: In\u00a0[13]: Copied! <pre># Get sections across the first two pages\nmulti_page_sections = [] # Initialize as a list\n\nfor page_num in range(min(2, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page (returns ElementCollection)\n    page_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Add elements from the collection to our list\n    multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection\n\n# Display info about each section (showing first 3)\n[{\n    \"page\": section.page.number + 1,  # 1-indexed page number for display\n    \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text()\n} for section in multi_page_sections]\n</pre> # Get sections across the first two pages multi_page_sections = [] # Initialize as a list  for page_num in range(min(2, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page (returns ElementCollection)     page_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Add elements from the collection to our list     multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection  # Display info about each section (showing first 3) [{     \"page\": section.page.number + 1,  # 1-indexed page number for display     \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text() } for section in multi_page_sections] Out[13]: <pre>[{'page': 2, 'text': 'Tristan Strong punches a hole in the sky (Removed:...'},\n {'page': 2, 'text': 'Upside down in the middle of nowhere (Removed: 1)\\n...'},\n {'page': 2, 'text': 'Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Bazaz. ...'},\n {'page': 2, 'text': 'Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book edito...'},\n {'page': 2, 'text': 'The Abenaki (Removed: 1)\\nAuthor: Landau, Elaine. I...'},\n {'page': 2, 'text': 'Afghanistan (Removed: 1)\\nAuthor: Milivojevic, Jova...'},\n {'page': 2, 'text': 'Alexander the Great rocks the world (Removed: 1)\\nA...'},\n {'page': 3, 'text': 'The Anasazi (Removed: 1)\\nAuthor: Petersen, David. ...'},\n {'page': 3, 'text': 'And then what happened, Paul Revere? (Removed: 1)\\n...'},\n {'page': 3, 'text': 'The assassination of Martin Luther King Jr (Remove...'},\n {'page': 3, 'text': 'Barbara Jordan. (Removed: 1)\\nAuthor: Wexler, Diane...'},\n {'page': 3, 'text': 'Bedtime for Batman (Removed: 1)\\nAuthor: Dahl, Mich...'},\n {'page': 3, 'text': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskeg...'},\n {'page': 3, 'text': 'Bigfoot Wallace (Removed: 1)\\nAuthor: Harper,Jo. IS...'},\n {'page': 3, 'text': 'The blaze engulfs : January 1939 to December 1941 ...'}]</pre> In\u00a0[14]: Copied! <pre># Extract all book entries across multiple pages\nbook_database = []\n\n# Process first 3 pages (or fewer if the document is shorter)\nfor page_num in range(min(3, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page\n    book_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Process each book section\n    for section in book_sections:\n        # Skip sections that are too short (might be headers/footers)\n        if len(section.extract_text()) &lt; 50:\n            continue\n            \n        # Extract book information\n        book_info = {\"page\": page_num + 1}\n        \n        for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.strip(':').lower()\n                field_value = field_element.extract_text().replace(field, '').strip()\n                book_info[field_name] = field_value\n\n        # Below the field name\n        for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.lower()\n                field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()\n                book_info[field_name] = field_value\n\n        book_database.append(book_info)\n\n# Display sample entries (first 3)\nimport pandas as pd\n\ndf = pd.json_normalize(book_database)\ndf.head()\n</pre> # Extract all book entries across multiple pages book_database = []  # Process first 3 pages (or fewer if the document is shorter) for page_num in range(min(3, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page     book_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Process each book section     for section in book_sections:         # Skip sections that are too short (might be headers/footers)         if len(section.extract_text()) &lt; 50:             continue                      # Extract book information         book_info = {\"page\": page_num + 1}                  for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.strip(':').lower()                 field_value = field_element.extract_text().replace(field, '').strip()                 book_info[field_name] = field_value          # Below the field name         for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.lower()                 field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()                 book_info[field_name] = field_value          book_database.append(book_info)  # Display sample entries (first 3) import pandas as pd  df = pd.json_normalize(book_database) df.head() Out[14]: page author isbn price acquired barcode removed by 0 1 Mbalia, Kwame. 978-1-36803993-2 6/11/2021 11 32441014018707 113396-42441 1 1 Lamana, Julie T. 978-1-45212456-8 (alk. $15.00 6/12/2023 11 32441012580849 113396-42441 2 1 Wangu, Madhu Bazaz. 0-8160-2442-1 $10.00 4/19/2018 ch 33343000017835 christen.mcclain 3 1 Kelly Wand, book editor. 0-7377-1314-3 (lib.) $19.95 3/21/2006 ch *3431000028742 christen.mcclain 4 1 Landau, Elaine. 0-531-20227-5 $16.50 2/21/2000 33 33170000506628 33554-43170 <p>Section extraction lets you break down documents into logical parts, making it easier to generate summaries, extract specific content, and create structured data from semi-structured documents. In this example, we've shown how to convert a PDF library catalog into a structured book database.</p>"},{"location":"tutorials/09-section-extraction/#section-extraction","title":"Section Extraction\u00b6","text":"<p>Documents are often organized into logical sections like chapters, articles, or content blocks. This tutorial shows how to extract these sections using natural-pdf, using a library weeding log as an example.</p>"},{"location":"tutorials/09-section-extraction/#basic-section-extraction","title":"Basic Section Extraction\u00b6","text":""},{"location":"tutorials/09-section-extraction/#working-with-section-content","title":"Working with Section Content\u00b6","text":""},{"location":"tutorials/09-section-extraction/#using-different-section-boundaries","title":"Using Different Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#section-boundary-inclusion-options","title":"Section Boundary Inclusion Options\u00b6","text":""},{"location":"tutorials/09-section-extraction/#custom-section-boundaries","title":"Custom Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#multi-page-sections","title":"Multi-page Sections\u00b6","text":""},{"location":"tutorials/09-section-extraction/#building-a-book-database","title":"Building a Book Database\u00b6","text":""},{"location":"tutorials/09-section-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Demonstrate using <code>page.init_search()</code> to pre-index all pages and retrieve section headings quickly.</li> <li>Add an example that merges multi-page sections by passing <code>new_section_on_page_break=False</code>.</li> <li>Include tips for detecting numbered headings (\"1.\", \"2.\") when ruling lines are absent.</li> <li>Provide a performance note on large PDFs and how to stream through pages lazily.</li> </ul>"},{"location":"tutorials/10-form-field-extraction/","title":"Form Field Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"\n</pre> #%pip install \"natural-pdf[ai]\" <p>If you already have the core library, simply run <code>npdf install ai</code> to add the extra ML packages.</p> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find fields with labels ending in colon\nlabels = page.find_all('text:contains(\":\")')\n\n# Visualize the found labels\nlabels.show(color=\"blue\", label=\"Field Labels\")\n\n# Count how many potential fields we found\nlen(labels)\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find fields with labels ending in colon labels = page.find_all('text:contains(\":\")')  # Visualize the found labels labels.show(color=\"blue\", label=\"Field Labels\")  # Count how many potential fields we found len(labels) Out[2]: <pre>4</pre> In\u00a0[3]: Copied! <pre># Extract the value for each field label\nform_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    field_name = label.text.strip().rstrip(':')\n    \n    # Find the value to the right of the label\n    value_region = label.right(width=200)\n    value = value_region.extract_text().strip()\n    \n    # Store in our dictionary\n    form_data[field_name] = value\n\n# Display the extracted data\nform_data\n</pre> # Extract the value for each field label form_data = {}  for label in labels:     # Clean up the label text     field_name = label.text.strip().rstrip(':')          # Find the value to the right of the label     value_region = label.right(width=200)     value = value_region.extract_text().strip()          # Store in our dictionary     form_data[field_name] = value  # Display the extracted data form_data Out[3]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> In\u00a0[4]: Copied! <pre># Clear previous highlights\npage.clear_highlights()\n\n# Highlight both labels and their values\nfor label in labels:\n    # Highlight the label in red\n    label.show(color=\"red\", label=\"Label\")\n    \n    # Highlight the value area in blue\n    label.right(width=200).show(color=\"blue\", label=\"Value\")\n\n# Show the page image with highlighted elements\npage.to_image()\n</pre> # Clear previous highlights page.clear_highlights()  # Highlight both labels and their values for label in labels:     # Highlight the label in red     label.show(color=\"red\", label=\"Label\")          # Highlight the value area in blue     label.right(width=200).show(color=\"blue\", label=\"Value\")  # Show the page image with highlighted elements page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Extract values that might span multiple lines\nmulti_line_data = {}\n\nfor label in labels:\n    # Get the field name\n    field_name = label.text.strip().rstrip(':')\n    \n    # Look both to the right and below\n    right_value = label.right(width=200).extract_text().strip()\n    below_value = label.below(height=50).extract_text().strip()\n    \n    # Combine the values if they're different\n    if right_value in below_value:\n        value = below_value\n    else:\n        value = f\"{right_value} {below_value}\".strip()\n    \n    # Add to results\n    multi_line_data[field_name] = value\n\n# Show fields with potential multi-line values\nmulti_line_data\n</pre> # Extract values that might span multiple lines multi_line_data = {}  for label in labels:     # Get the field name     field_name = label.text.strip().rstrip(':')          # Look both to the right and below     right_value = label.right(width=200).extract_text().strip()     below_value = label.below(height=50).extract_text().strip()          # Combine the values if they're different     if right_value in below_value:         value = below_value     else:         value = f\"{right_value} {below_value}\".strip()          # Add to results     multi_line_data[field_name] = value  # Show fields with potential multi-line values multi_line_data Out[5]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt Date: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health Violation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'}</pre> In\u00a0[6]: Copied! <pre>import re\n\n# Find dates in the format July 31, YYY\ndate_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'\n\n# Search all text elements for dates\ntext_elements = page.find_all('text')\nprint([elem.text for elem in text_elements])\ndates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))\n\n# Visualize the date fields\ndates.show(color=\"green\", label=\"Date\")\n\n# Extract just the date values\ndate_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates]\ndate_texts\n</pre> import re  # Find dates in the format July 31, YYY date_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'  # Search all text elements for dates text_elements = page.find_all('text') print([elem.text for elem in text_elements]) dates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))  # Visualize the date fields dates.show(color=\"green\", label=\"Date\")  # Extract just the date values date_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates] date_texts <pre>['Jungle Health and Safety Inspection Service', 'INS-UP70N51NCL41R', 'Site: ', 'Durham\u2019s Meatpacking  ', 'Chicago, Ill.', 'Date:  ', 'February 3, 1905', 'Violation Count: ', '7', 'Summary: ', 'Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.', 'These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary ', 'visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in ', 'some of which there were open vats near the level of the floor, their peculiar trouble was that they fell', 'into the vats; and when they were fished out, there was never enough of them left to be worth ', 'exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out', 'to the world as Durham\u2019s Pure Leaf Lard!', 'Violations', 'Statute', 'Description', 'Level', 'Repeat?', '4.12.7', 'Unsanitary Working Conditions.', 'Critical', '5.8.3', 'Inadequate Protective Equipment.', 'Serious', '6.3.9', 'Ineffective Injury Prevention.', 'Serious', '7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', '8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', '9.6.4', 'Inadequate Ventilation Systems.', 'Serious', '10.2.7', 'Insufficient Employee Training for Safe Work Practices.', 'Serious', 'Jungle Health and Safety Inspection Service']\n</pre> Out[6]: <pre>['February 3, 1905']</pre> In\u00a0[7]: Copied! <pre># Run layout analysis to find table structures\npage.analyze_layout()\n\n# Find possible form tables\ntables = page.find_all('region[type=table]')\n\nif tables:\n    # Visualize the tables\n    tables.show(color=\"purple\", label=\"Form Table\")\n    \n    # Extract data from the first table\n    first_table = tables[0]\n    table_data = first_table.extract_table()\n    table_data\nelse:\n    # Try to find form-like structure using text alignment\n    # Create a region where a form might be\n    form_region = page.create_region(50, 200, page.width - 50, 500)\n    \n    # Group text by vertical position\n    rows = {}\n    text_elements = form_region.find_all('text')\n    \n    for elem in text_elements:\n        # Round y-position to group elements in the same row\n        row_pos = round(elem.top / 5) * 5\n        if row_pos not in rows:\n            rows[row_pos] = []\n        rows[row_pos].append(elem)\n    \n    # Extract data from rows (first 5 rows)\n    row_data = []\n    for y in sorted(rows.keys())[:5]:\n        # Sort elements by x-position (left to right)\n        elements = sorted(rows[y], key=lambda e: e.x0)\n        \n        # Show the row\n        row_box = form_region.create_region(\n            min(e.x0 for e in elements), \n            min(e.top for e in elements),\n            max(e.x1 for e in elements),\n            max(e.bottom for e in elements)\n        )\n        row_box.show(color=None, use_color_cycling=True)\n        \n        # Extract text from row\n        row_text = [e.text for e in elements]\n        row_data.append(row_text)\n    \n    # Show the extracted rows\n    row_data\n</pre> # Run layout analysis to find table structures page.analyze_layout()  # Find possible form tables tables = page.find_all('region[type=table]')  if tables:     # Visualize the tables     tables.show(color=\"purple\", label=\"Form Table\")          # Extract data from the first table     first_table = tables[0]     table_data = first_table.extract_table()     table_data else:     # Try to find form-like structure using text alignment     # Create a region where a form might be     form_region = page.create_region(50, 200, page.width - 50, 500)          # Group text by vertical position     rows = {}     text_elements = form_region.find_all('text')          for elem in text_elements:         # Round y-position to group elements in the same row         row_pos = round(elem.top / 5) * 5         if row_pos not in rows:             rows[row_pos] = []         rows[row_pos].append(elem)          # Extract data from rows (first 5 rows)     row_data = []     for y in sorted(rows.keys())[:5]:         # Sort elements by x-position (left to right)         elements = sorted(rows[y], key=lambda e: e.x0)                  # Show the row         row_box = form_region.create_region(             min(e.x0 for e in elements),              min(e.top for e in elements),             max(e.x1 for e in elements),             max(e.bottom for e in elements)         )         row_box.show(color=None, use_color_cycling=True)                  # Extract text from row         row_text = [e.text for e in elements]         row_data.append(row_text)          # Show the extracted rows     row_data <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpl0o4g880/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 2263.2ms\n</pre> <pre>Speed: 7.9ms preprocess, 2263.2ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[8]: Copied! <pre># Combine label-based and pattern-based extraction\nall_fields = {}\n\n# 1. First get fields with explicit labels\nfor label in labels:\n    field_name = label.text.strip().rstrip(':')\n    value = label.right(width=200).extract_text().strip()\n    all_fields[field_name] = value\n\n# 2. Add date fields that we found with pattern matching\nfor date_elem in dates:\n    # Find the nearest label\n    nearby_label = date_elem.nearest('text:contains(\":\")')\n    \n    if nearby_label:\n        # Extract the label text\n        label_text = nearby_label.text.strip().rstrip(':')\n        \n        # Get the date value\n        date_value = re.search(date_pattern, date_elem.text).group(0)\n        \n        # Add to our results if not already present\n        if label_text not in all_fields:\n            all_fields[label_text] = date_value\n\n# Show all extracted fields\nall_fields\n</pre> # Combine label-based and pattern-based extraction all_fields = {}  # 1. First get fields with explicit labels for label in labels:     field_name = label.text.strip().rstrip(':')     value = label.right(width=200).extract_text().strip()     all_fields[field_name] = value  # 2. Add date fields that we found with pattern matching for date_elem in dates:     # Find the nearest label     nearby_label = date_elem.nearest('text:contains(\":\")')          if nearby_label:         # Extract the label text         label_text = nearby_label.text.strip().rstrip(':')                  # Get the date value         date_value = re.search(date_pattern, date_elem.text).group(0)                  # Add to our results if not already present         if label_text not in all_fields:             all_fields[label_text] = date_value  # Show all extracted fields all_fields Out[8]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> In\u00a0[9]: Copied! <pre>answer = page.ask(\"What is the invoice total?\")\n</pre> answer = page.ask(\"What is the invoice total?\") <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n</pre> <p><code>answer['answer']</code> is the literal text found on the page.</p> <p>For a deep dive into Question Answering\u2014including confidence tuning, batching, and answer-span highlighting\u2014see Tutorial 06: Document Question Answering.</p> <p>Form field extraction enables you to automate data entry and document processing. By combining different techniques like label detection, spatial navigation, and pattern matching, you can handle a wide variety of form layouts.</p>"},{"location":"tutorials/10-form-field-extraction/#form-field-extraction","title":"Form Field Extraction\u00b6","text":"<p>Extracting key-value pairs from documents can be tackled in two complementary ways:</p> <ul> <li>Rule-based / spatial heuristics \u2013 look for label text, navigate rightward or downward, group elements into rows, etc.</li> <li>Extractive Document QA \u2013 feed the page image and its words to a fine-tuned LayoutLM model and ask natural-language questions such as \"What is the invoice total?\". The model returns the answer span exactly as it appears in the document along with a confidence score.</li> </ul> <p>This tutorial starts with classical heuristics and then upgrades to the LayoutLM-based DocumentQA engine built into <code>natural-pdf</code>. Because DocumentQA relies on <code>torch</code>, <code>transformers</code>, and <code>vision</code> extras, install the AI optional dependencies first:</p>"},{"location":"tutorials/10-form-field-extraction/#extracting-field-values","title":"Extracting Field Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#visualizing-labels-and-values","title":"Visualizing Labels and Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#handling-multi-line-values","title":"Handling Multi-line Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#finding-pattern-based-fields","title":"Finding Pattern-Based Fields\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#working-with-form-tables","title":"Working with Form Tables\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#combining-different-extraction-techniques","title":"Combining Different Extraction Techniques\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#asking-questions-with-layoutlm-documentqa","title":"Asking Questions with LayoutLM (DocumentQA)\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#optional-one-liner-qa","title":"Optional one-liner QA\u00b6","text":"<p>Need a single field but can't locate the right label?  You can fall back to <code>page.ask()</code> which runs the LayoutLM extractive QA model:</p>"},{"location":"tutorials/10-form-field-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Showcase the new <code>init_search</code> workflow for quickly locating form labels across multi-page documents.</li> <li>Compare heuristics for multi-col forms (e.g., left/right alignment vs. table structures) and when to switch strategies.</li> <li>Demonstrate embedding page classification (e.g., \"invoice\" vs \"purchase order\") before field extraction to route documents to the correct template.</li> <li>Provide an end-to-end example saving the extracted dictionary to JSON and a searchable PDF via <code>pdf.save_searchable()</code>.</li> <li>Add a sidebar contrasting extractive QA with generative LLM approaches and notes on when to choose each.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/","title":"Enhanced Table Processing","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Optional fine-tuning for pdfplumber.  Typical tweaks are vertical/horizontal strategies.\nsettings = {\n    \"vertical_strategy\": \"lines\",\n    \"horizontal_strategy\": \"lines\",\n    \"intersection_tolerance\": 3,\n}\n\nrows = page.extract_table(method=\"pdfplumber\", table_settings=settings)\nrows  # \u25b6\ufe0e returns a list of lists\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Optional fine-tuning for pdfplumber.  Typical tweaks are vertical/horizontal strategies. settings = {     \"vertical_strategy\": \"lines\",     \"horizontal_strategy\": \"lines\",     \"intersection_tolerance\": 3, }  rows = page.extract_table(method=\"pdfplumber\", table_settings=settings) rows  # \u25b6\ufe0e returns a list of lists Out[1]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>Expected output: a small list of rows containing the text exactly as it appears in the digital table.</p> In\u00a0[2]: Copied! <pre>settings_text = {\n    \"vertical_strategy\": \"text\",   # look for whitespace gutters\n    \"horizontal_strategy\": \"text\", # group into rows by vertical gaps\n    \"text_x_tolerance\": 2,          # tune for narrow columns\n    \"text_y_tolerance\": 2,\n}\n\nrows_text = page.extract_table(method=\"pdfplumber\", table_settings=settings_text)\n</pre> settings_text = {     \"vertical_strategy\": \"text\",   # look for whitespace gutters     \"horizontal_strategy\": \"text\", # group into rows by vertical gaps     \"text_x_tolerance\": 2,          # tune for narrow columns     \"text_y_tolerance\": 2, }  rows_text = page.extract_table(method=\"pdfplumber\", table_settings=settings_text) <p>Compare <code>rows_text</code> with the earlier <code>rows</code> list\u2014if your PDF omits the grid, the whitespace strategy will usually outperform line-based detection.</p> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# If the page is scanned, run OCR first so each cell has text\npage.apply_ocr(engine=\"easyocr\", languages=[\"en\"], resolution=200)\n\n# Table Transformer needs the layout model; specify device if you have GPU\nrows = page.extract_table(method=\"tatr\")\nrows\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # If the page is scanned, run OCR first so each cell has text page.apply_ocr(engine=\"easyocr\", languages=[\"en\"], resolution=200)  # Table Transformer needs the layout model; specify device if you have GPU rows = page.extract_table(method=\"tatr\") rows <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> Out[3]: <pre>[]</pre> <p>Expected output: the table rows\u2014even when the grid is just implied\u2014arrive with text already OCR-corrected.</p> In\u00a0[4]: Copied! <pre># from natural_pdf import PDF\n\n# pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/whitespace-table.pdf\")\n# page = pdf.pages[0]\n\n# rows = page.extract_table(method=\"text\", table_settings={\"min_words_horizontal\": 2})\n# for row in rows:\n#     print(row)\n</pre> # from natural_pdf import PDF  # pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/whitespace-table.pdf\") # page = pdf.pages[0]  # rows = page.extract_table(method=\"text\", table_settings={\"min_words_horizontal\": 2}) # for row in rows: #     print(row) <p>Expected output: printed rows that roughly match the visual columns; best effort on ragged layouts.</p> In\u00a0[5]: Copied! <pre>page.detect_lines(resolution=200, source_label=\"detected\", horizontal=True, vertical=True)\n\n# (Optional) visual check\npage.find_all(\"line[source=detected]\").show(group_by=\"orientation\")\n\n# Convert lines \u2192 regions\npage.detect_table_structure_from_lines(source_label=\"detected\", cell_padding=0.5)\n\ntable = page.find(\"region[type='table']\")\n</pre> page.detect_lines(resolution=200, source_label=\"detected\", horizontal=True, vertical=True)  # (Optional) visual check page.find_all(\"line[source=detected]\").show(group_by=\"orientation\")  # Convert lines \u2192 regions page.detect_table_structure_from_lines(source_label=\"detected\", cell_padding=0.5)  table = page.find(\"region[type='table']\")"},{"location":"tutorials/11-enhanced-table-processing/#enhanced-table-processing","title":"Enhanced Table Processing\u00b6","text":"<p>Tables can appear in PDFs in wildly different ways\u2014cleanly tagged in the PDF structure, drawn with ruling lines, or simply implied by visual spacing.  <code>natural-pdf</code> exposes several back-ends under the single method <code>extract_table()</code> so you can choose the strategy that matches your document.</p> <p>Below we walk through the three main options, when to reach for each one, and sample code you can adapt (replace the example PDF URLs with your own files).</p>"},{"location":"tutorials/11-enhanced-table-processing/#1-methodpdfplumber-default","title":"1. <code>method=\"pdfplumber\"</code>  (default)\u00b6","text":"<ul> <li>How it works \u2013 delegates to pdfplumber's ruling-line heuristics; looks for vertical/horizontal lines and whitespace gutters.</li> <li>Best for \u2013 digitally-born PDFs where the table grid is drawn or where columns have consistent whitespace.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example-a-grid-based-line-detection","title":"Example A \u2013 Grid-based (line) detection\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#example-b-whitespace-driven-detection","title":"Example B \u2013 Whitespace-driven detection\u00b6","text":"<p>Sometimes a table is drawn without ruling lines (or the PDF stores them as thick rectangles so the line detector ignores them).  In that case you can switch both strategies to <code>\"text\"</code> so pdfplumber clusters by the gaps between words rather than relying on graphics commands:</p>"},{"location":"tutorials/11-enhanced-table-processing/#2-methodtatr-table-transformer","title":"2. <code>method=\"tatr\"</code>  (Table Transformer)\u00b6","text":"<ul> <li>How it works \u2013 runs Microsoft's Table Transformer (LayoutLM-based) to detect tables, rows and cells visually, then reads the text inside each cell.</li> <li>Best for \u2013 scanned or camera-based documents, or born-digital files where ruling lines are missing/irregular.</li> <li>Dependencies \u2013 requires the AI extra (<code>pip install \"natural-pdf[ai]\"</code>) because it needs <code>torch</code>, <code>transformers</code>, and <code>torchvision</code>.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example","title":"Example\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#3-methodtext-whitespace-heuristic","title":"3. <code>method=\"text\"</code>  (Whitespace heuristic)\u00b6","text":"<ul> <li>How it works \u2013 groups words into lines, then uses whitespace clustering (Jenks breaks) to infer columns; no layout model.</li> <li>Best for \u2013 simple, left-aligned tables with consistent columns but no ruling lines; fastest option.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example","title":"Example\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#4-lines-first-workflow-when-pdfplumber-misses-rowscols","title":"4. Lines-first workflow (when pdfplumber misses rows/cols)\u00b6","text":"<p>If <code>method=\"pdfplumber\"</code> cannot find the grid, detect lines explicitly and build the table structure yourself.</p>"},{"location":"tutorials/11-enhanced-table-processing/#todo","title":"TODO\u00b6","text":"<ul> <li>Provide a benchmark matrix of speed vs. accuracy for the three methods.</li> <li>Add a snippet showing how to export cell regions directly to a pandas DataFrame.</li> <li>Document edge-cases: rotated tables, merged cells, or header repetition across pages.</li> <li>Include guidance on mixing methods\u2014e.g., run <code>detect_lines</code> first, fall back to <code>text</code> for cells lacking grid.</li> </ul>"},{"location":"tutorials/12-ocr-integration/","title":"OCR Integration for Scanned Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Try extracting text without OCR\ntext_without_ocr = page.extract_text()\nf\"Without OCR: {len(text_without_ocr)} characters extracted\"\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # Try extracting text without OCR text_without_ocr = page.extract_text() f\"Without OCR: {len(text_without_ocr)} characters extracted\" Out[2]: <pre>'Without OCR: 0 characters extracted'</pre> In\u00a0[3]: Copied! <pre># Apply OCR using the default engine (EasyOCR) for English\npage.apply_ocr(languages=['en'])\n\n# Select all text pieces found by OCR\ntext_elements = page.find_all('text[source=ocr]')\nprint(f\"Found {len(text_elements)} text elements using default OCR\")\n\n# Visualize the elements\ntext_elements.show()\n</pre> # Apply OCR using the default engine (EasyOCR) for English page.apply_ocr(languages=['en'])  # Select all text pieces found by OCR text_elements = page.find_all('text[source=ocr]') print(f\"Found {len(text_elements)} text elements using default OCR\")  # Visualize the elements text_elements.show() <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> <pre>Found 44 text elements using default OCR\n</pre> Out[3]: In\u00a0[4]: Copied! <pre># Apply OCR using PaddleOCR for English\npage.apply_ocr(engine='paddle', languages=['en'])\nprint(f\"Found {len(page.find_all('text[source=ocr]'))} elements after English OCR.\")\n\n# Apply OCR using PaddleOCR for Chinese\npage.apply_ocr(engine='paddle', languages=['ch'])\nprint(f\"Found {len(page.find_all('text[source=ocr]'))} elements after Chinese OCR.\")\n\ntext_with_ocr = page.extract_text()\nprint(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\")\n</pre> # Apply OCR using PaddleOCR for English page.apply_ocr(engine='paddle', languages=['en']) print(f\"Found {len(page.find_all('text[source=ocr]'))} elements after English OCR.\")  # Apply OCR using PaddleOCR for Chinese page.apply_ocr(engine='paddle', languages=['ch']) print(f\"Found {len(page.find_all('text[source=ocr]'))} elements after Chinese OCR.\")  text_with_ocr = page.extract_text() print(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\") <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Found 43 elements after English OCR.\n</pre> <pre>Found 43 elements after Chinese OCR.\n\nExtracted text after OCR:\nRed (RGB tuple)\nJungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham's Meatpacking Chicago, .\nDate:February 3.1905\nViolation Cou...\n</pre> <p>You can also use <code>.describe()</code> to see a summary of the OCR outcome...</p> In\u00a0[5]: Copied! <pre>page.describe()\n</pre> page.describe() Out[5]: <p>...or <code>.inspect()</code> on the text elements for individual details.</p> In\u00a0[6]: Copied! <pre>page.find_all('text').inspect()\n</pre> page.find_all('text').inspect() Out[6]: In\u00a0[7]: Copied! <pre>import natural_pdf as npdf\n\n# Set global OCR defaults\nnpdf.options.ocr.engine = 'surya'          # Default OCR engine\nnpdf.options.ocr.min_confidence = 0.7      # Default confidence threshold\n\n# Now all OCR calls use these defaults\npdf = npdf.PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npdf.pages[0].apply_ocr()  # Uses: engine='surya', languages=['en', 'es'], min_confidence=0.7\n\n# You can still override defaults for specific calls\npdf.pages[0].apply_ocr(engine='easyocr', languages=['fr'])  # Override engine and languages\n</pre> import natural_pdf as npdf  # Set global OCR defaults npdf.options.ocr.engine = 'surya'          # Default OCR engine npdf.options.ocr.min_confidence = 0.7      # Default confidence threshold  # Now all OCR calls use these defaults pdf = npdf.PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") pdf.pages[0].apply_ocr()  # Uses: engine='surya', languages=['en', 'es'], min_confidence=0.7  # You can still override defaults for specific calls pdf.pages[0].apply_ocr(engine='easyocr', languages=['fr'])  # Override engine and languages <pre>Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n</pre> <pre>Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n</pre> <pre>\rDetecting bboxes:   0%|                                                          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.38it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.38it/s]</pre> <pre>\n</pre> <pre>\rRecognizing Text:   0%|                                                          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:13&lt;00:00, 13.82s/it]</pre> <pre>\rRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:13&lt;00:00, 13.82s/it]</pre> <pre>\n</pre> <pre>[2025-06-17 22:10:07,050] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> Out[7]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>This is especially useful when processing many documents with the same OCR settings, as you don't need to specify the parameters repeatedly.</p> In\u00a0[8]: Copied! <pre>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\n\n# Re-apply OCR using EasyOCR with specific options\neasy_opts = EasyOCROptions(\n    paragraph=False,\n)\npage.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)\n\npaddle_opts = PaddleOCROptions()\npage.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)\n\nsurya_opts = SuryaOCROptions()\npage.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts)\n</pre> from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions  # Re-apply OCR using EasyOCR with specific options easy_opts = EasyOCROptions(     paragraph=False, ) page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)  paddle_opts = PaddleOCROptions() page.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)  surya_opts = SuryaOCROptions() page.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts) <pre>Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n</pre> <pre>Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n</pre> <pre>\rDetecting bboxes:   0%|                                                          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.70it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.69it/s]</pre> <pre>\n</pre> Out[8]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[9]: Copied! <pre># Process all pages in the document\n\n# Apply OCR to all pages (example using EasyOCR)\npdf.apply_ocr(engine='easyocr', languages=['en'])\nprint(f\"Applied OCR to {len(pdf.pages)} pages.\")\n\n# Or apply layout analysis to all pages (example using Paddle)\n# pdf.apply_layout(engine='paddle')\n# print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")\n\n# Extract text from all pages (uses OCR results if available)\nall_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")\n\nprint(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\")\n</pre> # Process all pages in the document  # Apply OCR to all pages (example using EasyOCR) pdf.apply_ocr(engine='easyocr', languages=['en']) print(f\"Applied OCR to {len(pdf.pages)} pages.\")  # Or apply layout analysis to all pages (example using Paddle) # pdf.apply_layout(engine='paddle') # print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")  # Extract text from all pages (uses OCR results if available) all_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")  print(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\") <pre>[2025-06-17 22:10:46,581] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'page_separator'\n</pre> <pre>Applied OCR to 1 pages.\n\nCombined text from all pages:\nRed (RGB tuple )\nJungle Health and Safety Inspection Service\nDate: February 3, 1905\nViolation Count:\nSummary: Worst of any, however; were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor\nsome of which there were open vats near the level of the floor; their peculiar trouble was that they fell\nexhibitingsometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham's Pure Leaf Lardl\nViolat...\n</pre>"},{"location":"tutorials/12-ocr-integration/#ocr-integration-for-scanned-documents","title":"OCR Integration for Scanned Documents\u00b6","text":"<p>Optical Character Recognition (OCR) allows you to extract text from scanned documents where the text isn't embedded in the PDF. This tutorial demonstrates how to work with scanned documents.</p>"},{"location":"tutorials/12-ocr-integration/#applying-ocr-and-finding-elements","title":"Applying OCR and Finding Elements\u00b6","text":"<p>The core method is <code>page.apply_ocr()</code>. This runs the OCR process and adds <code>TextElement</code> objects to the page. You can specify the engine and languages.</p> <p>Note: Re-applying OCR to the same page or region will automatically remove any previously generated OCR elements for that area before adding the new ones.</p>"},{"location":"tutorials/12-ocr-integration/#page-1-summary","title":"Page 1 Summary\u00b6","text":"<p>Elements:</p> <ul> <li>text: 43 elements</li> </ul> <p>Text Analysis:</p> <ul> <li>typography:<ul> <li>fonts:<ul> <li>OCR: 43</li> </ul> </li> <li>sizes:<ul> <li>11.0pt: 13</li> <li>12.0pt: 10</li> <li>13.0pt: 7</li> <li>15.0pt: 6</li> <li>14.0pt: 4</li> <li>10.0pt: 1</li> <li>7.0pt: 1</li> <li>8.0pt: 1</li> </ul> </li> </ul> </li> <li>ocr quality:<ul> <li>confidence stats:<ul> <li>mean: 0.95</li> <li>min: 0.64</li> <li>max: 1.00</li> </ul> </li> <li>quality distribution:<ul> <li>99%+ (21/43) 49%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591</code></li> <li>95%+ (32/43) 74%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591</code></li> <li>90%+ (38/43) 88%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591</code></li> </ul> </li> <li>lowest scoring:<ul> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> </ul> </li> </ul> </li> </ul>"},{"location":"tutorials/12-ocr-integration/#1-064","title":"1: 0.64: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#2-066","title":"2: 0.66: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#3-068","title":"3: 0.68: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#4-070","title":"4: 0.70: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#5-089-some-of-which-there-were-open-vats-near-the-level-of-thefloo","title":"5: 0.89: some of which there were open vats near the level of thefloo...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#6-094-insufficient-employee-training-for-safe-work-practices","title":"6: 0.94: Insufficient Employee Training for Safe Work Practices.\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#7-094-these-people-could-not-be-shown-to-the-visitor-for-the-odo","title":"7: 0.94: These people could not be shown to the visitor - for the odo...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#8-094-summary-worst-of-any-however-were-the-fertilizer-men-and","title":"8: 0.94: Summary: Worst of any, however, were the fertilizer men, and...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#9-094-visitor-at-a-hundred-yardsand-as-for-the-other-men-who-wor","title":"9: 0.94: visitor at a hundred yards,and as for the other men, who wor...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#10-095-jungle-health-and-safety-inspection-service","title":"10: 0.95: Jungle Health and Safety Inspection Service\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#collection-inspection-43-elements","title":"Collection Inspection (43 elements)\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#word-elements","title":"Word Elements\u00b6","text":"text x0 top x1 bottom font_family size bold italic source confidence color Red (RGB tuple) 541 57 583 68 OCR 11 False False ocr 0.98 #000000 Jungle Health and Safety Inspection Service 329 83 467 94 OCR 11 False False ocr 0.95 #000000 INS-UP70N51NCL41R 328 92 402 101 OCR 10 False False ocr 1.00 #000000 Site: Durham's Meatpacking Chicago, . 41 125 201 136 OCR 11 False False ocr 0.95 #000000 Date:February 3.1905 40 141 134 152 OCR 11 False False ocr 0.95 #000000 Violation Count: 7 40 159 117 170 OCR 11 False False ocr 0.99 #000000 Summary: Worst of any, however, were the fertilize... 41 177 434 187 OCR 11 False False ocr 0.94 #000000 These people could not be shown to the visitor - f... 41 190 438 201 OCR 11 False False ocr 0.94 #000000 visitor at a hundred yards,and as for the other me... 40 204 420 215 OCR 11 False False ocr 0.94 #000000 some of which there were open vats near the level ... 40 218 427 229 OCR 11 False False ocr 0.89 #000000 into the yats: and when they were fished out. ther... 43 234 397 241 OCR 7 False False ocr 0.95 #000000 exhibiting - sometimes they would be overlooked fo... 43 248 423 255 OCR 8 False False ocr 0.96 #000000 to the world as Durham's Pure Leaf Lard! 41 260 200 270 OCR 11 False False ocr 0.97 #000000 Violations 40 372 91 384 OCR 12 False False ocr 1.00 #000000 Statute 44 393 80 408 OCR 15 False False ocr 1.00 #000000 Repeat? 430 393 471 408 OCR 15 False False ocr 1.00 #000000 Description 87 394 138 406 OCR 12 False False ocr 1.00 #000000 Level 389 394 415 407 OCR 13 False False ocr 1.00 #000000 4.12.7 45 411 74 425 OCR 14 False False ocr 1.00 #000000 Unsanitary Working Conditions. 88 411 211 425 OCR 14 False False ocr 0.99 #000000 Critical 389 412 419 424 OCR 12 False False ocr 1.00 #000000 5.8.3 45 428 68 441 OCR 13 False False ocr 1.00 #000000 Inadequate Protective Equipment. 88 428 221 442 OCR 14 False False ocr 0.96 #000000 Serious 389 429 421 441 OCR 12 False False ocr 1.00 #000000 \u25a1 442 444 458 460 OCR 15 False False ocr 0.70 #000000 6.3.9 45 446 68 459 OCR 13 False False ocr 1.00 #000000 Ineffective Injury Prevention. 88 446 200 460 OCR 14 False False ocr 0.97 #000000 Serious 389 447 421 459 OCR 12 False False ocr 1.00 #000000 \u25a1 442 461 458 477 OCR 15 False False ocr 0.68 #000000 7.1.5 44 463 68 475 OCR 12 False False ocr 1.00 #000000 <p>Showing 30 of 43 elements (pass limit= to see more)</p>"},{"location":"tutorials/12-ocr-integration/#setting-default-ocr-options","title":"Setting Default OCR Options\u00b6","text":"<p>You can set global default OCR options using <code>natural_pdf.options</code>. These defaults will be used automatically when you call <code>apply_ocr()</code> without specifying parameters.</p>"},{"location":"tutorials/12-ocr-integration/#advanced-ocr-configuration","title":"Advanced OCR Configuration\u00b6","text":"<p>For more control, import and use the specific <code>Options</code> class for your chosen engine within the <code>apply_ocr</code> call.</p>"},{"location":"tutorials/12-ocr-integration/#interactive-ocr-correction-debugging","title":"Interactive OCR Correction / Debugging\u00b6","text":"<p>If OCR results aren't perfect, you can use the bundled interactive web application (SPA) to review and correct them.</p> <ol> <li><p>Package the data: After running <code>apply_ocr</code> (or <code>apply_layout</code>), use <code>create_correction_task_package</code> to create a zip file containing the PDF images and detected elements.</p> <pre>from natural_pdf.utils.packaging import create_correction_task_package\n\npage.apply_ocr()\n\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</pre> </li> <li><p>Run the SPA: Navigate to the SPA directory within the installed <code>natural_pdf</code> library in your terminal and start a simple web server.</p> </li> <li><p>Use the SPA: Open <code>http://localhost:8000</code> in your browser. Drag the <code>correction_package.zip</code> file onto the page to load the document. You can then click on text elements to correct the OCR results.</p> </li> </ol>"},{"location":"tutorials/12-ocr-integration/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>Apply OCR or layout analysis to all pages using the <code>PDF</code> object.</p>"},{"location":"tutorials/12-ocr-integration/#saving-pdfs-with-searchable-text","title":"Saving PDFs with Searchable Text\u00b6","text":"<p>After applying OCR to a PDF, you can save a new version of the PDF where the recognized text is embedded as an invisible layer. This makes the text searchable and copyable in standard PDF viewers.</p> <p>Use the <code>save_searchable()</code> method on the <code>PDF</code></p>"},{"location":"tutorials/12-ocr-integration/#todo","title":"TODO\u00b6","text":"<ul> <li>Add guidance on installing only the OCR engines you need (e.g. <code>pip install \"natural-pdf[ai] easyocr\"</code>) instead of the heavy <code>[all]</code> extra.</li> <li>Show how to use <code>detect_only=True</code> to combine OCR detection with external recognition for higher accuracy (ties into fine-tuning tutorial).</li> <li>Include an example of saving a searchable PDF via <code>pdf.save_searchable(\"output.pdf\")</code> after OCR.</li> <li>Mention <code>resolution</code> parameter trade-offs (speed vs accuracy) when calling <code>apply_ocr</code>.</li> <li>Provide a quick snippet demonstrating <code>.viewer()</code> for interactive visual QC of OCR results.</li> </ul>"},{"location":"tutorials/13-semantic-search/","title":"Semantic Search Across Multiple Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[search]\"\n</pre> #%pip install \"natural-pdf[search]\" In\u00a0[2]: Copied! <pre>import natural_pdf\n\n# Define the paths to your PDF files\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n]\n\n# Or use glob patterns\n# collection = natural_pdf.PDFCollection(\"pdfs/*.pdf\")\n\n# Create a PDFCollection\ncollection = natural_pdf.PDFCollection(pdf_paths)\nprint(f\"Created collection with {len(collection.pdfs)} PDFs.\")\n</pre> import natural_pdf  # Define the paths to your PDF files pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\" ]  # Or use glob patterns # collection = natural_pdf.PDFCollection(\"pdfs/*.pdf\")  # Create a PDFCollection collection = natural_pdf.PDFCollection(pdf_paths) print(f\"Created collection with {len(collection.pdfs)} PDFs.\") <pre>Created collection with 2 PDFs.\n</pre> In\u00a0[3]: Copied! <pre># Initialize search.\n# index=True will build the serachable database immediately\n# persist=True will save it so you don't need to do it every time\ncollection.init_search(index=True)\nprint(\"Search index initialized.\")\n</pre> # Initialize search. # index=True will build the serachable database immediately # persist=True will save it so you don't need to do it every time collection.init_search(index=True) print(\"Search index initialized.\") <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n</pre> <pre>Search index initialized.\n</pre> In\u00a0[4]: Copied! <pre># Perform a search query\nquery = \"american president\"\nresults = collection.find_relevant(query)\n\nprint(f\"Found {len(results)} results for '{query}':\")\n</pre> # Perform a search query query = \"american president\" results = collection.find_relevant(query)  print(f\"Found {len(results)} results for '{query}':\") <pre>Found 6 results for 'american president':\n</pre> In\u00a0[5]: Copied! <pre># Process and display the results\nif results:\n    for i, result in enumerate(results):\n        print(f\"  {i+1}. PDF: {result['pdf_path']}\")\n        print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")\n        # Display a snippet of the content\n        snippet = result.get('content_snippet', '')\n        print(f\"     Snippet: {snippet}...\") \nelse:\n    print(\"  No relevant results found.\")\n\n# You can access the full content if needed via the result object, \n# though 'content_snippet' is usually sufficient for display.\n</pre> # Process and display the results if results:     for i, result in enumerate(results):         print(f\"  {i+1}. PDF: {result['pdf_path']}\")         print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")         # Display a snippet of the content         snippet = result.get('content_snippet', '')         print(f\"     Snippet: {snippet}...\")  else:     print(\"  No relevant results found.\")  # You can access the full content if needed via the result object,  # though 'content_snippet' is usually sufficient for display. <pre>  1. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 2 (Score: -0.8584)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nThe Anasazi (Removed: 1)\nAuthor: Petersen, David. ISBN: 0-516-01121-9 (trade) Published: 1991\nSit...\n  2. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 5 (Score: -0.8661)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000562167 $13.10 11/5/1999 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  3. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\n     Page: 1 (Score: -1.0080)\n     Snippet: Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men...\n  4. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 4 (Score: -1.0489)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nChildren of the Philippines (Removed: 1)\nAuthor: Kinkade, Sheila, 1962- ISBN: 0-87614-993-X Publi...\n  5. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 3 (Score: -1.0890)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000507600 $19.45 2/21/2000 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  6. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 1 (Score: -1.0946)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/12/2023 - Copies Removed: 2\nTristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-...\n</pre> <p>Semantic search allows you to efficiently query large sets of documents to find the most relevant information without needing exact keyword matches, leveraging the meaning and context of your query.</p>"},{"location":"tutorials/13-semantic-search/#semantic-search-across-multiple-documents","title":"Semantic Search Across Multiple Documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to find information relevant to a specific query across all documents, not just within a single one. This tutorial demonstrates how to perform semantic search over a <code>PDFCollection</code>.</p> <p>You can do semantic search with the default install, but for increased performance with LanceDB I recommend installing the search extension.</p>"},{"location":"tutorials/13-semantic-search/#initializing-the-search-index","title":"Initializing the Search Index\u00b6","text":"<p>Before performing a search, you need to initialize the search capabilities for the collection. This involves processing the documents and building an index.</p>"},{"location":"tutorials/13-semantic-search/#performing-a-semantic-search","title":"Performing a Semantic Search\u00b6","text":"<p>Once the index is ready, you can use the <code>find_relevant()</code> method to search for content semantically related to your query.</p>"},{"location":"tutorials/13-semantic-search/#understanding-search-results","title":"Understanding Search Results\u00b6","text":"<p>The <code>find_relevant()</code> method returns a list of dictionaries, each representing a relevant text chunk found in one of the PDFs. Each result includes:</p> <ul> <li><code>pdf_path</code>: The path to the PDF document where the result was found.</li> <li><code>page_number</code>: The page number within the PDF.</li> <li><code>score</code>: A relevance score (higher means more relevant).</li> <li><code>content_snippet</code>: A snippet of the text chunk that matched the query.</li> </ul> <p>In the future we should be able to easily look at the PDF!</p>"},{"location":"tutorials/13-semantic-search/#todo","title":"TODO\u00b6","text":"<ul> <li>Add example for using <code>persist=True</code> and <code>collection_name</code> in <code>init_search</code> to create a persistent on-disk index.</li> <li>Show how to override the embedding model (e.g. <code>embedding_model=\"all-MiniLM-L12-v2\"</code>).</li> <li>Mention <code>top_k</code> and filtering options available through <code>SearchOptions</code> when calling <code>find_relevant</code>.</li> <li>Provide a short snippet on visualising matched pages/elements once highlighting support lands (future feature).</li> <li>Clarify that installing the AI stack (<code>natural-pdf[ai]</code>) also pulls in <code>sentence-transformers</code>, which is needed for in-memory NumPy fallback.</li> </ul>"},{"location":"tutorials/14-categorizing-documents/","title":"Categorizing documents","text":"In\u00a0[1]: Copied! <pre># Install the AI extra\n#%pip install \"natural-pdf[ai]\"\n</pre> # Install the AI extra #%pip install \"natural-pdf[ai]\" In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/cia-doc.pdf\")\npdf.pages.to_image(cols=6)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/cia-doc.pdf\") pdf.pages.to_image(cols=6) <pre>CropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\nCropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre>pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='vision')\n\nfor page in pdf.pages:\n    print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\")\n</pre> pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='vision')  for page in pdf.pages:     print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\") <pre>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nDevice set to use mps:0\n</pre> <pre>Classifying batch (openai/clip-vit-base-patch16):   0%|          | 0/17 [00:00&lt;?, ?it/s]</pre> <pre>Page 1 is text - 0.633\nPage 2 is text - 0.957\nPage 3 is text - 0.921\nPage 4 is diagram - 0.895\nPage 5 is diagram - 0.891\nPage 6 is invoice - 0.919\nPage 7 is text - 0.834\nPage 8 is invoice - 0.594\nPage 9 is invoice - 0.971\nPage 10 is invoice - 0.987\nPage 11 is invoice - 0.994\nPage 12 is invoice - 0.992\nPage 13 is text - 0.822\nPage 14 is text - 0.936\nPage 15 is diagram - 0.913\nPage 16 is text - 0.617\nPage 17 is invoice - 0.868\n</pre> <p>How did it do?</p> In\u00a0[3]: Copied! <pre>(\n    pdf.pages\n    .filter(lambda page: page.category == 'diagram')\n    .to_image(show_category=True)\n)\n</pre> (     pdf.pages     .filter(lambda page: page.category == 'diagram')     .to_image(show_category=True) ) Out[3]: <p>Looks great! Note that I had to play around with the categories a bit before I got something that worked. Using \"blank\" doesn't ever show up, \"invoice\" did a lot better than \"form,\" etc etc. It's pretty quick and easy to sanity check so you shouldn't have to suffer too much.</p> <p>I can also save just those pages into a new PDF document.</p> In\u00a0[8]: skip-execution Copied! <pre>(\n    pdf.pages\n    .filter(lambda page: page.category == 'diagram')\n    .save_pdf(\"output.pdf\", original=True)\n)\n</pre> (     pdf.pages     .filter(lambda page: page.category == 'diagram')     .save_pdf(\"output.pdf\", original=True) ) In\u00a0[9]: Copied! <pre>pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='text')\n\nfor page in pdf.pages:\n    print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\")\n</pre> pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='text')  for page in pdf.pages:     print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\") <pre>Classifying batch (facebook/bart-large-mnli):   0%|          | 0/17 [00:00&lt;?, ?it/s]</pre> <pre>Page 1 is text - 0.514\nPage 2 is text - 0.587\nPage 3 is invoice - 0.603\nPage 4 is diagram - 0.65\nPage 5 is diagram - 0.567\nPage 6 is text - 0.654\nPage 7 is diagram - 0.466\nPage 8 is text - 0.626\nPage 9 is text - 0.513\nPage 10 is text - 0.542\nPage 11 is invoice - 0.506\nPage 12 is text - 0.78\nPage 13 is text - 0.456\nPage 14 is diagram - 0.721\nPage 15 is diagram - 0.8\nPage 16 is text - 0.499\nPage 17 is text - 0.78\n</pre> <p>How does it compare to our vision option?</p> In\u00a0[10]: Copied! <pre>pdf.pages.filter(lambda page: page.category == 'diagram').to_image(show_category=True)\n</pre> pdf.pages.filter(lambda page: page.category == 'diagram').to_image(show_category=True) Out[10]: <p>Yes, you can notice that it's wrong, but more importantly look at the confidence scores. Low scores are your best clue that something might not be perfect (beyond manually checking things, of course).</p> <p>If you're processing documents that are text-heavy you'll have much better luck with a text model as compared to a vision one.</p> In\u00a0[7]: Copied! <pre>import natural_pdf\n\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n]\n\n# Import your PDFs\npdfs = natural_pdf.PDFCollection(pdf_paths)\n\n# Run your classification\npdfs.classify_all(['school', 'business'], using='text')\n</pre> import natural_pdf  pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\" ]  # Import your PDFs pdfs = natural_pdf.PDFCollection(pdf_paths)  # Run your classification pdfs.classify_all(['school', 'business'], using='text') <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[7]: <pre>&lt;PDFCollection(count=2)&gt;</pre> <p>What's the first PDF?</p> In\u00a0[8]: Copied! <pre>print(f\"{pdfs[0].category} - confidence of {pdfs[0].category_confidence:0.3}\")\n\n# Look at the first page\npdfs[0].pages[0].to_image(width=500)\n</pre> print(f\"{pdfs[0].category} - confidence of {pdfs[0].category_confidence:0.3}\")  # Look at the first page pdfs[0].pages[0].to_image(width=500) <pre>business - confidence of 0.837\n</pre> Out[8]: <p>How about the second?</p> In\u00a0[9]: Copied! <pre>print(f\"{pdfs[1].category} - confidence of {pdfs[1].category_confidence:0.3}\")\n\n# Look at the first page\npdfs[1].pages[0].to_image(width=500)\n</pre> print(f\"{pdfs[1].category} - confidence of {pdfs[1].category_confidence:0.3}\")  # Look at the first page pdfs[1].pages[0].to_image(width=500) <pre>school - confidence of 0.569\n</pre> Out[9]:"},{"location":"tutorials/14-categorizing-documents/#categorizing-documents","title":"Categorizing documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to automatically categorize pages of PDFs or entire collections of PDFs.</p>"},{"location":"tutorials/14-categorizing-documents/#vision-classification","title":"Vision classification\u00b6","text":"<p>These pages are easily differentiable based on how they look, so we can most likely use a vision model to tell them apart.</p>"},{"location":"tutorials/14-categorizing-documents/#text-classification-default","title":"Text classification (default)\u00b6","text":"<p>By default the search is done using text. It takes the text on the page and feeds it to the classifier along with the categories. Note that you might need to OCR your content first!</p>"},{"location":"tutorials/14-categorizing-documents/#pdf-classification","title":"PDF classification\u00b6","text":"<p>If you want to classify entire PDFs, the process is similar. The only gotcha is you can't use <code>using=\"vision\"</code> with multi-page PDFs (yet?).</p>"},{"location":"tutorials/14-categorizing-documents/#todo","title":"TODO\u00b6","text":"<ul> <li>Document advanced parameters for classification helpers (<code>min_confidence</code>, <code>multi_label</code>, <code>analysis_key</code>) so users can fine-tune behaviour or store multiple result sets.</li> <li>Add an example that passes an explicit Hugging Face model ID (e.g. <code>model=\"openai/clip-vit-base-patch16\"</code>) for reproducibility.</li> <li>Note that vision classification only works for single-page PDFs or per-page classification, not whole multi-page PDFs.</li> <li>Suggest using <code>pdf.pages.to_image(show_category=True)</code> to visually QC an entire document after classification.</li> </ul>"},{"location":"visual-debugging/","title":"Visual Debugging","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find a specific element and add a persistent highlight\npage.find_all('text:contains(\"Summary\")').highlight()\npage.find_all('text:contains(\"Date\")').highlight()\npage.find_all('line').highlight()\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find a specific element and add a persistent highlight page.find_all('text:contains(\"Summary\")').highlight() page.find_all('text:contains(\"Date\")').highlight() page.find_all('line').highlight() page.to_image(width=700) Out[1]: In\u00a0[2]: Copied! <pre>page.clear_highlights()\n\ntitle = page.find('text:bold[size&gt;=12]')\n\n# Highlight with a specific color (string name, hex, or RGB/RGBA tuple)\n# title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity\n# title.highlight(color=\"#FF0000\")        # Hex color\ntitle.highlight(color=\"red\")           # Color name\n\ntext = page.find('text:contains(\"Critical\")')\n\n# Add a label to the highlight (appears in legend)\ntext.highlight(label=\"Critical\")\n\n# Combine color and label\nrect = page.find('rect')\nrect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")\n\npage.to_image(width=700)\n</pre> page.clear_highlights()  title = page.find('text:bold[size&gt;=12]')  # Highlight with a specific color (string name, hex, or RGB/RGBA tuple) # title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity # title.highlight(color=\"#FF0000\")        # Hex color title.highlight(color=\"red\")           # Color name  text = page.find('text:contains(\"Critical\")')  # Add a label to the highlight (appears in legend) text.highlight(label=\"Critical\")  # Combine color and label rect = page.find('rect') rect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")  page.to_image(width=700) Out[2]: In\u00a0[3]: Copied! <pre># Find and highlight all headings with a single color/label\nheadings = page.find_all('text[size&gt;=14]:bold')\nheadings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")\n\n# Find and highlight all tables\ntables = page.find_all('region[type=table]')\ntables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")\n\n# View the result\npage.viewer()\n</pre> # Find and highlight all headings with a single color/label headings = page.find_all('text[size&gt;=14]:bold') headings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")  # Find and highlight all tables tables = page.find_all('region[type=table]') tables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")  # View the result page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[3]: In\u00a0[4]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Highlight the region\ncontent.show()\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Highlight the region content.show() Out[4]: <p>Or look at just the region by itself</p> In\u00a0[5]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Crop to the region\ncontent.to_image(crop_only=True, include_highlights=False)\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Crop to the region content.to_image(crop_only=True, include_highlights=False) Out[5]: In\u00a0[6]: Copied! <pre># Analyze and highlight text styles\npage.clear_highlights()\n\npage.analyze_text_styles()\npage.find_all('text').highlight(group_by='style_label')\n\npage.to_image(width=700)\n</pre> # Analyze and highlight text styles page.clear_highlights()  page.analyze_text_styles() page.find_all('text').highlight(group_by='style_label')  page.to_image(width=700) Out[6]: In\u00a0[7]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\ntext = page.find_all('line')\ntext.highlight(include_attrs=['width', 'color'])\n\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  text = page.find_all('line') text.highlight(include_attrs=['width', 'color'])  page.to_image(width=700) Out[7]: <p>Does it get busy? YES.</p> In\u00a0[8]: Copied! <pre># Clear all highlights on the page\npage.clear_highlights()\n\n# Apply new highlights\npage.find_all('text:bold').highlight(label=\"Bold Text\")\npage.viewer()\n</pre> # Clear all highlights on the page page.clear_highlights()  # Apply new highlights page.find_all('text:bold').highlight(label=\"Bold Text\") page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[8]: In\u00a0[9]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\")\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\") page = pdf.pages[0] page.to_image(width=700) Out[9]: In\u00a0[10]: Copied! <pre>response = page.ask(\"How many votes did Kamala Harris get on Election Day?\")\nresponse\n</pre> response = page.ask(\"How many votes did Kamala Harris get on Election Day?\") response <pre>Device set to use mps:0\n</pre> Out[10]: <pre>{'answer': '60',\n 'confidence': 0.31857365369796753,\n 'start': 31,\n 'end': 31,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[11]: Copied! <pre>response['source_elements'].show()\n</pre> response['source_elements'].show() Out[11]:"},{"location":"visual-debugging/#visual-debugging","title":"Visual Debugging\u00b6","text":"<p>Sometimes it's hard to understand what's happening when working with PDFs. Natural PDF provides powerful visual debugging tools to help you see what you're extracting.</p>"},{"location":"visual-debugging/#adding-persistent-highlights","title":"Adding Persistent Highlights\u00b6","text":"<p>Use the <code>.highlight()</code> method on <code>Element</code> or <code>ElementCollection</code> objects to add persistent highlights to a page. These highlights are stored and will appear when viewing the page later.</p>"},{"location":"visual-debugging/#customizing-persistent-highlights","title":"Customizing Persistent Highlights\u00b6","text":"<p>Customize the appearance of persistent highlights added with <code>.highlight()</code>:</p>"},{"location":"visual-debugging/#highlighting-multiple-elements","title":"Highlighting Multiple Elements\u00b6","text":"<p>Highlighting an <code>ElementCollection</code> applies the highlight to all elements within it. By default, all elements in the collection get the same color and a label based on their type.</p>"},{"location":"visual-debugging/#highlighting-regions","title":"Highlighting Regions\u00b6","text":"<p>You can highlight regions to see what area you're working with:</p>"},{"location":"visual-debugging/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>Visualize text styles to understand the document structure:</p>"},{"location":"visual-debugging/#displaying-attributes","title":"Displaying Attributes\u00b6","text":"<p>You can display element attributes directly on the highlights:</p>"},{"location":"visual-debugging/#clearing-highlights","title":"Clearing Highlights\u00b6","text":"<p>You can clear persistent highlights from a page:</p>"},{"location":"visual-debugging/#document-qa-visualization","title":"Document QA Visualization\u00b6","text":"<p>Visualize document QA results:</p>"},{"location":"visual-debugging/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to visualize PDF content, you might want to explore:</p> <ul> <li>OCR capabilities for working with scanned documents</li> <li>Layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"}]}