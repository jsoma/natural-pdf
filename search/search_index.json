{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Natural PDF","text":"<p>A friendly library for working with PDFs, built on top of pdfplumber.</p> <p>Natural PDF lets you find and extract content from PDFs using simple code that makes sense.</p> <ul> <li>Live demo here</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install natural_pdf\n# All the extras\npip install \"natural_pdf[all]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from natural_pdf import PDF\n\npdf = PDF('https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf')\npage = pdf.pages[0]\n\n# Find the title and get content below it\ntitle = page.find('text:contains(\"Summary\"):bold')\ncontent = title.below().extract_text()\n\n# Exclude everything above 'CONFIDENTIAL' and below last line on page\npage.add_exclusion(page.find('text:contains(\"CONFIDENTIAL\")').above())\npage.add_exclusion(page.find_all('line')[-1].below())\n\n# Get the clean text without header/footer\nclean_text = page.extract_text()\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#new-to-natural-pdf","title":"New to Natural PDF?","text":"<ul> <li>Installation - Get Natural PDF installed and run your first extraction</li> <li>Quick Reference - Essential commands and patterns in one place</li> <li>Tutorial Series - Step-by-step learning path through all features</li> </ul>"},{"location":"#learning-the-basics","title":"Learning the Basics","text":"<p>Follow the tutorial series to learn Natural PDF systematically:</p> <ol> <li>Loading and Basic Text Extraction</li> <li>Finding Specific Elements </li> <li>Extracting Content Blocks</li> <li>Table Extraction</li> <li>Excluding Unwanted Content</li> <li>Document Question Answering</li> <li>Layout Analysis</li> <li>Spatial Navigation</li> <li>Section Extraction</li> <li>Form Field Extraction</li> <li>Enhanced Table Processing</li> <li>OCR Integration</li> <li>Semantic Search</li> <li>Categorizing Documents</li> </ol>"},{"location":"#solving-specific-problems","title":"Solving Specific Problems","text":""},{"location":"#text-extraction-issues","title":"Text Extraction Issues","text":"<ul> <li>Extract Clean Text Without Headers and Footers - Remove repeated content that's cluttering your text extraction</li> <li>Getting Text from Scanned Documents - Use OCR to extract text from image-based PDFs</li> </ul>"},{"location":"#table-problems","title":"Table Problems","text":"<ul> <li>Fix Messy Table Extraction - Handle tables with no borders, merged cells, or poor alignment</li> <li>Getting Tables Out of PDFs - Basic to advanced table extraction techniques</li> </ul>"},{"location":"#data-extraction","title":"Data Extraction","text":"<ul> <li>Extract Data from Forms and Invoices - Pull structured information from standardized documents</li> <li>Pulling Structured Data from PDFs - Use AI to extract specific fields from any document</li> </ul>"},{"location":"#document-analysis","title":"Document Analysis","text":"<ul> <li>Ask Questions to Your Documents - Use natural language to find information</li> <li>Categorizing Pages and Regions - Automatically classify document types and content</li> </ul>"},{"location":"#finding-content","title":"Finding Content","text":"<ul> <li>Finding What You Need in PDFs - Master selectors to locate any element</li> <li>PDF Navigation - Move around documents and work with multiple pages</li> </ul>"},{"location":"#layout-and-structure","title":"Layout and Structure","text":"<ul> <li>Document Layout Analysis - Automatically detect titles, tables, and document structure</li> <li>Working with Regions - Define and work with specific areas of pages</li> <li>Visual Debugging - See what you're extracting and debug selector issues</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#find-elements-with-selectors","title":"Find Elements with Selectors","text":"<p>Use CSS-like selectors to find text, shapes, and more.</p> <pre><code># Find bold text containing \"Revenue\"\npage.find('text:contains(\"Revenue\"):bold').extract_text()\n\n# Find all large text\npage.find_all('text[size&gt;=12]').extract_text()\n</code></pre>"},{"location":"#navigate-spatially","title":"Navigate Spatially","text":"<p>Move around the page relative to elements, not just coordinates.</p> <pre><code># Extract text below a specific heading\nintro_text = page.find('text:contains(\"Introduction\")').below().extract_text()\n\n# Extract text from one heading to the next\nmethods_text = page.find('text:contains(\"Methods\")').below(\n    until='text:contains(\"Results\")'\n).extract_text()\n</code></pre>"},{"location":"#extract-clean-text","title":"Extract Clean Text","text":"<p>Easily extract text content, automatically handling common page elements like headers and footers (if exclusions are set).</p> <pre><code># Extract all text from the page (respecting exclusions)\npage_text = page.extract_text()\n\n# Extract text from a specific region\nsome_region = page.find(...)\nregion_text = some_region.extract_text()\n</code></pre>"},{"location":"#apply-ocr","title":"Apply OCR","text":"<p>Extract text from scanned documents using various OCR engines.</p> <pre><code># Apply OCR using the default engine\nocr_elements = page.apply_ocr()\n\n# Extract text (will use OCR results if available)\ntext = page.extract_text()\n</code></pre>"},{"location":"#analyze-document-layout","title":"Analyze Document Layout","text":"<p>Use AI models to detect document structures like titles, paragraphs, and tables.</p> <pre><code># Detect document structure\npage.analyze_layout()\n\n# Highlight titles and tables\npage.find_all('region[type=title]').highlight(color=\"purple\")\npage.find_all('region[type=table]').highlight(color=\"blue\")\n\n# Extract data from the first table\ntable_data = page.find('region[type=table]').extract_table()\n</code></pre>"},{"location":"#document-question-answering","title":"Document Question Answering","text":"<p>Ask natural language questions directly to your documents.</p> <pre><code># Ask a question\nresult = page.ask(\"What was the company's revenue in 2022?\")\nif result.found:\n    print(f\"Answer: {result.answer}\")\n    result.show()  # Highlight where the answer was found\n</code></pre>"},{"location":"#classify-pages-and-regions","title":"Classify Pages and Regions","text":"<p>Categorize pages or specific regions based on their content using text or vision models.</p> <pre><code># Classify a page based on text\nlabels = [\"invoice\", \"scientific article\", \"presentation\"]\npage.classify(labels, using=\"text\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n\n# Classify a page based on what it looks like\npage.classify(labels, using=\"vision\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n</code></pre>"},{"location":"#visualize-your-work","title":"Visualize Your Work","text":"<p>Debug and understand your extractions visually.</p> <pre><code># Highlight headings\npage.find_all('text[size&gt;=14]').show(color=\"red\", label=\"Headings\")\n\n# Launch the interactive viewer (Jupyter)\npage.viewer()\n</code></pre>"},{"location":"#reference-documentation","title":"Reference Documentation","text":"<ul> <li>Quick Reference - Cheat sheet of essential commands and patterns</li> <li>API Reference - Complete library reference</li> </ul>"},{"location":"#understanding-natural-pdf","title":"Understanding Natural PDF","text":"<p>Coming soon: Conceptual guides explaining how Natural PDF thinks about PDFs and when to use different approaches.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all the classes and methods in Natural PDF.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#pdf-class","title":"PDF Class","text":"<p>The main entry point for working with PDFs.</p> <pre><code>class PDF:\n    \"\"\"\n    The main entry point for working with PDFs.\n\n    Parameters:\n        path (str): Path to the PDF file.\n        password (str, optional): Password for encrypted PDFs. Default: None\n        reading_order (bool, optional): Sort elements in reading order. Default: True\n        keep_spaces (bool, optional): Keep spaces in word elements. Default: True\n        font_attrs (list, optional): Font attributes to use for text grouping. \n                                    Default: ['fontname', 'size']\n        ocr (bool/dict/str, optional): OCR configuration. Default: False\n        ocr_engine (str/Engine, optional): OCR engine to use. Default: \"easyocr\"\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>pages</code> Access pages in the document N/A (property) <code>PageCollection</code> <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>add_exclusion(func, label=None)</code> Add a document-wide exclusion zone <code>func</code>: Function taking a page and returning region<code>label</code>: Optional label for the exclusion <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections across all pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries ('start', 'end', 'both', 'none') <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None)</code> Ask a question about the document content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path <code>dict</code>: Result with answer and metadata"},{"location":"api/#page-class","title":"Page Class","text":"<p>Represents a single page in a PDF document.</p> <pre><code>class Page:\n    \"\"\"\n    Represents a single page in a PDF document.\n\n    Properties:\n        page_number (int): 1-indexed page number\n        page_index (int): 0-indexed page position\n        width (float): Page width in points\n        height (float): Page height in points\n        pdf (PDF): Parent PDF object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the page <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>create_region(x0, top, x1, bottom)</code> Create a region at specific coordinates <code>x0</code>: Left coordinate<code>top</code>: Top coordinate<code>x1</code>: Right coordinate<code>bottom</code>: Bottom coordinate <code>Region</code> <code>highlight(elements, color=None, label=None)</code> Highlight elements on the page <code>elements</code>: Elements to highlight<code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight <code>Page</code> (self) <code>highlight_all(include_types=None, include_text_styles=False, include_layout_regions=False)</code> Highlight all elements on the page <code>include_types</code>: Element types to include<code>include_text_styles</code>: Whether to include text styles<code>include_layout_regions</code>: Whether to include layout regions <code>Page</code> (self) <code>save_image(path, resolution=72, labels=True)</code> Save an image of the page with highlights <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>None</code> <code>to_image(resolution=72, labels=True)</code> Get a PIL Image of the page with highlights <code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>PIL.Image</code> <code>analyze_text_styles()</code> Group text by visual style properties None <code>dict</code>: Mapping of style name to elements <code>analyze_layout(engine=\"yolo\", confidence=0.2, existing=\"replace\")</code> Detect layout regions using ML models <code>model</code>: Model to use (\"yolo\", \"tatr\")<code>confidence</code>: Confidence threshold<code>existing</code>: How to handle existing regions <code>ElementCollection</code>: Detected regions <code>add_exclusion(region, label=None)</code> Add an exclusion zone to the page <code>region</code>: Region to exclude<code>label</code>: Optional label for the exclusion <code>Region</code>: The exclusion region <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections from the page <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the page content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>apply_ocr(languages=None, min_confidence=0.0, **kwargs)</code> Apply OCR to the page <code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold<code>**kwargs</code>: Additional OCR engine parameters <code>ElementCollection</code>: OCR text elements"},{"location":"api/#region-class","title":"Region Class","text":"<p>Represents a rectangular area on a page.</p> <pre><code>class Region:\n    \"\"\"\n    Represents a rectangular area on a page.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the region\n        height (float): Height of the region\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the region <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>expand(left=0, top=0, right=0, bottom=0, width_factor=1.0, height_factor=1.0)</code> Expand the region in specified directions <code>left/top/right/bottom</code>: Points to expand in each direction<code>width_factor/height_factor</code>: Scale width/height by this factor <code>Region</code>: Expanded region <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight the region <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Region attributes to display <code>Region</code> (self) <code>to_image(resolution=72, crop=False)</code> Get a PIL Image of just the region <code>resolution</code>: Image resolution in DPI<code>crop</code>: Whether to exclude border <code>PIL.Image</code> <code>save_image(path, resolution=72, crop=False)</code> Save an image of just the region <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>crop</code>: Whether to exclude border <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections within the region <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the region content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>extract_table(method=None, table_settings=None, use_ocr=False)</code> Extract table data from the region <code>method</code>: Extraction method (\"pdfplumber\", \"tatr\")<code>table_settings</code>: Custom settings for extraction<code>use_ocr</code>: Whether to use OCR text <code>list</code>: Table data as rows and columns <code>intersects(other)</code> Check if this region intersects with another <code>other</code>: Another region <code>bool</code>: True if regions intersect <code>contains(x, y)</code> Check if a point is within the region <code>x</code>: X coordinate<code>y</code>: Y coordinate <code>bool</code>: True if point is in region"},{"location":"api/#element-types","title":"Element Types","text":""},{"location":"api/#element-base-class","title":"Element Base Class","text":"<p>The base class for all PDF elements.</p> <pre><code>class Element:\n    \"\"\"\n    Base class for all PDF elements.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the element\n        height (float): Height of the element\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>above(height=None, full_width=True, until=None, include_until=True)</code> Create a region above the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>below(height=None, full_width=True, until=None, include_until=True)</code> Create a region below the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>select_until(selector, include_endpoint=True, full_width=True)</code> Create a region from this element to another <code>selector</code>: Selector for endpoint<code>include_endpoint</code>: Whether to include endpoint<code>full_width</code>: Whether to span page width <code>Region</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight this element <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Element attributes to display <code>Element</code> (self) <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from this element <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>next(selector=None, limit=None, apply_exclusions=True)</code> Get the next element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>prev(selector=None, limit=None, apply_exclusions=True)</code> Get the previous element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>nearest(selector, max_distance=None, apply_exclusions=True)</code> Get the nearest element matching selector <code>selector</code>: Selector for elements<code>max_distance</code>: Maximum distance in points<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code>"},{"location":"api/#textelement","title":"TextElement","text":"<p>Represents text elements in the PDF.</p> <pre><code>class TextElement(Element):\n    \"\"\"\n    Represents text elements in the PDF.\n\n    Additional Properties:\n        text (str): The text content\n        fontname (str): The font name\n        size (float): The font size\n        bold (bool): Whether the text is bold\n        italic (bool): Whether the text is italic\n        color (tuple): The text color as RGB tuple\n        confidence (float): OCR confidence (for OCR text)\n        source (str): 'pdf' or 'ocr'\n    \"\"\"\n</code></pre> <p>Main Properties</p> Property Type Description <code>text</code> <code>str</code> The text content <code>fontname</code> <code>str</code> The font name <code>size</code> <code>float</code> The font size <code>bold</code> <code>bool</code> Whether the text is bold <code>italic</code> <code>bool</code> Whether the text is italic <code>color</code> <code>tuple</code> The text color as RGB tuple <code>confidence</code> <code>float</code> OCR confidence (for OCR text) <code>source</code> <code>str</code> 'pdf' or 'ocr' <code>font_variant</code> <code>str</code> Font variant identifier (e.g., 'AAAAAB+') <p>Additional Methods</p> Method Description Parameters Returns <code>font_info()</code> Get detailed font information None <code>dict</code>: Font properties"},{"location":"api/#collections","title":"Collections","text":""},{"location":"api/#elementcollection","title":"ElementCollection","text":"<p>A collection of elements with batch operations.</p> <pre><code>class ElementCollection:\n    \"\"\"\n    A collection of elements with batch operations.\n\n    This class provides operations that can be applied to multiple elements at once.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all elements <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>filter(selector)</code> Filter elements by selector <code>selector</code>: CSS-like selector string <code>ElementCollection</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight all elements <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Attributes to display <code>ElementCollection</code> (self) <code>first</code> Get the first element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>last</code> Get the last element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>highest()</code> Get the highest element on the page None <code>Element</code> or <code>None</code> <code>lowest()</code> Get the lowest element on the page None <code>Element</code> or <code>None</code> <code>leftmost()</code> Get the leftmost element on the page None <code>Element</code> or <code>None</code> <code>rightmost()</code> Get the rightmost element on the page None <code>Element</code> or <code>None</code> <code>__len__()</code> Get the number of elements None <code>int</code> <code>__getitem__(index)</code> Get an element by index <code>index</code>: Index or slice <code>Element</code> or <code>ElementCollection</code>"},{"location":"api/#pagecollection","title":"PageCollection","text":"<p>A collection of pages with cross-page operations.</p> <pre><code>class PageCollection:\n    \"\"\"\n    A collection of pages with cross-page operations.\n\n    This class provides operations that can be applied across multiple pages.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start', new_section_on_page_break=False)</code> Get sections spanning multiple pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries<code>new_section_on_page_break</code>: Whether to start new sections at page breaks <code>list[Region]</code> <code>__len__()</code> Get the number of pages None <code>int</code> <code>__getitem__(index)</code> Get a page by index <code>index</code>: Index or slice <code>Page</code> or <code>PageCollection</code>"},{"location":"api/#ocr-classes","title":"OCR Classes","text":""},{"location":"api/#ocrengine","title":"OCREngine","text":"<p>Base class for OCR engines.</p> <pre><code>class OCREngine:\n    \"\"\"\n    Base class for OCR engines.\n\n    This class provides the interface for OCR engines.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>process_image(image, languages=None, min_confidence=0.0, **kwargs)</code> Process an image with OCR <code>image</code>: PIL Image<code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold <code>list</code>: OCR results"},{"location":"api/#easyocrengine","title":"EasyOCREngine","text":"<p>OCR engine using EasyOCR.</p> <pre><code>class EasyOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using EasyOCR.\n\n    Parameters:\n        model_dir (str, optional): Directory for models. Default: None\n    \"\"\"\n</code></pre>"},{"location":"api/#paddleocrengine","title":"PaddleOCREngine","text":"<p>OCR engine using PaddleOCR.</p> <pre><code>class PaddleOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using PaddleOCR.\n\n    Parameters:\n        use_angle_cls (bool, optional): Use text direction classification. Default: False\n        lang (str, optional): Language code. Default: \"en\"\n        det (bool, optional): Use text detection. Default: True\n        rec (bool, optional): Use text recognition. Default: True\n        cls (bool, optional): Use text direction classification. Default: False\n        det_model_dir (str, optional): Detection model directory. Default: None\n        rec_model_dir (str, optional): Recognition model directory. Default: None\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre>"},{"location":"api/#document-qa-classes","title":"Document QA Classes","text":""},{"location":"api/#documentqa","title":"DocumentQA","text":"<p>Class for document question answering.</p> <pre><code>class DocumentQA:\n    \"\"\"\n    Class for document question answering.\n\n    Parameters:\n        model (str, optional): Model name or path. Default: \"microsoft/layoutlmv3-base\"\n        device (str, optional): Device to use. Default: \"cpu\"\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>ask(question, image, word_boxes, min_confidence=0.0, max_answer_length=None, language=None)</code> Ask a question about a document <code>question</code>: Question to ask<code>image</code>: Document image<code>word_boxes</code>: Text positions<code>min_confidence</code>: Minimum confidence threshold<code>max_answer_length</code>: Maximum answer length<code>language</code>: Language code <code>dict</code>: Result with answer and metadata"},{"location":"api/#selector-syntax","title":"Selector Syntax","text":"<p>Natural PDF uses a CSS-like selector syntax to find elements in PDFs.</p>"},{"location":"api/#basic-selectors","title":"Basic Selectors","text":"Selector Description Example <code>element_type</code> Match elements of this type <code>text</code>, <code>rect</code>, <code>line</code> <code>[attribute=value]</code> Match elements with this attribute value <code>[fontname=Arial]</code>, <code>[size=12]</code> <code>[attribute&gt;=value]</code> Match elements with attribute &gt;= value <code>[size&gt;=12]</code> <code>[attribute&lt;=value]</code> Match elements with attribute &lt;= value <code>[size&lt;=10]</code> <code>[attribute~=value]</code> Match elements with attribute approximately equal <code>[color~=red]</code>, <code>[color~=(1,0,0)]</code> <code>[attribute*=value]</code> Match elements with attribute containing value <code>[fontname*=Arial]</code>"},{"location":"api/#pseudo-classes","title":"Pseudo-Classes","text":"Pseudo-Class Description Example <code>:contains(\"text\")</code> Match elements containing text <code>text:contains(\"Summary\")</code> <code>:starts-with(\"text\")</code> Match elements starting with text <code>text:starts-with(\"Summary\")</code> <code>:ends-with(\"text\")</code> Match elements ending with text <code>text:ends-with(\"2023\")</code> <code>:bold</code> Match bold text <code>text:bold</code> <code>:italic</code> Match italic text <code>text:italic</code>"},{"location":"api/#attribute-names","title":"Attribute Names","text":"Attribute Element Types Description <code>fontname</code> text Font name <code>size</code> text Font size <code>color</code> text, rect, line Color <code>width</code> rect, line Width <code>height</code> rect Height <code>confidence</code> text (OCR) OCR confidence score <code>source</code> text Source ('pdf' or 'ocr') <code>type</code> region Region type (e.g., 'table', 'title') <code>model</code> region Layout model that detected the region <code>font-variant</code> text Font variant identifier"},{"location":"api/#constants-and-configuration","title":"Constants and Configuration","text":""},{"location":"api/#color-names","title":"Color Names","text":"<p>Natural PDF supports color names in selectors.</p> Color Name RGB Value Example <code>red</code> (1, 0, 0) <code>[color~=red]</code> <code>green</code> (0, 1, 0) <code>[color~=green]</code> <code>blue</code> (0, 0, 1) <code>[color~=blue]</code> <code>black</code> (0, 0, 0) <code>[color~=black]</code> <code>white</code> (1, 1, 1) <code>[color~=white]</code>"},{"location":"api/#region-types","title":"Region Types","text":"<p>Layout analysis models detect the following region types:</p> Model Region Types YOLO <code>title</code>, <code>plain-text</code>, <code>table</code>, <code>figure</code>, <code>figure_caption</code>, <code>table_caption</code>, <code>table_footnote</code>, <code>isolate_formula</code>, <code>formula_caption</code>, <code>abandon</code> TATR <code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>"},{"location":"categorizing-documents/","title":"Categorizing Pages and Regions","text":"<p>Natural PDF allows you to automatically categorize pages or specific regions within a page using machine learning models. This is incredibly useful for filtering large collections of documents or understanding the structure and content of individual PDFs.</p>"},{"location":"categorizing-documents/#installation","title":"Installation","text":"<p>To use the classification features, you need to install the optional dependencies:</p> <pre><code>pip install \"natural-pdf[ai]\"\n</code></pre> <p>This installs necessary libraries like <code>torch</code>, <code>transformers</code>, and others.</p>"},{"location":"categorizing-documents/#core-concept-the-classify-method","title":"Core Concept: The <code>.classify()</code> Method","text":"<p>The primary way to perform categorization is using the <code>.classify()</code> method available on <code>Page</code> and <code>Region</code> objects.</p> <pre><code>from natural_pdf import PDF\n\n# Example: Classify a Page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\nlabels = [\"invoice\", \"letter\", \"report cover\", \"data table\"]\npage.classify(labels, using=\"text\")\n\n# Access the top result\nprint(f\"Top Category: {page.category}\")\nprint(f\"Confidence: {page.category_confidence:.3f}\")\n</code></pre> <p>Key Arguments:</p> <ul> <li><code>labels</code> (required): A list of strings representing the potential labels you want to classify the item into.</li> <li><code>using</code> (optional): Specifies which classification model or strategy to use. Defaults to <code>\"text\"</code>.<ul> <li><code>\"text\"</code>: Uses a text-based model (default: <code>facebook/bart-large-mnli</code>) suitable for classifying based on language content.</li> <li><code>\"vision\"</code>: Uses a vision-based model (default: <code>openai/clip-vit-base-patch32</code>) suitable for classifying based on visual layout and appearance.</li> <li>Specific Model ID: You can provide a Hugging Face model ID (e.g., <code>\"google/siglip-base-patch16-224\"</code>, <code>\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"</code>) compatible with zero-shot text or image classification. The library attempts to infer whether it's text or vision, but you might need <code>using</code>.</li> </ul> </li> <li><code>model</code> (optional): Explicitly model ID (HuggingFace repo name)</li> <li><code>min_confidence</code> (optional): A float between 0.0 and 1.0. Only labels with a confidence score greater than or equal to this threshold will be included in the results (default: 0.0).</li> </ul>"},{"location":"categorizing-documents/#text-vs-vision-classification","title":"Text vs. Vision Classification","text":"<p>Choosing the right model type depends on your goal:</p>"},{"location":"categorizing-documents/#text-classification-usingtext","title":"Text Classification (<code>using=\"text\"</code>)","text":"<ul> <li>How it works: Extracts the text from the page or region and analyzes the language content.</li> <li>Best for:<ul> <li>Topic Identification: Determining what a page or section is about (e.g., \"budget discussion,\" \"environmental impact,\" \"legal terms\").</li> <li>Content-Driven Document Types: Identifying document types primarily defined by their text (e.g., emails, meeting minutes, news articles, reports).</li> </ul> </li> <li>Data Journalism Example: You have thousands of pages of government reports. You can use text classification to find all pages discussing \"public health funding\" or classify paragraphs within environmental impact statements to find mentions of specific endangered species.</li> </ul> <pre><code># Find pages related to finance\nfinancial_labels = [\"budget\", \"revenue\", \"expenditure\", \"forecast\"]\npdf.classify_pages(financial_labels, using=\"text\")\nbudget_pages = [p for p in pdf.pages if p.category == \"budget\"]\n</code></pre>"},{"location":"categorizing-documents/#vision-classification-usingvision","title":"Vision Classification (<code>using=\"vision\"</code>)","text":"<ul> <li>How it works: Renders the page or region as an image and analyzes its visual layout, structure, and appearance.</li> <li>Best for:<ul> <li>Layout-Driven Document Types: Identifying documents recognizable by their structure (e.g., invoices, receipts, forms, presentation slides, title pages).</li> <li>Identifying Visual Elements: Distinguishing between pages dominated by text, tables, charts, or images.</li> </ul> </li> <li>Data Journalism Example: You have a scanned archive of campaign finance filings containing various document types. You can use vision classification to quickly isolate all the pages that look like donation receipts or expenditure forms, even if the OCR quality is poor.</li> </ul> <pre><code># Find pages that look like invoices or receipts\nvisual_labels = [\"invoice\", \"receipt\", \"letter\", \"form\"]\npage.classify(visual_labels, using=\"vision\")\nif page.category in [\"invoice\", \"receipt\"]:\n    print(f\"Page {page.number} looks like an invoice or receipt.\")\n</code></pre>"},{"location":"categorizing-documents/#classifying-specific-objects","title":"Classifying Specific Objects","text":""},{"location":"categorizing-documents/#pages-pageclassify","title":"Pages (<code>page.classify(...)</code>)","text":"<p>Classifying a whole page is useful for sorting documents or identifying the overall purpose of a page within a larger document.</p> <pre><code># Classify the first page\npage = pdf.pages[0]\npage_types = [\"cover page\", \"table of contents\", \"chapter start\", \"appendix\"]\npage.classify(page_types, using=\"vision\") # Vision often good for page structure\nprint(f\"Page 1 Type: {page.category}\")\n</code></pre>"},{"location":"categorizing-documents/#regions-regionclassify","title":"Regions (<code>region.classify(...)</code>)","text":"<p>Classifying a specific region allows for more granular analysis within a page. You might first detect regions using Layout Analysis and then classify those regions.</p> <pre><code># Assume layout analysis has run, find paragraphs\nparagraphs = page.find_all(\"region[type=paragraph]\")\nif paragraphs:\n    # Classify the topic of the first paragraph\n    topic_labels = [\"introduction\", \"methodology\", \"results\", \"conclusion\"]\n    # Use text model for topic\n    paragraphs[0].classify(topic_labels, using=\"text\")\n    print(f\"First paragraph category: {paragraphs[0].category}\")\n</code></pre>"},{"location":"categorizing-documents/#accessing-classification-results","title":"Accessing Classification Results","text":"<p>After running <code>.classify()</code>, you can access the results:</p> <ul> <li><code>page.category</code> or <code>region.category</code>: Returns the string label of the category with the highest confidence score from the last classification run. Returns <code>None</code> if no classification has been run or no category met the threshold.</li> <li><code>page.category_confidence</code> or <code>region.category_confidence</code>: Returns the float confidence score (0.0-1.0) for the top category. Returns <code>None</code> otherwise.</li> <li><code>page.classification_results</code> or <code>region.classification_results</code>: Returns the full result dictionary stored in the object's <code>.metadata['classification']</code>, containing the model used, engine type, labels provided, timestamp, and a list of all scores above the threshold sorted by confidence. Returns <code>None</code> if no classification has been run.</li> </ul> <pre><code>results = page.classify([\"invoice\", \"letter\"], using=\"text\", min_confidence=0.5)\n\nif page.category == \"invoice\":\n    print(f\"Found an invoice with confidence {page.category_confidence:.2f}\")\n\n# See all results above the threshold\n# print(page.classification_results['scores'])\n</code></pre>"},{"location":"categorizing-documents/#classifying-collections","title":"Classifying Collections","text":"<p>For batch processing, use the <code>.classify_all()</code> method on <code>PDFCollection</code> or <code>ElementCollection</code> objects. This displays a progress bar tracking individual items (pages or elements).</p>"},{"location":"categorizing-documents/#pdfcollection-collectionclassify_all","title":"PDFCollection (<code>collection.classify_all(...)</code>)","text":"<p>Classifies pages across all PDFs in the collection. Use <code>max_workers</code> for parallel processing across different PDF files.</p> <pre><code>collection = natural_pdf.PDFCollection.from_directory(\"./documents/\")\nlabels = [\"form\", \"datasheet\", \"image\", \"text document\"]\n\n# Classify all pages using vision model, processing 4 PDFs concurrently\ncollection.classify_all(labels, using=\"vision\", max_workers=4)\n\n# Filter PDFs containing forms\nform_pdfs = []\nfor pdf in collection:\n    if any(p.category == \"form\" for p in pdf.pages if p.category):\n        form_pdfs.append(pdf.path)\n    pdf.close() # Remember to close PDFs\n\nprint(f\"Found forms in: {form_pdfs}\")\n</code></pre>"},{"location":"categorizing-documents/#elementcollection-element_collectionclassify_all","title":"ElementCollection (<code>element_collection.classify_all(...)</code>)","text":"<p>Classifies all classifiable elements (currently <code>Page</code> and <code>Region</code>) within the collection.</p> <pre><code># Assume 'pdf' is loaded and 'layout_regions' is an ElementCollection of Regions\nlayout_regions = pdf.find_all(\"region\")\nregion_types = [\"paragraph\", \"list\", \"table\", \"figure\", \"caption\"]\n\n# Classify all detected regions based on vision\nlayout_regions.classify_all(region_types, model=\"vision\")\n\n# Count table regions using filter()\ntable_regions = layout_regions.filter(lambda region: region.category == \"table\")\nprint(f\"Found {len(table_regions)} regions classified as tables.\")\n</code></pre>"},{"location":"data-extraction/","title":"Pulling Structured Data from PDFs","text":"<p>Ever had a pile of invoices, reports, or forms where you need to extract the same pieces of information from each one? That's where structured data extraction shines. Instead of manually copying invoice numbers and dates, you can tell Natural PDF exactly what information you want and let it find those details automatically.</p> <p>You'll need more than the basic install for this: <pre><code># Install the OpenAI (or compatible) client library\npip install openai\n\n# Or pull in the full AI stack (classification, QA, search, etc.)\npip install \"natural_pdf[ai]\"\n</code></pre></p>"},{"location":"data-extraction/#the-simple-approach-just-tell-it-what-you-want","title":"The Simple Approach: Just Tell It What You Want","text":"<p>Don't want to mess around with schemas? Just make a list of what you're looking for:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Extract data using just a list - no schema required!\ndata = page.extract(schema=[\"site\", \"date\", \"violation count\", \"inspector\"]).extracted()\n\nprint(data.site)  # \"ACME Manufacturing Plant\"\nprint(data.date)  # \"2024-03-15\"\nprint(data.violation_count)  # \"3\"\n</code></pre> <p>Natural PDF automatically builds a schema behind the scenes and extracts the data. Each field becomes a string, and you get confidence scores for free:</p> <pre><code># Check how confident the extraction was\nprint(data.site_confidence)  # 0.89\nprint(data.date_confidence)  # 0.95\n</code></pre> <p>This works completely offline - no API keys or internet connection needed. It uses a local document question-answering model that understands both text and layout.</p>"},{"location":"data-extraction/#working-offline-no-internet-required","title":"Working Offline (No Internet Required)","text":"<p>Maybe you're dealing with sensitive documents or just don't want to send everything to the cloud:</p> <pre><code># This works completely offline\npage.extract(schema=[\"company\", \"total\", \"due_date\"])\n</code></pre> <p>The offline engine is pretty smart - it looks at both the text content and how things are visually laid out on the page. For sketchy results, you can set a confidence threshold:</p> <pre><code># Only accept answers the model is confident about\npage.extract(schema=[\"amount\", \"date\"], min_confidence=0.8)\n</code></pre> <p>If an answer falls below your threshold, it gets set to <code>None</code> instead of giving you questionable data.</p> <p>Want to use a local LLM instead? Tools like LM Studio or Msty can run models locally with an OpenAI-compatible API:</p> <pre><code>from openai import OpenAI\n\n# Point to your local LLM server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n\npage.extract(schema=InvoiceSchema, client=client)\n</code></pre> <p>Just heads up - local LLMs are much slower than the document QA approach for simple extractions!</p>"},{"location":"data-extraction/#building-custom-schemas","title":"Building Custom Schemas","text":"<p>For more complex extractions, you can define exactly what you want using Pydantic:</p> <pre><code>from natural_pdf import PDF\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Set up your LLM client (using Gemini here)\nclient = OpenAI(\n    api_key=\"YOUR_API_KEY\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" \n)\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Define exactly what you want to extract\nclass ReportInfo(BaseModel):\n    inspection_number: str = Field(description=\"The main report identifier\")\n    inspection_date: str = Field(description=\"When the inspection happened\")\n    inspection_service: str = Field(description=\"Name of inspection service\")\n    site_name: str = Field(description=\"Location that was inspected\")\n    summary: str = Field(description=\"Visit summary\")\n    violation_count: int = Field(description=\"Number of violations found\")\n\n# Extract the data\npage.extract(schema=ReportInfo, client=client, model=\"gemini-2.5-flash\") \n\n# Get your structured data\nreport_data = page.extracted() \nprint(report_data.inspection_number)\nprint(report_data.violation_count)\n</code></pre>"},{"location":"data-extraction/#managing-multiple-extractions","title":"Managing Multiple Extractions","text":"<ul> <li>Results get stored under the key <code>\"default-structured\"</code> by default</li> <li>Use <code>analysis_key</code> to store multiple different extractions from the same document</li> <li>Trying to extract with an existing key will fail unless you use <code>overwrite=True</code></li> </ul> <pre><code># Extract using a specific key\npage.extract(InvoiceInfo, client=client, analysis_key=\"invoice_header\")\n\n# Access that specific extraction\nheader_data = page.extracted(analysis_key=\"invoice_header\") \ncompany = page.extracted('company_name', analysis_key=\"invoice_header\")\n</code></pre>"},{"location":"data-extraction/#text-vs-vision-extraction","title":"Text vs Vision Extraction","text":"<p>You can choose how to send the document to the LLM:</p> <ul> <li><code>using='text'</code> (default): Sends the text content with layout preserved</li> <li><code>using='vision'</code>: Sends an image of the page</li> </ul> <pre><code># Send text content (faster, cheaper)\npage.extract(schema=MySchema, client=client, using='text')\n\n# Send page image (better for visual layouts)\npage.extract(schema=MySchema, client=client, using='vision')\n</code></pre>"},{"location":"data-extraction/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<p>The extraction methods work on any part of a PDF - regions, pages, collections - making it easy to process lots of documents:</p> <pre><code># Extract from a specific region\nheader_region.extract(InvoiceInfo, client=client)\ncompany = header_region.extracted('company_name')\n\n# Process multiple pages at once\nresults = pdf.pages[:5].apply(\n    lambda page: page.extract(\n        schema=InvoiceInfo, \n        client=client, \n        analysis_key=\"page_invoice_info\"\n    )\n)\n\n# Access results for any page\npdf.pages[0].extracted('company_name', analysis_key=\"page_invoice_info\")\n</code></pre> <p>This approach lets you turn unstructured PDF content into clean, structured data you can actually work with.</p>"},{"location":"describe/","title":"Describe Functionality","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.describe()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.describe() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Describe all elements on the page\npage.find_all('text').describe()\n</pre> # Describe all elements on the page page.find_all('text').describe() Out[2]: In\u00a0[3]: Copied! <pre># Describe all elements on the page\npage.find_all('rect').describe()\n</pre> # Describe all elements on the page page.find_all('rect').describe() Out[3]: In\u00a0[4]: Copied! <pre>page.find_all('text').inspect()\n</pre> page.find_all('text').inspect() Out[4]: In\u00a0[5]: Copied! <pre>page.find_all('line').inspect()\n</pre> page.find_all('line').inspect() Out[5]:"},{"location":"describe/#describe-functionality","title":"Describe Functionality\u00b6","text":"<p>The <code>describe()</code> and <code>inspect()</code> methods provide an easy way to understand the contents of your PDF elements without having to visualize them as images.</p>"},{"location":"describe/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Get a summary of an entire page:</p>"},{"location":"describe/#page-1-summary","title":"Page 1 Summary\u00b6","text":"<p>Elements:</p> <ul> <li>text: 44 elements</li> <li>line: 21 elements</li> <li>rect: 8 elements</li> </ul> <p>Text Analysis:</p> <ul> <li>typography:<ul> <li>fonts:<ul> <li>Helvetica: 44</li> </ul> </li> <li>sizes:<ul> <li>10.0pt: 40</li> <li>8.0pt: 3</li> <li>12.0pt: 1</li> </ul> </li> <li>styles: 9 bold</li> <li>colors:<ul> <li>black: 43</li> <li>other: 1</li> </ul> </li> </ul> </li> </ul>"},{"location":"describe/#element-collection-summaries","title":"Element collection summaries\u00b6","text":"<p>You can describe element collections on a page with <code>.describe()</code>.</p>"},{"location":"describe/#collection-summary-44-elements","title":"Collection Summary (44 elements)\u00b6","text":"<p>Typography:</p> <ul> <li>fonts:<ul> <li>Helvetica: 44</li> </ul> </li> <li>sizes:<ul> <li>10.0pt: 40</li> <li>8.0pt: 3</li> <li>12.0pt: 1</li> </ul> </li> <li>styles: 9 bold</li> <li>colors:<ul> <li>black: 43</li> <li>other: 1</li> </ul> </li> </ul>"},{"location":"describe/#collection-summary-8-elements","title":"Collection Summary (8 elements)\u00b6","text":"<p>Size Stats:</p> <ul> <li>width range: 8-180</li> <li>height range: 8-35</li> <li>avg area: 844 sq pts</li> </ul> <p>Styles:</p> <ul> <li>stroke widths:<ul> <li>0.5: 7</li> </ul> </li> <li>colors:<ul> <li>black: 8</li> </ul> </li> </ul>"},{"location":"describe/#inspecting-lists-of-elements","title":"Inspecting lists of elements\u00b6","text":"<p>For more detail, you can view specific details of element collections with <code>inspect()</code>.</p>"},{"location":"describe/#collection-inspection-44-elements","title":"Collection Inspection (44 elements)\u00b6","text":""},{"location":"describe/#word-elements","title":"Word Elements\u00b6","text":"text x0 top x1 bottom font_family size bold italic source confidence color Jungle Health and Safety Inspection Service 385 36 542 44 Helvetica 8 False False native 1.00 #000000 INS-UP70N51NCL41R 385 46 466 54 Helvetica 8 False False native 1.00 #ff0000 Site: 50 84 74 94 Helvetica 10 True False native 1.00 #000000 Durham\u2019s Meatpacking 74 84 182 94 Helvetica 10 False False native 1.00 #000000 Chicago, Ill. 182 84 235 94 Helvetica 10 False False native 1.00 #000000 Date: 50 104 81 114 Helvetica 10 True False native 1.00 #000000 February 3, 1905 81 104 157 114 Helvetica 10 False False native 1.00 #000000 Violation Count: 50 124 130 134 Helvetica 10 True False native 1.00 #000000 7 130 124 136 134 Helvetica 10 False False native 1.00 #000000 Summary: 50 144 102 154 Helvetica 10 True False native 1.00 #000000 Worst of any, however, were the fertilizer men, an... 102 144 506 154 Helvetica 10 False False native 1.00 #000000 These people could not be shown to the visitor - f... 50 160 512 170 Helvetica 10 False False native 1.00 #000000 visitor at a hundred yards, and as for the other m... 50 176 491 186 Helvetica 10 False False native 1.00 #000000 some of which there were open vats near the level ... 50 192 496 202 Helvetica 10 False False native 1.00 #000000 into the vats; and when they were fished out, ther... 50 208 465 218 Helvetica 10 False False native 1.00 #000000 exhibiting - sometimes they would be overlooked fo... 50 224 492 234 Helvetica 10 False False native 1.00 #000000 to the world as Durham\u2019s Pure Leaf Lard! 50 240 232 250 Helvetica 10 False False native 1.00 #000000 Violations 50 372 107 384 Helvetica 12 True False native 1.00 #000000 Statute 55 398 89 408 Helvetica 10 True False native 1.00 #000000 Description 105 398 160 408 Helvetica 10 True False native 1.00 #000000 Level 455 398 481 408 Helvetica 10 True False native 1.00 #000000 Repeat? 505 398 544 408 Helvetica 10 True False native 1.00 #000000 4.12.7 55 418 83 428 Helvetica 10 False False native 1.00 #000000 Unsanitary Working Conditions. 105 418 245 428 Helvetica 10 False False native 1.00 #000000 Critical 455 418 486 428 Helvetica 10 False False native 1.00 #000000 5.8.3 55 438 77 448 Helvetica 10 False False native 1.00 #000000 Inadequate Protective Equipment. 105 438 256 448 Helvetica 10 False False native 1.00 #000000 Serious 455 438 489 448 Helvetica 10 False False native 1.00 #000000 6.3.9 55 458 77 468 Helvetica 10 False False native 1.00 #000000 Ineffective Injury Prevention. 105 458 231 468 Helvetica 10 False False native 1.00 #000000 <p>Showing 30 of 44 elements (pass limit= to see more)</p>"},{"location":"describe/#collection-inspection-21-elements","title":"Collection Inspection (21 elements)\u00b6","text":""},{"location":"describe/#line-elements","title":"Line Elements\u00b6","text":"x0 top x1 bottom width is_horizontal is_vertical 50 352 550 352 2 True False 50 392 550 392 0 True False 50 392 50 552 0 False True 100 392 100 552 0 False True 450 392 450 552 0 False True 500 392 500 552 0 False True 550 392 550 552 0 False True 50 412 550 412 0 True False 520 418 528 426 0 False False 520 418 528 426 0 False False 50 432 550 432 0 True False 520 438 528 446 0 False False 520 438 528 446 0 False False 50 452 550 452 0 True False 50 472 550 472 0 True False 50 492 550 492 0 True False 50 512 550 512 0 True False 520 518 528 526 0 False False 520 518 528 526 0 False False 50 532 550 532 0 True False 50 552 550 552 0 True False"},{"location":"describe/","title":"Describe Functionality","text":"<p>The <code>describe()</code> and <code>inspect()</code> methods provide an easy way to understand the contents of your PDF elements without having to visualize them as images.</p>"},{"location":"describe/#basic-usage","title":"Basic Usage","text":"<p>Get a summary of an entire page:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.describe()\n</code></pre>"},{"location":"describe/#element-collection-summaries","title":"Element collection summaries","text":"<p>You can describe element collections on a page with <code>.describe()</code>.</p> <pre><code># Describe all elements on the page\npage.find_all('text').describe()\n</code></pre> <pre><code># Describe all elements on the page\npage.find_all('rect').describe()\n</code></pre>"},{"location":"describe/#inspecting-lists-of-elements","title":"Inspecting lists of elements","text":"<p>For more detail, you can view specific details of element collections with <code>inspect()</code>.</p> <pre><code>page.find_all('text').inspect()\n</code></pre> <pre><code>page.find_all('line').inspect()\n</code></pre>"},{"location":"document-qa/","title":"Document Question Answering","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n\n# Display the first page \npage = pdf.pages[0]\npage.show()\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")  # Display the first page  page = pdf.pages[0] page.show() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Ask a question about the entire document\npage.ask(\"How many votes did Harris and Waltz get?\")\n</pre> # Ask a question about the entire document page.ask(\"How many votes did Harris and Waltz get?\") <pre>Device set to use mps:0\n</pre> Out[2]: <pre>{'answer': '148',\n 'confidence': 0.9995507001876831,\n 'start': 20,\n 'end': 20,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre>page.ask(\"Who got the most votes for Attorney General?\")\n</pre> page.ask(\"Who got the most votes for Attorney General?\") Out[3]: <pre>{'answer': 'DEM EUGENE DEPASQUALE',\n 'confidence': 0.9180722236633301,\n 'start': 63,\n 'end': 63,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre>page.ask(\"Who was the Republican candidate for Attorney General?\")\n</pre> page.ask(\"Who was the Republican candidate for Attorney General?\") Out[4]: <pre>{'answer': 'LIB ROBERT COWBURN',\n 'confidence': 0.21592436730861664,\n 'start': 67,\n 'end': 67,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[5]: Copied! <pre># Get a specific page\nregion = page.find('text:contains(\"Attorney General\")').below()\nregion.show()\n</pre> # Get a specific page region = page.find('text:contains(\"Attorney General\")').below() region.show() Out[5]: In\u00a0[6]: Copied! <pre>region.ask(\"How many write-in votes were cast?\")\n</pre> region.ask(\"How many write-in votes were cast?\") Out[6]: <pre>{'answer': '498',\n 'confidence': 0.9988918304443359,\n 'start': 17,\n 'end': 17,\n 'found': True,\n 'region': &lt;Region bbox=(0, 553.663, 612, 792)&gt;,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\n\nquestions = [\n    \"How many votes did Harris and Walz get?\",\n    \"How many votes did Trump get?\",\n    \"How many votes did Natural PDF get?\",\n    \"What was the date of this form?\"\n]\n\n# You can actually do this but with multiple questions\n# in the model itself buuuut Natural PDF can'd do it yet\nresults = [page.ask(q) for q in questions]\n\ndf = pd.json_normalize(results)\ndf.insert(0, 'question', questions)\ndf\n</pre> import pandas as pd  questions = [     \"How many votes did Harris and Walz get?\",     \"How many votes did Trump get?\",     \"How many votes did Natural PDF get?\",     \"What was the date of this form?\" ]  # You can actually do this but with multiple questions # in the model itself buuuut Natural PDF can'd do it yet results = [page.ask(q) for q in questions]  df = pd.json_normalize(results) df.insert(0, 'question', questions) df Out[7]: question answer confidence start end found page_num source_elements 0 How many votes did Harris and Walz get? 148 0.999671 20 20 True 0 [&lt;TextElement text='148' font='Helvetica' size... 1 How many votes did Trump get? 348 0.310203 22 22 True 0 [&lt;TextElement text='348' font='Helvetica' size... 2 How many votes did Natural PDF get? November 5, 2024 0.237136 3 3 True 0 [&lt;TextElement text='November 5...' font='Helve... 3 What was the date of this form? November 5, 2024 0.792696 3 3 True 0 [&lt;TextElement text='November 5...' font='Helve... In\u00a0[8]: Copied! <pre>result = page.ask(\"Who got the most votes for Attorney General?\")\n\n# See the answer\nprint(result.answer)  # \"John Smith\"\n\n# Show exactly where it found that answer\nresult.show()\n</pre> result = page.ask(\"Who got the most votes for Attorney General?\")  # See the answer print(result.answer)  # \"John Smith\"  # Show exactly where it found that answer result.show() <pre>DEM EUGENE DEPASQUALE\n</pre> Out[8]: <p>The <code>result.show()</code> method highlights the specific text elements the model used to answer your question - super helpful for debugging or when you need to double-check the results.</p> <p>You can also access result data like a normal dictionary or use dot notation if you prefer:</p> In\u00a0[9]: Copied! <pre># Both of these work the same way\nprint(result[\"confidence\"])  # 0.97\nprint(result.confidence)     # 0.97\n</pre> # Both of these work the same way print(result[\"confidence\"])  # 0.97 print(result.confidence)     # 0.97 <pre>0.9180722236633301\n0.9180722236633301\n</pre> <p>If the model couldn't find a confident answer, <code>result.found</code> will be <code>False</code> and calling <code>result.show()</code> will let you know there's nothing to visualize.</p>"},{"location":"document-qa/#document-question-answering","title":"Document Question Answering\u00b6","text":"<p>Natural PDF includes document QA functionality that allows you to ask natural language questions about your PDFs and get relevant answers. This feature uses LayoutLM models to understand both the text content and the visual layout of your documents.</p>"},{"location":"document-qa/#setup","title":"Setup\u00b6","text":"<p>Let's start by loading a sample PDF to experiment with question answering.</p>"},{"location":"document-qa/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here's how to ask questions to a PDF page:</p>"},{"location":"document-qa/#asking-questions-to-part-of-a-page-questions","title":"Asking questions to part of a page questions\u00b6","text":"<p>You can also ask questions to a specific region of a page*:</p>"},{"location":"document-qa/#asking-multiple-questions","title":"Asking multiple questions\u00b6","text":""},{"location":"document-qa/#visualizing-where-answers-come-from","title":"Visualizing where answers come from\u00b6","text":"<p>Sometimes you'll want to see exactly where the model found an answer in your document. Maybe you're checking if it grabbed the right table cell, or you want to verify it didn't confuse similar-looking sections.</p>"},{"location":"document-qa/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you've learned about document QA, explore:</p> <ul> <li>Element Selection: Find specific elements to focus your questions.</li> <li>Layout Analysis: Automatically detect document structure.</li> <li>Working with Regions: Define custom areas for targeted questioning.</li> <li>Text Extraction: Extract and preprocess text before QA.</li> </ul>"},{"location":"element-selection/","title":"Finding What You Need in PDFs","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Find the first text element containing \"Summary\"\nsummary_text = page.find('text:contains(\"Summary\")')\nsummary_text\n</pre> # Find the first text element containing \"Summary\" summary_text = page.find('text:contains(\"Summary\")') summary_text Out[2]: <pre>&lt;TextElement text='Summary: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 144.07000000000005, 101.68, 154.07000000000005)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all text elements containing \"Inadequate\"\ncontains_inadequate = page.find_all('text:contains(\"Inadequate\")')\nlen(contains_inadequate)\n</pre> # Find all text elements containing \"Inadequate\" contains_inadequate = page.find_all('text:contains(\"Inadequate\")') len(contains_inadequate) Out[3]: <pre>2</pre> In\u00a0[4]: Copied! <pre># Let's see what we found\nsummary_text.highlight(label='summary')\ncontains_inadequate.highlight(label=\"inadequate\")\npage.to_image(width=700)\n</pre> # Let's see what we found summary_text.highlight(label='summary') contains_inadequate.highlight(label=\"inadequate\") page.to_image(width=700) Out[4]: In\u00a0[5]: Copied! <pre># Find all text elements\nall_text = page.find_all('text')\nlen(all_text)\n</pre> # Find all text elements all_text = page.find_all('text') len(all_text) Out[5]: <pre>44</pre> In\u00a0[6]: Copied! <pre># Find all rectangle elements\nall_rects = page.find_all('rect')\nlen(all_rects)\n</pre> # Find all rectangle elements all_rects = page.find_all('rect') len(all_rects) Out[6]: <pre>8</pre> In\u00a0[7]: Copied! <pre># Find all line elements\nall_lines = page.find_all('line')\nlen(all_lines)\n</pre> # Find all line elements all_lines = page.find_all('line') len(all_lines) Out[7]: <pre>21</pre> In\u00a0[8]: Copied! <pre># Show where all the lines are\npage.find_all('line').show()\n</pre> # Show where all the lines are page.find_all('line').show() Out[8]: In\u00a0[9]: Copied! <pre># Find large text (probably headings)\npage.find_all('text[size&gt;=11]')\n</pre> # Find large text (probably headings) page.find_all('text[size&gt;=11]') Out[9]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[10]: Copied! <pre># Find text that uses Helvetica font\npage.find_all('text[fontname*=Helvetica]')\n</pre> # Find text that uses Helvetica font page.find_all('text[fontname*=Helvetica]') Out[10]: <pre>&lt;ElementCollection[TextElement](count=44)&gt;</pre> In\u00a0[11]: Copied! <pre># Find red text in this PDF\nred_text = page.find_all('text[color~=red]')\nred_text.show()\n</pre> # Find red text in this PDF red_text = page.find_all('text[color~=red]') red_text.show() Out[11]: In\u00a0[12]: Copied! <pre># Find thick lines (might be important borders)\npage.find_all('line[width&gt;=2]')\n</pre> # Find thick lines (might be important borders) page.find_all('line[width&gt;=2]') Out[12]: <pre>&lt;ElementCollection[LineElement](count=1)&gt;</pre> In\u00a0[13]: Copied! <pre># Find bold text (probably important)\npage.find_all('text:bold').show()\n</pre> # Find bold text (probably important) page.find_all('text:bold').show() Out[13]: In\u00a0[14]: Copied! <pre># Combine filters: large bold text (definitely headings)\npage.find_all('text[size&gt;=11]:bold')\n</pre> # Combine filters: large bold text (definitely headings) page.find_all('text[size&gt;=11]:bold') Out[14]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[15]: Copied! <pre># Find all text that's NOT bold\nnon_bold_text = page.find_all('text:not(:bold)')\n\n# Find all elements that are NOT tables  \nnot_tables = page.find_all(':not(region[type=table])')\n\n# Find text that doesn't contain \"Total\"\nrelevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)\n\n# Find text that isn't empty\nnon_empty_text = page.find_all('text:not(:empty)')\n</pre> # Find all text that's NOT bold non_bold_text = page.find_all('text:not(:bold)')  # Find all elements that are NOT tables   not_tables = page.find_all(':not(region[type=table])')  # Find text that doesn't contain \"Total\" relevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)  # Find text that isn't empty non_empty_text = page.find_all('text:not(:empty)') In\u00a0[16]: Copied! <pre># First, find a thick horizontal line\nref_line = page.find('line[width&gt;=2]')\n\n# Now find text that's above that line\ntext_above_line = page.find_all('text:above(\"line[width&gt;=2]\")')\ntext_above_line\n</pre> # First, find a thick horizontal line ref_line = page.find('line[width&gt;=2]')  # Now find text that's above that line text_above_line = page.find_all('text:above(\"line[width&gt;=2]\")') text_above_line Out[16]: <pre>&lt;ElementCollection[TextElement](count=17)&gt;</pre> In\u00a0[17]: Copied! <pre># Case-insensitive search\npage.find_all('text:contains(\"summary\")', case=False)\n</pre> # Case-insensitive search page.find_all('text:contains(\"summary\")', case=False) Out[17]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[18]: Copied! <pre># Regular expression search (for patterns like inspection IDs)\npage.find_all('text:contains(\"INS-\\\\w+\")', regex=True)\n</pre> # Regular expression search (for patterns like inspection IDs) page.find_all('text:contains(\"INS-\\\\w+\")', regex=True) Out[18]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[19]: Copied! <pre># Combine regex with case-insensitivity\npage.find_all('text:contains(\"jungle health\")', regex=True, case=False)\n</pre> # Combine regex with case-insensitivity page.find_all('text:contains(\"jungle health\")', regex=True, case=False) Out[19]: <pre>&lt;ElementCollection[TextElement](count=2)&gt;</pre> In\u00a0[20]: Copied! <pre># Get all headings (large, bold text)\nheadings = page.find_all('text[size&gt;=11]:bold')\nheadings\n</pre> # Get all headings (large, bold text) headings = page.find_all('text[size&gt;=11]:bold') headings Out[20]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[21]: Copied! <pre># Get the first and last heading in reading order\nfirst = headings.first\nlast = headings.last\n(first, last)\n</pre> # Get the first and last heading in reading order first = headings.first last = headings.last (first, last) Out[21]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[22]: Copied! <pre># Get the physically highest/lowest element\nhighest = headings.highest()\nlowest = headings.lowest()\n(highest, lowest)\n</pre> # Get the physically highest/lowest element highest = headings.highest() lowest = headings.lowest() (highest, lowest) Out[22]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[23]: Copied! <pre># Filter the collection further\nservice_headings = headings.filter(lambda heading: 'Service' in heading.extract_text())\n</pre> # Filter the collection further service_headings = headings.filter(lambda heading: 'Service' in heading.extract_text()) In\u00a0[24]: Copied! <pre># Extract text from all elements at once\nheadings.extract_text()\n</pre> # Extract text from all elements at once headings.extract_text() Out[24]: <pre>'Violations'</pre> <p>Note: <code>.highest()</code>, <code>.lowest()</code>, etc. will complain if your collection spans multiple pages.</p> In\u00a0[25]: Copied! <pre># Find text with specific font variants (if they exist)\npage.find_all('text[font-variant=AAAAAB]')\n</pre> # Find text with specific font variants (if they exist) page.find_all('text[font-variant=AAAAAB]') Out[25]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre> In\u00a0[26]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/geometry.pdf\")\npage = pdf.pages[0]\n\nrect = page.find('rect')\nrect.show(width=500)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/geometry.pdf\") page = pdf.pages[0]  rect = page.find('rect') rect.show(width=500) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[26]:"},{"location":"element-selection/#finding-what-you-need-in-pdfs","title":"Finding What You Need in PDFs\u00b6","text":"<p>Finding specific content in PDFs is like being a detective - you need the right tools to hunt down exactly what you're looking for. Natural PDF uses CSS-like selectors to help you find text, lines, images, and other elements in your documents. Think of it like using browser developer tools, but for PDFs.</p>"},{"location":"element-selection/#setup","title":"Setup\u00b6","text":"<p>Let's load up a sample PDF to experiment with. This one has various elements we can practice finding.</p>"},{"location":"element-selection/#the-basics-finding-elements","title":"The Basics: Finding Elements\u00b6","text":"<p>You have two main tools: <code>find()</code> (gets the first match) and <code>find_all()</code> (gets everything that matches). The basic pattern is <code>element_type[attribute_filter]:pseudo_class</code>.</p>"},{"location":"element-selection/#finding-text-by-what-it-says","title":"Finding Text by What It Says\u00b6","text":""},{"location":"element-selection/#finding-different-types-of-elements","title":"Finding Different Types of Elements\u00b6","text":"<p>PDFs contain more than just text - there are rectangles, lines, images, and other shapes.</p>"},{"location":"element-selection/#filtering-by-properties","title":"Filtering by Properties\u00b6","text":"<p>Use square brackets <code>[]</code> to filter elements by their characteristics - size, color, font, etc.</p>"},{"location":"element-selection/#common-properties-you-can-filter-on","title":"Common Properties You Can Filter On\u00b6","text":"Property Example Usage What It Does Notes <code>size</code> (text) <code>text[size&gt;=12]</code> Font size in points Use <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> <code>fontname</code> <code>text[fontname*=Bold]</code> Font family name <code>*=</code> means \"contains\" <code>color</code> (text) <code>text[color~=red]</code> Text color <code>~=</code> for approximate match <code>width</code> (line) <code>line[width&gt;1]</code> Line thickness Useful for finding borders <code>source</code> <code>text[source=ocr]</code> Where text came from <code>pdf</code>, <code>ocr</code>, or <code>detected</code> <code>type</code> (region) <code>region[type=table]</code> Layout analysis result From layout detection models"},{"location":"element-selection/#using-special-conditions-pseudo-classes","title":"Using Special Conditions (Pseudo-Classes)\u00b6","text":"<p>These are powerful filters that let you find elements based on their content or relationship to other elements.</p>"},{"location":"element-selection/#common-pseudo-classes","title":"Common Pseudo-Classes\u00b6","text":"Pseudo-Class Example What It Finds <code>:contains('text')</code> <code>text:contains('Report')</code> Elements containing specific text <code>:bold</code> <code>text:bold</code> Bold text (detected automatically) <code>:italic</code> <code>text:italic</code> Italic text <code>:below(selector)</code> <code>text:below('line[width&gt;=2]')</code> Elements below another element <code>:above(selector)</code> <code>text:above('text:contains(\"Summary\")')</code> Elements above another element <code>:near(selector)</code> <code>text:near('image')</code> Elements close to another element <p>Spatial pseudo-classes like <code>:below</code> and <code>:above</code> work based on the first element that matches the inner selector.</p>"},{"location":"element-selection/#excluding-things-with-not","title":"Excluding Things with <code>:not()</code>\u00b6","text":"<p>Sometimes it's easier to say what you don't want than what you do want.</p>"},{"location":"element-selection/#finding-things-relative-to-other-things","title":"Finding Things Relative to Other Things\u00b6","text":"<p>This is super useful when you know the structure of your document.</p>"},{"location":"element-selection/#advanced-text-searching","title":"Advanced Text Searching\u00b6","text":"<p>When you need more control over how text matching works:</p>"},{"location":"element-selection/#working-with-groups-of-elements","title":"Working with Groups of Elements\u00b6","text":"<p><code>find_all()</code> returns an <code>ElementCollection</code> - like a list, but with PDF-specific superpowers.</p>"},{"location":"element-selection/#dealing-with-weird-font-names","title":"Dealing with Weird Font Names\u00b6","text":"<p>PDFs sometimes have bizarre font names that don't look like normal fonts. Don't worry - they're usually normal fonts with weird internal names.</p>"},{"location":"element-selection/#testing-relationships-between-elements","title":"Testing Relationships Between Elements\u00b6","text":"<p>Want to see how elements relate to each other spatially? Let's try a different PDF:</p>"},{"location":"extracting-clean-text/","title":"Extract Clean Text Without Headers and Footers","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find and exclude the header (top 10% of page)\nheader_region = page.create_region(0, 0, page.width, page.height * 0.1)\npage.add_exclusion(header_region)\n\n# Find and exclude footer (bottom 10% of page)  \nfooter_region = page.create_region(0, page.height * 0.9, page.width, page.height)\npage.add_exclusion(footer_region)\n\n# Now extract clean text\nclean_text = page.extract_text()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find and exclude the header (top 10% of page) header_region = page.create_region(0, 0, page.width, page.height * 0.1) page.add_exclusion(header_region)  # Find and exclude footer (bottom 10% of page)   footer_region = page.create_region(0, page.height * 0.9, page.width, page.height) page.add_exclusion(footer_region)  # Now extract clean text clean_text = page.extract_text() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> In\u00a0[2]: Copied! <pre># Exclude anything containing \"CONFIDENTIAL\"\nconfidential = page.find('text:contains(\"CONFIDENTIAL\")')\nif confidential:\n    page.add_exclusion(confidential.above())  # Everything above it\n\n# Exclude page numbers (usually small text with just numbers)\npage_nums = page.find_all('text:contains(\"^\\\\d+$\")', regex=True)\nfor num in page_nums:\n    page.add_exclusion(num)\n\n# Exclude elements by position (like top-right logos)\ntop_right = page.create_region(page.width * 0.7, 0, page.width, page.height * 0.15)\npage.add_exclusion(top_right)\n</pre> # Exclude anything containing \"CONFIDENTIAL\" confidential = page.find('text:contains(\"CONFIDENTIAL\")') if confidential:     page.add_exclusion(confidential.above())  # Everything above it  # Exclude page numbers (usually small text with just numbers) page_nums = page.find_all('text:contains(\"^\\\\d+$\")', regex=True) for num in page_nums:     page.add_exclusion(num)  # Exclude elements by position (like top-right logos) top_right = page.create_region(page.width * 0.7, 0, page.width, page.height * 0.15) page.add_exclusion(top_right) Out[2]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[3]: Copied! <pre># Define exclusions that adapt to each page\ndef exclude_header(page):\n    # Top 50 points of every page\n    return page.create_region(0, 0, page.width, 50)\n\ndef exclude_footer(page):\n    # Bottom 30 points of every page  \n    return page.create_region(0, page.height - 30, page.width, page.height)\n\ndef exclude_watermark(page):\n    # Find \"DRAFT\" watermark if it exists\n    draft = page.find('text:contains(\"DRAFT\")')\n    return draft.create_region() if draft else None\n\n# Apply to entire PDF\npdf.add_exclusion(exclude_header, label=\"Headers\")\npdf.add_exclusion(exclude_footer, label=\"Footers\") \npdf.add_exclusion(exclude_watermark, label=\"Watermarks\")\n\n# Extract clean text from any page\nclean_text = pdf.pages[0].extract_text()  # Headers/footers automatically excluded\n</pre> # Define exclusions that adapt to each page def exclude_header(page):     # Top 50 points of every page     return page.create_region(0, 0, page.width, 50)  def exclude_footer(page):     # Bottom 30 points of every page       return page.create_region(0, page.height - 30, page.width, page.height)  def exclude_watermark(page):     # Find \"DRAFT\" watermark if it exists     draft = page.find('text:contains(\"DRAFT\")')     return draft.create_region() if draft else None  # Apply to entire PDF pdf.add_exclusion(exclude_header, label=\"Headers\") pdf.add_exclusion(exclude_footer, label=\"Footers\")  pdf.add_exclusion(exclude_watermark, label=\"Watermarks\")  # Extract clean text from any page clean_text = pdf.pages[0].extract_text()  # Headers/footers automatically excluded In\u00a0[4]: Copied! <pre># Apply OCR\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Only use high-confidence OCR text\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = reliable_text.extract_text()\n\n# Or exclude low-confidence noise\nnoisy_text = page.find_all('text[source=ocr][confidence&lt;0.5]')\nfor noise in noisy_text:\n    page.add_exclusion(noise)\n</pre> # Apply OCR page.apply_ocr(engine='easyocr', languages=['en'])  # Only use high-confidence OCR text reliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]') clean_text = reliable_text.extract_text()  # Or exclude low-confidence noise noisy_text = page.find_all('text[source=ocr][confidence&lt;0.5]') for noise in noisy_text:     page.add_exclusion(noise) <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> In\u00a0[5]: Copied! <pre># Extract just the main content column (avoiding sidebars)\nmain_column = page.create_region(\n    x0=page.width * 0.1,      # Start 10% from left\n    top=page.height * 0.15,   # Skip header area\n    x1=page.width * 0.7,      # End before sidebar\n    bottom=page.height * 0.9   # Stop before footer\n)\n\ncolumn_text = main_column.extract_text()\n</pre> # Extract just the main content column (avoiding sidebars) main_column = page.create_region(     x0=page.width * 0.1,      # Start 10% from left     top=page.height * 0.15,   # Skip header area     x1=page.width * 0.7,      # End before sidebar     bottom=page.height * 0.9   # Stop before footer )  column_text = main_column.extract_text() In\u00a0[6]: Copied! <pre># Highlight what you're about to exclude\nheader = page.create_region(0, 0, page.width, 50)\nfooter = page.create_region(0, page.height - 30, page.width, page.height)\n\nheader.highlight(color=\"red\", label=\"Will exclude\")\nfooter.highlight(color=\"red\", label=\"Will exclude\") \n\n# Show the page to verify\npage.show()\n\n# If it looks right, apply the exclusions\npage.add_exclusion(header)\npage.add_exclusion(footer)\n</pre> # Highlight what you're about to exclude header = page.create_region(0, 0, page.width, 50) footer = page.create_region(0, page.height - 30, page.width, page.height)  header.highlight(color=\"red\", label=\"Will exclude\") footer.highlight(color=\"red\", label=\"Will exclude\")   # Show the page to verify page.show()  # If it looks right, apply the exclusions page.add_exclusion(header) page.add_exclusion(footer) Out[6]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[7]: Copied! <pre># Extract with and without exclusions to see the difference\nfull_text = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text(use_exclusions=True)\n\nprint(f\"Original: {len(full_text)} characters\")\nprint(f\"Clean: {len(clean_text)} characters\")\nprint(f\"Removed: {len(full_text) - len(clean_text)} characters\")\n</pre> # Extract with and without exclusions to see the difference full_text = page.extract_text(use_exclusions=False) clean_text = page.extract_text(use_exclusions=True)  print(f\"Original: {len(full_text)} characters\") print(f\"Clean: {len(clean_text)} characters\") print(f\"Removed: {len(full_text) - len(clean_text)} characters\") <pre>Original: 2247 characters\nClean: 2247 characters\nRemoved: 0 characters\n</pre> In\u00a0[8]: Copied! <pre># Remove headers with logos and contact info\npage.add_exclusion(page.create_region(0, 0, page.width, 80))\n\n# Remove footers with page numbers and dates\npage.add_exclusion(page.create_region(0, page.height - 40, page.width, page.height))\n</pre> # Remove headers with logos and contact info page.add_exclusion(page.create_region(0, 0, page.width, 80))  # Remove footers with page numbers and dates page.add_exclusion(page.create_region(0, page.height - 40, page.width, page.height)) Out[8]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[9]: Copied! <pre># Remove running headers with paper title\nheader = page.find('text[size&lt;=10]').above() if page.find('text[size&lt;=10]') else None\nif header:\n    page.add_exclusion(header)\n\n# Remove footnotes (small text at bottom)\nfootnotes = page.find_all('text[size&lt;=8]')\nfor note in footnotes:\n    if note.top &gt; page.height * 0.8:  # Bottom 20% of page\n        page.add_exclusion(note)\n</pre> # Remove running headers with paper title header = page.find('text[size&lt;=10]').above() if page.find('text[size&lt;=10]') else None if header:     page.add_exclusion(header)  # Remove footnotes (small text at bottom) footnotes = page.find_all('text[size&lt;=8]') for note in footnotes:     if note.top &gt; page.height * 0.8:  # Bottom 20% of page         page.add_exclusion(note) In\u00a0[10]: Copied! <pre># Remove classification markings\nclassifications = page.find_all('text:contains(\"CONFIDENTIAL|SECRET|UNCLASSIFIED\")', regex=True)\nfor mark in classifications:\n    page.add_exclusion(mark)\n\n# Remove agency headers\nagency_header = page.find('text:contains(\"Department of|Agency|Office of\")', regex=True)\nif agency_header:\n    page.add_exclusion(agency_header.above())\n</pre> # Remove classification markings classifications = page.find_all('text:contains(\"CONFIDENTIAL|SECRET|UNCLASSIFIED\")', regex=True) for mark in classifications:     page.add_exclusion(mark)  # Remove agency headers agency_header = page.find('text:contains(\"Department of|Agency|Office of\")', regex=True) if agency_header:     page.add_exclusion(agency_header.above())"},{"location":"extracting-clean-text/#extract-clean-text-without-headers-and-footers","title":"Extract Clean Text Without Headers and Footers\u00b6","text":"<p>You've got a PDF where you need the main content, but every page has headers, footers, watermarks, or other junk that's messing up your text extraction. Here's how to get just the content you want.</p>"},{"location":"extracting-clean-text/#the-problem","title":"The Problem\u00b6","text":"<p>PDFs often have repeated content on every page that you don't want:</p> <ul> <li>Company headers with logos and contact info</li> <li>Page numbers and footers</li> <li>\"CONFIDENTIAL\" watermarks</li> <li>Navigation elements from web-to-PDF conversions</li> </ul> <p>When you extract text normally, all this noise gets mixed in with your actual content.</p>"},{"location":"extracting-clean-text/#quick-solution-exclude-by-pattern","title":"Quick Solution: Exclude by Pattern\u00b6","text":"<p>If the unwanted content is consistent across pages, you can exclude it once:</p>"},{"location":"extracting-clean-text/#exclude-specific-elements","title":"Exclude Specific Elements\u00b6","text":"<p>For more precision, exclude specific text or elements:</p>"},{"location":"extracting-clean-text/#apply-exclusions-to-all-pages","title":"Apply Exclusions to All Pages\u00b6","text":"<p>Set up exclusions that work across your entire document:</p>"},{"location":"extracting-clean-text/#remove-noise-from-scanned-documents","title":"Remove Noise from Scanned Documents\u00b6","text":"<p>For scanned PDFs, apply OCR first, then filter by confidence:</p>"},{"location":"extracting-clean-text/#handle-multi-column-layouts","title":"Handle Multi-Column Layouts\u00b6","text":"<p>Extract text from specific columns or sections:</p>"},{"location":"extracting-clean-text/#visual-debugging","title":"Visual Debugging\u00b6","text":"<p>See what you're excluding before committing:</p>"},{"location":"extracting-clean-text/#compare-before-and-after","title":"Compare Before and After\u00b6","text":""},{"location":"extracting-clean-text/#common-patterns","title":"Common Patterns\u00b6","text":""},{"location":"extracting-clean-text/#corporate-reports","title":"Corporate Reports\u00b6","text":""},{"location":"extracting-clean-text/#academic-papers","title":"Academic Papers\u00b6","text":""},{"location":"extracting-clean-text/#government-documents","title":"Government Documents\u00b6","text":""},{"location":"extracting-clean-text/#when-things-go-wrong","title":"When Things Go Wrong\u00b6","text":"<ul> <li>Problem: Headers vary between pages</li> <li>Solution: Use adaptive exclusions</li> </ul> <pre>def smart_header_exclusion(page):\n    # Look for common header patterns\n    logo = page.find('image')\n    company_name = page.find('text:contains(\"ACME Corp\")')\n    \n    if logo:\n        return logo.above()\n    elif company_name and company_name.top &lt; page.height * 0.2:\n        return company_name.above()\n    else:\n        return page.create_region(0, 0, page.width, 60)  # Fallback\n\npdf.add_exclusion(smart_header_exclusion)\n</pre> <ul> <li>Problem: Need to preserve some header information</li> <li>Solution: Extract before excluding</li> </ul> <pre># Get the document title from the header first\ntitle = page.find('text[size&gt;=14]:bold')\ndocument_title = title.text if title else \"Unknown\"\n\n# Then exclude the header for clean body text\npage.add_exclusion(page.create_region(0, 0, page.width, 100))\nbody_text = page.extract_text()\n</pre>"},{"location":"extracting-clean-text/","title":"Extract Clean Text Without Headers and Footers","text":"<p>You've got a PDF where you need the main content, but every page has headers, footers, watermarks, or other junk that's messing up your text extraction. Here's how to get just the content you want.</p>"},{"location":"extracting-clean-text/#the-problem","title":"The Problem","text":"<p>PDFs often have repeated content on every page that you don't want: - Company headers with logos and contact info - Page numbers and footers - \"CONFIDENTIAL\" watermarks - Navigation elements from web-to-PDF conversions</p> <p>When you extract text normally, all this noise gets mixed in with your actual content.</p>"},{"location":"extracting-clean-text/#quick-solution-exclude-by-pattern","title":"Quick Solution: Exclude by Pattern","text":"<p>If the unwanted content is consistent across pages, you can exclude it once:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find and exclude the header (top 10% of page)\nheader_region = page.create_region(0, 0, page.width, page.height * 0.1)\npage.add_exclusion(header_region)\n\n# Find and exclude footer (bottom 10% of page)  \nfooter_region = page.create_region(0, page.height * 0.9, page.width, page.height)\npage.add_exclusion(footer_region)\n\n# Now extract clean text\nclean_text = page.extract_text()\n</code></pre>"},{"location":"extracting-clean-text/#exclude-specific-elements","title":"Exclude Specific Elements","text":"<p>For more precision, exclude specific text or elements:</p> <pre><code># Exclude anything containing \"CONFIDENTIAL\"\nconfidential = page.find('text:contains(\"CONFIDENTIAL\")')\nif confidential:\n    page.add_exclusion(confidential.above())  # Everything above it\n\n# Exclude page numbers (usually small text with just numbers)\npage_nums = page.find_all('text:contains(\"^\\\\d+$\")', regex=True)\nfor num in page_nums:\n    page.add_exclusion(num)\n\n# Exclude elements by position (like top-right logos)\ntop_right = page.create_region(page.width * 0.7, 0, page.width, page.height * 0.15)\npage.add_exclusion(top_right)\n</code></pre>"},{"location":"extracting-clean-text/#apply-exclusions-to-all-pages","title":"Apply Exclusions to All Pages","text":"<p>Set up exclusions that work across your entire document:</p> <pre><code># Define exclusions that adapt to each page\ndef exclude_header(page):\n    # Top 50 points of every page\n    return page.create_region(0, 0, page.width, 50)\n\ndef exclude_footer(page):\n    # Bottom 30 points of every page  \n    return page.create_region(0, page.height - 30, page.width, page.height)\n\ndef exclude_watermark(page):\n    # Find \"DRAFT\" watermark if it exists\n    draft = page.find('text:contains(\"DRAFT\")')\n    return draft.create_region() if draft else None\n\n# Apply to entire PDF\npdf.add_exclusion(exclude_header, label=\"Headers\")\npdf.add_exclusion(exclude_footer, label=\"Footers\") \npdf.add_exclusion(exclude_watermark, label=\"Watermarks\")\n\n# Extract clean text from any page\nclean_text = pdf.pages[0].extract_text()  # Headers/footers automatically excluded\n</code></pre>"},{"location":"extracting-clean-text/#remove-noise-from-scanned-documents","title":"Remove Noise from Scanned Documents","text":"<p>For scanned PDFs, apply OCR first, then filter by confidence:</p> <pre><code># Apply OCR\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Only use high-confidence OCR text\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = reliable_text.extract_text()\n\n# Or exclude low-confidence noise\nnoisy_text = page.find_all('text[source=ocr][confidence&lt;0.5]')\nfor noise in noisy_text:\n    page.add_exclusion(noise)\n</code></pre>"},{"location":"extracting-clean-text/#handle-multi-column-layouts","title":"Handle Multi-Column Layouts","text":"<p>Extract text from specific columns or sections:</p> <pre><code># Extract just the main content column (avoiding sidebars)\nmain_column = page.create_region(\n    x0=page.width * 0.1,      # Start 10% from left\n    top=page.height * 0.15,   # Skip header area\n    x1=page.width * 0.7,      # End before sidebar\n    bottom=page.height * 0.9   # Stop before footer\n)\n\ncolumn_text = main_column.extract_text()\n</code></pre>"},{"location":"extracting-clean-text/#visual-debugging","title":"Visual Debugging","text":"<p>See what you're excluding before committing:</p> <pre><code># Highlight what you're about to exclude\nheader = page.create_region(0, 0, page.width, 50)\nfooter = page.create_region(0, page.height - 30, page.width, page.height)\n\nheader.highlight(color=\"red\", label=\"Will exclude\")\nfooter.highlight(color=\"red\", label=\"Will exclude\") \n\n# Show the page to verify\npage.show()\n\n# If it looks right, apply the exclusions\npage.add_exclusion(header)\npage.add_exclusion(footer)\n</code></pre>"},{"location":"extracting-clean-text/#compare-before-and-after","title":"Compare Before and After","text":"<pre><code># Extract with and without exclusions to see the difference\nfull_text = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text(use_exclusions=True)\n\nprint(f\"Original: {len(full_text)} characters\")\nprint(f\"Clean: {len(clean_text)} characters\")\nprint(f\"Removed: {len(full_text) - len(clean_text)} characters\")\n</code></pre>"},{"location":"extracting-clean-text/#common-patterns","title":"Common Patterns","text":""},{"location":"extracting-clean-text/#corporate-reports","title":"Corporate Reports","text":"<pre><code># Remove headers with logos and contact info\npage.add_exclusion(page.create_region(0, 0, page.width, 80))\n\n# Remove footers with page numbers and dates\npage.add_exclusion(page.create_region(0, page.height - 40, page.width, page.height))\n</code></pre>"},{"location":"extracting-clean-text/#academic-papers","title":"Academic Papers","text":"<pre><code># Remove running headers with paper title\nheader = page.find('text[size&lt;=10]').above() if page.find('text[size&lt;=10]') else None\nif header:\n    page.add_exclusion(header)\n\n# Remove footnotes (small text at bottom)\nfootnotes = page.find_all('text[size&lt;=8]')\nfor note in footnotes:\n    if note.top &gt; page.height * 0.8:  # Bottom 20% of page\n        page.add_exclusion(note)\n</code></pre>"},{"location":"extracting-clean-text/#government-documents","title":"Government Documents","text":"<pre><code># Remove classification markings\nclassifications = page.find_all('text:contains(\"CONFIDENTIAL|SECRET|UNCLASSIFIED\")', regex=True)\nfor mark in classifications:\n    page.add_exclusion(mark)\n\n# Remove agency headers\nagency_header = page.find('text:contains(\"Department of|Agency|Office of\")', regex=True)\nif agency_header:\n    page.add_exclusion(agency_header.above())\n</code></pre>"},{"location":"extracting-clean-text/#when-things-go-wrong","title":"When Things Go Wrong","text":"<ul> <li>Problem: Headers vary between pages</li> <li>Solution: Use adaptive exclusions</li> </ul> <pre><code>def smart_header_exclusion(page):\n    # Look for common header patterns\n    logo = page.find('image')\n    company_name = page.find('text:contains(\"ACME Corp\")')\n\n    if logo:\n        return logo.above()\n    elif company_name and company_name.top &lt; page.height * 0.2:\n        return company_name.above()\n    else:\n        return page.create_region(0, 0, page.width, 60)  # Fallback\n\npdf.add_exclusion(smart_header_exclusion)\n</code></pre> <ul> <li>Problem: Need to preserve some header information  </li> <li>Solution: Extract before excluding</li> </ul> <pre><code># Get the document title from the header first\ntitle = page.find('text[size&gt;=14]:bold')\ndocument_title = title.text if title else \"Unknown\"\n\n# Then exclude the header for clean body text\npage.add_exclusion(page.create_region(0, 0, page.width, 100))\nbody_text = page.extract_text()\n</code></pre>"},{"location":"finetuning/","title":"OCR Fine-tuning","text":"<p>While the built-in OCR engines (EasyOCR, PaddleOCR, Surya) offer good general performance, you might encounter situations where their accuracy isn't sufficient for your specific needs. This is often the case with:</p> <ul> <li>Unique Fonts: Documents using unusual or stylized fonts.</li> <li>Specific Languages: Languages or scripts not perfectly covered by the default models.</li> <li>Low Quality Scans: Noisy or degraded document images.</li> <li>Specialized Layouts: Text within complex tables, forms, or unusual arrangements.</li> </ul> <p>Fine-tuning allows you to adapt a pre-trained OCR recognition model to your specific data, significantly improving its accuracy on documents similar to those used for training.</p>"},{"location":"finetuning/#why-fine-tune","title":"Why Fine-tune?","text":"<ul> <li>Higher Accuracy: Achieve better text extraction results on your specific document types.</li> <li>Adaptability: Train the model to recognize domain-specific terms, symbols, or layouts.</li> <li>Reduced Errors: Minimize downstream errors in data extraction and processing pipelines.</li> </ul>"},{"location":"finetuning/#strategy-detect-llm-correct-export","title":"Strategy: Detect + LLM Correct + Export","text":"<p>Training an OCR model requires accurate ground truth: images of text snippets paired with their correct transcriptions. Manually creating this data is tedious. A powerful alternative leverages the strengths of different models:</p> <ol> <li>Detect Text Regions: Use a robust local OCR engine (like Surya or PaddleOCR) primarily for its detection capabilities (<code>detect_only=True</code>). This identifies the locations of text on the page, even if the initial recognition isn't perfect. You can combine this with layout analysis or region selections (<code>.region()</code>, <code>.below()</code>, <code>.add_exclusion()</code>) to focus on the specific areas you care about.</li> <li>Correct with LLM: For each detected text region, send the image snippet to a powerful Large Language Model (LLM) with multimodal capabilities (like GPT-4o, Claude 3.5 Sonnet/Haiku) using the <code>direct_ocr_llm</code> utility. The LLM performs high-accuracy OCR on the snippet, providing a \"ground truth\" transcription.</li> <li>Export for Fine-tuning: Use the <code>PaddleOCRRecognitionExporter</code> to package the original image snippets (from step 1) along with their corresponding LLM-generated text labels (from step 2) into the specific format required by PaddleOCR for fine-tuning its recognition model.</li> </ol> <p>This approach combines the efficient spatial detection of local models with the superior text recognition of large generative models to create a high-quality fine-tuning dataset with minimal manual effort.</p>"},{"location":"finetuning/#example-fine-tuning-for-greek-spreadsheet-text","title":"Example: Fine-tuning for Greek Spreadsheet Text","text":"<p>Let's walk through an example of preparing data to fine-tune PaddleOCR for text from a scanned Greek spreadsheet, adapting the process described above.</p> <pre><code># --- 1. Setup and Load PDF ---\nfrom natural_pdf import PDF\nfrom natural_pdf.ocr.utils import direct_ocr_llm\nfrom natural_pdf.exporters import PaddleOCRRecognitionExporter\nimport openai # Or your preferred LLM client library\nimport os\n\n# Ensure your LLM API key is set (using environment variables is recommended)\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" \n# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" \n\n# pdf_path = \"path/to/your/document.pdf\" \npdf_path = \"path/to/your/document.pdf\" \n# For demonstration we use a public sample PDF; replace with your own.\npdf_path = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\"\npdf = PDF(pdf_path)\n\n# --- 2. (Optional) Exclude Irrelevant Areas ---\n# If the document has consistent headers, footers, or margins you want to ignore\n# Use exclusions *before* detection\npdf.add_exclusion(lambda page: page.region(right=45)) # Exclude left margin/line numbers\npdf.add_exclusion(lambda page: page.region(left=500)) # Exclude right margin\n\n# --- 3. Detect Text Regions ---\n# Use a good detection engine. Surya is often robust for line detection.\n# We only want the bounding boxes, not the initial (potentially inaccurate) OCR text.\nprint(\"Detecting text regions...\")\n# Process only a subset of pages for demonstration if needed\nfor page in pdf.pages[:10]:\n    # Use a moderate resolution for detection; higher res used for LLM correction later\n    page.apply_ocr(engine='surya', resolution=120, detect_only=True) \nprint(f\"Detection complete for {num_pages_to_process} pages.\")\n\n# (Optional) Visualize detected boxes on a sample page\n# pdf.pages[9].find_all('text[source=ocr]').show() \n\n# --- 4. Correct with LLM ---\n# Configure your LLM client (example using OpenAI client, adaptable for others)\n# For Anthropic: client = openai.OpenAI(base_url=\"https://api.anthropic.com/v1/\", api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\nclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) \n\n# Craft a clear prompt for the LLM\n# Be as specific as possible! If it's in a specific language, what kinds\n# of characters, etc.\nprompt = \"\"\"OCR this image patch. Return only the exact text content visible in the image. \nPreserve original spelling, capitalization, punctuation, and symbols. \nDo not add any explanatory text, translations, comments, or quotation marks around the result.\nThe text is likely from a Greek document, potentially a spreadsheet, containing Modern Greek words or numbers.\"\"\"\n\n# Define the correction function using direct_ocr_llm\ndef correct_text_region(region):\n    # Use a high resolution for the LLM call for best accuracy\n    return direct_ocr_llm(\n        region, \n        client, \n        prompt=prompt, \n        resolution=300, \n        # model=\"claude-3-5-sonnet-20240620\" # Example Anthropic model\n        model=\"gpt-4o-mini\" # Example OpenAI model\n    )\n\n# Apply the correction function to the detected text regions\nprint(\"Applying LLM correction to detected regions...\")\nfor page in pdf.pages[:num_pages_to_process]:\n    # This finds elements added by apply_ocr and passes their regions to 'correct_text_region'\n    # The returned text from the LLM replaces the original OCR text for these elements\n    # The source attribute is updated (e.g., to 'ocr-llm-corrected')\n    page.correct_ocr(correct_text_region) \nprint(\"LLM correction complete.\")\n\n# --- 5. Export for PaddleOCR Fine-tuning ---\nprint(\"Configuring exporter...\")\nexporter = PaddleOCRRecognitionExporter(\n    # Select all of the non-blank OCR text\n    # Hopefully it's all been LLM-corrected! \n    selector=\"text[source^=ocr][text!='']\", \n    resolution=300,     # Resolution for the exported image crops\n    padding=2,          # Add slight padding around text boxes\n    split_ratio=0.9,    # 90% for training, 10% for validation\n    random_seed=42,     # For reproducible train/val split\n    include_guide=True  # Include the Colab fine-tuning notebook\n)\n\n# Define the output directory\noutput_directory = \"./my_paddleocr_finetune_data\"\nprint(f\"Exporting data to {output_directory}...\")\n\n# Run the export process\nexporter.export(pdf, output_directory)\n\nprint(\"Export complete.\")\nprint(f\"Dataset ready for fine-tuning in: {output_directory}\")\nprint(f\"Next step: Upload '{os.path.join(output_directory, 'fine_tune_paddleocr.ipynb')}' and the rest of the contents to Google Colab.\")\n\n# --- Cleanup ---\npdf.close() \n</code></pre>"},{"location":"finetuning/#running-the-fine-tuning","title":"Running the Fine-tuning","text":"<p>The <code>PaddleOCRRecognitionExporter</code> automatically includes a Jupyter Notebook (<code>fine_tune_paddleocr.ipynb</code>) in the output directory. This notebook is pre-configured to guide you through the fine-tuning process on Google Colab (which offers free GPU access):</p> <ol> <li>Upload: Upload the entire output directory (e.g., <code>my_paddleocr_finetune_data</code>) to your Google Drive or directly to your Colab instance.</li> <li>Open Notebook: Open the <code>fine_tune_paddleocr.ipynb</code> notebook in Google Colab.</li> <li>Set Runtime: Ensure the Colab runtime is set to use a GPU (Runtime -&gt; Change runtime type -&gt; GPU).</li> <li>Run Cells: Execute the cells in the notebook sequentially. It will:<ul> <li>Install necessary libraries (PaddlePaddle, PaddleOCR).</li> <li>Point the training configuration to your uploaded dataset (<code>images/</code>, <code>train.txt</code>, <code>val.txt</code>, <code>dict.txt</code>).</li> <li>Download a pre-trained PaddleOCR model (usually a multilingual one).</li> <li>Start the fine-tuning process using your data.</li> <li>Save the fine-tuned model checkpoints.</li> <li>Export the best model into an \"inference format\" suitable for use with <code>natural-pdf</code>.</li> </ul> </li> <li>Download Model: Download the resulting <code>inference_model</code> directory from Colab.</li> </ol>"},{"location":"finetuning/#using-the-fine-tuned-model","title":"Using the Fine-tuned Model","text":"<p>Once you have the <code>inference_model</code> directory, you can instruct <code>natural-pdf</code> to use it for OCR:</p> <pre><code>from natural_pdf import PDF\nfrom natural_pdf.ocr import PaddleOCROptions\n\n# Path to the directory you downloaded from Colab\nfinetuned_model_dir = \"/path/to/your/downloaded/inference_model\" \n\n# Specify the path in PaddleOCROptions\npaddle_opts = PaddleOCROptions(\n    rec_model_dir=finetuned_model_dir,\n    rec_char_dict_path=os.path.join(finetuned_model_dir, 'your_dict.txt') # Or wherever your dict is\n    use_gpu=True # If using GPU locally\n)\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Apply OCR using your fine-tuned model\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# Extract text using the improved results\ntext = page.extract_text() \nprint(text)\n\npdf.close()\n</code></pre> <p>By following this process, you can significantly enhance OCR performance on your specific documents using the power of fine-tuning. </p>"},{"location":"fix-messy-tables/","title":"Fix Messy Table Extraction","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Detect table regions using AI\npage.analyze_layout(engine='yolo')\n\n# Find and extract detected tables\ntable_regions = page.find_all('region[type=table]')\nfor i, table in enumerate(table_regions):\n    data = table.extract_table()\n    print(f\"Table {i+1}: {len(data)} rows\")\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Detect table regions using AI page.analyze_layout(engine='yolo')  # Find and extract detected tables table_regions = page.find_all('region[type=table]') for i, table in enumerate(table_regions):     data = table.extract_table()     print(f\"Table {i+1}: {len(data)} rows\") <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpk1in5lxs/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 918.9ms\n</pre> <pre>Speed: 6.8ms preprocess, 918.9ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Table 1: 8 rows\n</pre> In\u00a0[2]: Copied! <pre># Use Table Transformer - understands table structure better\npage.clear_detected_layout_regions()  # Clear previous attempts\npage.analyze_layout(engine='tatr')\n\n# TATR understands rows, columns, and headers\ntable = page.find('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]') \nheaders = page.find_all('region[type=table-column-header][model=tatr]')\n\nprint(f\"Found: {len(rows)} rows, {len(cols)} columns, {len(headers)} headers\")\n\n# Extract using the detected structure\ndata = table.extract_table(method='tatr')\n</pre> # Use Table Transformer - understands table structure better page.clear_detected_layout_regions()  # Clear previous attempts page.analyze_layout(engine='tatr')  # TATR understands rows, columns, and headers table = page.find('region[type=table][model=tatr]') rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]')  headers = page.find_all('region[type=table-column-header][model=tatr]')  print(f\"Found: {len(rows)} rows, {len(cols)} columns, {len(headers)} headers\")  # Extract using the detected structure data = table.extract_table(method='tatr') <pre>Found: 8 rows, 4 columns, 1 headers\n</pre> In\u00a0[3]: Copied! <pre># Visually inspect the page to find table boundaries\npage.show()\n\n# Manually define the table region\ntable_area = page.create_region(\n    x0=50,      # Left edge of table\n    top=200,    # Top edge of table  \n    x1=550,     # Right edge of table\n    bottom=400  # Bottom edge of table\n)\n\n# Highlight to verify\ntable_area.highlight(color=\"blue\", label=\"Table area\")\npage.show()\n\n# Extract from the defined area\ndata = table_area.extract_table()\n</pre> # Visually inspect the page to find table boundaries page.show()  # Manually define the table region table_area = page.create_region(     x0=50,      # Left edge of table     top=200,    # Top edge of table       x1=550,     # Right edge of table     bottom=400  # Bottom edge of table )  # Highlight to verify table_area.highlight(color=\"blue\", label=\"Table area\") page.show()  # Extract from the defined area data = table_area.extract_table() In\u00a0[4]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find all header labels\nheaders = page.find_all(\n    'text:contains(\"Statute|Description|Level|Repeat\")',\n    regex=True,\n)\ncol_edges = headers.apply(lambda header: header.x0) + [page.width]\nprint(col_edges)\n\nheaders.show(crop=True)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find all header labels headers = page.find_all(     'text:contains(\"Statute|Description|Level|Repeat\")',     regex=True, ) col_edges = headers.apply(lambda header: header.x0) + [page.width] print(col_edges)  headers.show(crop=True) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>[55.0, 105.0, 455.0, 505.0, 612]\n</pre> Out[4]: In\u00a0[5]: Copied! <pre>table_area = (\n    headers[0]\n    .below(\n        until=\"text[size&lt;10]\",\n        include_endpoint=False\n    )\n)\ntable_area.show()\n</pre> table_area = (     headers[0]     .below(         until=\"text[size&lt;10]\",         include_endpoint=False     ) ) table_area.show() Out[5]: In\u00a0[6]: Copied! <pre>row_items = (\n    headers[0]\n    .below(width='element')\n    .find_all('text')\n)\n\nrow_borders = row_items.apply(lambda entry: entry.top) + [row_items[-1].bottom]\nprint(row_borders)\nrow_items.show()\n</pre> row_items = (     headers[0]     .below(width='element')     .find_all('text') )  row_borders = row_items.apply(lambda entry: entry.top) + [row_items[-1].bottom] print(row_borders) row_items.show() <pre>[418.07, 438.07, 458.07, 478.07, 498.07, 518.0699999999999, 538.0699999999999, 548.0699999999999]\n</pre> Out[6]: In\u00a0[7]: Copied! <pre>options = {\n    \"vertical_strategy\": \"explicit\", \n    \"horizontal_strategy\": \"explicit\",\n    \"explicit_vertical_lines\": col_edges,\n    \"explicit_horizontal_lines\": row_borders,\n}\ndata = table_area.extract_table('pdfplumber', table_settings=options)\ndata\n</pre> options = {     \"vertical_strategy\": \"explicit\",      \"horizontal_strategy\": \"explicit\",     \"explicit_vertical_lines\": col_edges,     \"explicit_horizontal_lines\": row_borders, } data = table_area.extract_table('pdfplumber', table_settings=options) data Out[7]: <pre>[['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[8]: Copied! <pre># Detect all tables\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\nprint(f\"Found {len(tables)} tables\")\n\n# Extract each table separately\nall_data = []\nfor i, table in enumerate(tables):\n    # Show which table we're working on\n    table.highlight(color=\"red\", label=f\"Table {i+1}\")\n    \n    data = table.extract_table()\n    all_data.append(data)\n    \npage.show()  # See all highlighted tables\n\n# Save each table separately\nimport pandas as pd\nfor i, data in enumerate(all_data):\n    df = pd.DataFrame(data)\n    df.to_csv(f\"table_{i+1}.csv\", index=False)\n</pre> # Detect all tables page.analyze_layout(engine='yolo') tables = page.find_all('region[type=table]')  print(f\"Found {len(tables)} tables\")  # Extract each table separately all_data = [] for i, table in enumerate(tables):     # Show which table we're working on     table.highlight(color=\"red\", label=f\"Table {i+1}\")          data = table.extract_table()     all_data.append(data)      page.show()  # See all highlighted tables  # Save each table separately import pandas as pd for i, data in enumerate(all_data):     df = pd.DataFrame(data)     df.to_csv(f\"table_{i+1}.csv\", index=False) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpuc37ygu0/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1019.0ms\n</pre> <pre>Speed: 6.6ms preprocess, 1019.0ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Found 1 tables\n</pre> In\u00a0[9]: Copied! <pre># For rotated tables, you might need to work with coordinates differently\n# First, see if layout detection picks it up\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\n# If the table appears rotated in the highlights, \n# the extraction might still work normally\nfor table in tables:\n    table.show()  # Check if this looks right\n    data = table.extract_table()\n    if data and len(data) &gt; 1:  # If we got reasonable data\n        print(\"Table extracted successfully despite rotation\")\n</pre> # For rotated tables, you might need to work with coordinates differently # First, see if layout detection picks it up page.analyze_layout(engine='yolo') tables = page.find_all('region[type=table]')  # If the table appears rotated in the highlights,  # the extraction might still work normally for table in tables:     table.show()  # Check if this looks right     data = table.extract_table()     if data and len(data) &gt; 1:  # If we got reasonable data         print(\"Table extracted successfully despite rotation\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp20q1d2_w/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 821.5ms\n</pre> <pre>Speed: 4.7ms preprocess, 821.5ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Table extracted successfully despite rotation\nTable extracted successfully despite rotation\n</pre> In\u00a0[10]: Copied! <pre># Apply OCR first to convert image text to searchable text\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Now try table detection on the OCR'd content\npage.analyze_layout(engine='tatr')\ntables = page.find_all('region[type=table]')\n\n# Extract using OCR text\nfor table in tables:\n    data = table.extract_table()\n</pre> # Apply OCR first to convert image text to searchable text page.apply_ocr(engine='easyocr', languages=['en'])  # Now try table detection on the OCR'd content page.analyze_layout(engine='tatr') tables = page.find_all('region[type=table]')  # Extract using OCR text for table in tables:     data = table.extract_table() <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> In\u00a0[11]: Copied! <pre># Step 1: See what elements exist in the table area\ntable_area = page.create_region(50, 200, 550, 400)\nelements = table_area.find_all('text')\n\nprint(f\"Found {len(elements)} text elements in table area\")\n\n# Step 2: See the actual text content\ntext_content = table_area.extract_text(layout=True)\nprint(\"Raw text content:\")\nprint(text_content)\n\n# Step 3: Check for lines that might define the table\nlines = table_area.find_all('line')\nprint(f\"Found {len(lines)} lines in table area\")\n\n# Step 4: Visualize everything\nelements.highlight(color=\"blue\", label=\"Text\")\nlines.highlight(color=\"red\", label=\"Lines\")\npage.show()\n</pre> # Step 1: See what elements exist in the table area table_area = page.create_region(50, 200, 550, 400) elements = table_area.find_all('text')  print(f\"Found {len(elements)} text elements in table area\")  # Step 2: See the actual text content text_content = table_area.extract_text(layout=True) print(\"Raw text content:\") print(text_content)  # Step 3: Check for lines that might define the table lines = table_area.find_all('line') print(f\"Found {len(lines)} lines in table area\")  # Step 4: Visualize everything elements.highlight(color=\"blue\", label=\"Text\") lines.highlight(color=\"red\", label=\"Lines\") page.show() <pre>Found 9 text elements in table area\nRaw text content:\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fellsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and wheninto the vats; and whenthey they were fished out; there was never enough of them left to be worthwere fished out, there was never enough of them left to be worth\nexhibitingexhibiting - sometimes they would be overlooked forsometimes they would be overlooked fordays, days, till all but the bones of them had gone outtill all but the bones of them had gone out\nto the world as Durham's Pure Leaf Lardlto the world as Durham\u2019s Pure Leaf Lard!\nViolationsViolations\nStatuteStatute DescriptionDescription LevelLevel Repeat?Repeat?\nFound 2 lines in table area\n</pre> Out[11]:"},{"location":"fix-messy-tables/#fix-messy-table-extraction","title":"Fix Messy Table Extraction\u00b6","text":"<p>Your PDF has tables, but when you try to extract them, you get garbled data, missing rows, or columns that don't line up. Here's how to fix the most common table extraction problems.</p>"},{"location":"fix-messy-tables/#the-problem","title":"The Problem\u00b6","text":"<p>PDF tables come in many flavors, and each one breaks in its own special way:</p> <ul> <li>No visible borders: Text that looks like a table but has no lines</li> <li>Merged cells: Headers that span multiple columns</li> <li>Inconsistent spacing: Columns that don't line up perfectly</li> <li>Multiple tables: Several tables crammed onto one page</li> <li>Rotated tables: Tables turned sideways or upside down</li> <li>Image tables: Tables that are actually pictures</li> </ul> <p>Simple extraction methods often fail on these cases.</p>"},{"location":"fix-messy-tables/#quick-fix-try-layout-detection-first","title":"Quick Fix: Try Layout Detection First\u00b6","text":"<p>Instead of extracting tables blind, detect them first:</p>"},{"location":"fix-messy-tables/#when-basic-detection-fails-use-tatr","title":"When Basic Detection Fails: Use TATR\u00b6","text":"<p>For tables without clear borders or with complex structure:</p>"},{"location":"fix-messy-tables/#manual-table-definition","title":"Manual Table Definition\u00b6","text":"<p>When detection fails completely, define the table area yourself:</p>"},{"location":"fix-messy-tables/#define-table-boundaries-using-header-labels-pdfplumber","title":"Define Table Boundaries Using Header Labels (pdfplumber)\u00b6","text":"<p>Sometimes the easiest trick is to let the column headers tell you exactly where the table starts and ends. Here's a quick, self-contained approach that works great on the violations table inside <code>01-practice.pdf</code>:</p>"},{"location":"fix-messy-tables/#2-compute-a-bounding-box-that-covers-every-header-element","title":"2) Compute a bounding box that covers every header element\u00b6","text":"<p>x0 = min(h.x0 for h in headers) x1 = max(h.x1 for h in headers) top = min(h.top for h in headers)</p>"},{"location":"fix-messy-tables/#3-extend-the-box-downward-until-the-next-thick-horizontal-line","title":"3) Extend the box downward until the next thick horizontal line\u00b6","text":"<p>bottom_line = headers[0].below(until='line:horizontal[width&gt;=400]') bottom = bottom_line.bottom if bottom_line else page.height</p> <p>table_region = page.create_region(x0, top, x1, bottom)</p>"},{"location":"fix-messy-tables/#4-extract-using-pdfplumber-simple-fast-no-ai","title":"4) Extract using pdfplumber (simple, fast, no AI)\u00b6","text":"<p>data = table_region.extract_table(method='pdfplumber') print(f\"Got {len(data)} rows from the table!\")</p> <pre><code>\nWhy this works:\n\n* Header labels are almost always the top-left anchors of a table.\n* pdfplumber's heuristics excel once you provide a tight bounding box.\n* No fancy ML models, so it's fast and deterministic.\n\nIf your headers have different names, just update the regular expression.  This trick often beats spending time on manual coordinate tweaking.\n\n## Fix Alignment Issues\n\nWhen columns don't line up properly:\n\n```python\n# Customize pdfplumber settings for better alignment\ntable_settings = {\n    \"vertical_strategy\": \"text\",        # Use text alignment instead of lines\n    \"horizontal_strategy\": \"lines\",     # Still use lines for rows\n    \"intersection_x_tolerance\": 10,     # More forgiving column alignment\n    \"intersection_y_tolerance\": 5,      # More forgiving row alignment\n    \"edge_min_length\": 3,              # Shorter minimum line length\n}\n\ndata = table_area.extract_table(table_settings=table_settings)\n</code></pre>"},{"location":"fix-messy-tables/#handle-multiple-tables-on-one-page","title":"Handle Multiple Tables on One Page\u00b6","text":"<p>Separate and extract each table individually:</p>"},{"location":"fix-messy-tables/#deal-with-rotated-tables","title":"Deal with Rotated Tables\u00b6","text":"<p>Some PDFs have sideways tables:</p>"},{"location":"fix-messy-tables/#extract-tables-from-images","title":"Extract Tables from Images\u00b6","text":"<p>When tables are embedded as images:</p>"},{"location":"fix-messy-tables/#debug-table-extraction-step-by-step","title":"Debug Table Extraction Step by Step\u00b6","text":"<p>When nothing works, debug systematically:</p>"},{"location":"fix-messy-tables/#troubleshooting-checklist","title":"Troubleshooting Checklist\u00b6","text":"<p>No tables detected?</p> <ul> <li>Try different engines: <code>yolo</code>, <code>tatr</code>, <code>surya</code></li> <li>Apply OCR first if it's a scanned document</li> <li>Manually define the table area</li> </ul> <p>Columns misaligned?</p> <ul> <li>Adjust <code>intersection_x_tolerance</code> in table settings</li> <li>Try <code>vertical_strategy=\"text\"</code> instead of <code>\"lines\"</code></li> <li>Check if there are actual lines vs. just text alignment</li> </ul> <p>Missing rows?</p> <ul> <li>Increase <code>intersection_y_tolerance</code></li> <li>Try <code>horizontal_strategy=\"text\"</code></li> <li>Look for merged cells that might be confusing the detector</li> </ul> <p>Garbled data?</p> <ul> <li>Extract manually row by row</li> <li>Check OCR quality if text looks wrong</li> <li>Verify the table region boundaries</li> </ul> <p>Multiple tables mixed together?</p> <ul> <li>Use layout detection to separate them first</li> <li>Define manual regions for each table</li> <li>Process tables in reading order</li> </ul>"},{"location":"fix-messy-tables/","title":"Fix Messy Table Extraction","text":"<p>Your PDF has tables, but when you try to extract them, you get garbled data, missing rows, or columns that don't line up. Here's how to fix the most common table extraction problems.</p>"},{"location":"fix-messy-tables/#the-problem","title":"The Problem","text":"<p>PDF tables come in many flavors, and each one breaks in its own special way:</p> <ul> <li>No visible borders: Text that looks like a table but has no lines</li> <li>Merged cells: Headers that span multiple columns </li> <li>Inconsistent spacing: Columns that don't line up perfectly</li> <li>Multiple tables: Several tables crammed onto one page</li> <li>Rotated tables: Tables turned sideways or upside down</li> <li>Image tables: Tables that are actually pictures</li> </ul> <p>Simple extraction methods often fail on these cases.</p>"},{"location":"fix-messy-tables/#quick-fix-try-layout-detection-first","title":"Quick Fix: Try Layout Detection First","text":"<p>Instead of extracting tables blind, detect them first:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Detect table regions using AI\npage.analyze_layout(engine='yolo')\n\n# Find and extract detected tables\ntable_regions = page.find_all('region[type=table]')\nfor i, table in enumerate(table_regions):\n    data = table.extract_table()\n    print(f\"Table {i+1}: {len(data)} rows\")\n</code></pre>"},{"location":"fix-messy-tables/#when-basic-detection-fails-use-tatr","title":"When Basic Detection Fails: Use TATR","text":"<p>For tables without clear borders or with complex structure:</p> <pre><code># Use Table Transformer - understands table structure better\npage.clear_detected_layout_regions()  # Clear previous attempts\npage.analyze_layout(engine='tatr')\n\n# TATR understands rows, columns, and headers\ntable = page.find('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]') \nheaders = page.find_all('region[type=table-column-header][model=tatr]')\n\nprint(f\"Found: {len(rows)} rows, {len(cols)} columns, {len(headers)} headers\")\n\n# Extract using the detected structure\ndata = table.extract_table(method='tatr')\n</code></pre>"},{"location":"fix-messy-tables/#manual-table-definition","title":"Manual Table Definition","text":"<p>When detection fails completely, define the table area yourself:</p> <pre><code># Visually inspect the page to find table boundaries\npage.show()\n\n# Manually define the table region\ntable_area = page.create_region(\n    x0=50,      # Left edge of table\n    top=200,    # Top edge of table  \n    x1=550,     # Right edge of table\n    bottom=400  # Bottom edge of table\n)\n\n# Highlight to verify\ntable_area.highlight(color=\"blue\", label=\"Table area\")\npage.show()\n\n# Extract from the defined area\ndata = table_area.extract_table()\n</code></pre>"},{"location":"fix-messy-tables/#define-table-boundaries-using-header-labels-pdfplumber","title":"Define Table Boundaries Using Header Labels (pdfplumber)","text":"<p>Sometimes the easiest trick is to let the column headers tell you exactly where the table starts and ends. Here's a quick, self-contained approach that works great on the violations table inside <code>01-practice.pdf</code>:</p> <pre><code>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find all header labels\nheaders = page.find_all(\n    'text:contains(\"Statute|Description|Level|Repeat\")',\n    regex=True,\n)\ncol_edges = headers.apply(lambda header: header.x0) + [page.width]\nprint(col_edges)\n\nheaders.show(crop=True)\n</code></pre> <pre><code>table_area = (\n    headers[0]\n    .below(\n        until=\"text[size&lt;10]\",\n        include_endpoint=False\n    )\n)\ntable_area.show()\n</code></pre> <pre><code>row_items = (\n    headers[0]\n    .below(width='element')\n    .find_all('text')\n)\n\nrow_borders = row_items.apply(lambda entry: entry.top) + [row_items[-1].bottom]\nprint(row_borders)\nrow_items.show()\n</code></pre> <pre><code>options = {\n    \"vertical_strategy\": \"explicit\", \n    \"horizontal_strategy\": \"explicit\",\n    \"explicit_vertical_lines\": col_edges,\n    \"explicit_horizontal_lines\": row_borders,\n}\ndata = table_area.extract_table('pdfplumber', table_settings=options)\ndata\n</code></pre>"},{"location":"fix-messy-tables/#2-compute-a-bounding-box-that-covers-every-header-element","title":"2) Compute a bounding box that covers every header element","text":"<p>x0 = min(h.x0 for h in headers) x1 = max(h.x1 for h in headers) top = min(h.top for h in headers)</p>"},{"location":"fix-messy-tables/#3-extend-the-box-downward-until-the-next-thick-horizontal-line","title":"3) Extend the box downward until the next thick horizontal line","text":"<p>bottom_line = headers[0].below(until='line:horizontal[width&gt;=400]') bottom = bottom_line.bottom if bottom_line else page.height</p> <p>table_region = page.create_region(x0, top, x1, bottom)</p>"},{"location":"fix-messy-tables/#4-extract-using-pdfplumber-simple-fast-no-ai","title":"4) Extract using pdfplumber (simple, fast, no AI)","text":"<p>data = table_region.extract_table(method='pdfplumber') print(f\"Got {len(data)} rows from the table!\") <pre><code>Why this works:\n\n* Header labels are almost always the top-left anchors of a table.\n* pdfplumber's heuristics excel once you provide a tight bounding box.\n* No fancy ML models, so it's fast and deterministic.\n\nIf your headers have different names, just update the regular expression.  This trick often beats spending time on manual coordinate tweaking.\n\n## Fix Alignment Issues\n\nWhen columns don't line up properly:\n\n```python\n# Customize pdfplumber settings for better alignment\ntable_settings = {\n    \"vertical_strategy\": \"text\",        # Use text alignment instead of lines\n    \"horizontal_strategy\": \"lines\",     # Still use lines for rows\n    \"intersection_x_tolerance\": 10,     # More forgiving column alignment\n    \"intersection_y_tolerance\": 5,      # More forgiving row alignment\n    \"edge_min_length\": 3,              # Shorter minimum line length\n}\n\ndata = table_area.extract_table(table_settings=table_settings)\n</code></pre></p>"},{"location":"fix-messy-tables/#handle-multiple-tables-on-one-page","title":"Handle Multiple Tables on One Page","text":"<p>Separate and extract each table individually:</p> <pre><code># Detect all tables\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\nprint(f\"Found {len(tables)} tables\")\n\n# Extract each table separately\nall_data = []\nfor i, table in enumerate(tables):\n    # Show which table we're working on\n    table.highlight(color=\"red\", label=f\"Table {i+1}\")\n\n    data = table.extract_table()\n    all_data.append(data)\n\npage.show()  # See all highlighted tables\n\n# Save each table separately\nimport pandas as pd\nfor i, data in enumerate(all_data):\n    df = pd.DataFrame(data)\n    df.to_csv(f\"table_{i+1}.csv\", index=False)\n</code></pre>"},{"location":"fix-messy-tables/#deal-with-rotated-tables","title":"Deal with Rotated Tables","text":"<p>Some PDFs have sideways tables:</p> <pre><code># For rotated tables, you might need to work with coordinates differently\n# First, see if layout detection picks it up\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\n# If the table appears rotated in the highlights, \n# the extraction might still work normally\nfor table in tables:\n    table.show()  # Check if this looks right\n    data = table.extract_table()\n    if data and len(data) &gt; 1:  # If we got reasonable data\n        print(\"Table extracted successfully despite rotation\")\n</code></pre>"},{"location":"fix-messy-tables/#extract-tables-from-images","title":"Extract Tables from Images","text":"<p>When tables are embedded as images:</p> <pre><code># Apply OCR first to convert image text to searchable text\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Now try table detection on the OCR'd content\npage.analyze_layout(engine='tatr')\ntables = page.find_all('region[type=table]')\n\n# Extract using OCR text\nfor table in tables:\n    data = table.extract_table()\n</code></pre>"},{"location":"fix-messy-tables/#debug-table-extraction-step-by-step","title":"Debug Table Extraction Step by Step","text":"<p>When nothing works, debug systematically:</p> <pre><code># Step 1: See what elements exist in the table area\ntable_area = page.create_region(50, 200, 550, 400)\nelements = table_area.find_all('text')\n\nprint(f\"Found {len(elements)} text elements in table area\")\n\n# Step 2: See the actual text content\ntext_content = table_area.extract_text(layout=True)\nprint(\"Raw text content:\")\nprint(text_content)\n\n# Step 3: Check for lines that might define the table\nlines = table_area.find_all('line')\nprint(f\"Found {len(lines)} lines in table area\")\n\n# Step 4: Visualize everything\nelements.highlight(color=\"blue\", label=\"Text\")\nlines.highlight(color=\"red\", label=\"Lines\")\npage.show()\n</code></pre>"},{"location":"fix-messy-tables/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<p>No tables detected? - Try different engines: <code>yolo</code>, <code>tatr</code>, <code>surya</code> - Apply OCR first if it's a scanned document - Manually define the table area</p> <p>Columns misaligned? - Adjust <code>intersection_x_tolerance</code> in table settings - Try <code>vertical_strategy=\"text\"</code> instead of <code>\"lines\"</code> - Check if there are actual lines vs. just text alignment</p> <p>Missing rows? - Increase <code>intersection_y_tolerance</code> - Try <code>horizontal_strategy=\"text\"</code> - Look for merged cells that might be confusing the detector</p> <p>Garbled data? - Extract manually row by row - Check OCR quality if text looks wrong - Verify the table region boundaries</p> <p>Multiple tables mixed together? - Use layout detection to separate them first - Define manual regions for each table - Process tables in reading order </p>"},{"location":"installation/","title":"Getting Started with Natural PDF","text":"<p>Let's get Natural PDF installed and run your first extraction.</p>"},{"location":"installation/#installation","title":"Installation","text":"<p>The base installation includes the core library which will allow you to select, extract, and use spatial navigation.</p> <pre><code>pip install natural-pdf\n</code></pre> <p>But! If you want to recognize text, do page layout analysis, document q-and-a or other things, you can install optional dependencies.</p> <p>Natural PDF has modular dependencies for different features. Install them based on your needs:</p> <pre><code># Full ML / QA / semantic-search stack\npip install natural-pdf[ai]\n\n# Deskewing\npip install natural-pdf[deskew]\n\n# Semantic search\npip install natural-pdf[search]\n</code></pre> <p>Other OCR and layout analysis engines like <code>surya</code>, <code>easyocr</code>, <code>paddle</code>, <code>doctr</code>, and <code>docling</code> can be installed via <code>pip</code> as needed. The library will provide you with an error message and installation command if you try to use an engine that isn't installed.</p> <p>After the core install you have two ways to add optional engines:</p>"},{"location":"installation/#1-helper-cli-recommended","title":"1 \u2013 Helper CLI (recommended)","text":"<pre><code># list optional groups and their install-status\nnpdf list\n\n# everything for classification, QA, semantic search, etc.\nnpdf install ai\n\n# install PaddleOCR stack\nnpdf install paddle\n\n# install Surya OCR + YOLO layout detector\nnpdf install surya yolo\n</code></pre> <p>The CLI runs each wheel in its own resolver pass, so it avoids strict version pins like <code>paddleocr \u2192 paddlex==3.0.1</code> while still upgrading to <code>paddlex 3.0.2</code>.</p>"},{"location":"installation/#2-classic-extras-for-the-light-stuff","title":"2 \u2013 Classic extras (for the light stuff)","text":"<pre><code># Full AI/ML stack\npip install \"natural-pdf[ai]\"\n\n# Deskewing\npip install \"natural-pdf[deskew]\"\n\n# Semantic search service\npip install \"natural-pdf[search]\"\n</code></pre> <p>If you attempt to use an engine that is missing, the library will raise an error that tells you which <code>npdf install \u2026</code> command to run.</p>"},{"location":"installation/#your-first-pdf-extraction","title":"Your First PDF Extraction","text":"<p>Here's a quick example to make sure everything is working:</p> <pre><code>from natural_pdf import PDF\n\n# Open a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Extract all text\ntext = page.extract_text()\nprint(text)\n\n# Find something specific\ntitle = page.find('text:bold')\nprint(f\"Found title: {title.text}\")\n</code></pre>"},{"location":"installation/#whats-next","title":"What's Next?","text":"<p>Now that you have Natural PDF installed, you can:</p> <ul> <li>Learn to navigate PDFs</li> <li>Explore how to select elements</li> <li>See how to extract text</li> </ul>"},{"location":"interactive-widget/","title":"Interactive widget","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.viewer()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[1]:"},{"location":"interactive-widget/#interactive-widget","title":"Interactive widget\u00b6","text":"<p>This is the best possible way, in all of history, to explore a PDF.</p>"},{"location":"layout-analysis/","title":"Document Layout Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.to_image(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Analyze the layout using the default engine (YOLO)\n# This adds 'region' elements to the page\npage.analyze_layout()\n</pre> # Analyze the layout using the default engine (YOLO) # This adds 'region' elements to the page page.analyze_layout() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpzwdl8uha/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 3911.9ms\n</pre> <pre>Speed: 12.5ms preprocess, 3911.9ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[2]: <pre>&lt;ElementCollection[Region](count=7)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all detected regions\nregions = page.find_all('region')\nlen(regions) # Show how many regions were detected\n</pre> # Find all detected regions regions = page.find_all('region') len(regions) # Show how many regions were detected Out[3]: <pre>7</pre> In\u00a0[4]: Copied! <pre>first_region = regions[0]\nf\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\"\n</pre> first_region = regions[0] f\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\" Out[4]: <pre>\"First region: type='abandon', confidence=0.81\"</pre> In\u00a0[5]: Copied! <pre># Highlight all detected regions, colored by type\nregions.highlight(group_by='type')\npage.to_image(width=700)\n</pre> # Highlight all detected regions, colored by type regions.highlight(group_by='type') page.to_image(width=700) Out[5]: In\u00a0[6]: Copied! <pre># Find all detected titles\ntitles = page.find_all('region[type=title]')\ntitles\n</pre> # Find all detected titles titles = page.find_all('region[type=title]') titles Out[6]: <pre>&lt;ElementCollection[Region](count=1)&gt;</pre> In\u00a0[7]: Copied! <pre>titles.show()\n</pre> titles.show() Out[7]: In\u00a0[8]: Copied! <pre>page.find_all('region[type=table]').show()\n</pre> page.find_all('region[type=table]').show() Out[8]: In\u00a0[9]: Copied! <pre>page.find('region[type=table]').extract_text(layout=True)\n</pre> page.find('region[type=table]').extract_text(layout=True) Out[9]: <pre>'Statute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[10]: Copied! <pre>page.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"paddle\")\npage.find_all('region[model=paddle]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"paddle\") page.find_all('region[model=paddle]').highlight(group_by='region_type') page.to_image(width=700) <pre>/Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> Out[10]: In\u00a0[11]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[11]: In\u00a0[12]: Copied! <pre># Analyze using Docling\n# https://docling-project.github.io/docling/\n\n# Docling has been weird, it's not included at the moment\n\n# page.clear_detected_layout_regions()\n# page.clear_highlights()\n\n# page.analyze_layout(engine=\"docling\")\n# page.find_all('region[model=docling]').highlight(group_by='region_type')\n# page.to_image(width=700)\n</pre> # Analyze using Docling # https://docling-project.github.io/docling/  # Docling has been weird, it's not included at the moment  # page.clear_detected_layout_regions() # page.clear_highlights()  # page.analyze_layout(engine=\"docling\") # page.find_all('region[model=docling]').highlight(group_by='region_type') # page.to_image(width=700) In\u00a0[13]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"surya\")\npage.find_all('region[model=surya]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"surya\") page.find_all('region[model=surya]').highlight(group_by='region_type') page.to_image(width=700) <pre>\rRecognizing layout:   0%|                                                 | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.69s/it]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.69s/it]</pre> <pre>\n</pre> <pre>\rRecognizing tables:   0%|                                                 | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing tables: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.83s/it]</pre> <pre>\rRecognizing tables: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.83s/it]</pre> <pre>\n</pre> Out[13]: <p>Note: Calling <code>analyze_layout</code> multiple times (even with the same engine) can add duplicate regions. You might want to use <code>page.clear_detected_layout_regions()</code> first, or filter by model using <code>region[model=yolo]</code>.</p> In\u00a0[14]: Copied! <pre># Re-run YOLO analysis (clearing previous results might be good practice)\npage.clear_detected_layout_regions()\npage.analyze_layout(engine=\"yolo\")\n\n# Find only high-confidence regions (e.g., &gt;= 0.8)\nhigh_conf_regions = page.find_all('region[confidence&gt;=0.8]')\nlen(high_conf_regions)\n</pre> # Re-run YOLO analysis (clearing previous results might be good practice) page.clear_detected_layout_regions() page.analyze_layout(engine=\"yolo\")  # Find only high-confidence regions (e.g., &gt;= 0.8) high_conf_regions = page.find_all('region[confidence&gt;=0.8]') len(high_conf_regions) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpceicz5i7/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 795.3ms\n</pre> <pre>Speed: 4.7ms preprocess, 795.3ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre># Ensure TATR analysis has been run\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Ensure TATR analysis has been run page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[15]: In\u00a0[16]: Copied! <pre># Find different structural elements from TATR\ntables = page.find_all('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\n\nf\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\"\n</pre> # Find different structural elements from TATR tables = page.find_all('region[type=table][model=tatr]') rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]')  f\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\" Out[16]: <pre>'Found: 2 tables, 8 rows, 4 columns, 1 headers (from TATR)'</pre> In\u00a0[17]: Copied! <pre># Find the TATR table region again\ntatr_table = page.find('region[type=table][model=tatr]')\n\n# This extraction uses the detected rows/columns\ntatr_table.extract_table()\n</pre> # Find the TATR table region again tatr_table = page.find('region[type=table][model=tatr]')  # This extraction uses the detected rows/columns tatr_table.extract_table() Out[17]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>if you'd like the normal approach instead of the \"intelligent\" one, you can ask for pdfplumber.</p> In\u00a0[18]: Copied! <pre># This extraction uses the detected rows/columns\ntatr_table.extract_table(method='pdfplumber')\n</pre> # This extraction uses the detected rows/columns tatr_table.extract_table(method='pdfplumber') Out[18]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre>"},{"location":"layout-analysis/#document-layout-analysis","title":"Document Layout Analysis\u00b6","text":"<p>Natural PDF can automatically detect the structure of a document (titles, paragraphs, tables, figures) using layout analysis models. This guide shows how to use this feature.</p>"},{"location":"layout-analysis/#setup","title":"Setup\u00b6","text":"<p>We'll use a sample PDF that includes various layout elements.</p>"},{"location":"layout-analysis/#running-basic-layout-analysis","title":"Running Basic Layout Analysis\u00b6","text":"<p>Use the <code>analyze_layout()</code> method. By default, it uses the YOLO model.</p>"},{"location":"layout-analysis/#visualizing-detected-layout","title":"Visualizing Detected Layout\u00b6","text":"<p>Use <code>highlight()</code> or <code>show()</code> on the detected regions.</p>"},{"location":"layout-analysis/#finding-specific-region-types","title":"Finding Specific Region Types\u00b6","text":"<p>Use attribute selectors to find regions of a specific type.</p>"},{"location":"layout-analysis/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>Detected regions are like any other <code>Region</code> object. You can extract text, find elements within them, etc.</p>"},{"location":"layout-analysis/#using-different-layout-models","title":"Using Different Layout Models\u00b6","text":"<p>Natural PDF supports multiple engines (<code>yolo</code>, <code>paddle</code>, <code>tatr</code>). Specify the engine when calling <code>analyze_layout</code>.</p> <p>Note: Using different engines requires installing the corresponding extras (e.g., <code>natural-pdf[layout_paddle]</code>). <code>yolo</code> is the default.</p>"},{"location":"layout-analysis/#controlling-confidence-threshold","title":"Controlling Confidence Threshold\u00b6","text":"<p>Filter detections by their confidence score.</p>"},{"location":"layout-analysis/#table-structure-with-tatr","title":"Table Structure with TATR\u00b6","text":"<p>The TATR engine provides detailed table structure elements (<code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>). This is very useful for precise table extraction.</p>"},{"location":"layout-analysis/#enhanced-table-extraction-with-tatr","title":"Enhanced Table Extraction with TATR\u00b6","text":"<p>When a <code>region[type=table]</code> comes from the TATR model, <code>extract_table()</code> can use the underlying row/column structure for more robust extraction.</p>"},{"location":"layout-analysis/#using-gemini-for-layout-analysis-advanced","title":"Using Gemini for Layout Analysis (Advanced)\u00b6","text":"<p>Natural PDF supports layout analysis using Google's Gemini models via an OpenAI-compatible API. This is an advanced feature and requires you to provide your own OpenAI client, API key, and endpoint.</p> <p>Example usage:</p> <pre>from openai import OpenAI\nfrom natural_pdf import PDF\nfrom natural_pdf.analyzers.layout.layout_options import GeminiLayoutOptions\n\n# Create a compatible OpenAI client for Gemini\nclient = OpenAI(\n    api_key=\"YOUR_GOOGLE_API_KEY\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\noptions = GeminiLayoutOptions(\n    model_name=\"gemini-2.0-flash\",\n    client=client,\n    classes=[\"text\", \"title\"]\n)\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\nregions = page.analyze_layout(engine=\"gemini\", options=options)\n</pre> <ul> <li>You must provide your own API key and endpoint for Gemini.</li> <li>The client must be compatible with the OpenAI API (see the <code>openai</code> Python package).</li> <li>This feature is intended for advanced users who need LLM-based layout analysis.</li> </ul>"},{"location":"layout-analysis/#next-steps","title":"Next Steps\u00b6","text":"<p>Layout analysis provides regions that you can use for:</p> <ul> <li>Table Extraction: Especially powerful with TATR regions.</li> <li>Text Extraction: Extract text only from specific region types (e.g., paragraphs).</li> <li>Document QA: Focus question answering on specific detected regions.</li> </ul>"},{"location":"loops-and-groups/","title":"Loops, groups and repetitive tasks","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Display the first page \npage = pdf.pages[0]\npage.to_image(width=500)\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Display the first page  page = pdf.pages[0] page.to_image(width=500) Out[1]: <p>We can find all of the book titles by finding (Removed: on the page...</p> In\u00a0[2]: Copied! <pre>page.find_all('text:contains(\"(Removed:\")').show()\n</pre> page.find_all('text:contains(\"(Removed:\")').show() Out[2]: <p>...but it's repeated on each following page, too!</p> In\u00a0[3]: Copied! <pre>pdf.pages[1].find_all('text:contains(\"(Removed:\")').show()\n</pre> pdf.pages[1].find_all('text:contains(\"(Removed:\")').show() Out[3]: <p>No problem, you can use <code>pdf.find_all</code> the same way to do with a single page - you just can't highlight them with <code>.show()</code> the same way.</p> In\u00a0[4]: Copied! <pre>pdf.find_all('text:contains(\"(Removed:\")')\n</pre> pdf.find_all('text:contains(\"(Removed:\")') Out[4]: <pre>&lt;ElementCollection[TextElement](count=37)&gt;</pre> <p>You can see there are 37 across the entire PDF.</p> In\u00a0[5]: Copied! <pre>titles = pdf.find_all('text:contains(\"(Removed:\")')\n\ntitles.extract_each_text()\n</pre> titles = pdf.find_all('text:contains(\"(Removed:\")')  titles.extract_each_text() Out[5]: <pre>['Tristan Strong punches a hole in the sky (Removed: 1)',\n 'Upside down in the middle of nowhere (Removed: 1)',\n 'Buddhism (Removed: 1)',\n 'Voodoo (Removed: 1)',\n 'The Abenaki (Removed: 1)',\n 'Afghanistan (Removed: 1)',\n 'Alexander the Great rocks the world (Removed: 1)',\n 'The Anasazi (Removed: 1)',\n 'And then what happened, Paul Revere? (Removed: 1)',\n 'The assassination of Martin Luther King Jr (Removed: 1)',\n 'Barbara Jordan. (Removed: 1)',\n 'Bedtime for Batman (Removed: 1)',\n 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskegee Airmen leader (Removed: 1)',\n 'Bigfoot Wallace (Removed: 1)',\n 'The blaze engulfs : January 1939 to December 1941 (Removed: 1)',\n 'The boys who challenged Hitler : Knud Pedersen and the Churchill Club (Removed: 1)',\n 'Brown v. Board of Education (Removed: 1)',\n 'The Cahuilla (Removed: 1)',\n 'Cambodia (Removed: 1)',\n 'Celebrate China (Removed: 1)',\n 'Cesar Chavez : a photo-illustrated biography (Removed: 1)',\n 'The Cherokee Indians (Removed: 1)',\n 'Children of the Philippines (Removed: 1)',\n 'The Chinook people (Removed: 1)',\n 'The Choctaw (Removed: 1)',\n 'Christopher Columbus (Removed: 1)',\n 'The Comanche Indians (Removed: 1)',\n 'Dare to dream : Coretta Scott King and the civil rights movement (Removed: 1)',\n 'A day in the life of a Native American (Removed: 1)',\n 'Dear Benjamin Banneker (Removed: 1)',\n 'Dolley Madison (Removed: 1)',\n 'Dreams from my father : a story of race and inheritance (Removed: 1)',\n 'Eleanor Roosevelt : a life of discovery (Removed: 1)',\n 'Elie Wiesel : bearing witness (Removed: 1)',\n 'Elizabeth Cady Stanton : a photo-illustrated biography (Removed: 1)',\n 'Family dinner (Removed: 1)',\n 'A firestorm unleashed : January 1942 - June 1943 (Removed: 1)']</pre> <p>You can also loop through them like a normal list...</p> In\u00a0[6]: Copied! <pre>for title in titles[:10]:\n    print(title.extract_text(), title.page.number)\n</pre> for title in titles[:10]:     print(title.extract_text(), title.page.number) <pre>Tristan Strong punches a hole in the sky (Removed: 1) 1\nUpside down in the middle of nowhere (Removed: 1) 1\nBuddhism (Removed: 1) 1\nVoodoo (Removed: 1) 1\nThe Abenaki (Removed: 1) 1\nAfghanistan (Removed: 1) 1\nAlexander the Great rocks the world (Removed: 1) 1\nThe Anasazi (Removed: 1) 2\nAnd then what happened, Paul Revere? (Removed: 1) 2\nThe assassination of Martin Luther King Jr (Removed: 1) 2\n</pre> <p>...but you can also use <code>.apply</code> for a little functional-programming flavor.</p> In\u00a0[7]: Copied! <pre>titles.apply(lambda title: {\n    'title': title.extract_text(),\n    'page': title.page.number\n})\n</pre> titles.apply(lambda title: {     'title': title.extract_text(),     'page': title.page.number }) Out[7]: <pre>[{'title': 'Tristan Strong punches a hole in the sky (Removed: 1)', 'page': 1},\n {'title': 'Upside down in the middle of nowhere (Removed: 1)', 'page': 1},\n {'title': 'Buddhism (Removed: 1)', 'page': 1},\n {'title': 'Voodoo (Removed: 1)', 'page': 1},\n {'title': 'The Abenaki (Removed: 1)', 'page': 1},\n {'title': 'Afghanistan (Removed: 1)', 'page': 1},\n {'title': 'Alexander the Great rocks the world (Removed: 1)', 'page': 1},\n {'title': 'The Anasazi (Removed: 1)', 'page': 2},\n {'title': 'And then what happened, Paul Revere? (Removed: 1)', 'page': 2},\n {'title': 'The assassination of Martin Luther King Jr (Removed: 1)',\n  'page': 2},\n {'title': 'Barbara Jordan. (Removed: 1)', 'page': 2},\n {'title': 'Bedtime for Batman (Removed: 1)', 'page': 2},\n {'title': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskegee Airmen leader (Removed: 1)',\n  'page': 2},\n {'title': 'Bigfoot Wallace (Removed: 1)', 'page': 2},\n {'title': 'The blaze engulfs : January 1939 to December 1941 (Removed: 1)',\n  'page': 2},\n {'title': 'The boys who challenged Hitler : Knud Pedersen and the Churchill Club (Removed: 1)',\n  'page': 3},\n {'title': 'Brown v. Board of Education (Removed: 1)', 'page': 3},\n {'title': 'The Cahuilla (Removed: 1)', 'page': 3},\n {'title': 'Cambodia (Removed: 1)', 'page': 3},\n {'title': 'Celebrate China (Removed: 1)', 'page': 3},\n {'title': 'Cesar Chavez : a photo-illustrated biography (Removed: 1)',\n  'page': 3},\n {'title': 'The Cherokee Indians (Removed: 1)', 'page': 3},\n {'title': 'Children of the Philippines (Removed: 1)', 'page': 4},\n {'title': 'The Chinook people (Removed: 1)', 'page': 4},\n {'title': 'The Choctaw (Removed: 1)', 'page': 4},\n {'title': 'Christopher Columbus (Removed: 1)', 'page': 4},\n {'title': 'The Comanche Indians (Removed: 1)', 'page': 4},\n {'title': 'Dare to dream : Coretta Scott King and the civil rights movement (Removed: 1)',\n  'page': 4},\n {'title': 'A day in the life of a Native American (Removed: 1)', 'page': 4},\n {'title': 'Dear Benjamin Banneker (Removed: 1)', 'page': 4},\n {'title': 'Dolley Madison (Removed: 1)', 'page': 5},\n {'title': 'Dreams from my father : a story of race and inheritance (Removed: 1)',\n  'page': 5},\n {'title': 'Eleanor Roosevelt : a life of discovery (Removed: 1)', 'page': 5},\n {'title': 'Elie Wiesel : bearing witness (Removed: 1)', 'page': 5},\n {'title': 'Elizabeth Cady Stanton : a photo-illustrated biography (Removed: 1)',\n  'page': 5},\n {'title': 'Family dinner (Removed: 1)', 'page': 5},\n {'title': 'A firestorm unleashed : January 1942 - June 1943 (Removed: 1)',\n  'page': 5}]</pre> <p>I think <code>.map</code> also works on that front?</p> In\u00a0[8]: Copied! <pre>elements = page.find_all('text:contains(\"Removed:\")')\nelements.show()\n</pre> elements = page.find_all('text:contains(\"Removed:\")') elements.show() Out[8]: <p>We can filter for the ones that don't say \"Copies Removed\"</p> In\u00a0[9]: Copied! <pre>titles = elements.filter(\n    lambda element: 'Copies Removed' not in element.extract_text()\n)\ntitles.show()\n</pre> titles = elements.filter(     lambda element: 'Copies Removed' not in element.extract_text() ) titles.show() Out[9]:"},{"location":"loops-and-groups/#loops-groups-and-repetitive-tasks","title":"Loops, groups and repetitive tasks\u00b6","text":"<p>Sometimes you need to do things again and again.</p>"},{"location":"loops-and-groups/#selecting-things","title":"Selecting things\u00b6","text":"<p>Let's say we have a lot of pages that all look like this:</p>"},{"location":"loops-and-groups/#extracting-data-from-elements","title":"Extracting data from elements\u00b6","text":"<p>If you just want the text, <code>.extract_text()</code> will smush it all together, but you can also get it as a list.</p>"},{"location":"loops-and-groups/#filtering","title":"Filtering\u00b6","text":"<p>You can also filter if you only want some of them. For example, maybe we weren't sure how to pick between the different Removed: text blocks.</p>"},{"location":"ocr/","title":"Getting Text from Scanned Documents","text":"<p>Got a PDF that's actually just a bunch of scanned images? Or maybe a PDF where the text got mangled somehow? OCR (Optical Character Recognition) is your friend. Natural PDF can extract text from image-based PDFs using several different OCR engines.</p>"},{"location":"ocr/#which-ocr-engine-should-you-use","title":"Which OCR Engine Should You Use?","text":"<p>Natural PDF supports multiple OCR engines, each with different strengths:</p> Engine Best For Speed Memory Notes EasyOCR General documents, handwritten text Moderate Higher Good all-around choice PaddleOCR Asian languages, when speed matters Fast Efficient Great for Chinese, Japanese, Korean Surya OCR Highest accuracy needed Moderate Higher (GPU helps) Best quality results Gemini Complex layouts (via API) Depends on API N/A Requires API key <p>If you try to use an engine that isn't installed, Natural PDF will tell you exactly what to install.</p>"},{"location":"ocr/#basic-ocr-just-make-it-work","title":"Basic OCR: Just Make It Work","text":"<p>The simplest approach - just apply OCR to a page and get the text:</p> <pre><code>from natural_pdf import PDF\n\n# Load a PDF that needs OCR\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Apply OCR using the default engine\nocr_elements = page.apply_ocr(languages=['en'])\n\n# Extract the text (uses OCR results automatically)\ntext = page.extract_text()\nprint(text)\n</code></pre>"},{"location":"ocr/#choosing-your-ocr-engine","title":"Choosing Your OCR Engine","text":"<p>Pick the engine that fits your needs:</p> <pre><code># Use PaddleOCR for Chinese and English documents\nocr_elements = page.apply_ocr(engine='paddle', languages=['zh-cn', 'en'])\n\n# Use EasyOCR with looser confidence requirements\nocr_elements = page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.3)\n</code></pre>"},{"location":"ocr/#fine-tuning-ocr-settings","title":"Fine-Tuning OCR Settings","text":"<p>For more control, use the engine-specific options classes:</p> <pre><code>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\n\n# Configure PaddleOCR with custom settings\npaddle_opts = PaddleOCROptions(\n    # Check the documentation for all available options\n)\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# Configure EasyOCR for better paragraph detection\neasy_opts = EasyOCROptions(\n    languages=['en', 'fr'],\n    gpu=True,            # Use GPU if available\n    paragraph=True,      # Group text into paragraphs\n    text_threshold=0.6,  # How confident to be about text detection\n    batch_size=8         # Process multiple regions at once\n)\nocr_elements = page.apply_ocr(engine='easyocr', options=easy_opts)\n\n# Configure Surya for high-accuracy line detection\nsurya_opts = SuryaOCROptions(\n    languages=['en', 'de'],\n    min_confidence=0.4   # Minimum confidence for results\n)\nocr_elements = page.apply_ocr(engine='surya', options=surya_opts)\n</code></pre>"},{"location":"ocr/#how-ocr-actually-works","title":"How OCR Actually Works","text":"<p>When you run <code>page.apply_ocr()</code>, here's what happens:</p> <pre><code># Apply OCR to a page - this adds text elements to the page\nocr_elements = page.apply_ocr(engine='easyocr')\nprint(f\"Found {len(ocr_elements)} text elements via OCR\")\n\n# You can also OCR just a specific region\ntitle = page.find('text:contains(\"Title\")')\nif title:\n    content_region = title.below(height=300)\n    region_ocr_elements = content_region.apply_ocr(engine='paddle', languages=['en'])\n</code></pre> <p>Note: Running OCR again on the same area will replace the previous OCR results.</p>"},{"location":"ocr/#working-with-ocr-results","title":"Working with OCR Results","text":"<p>Once you've run OCR, the text elements work just like regular PDF text:</p> <pre><code># Find all OCR-generated text\nocr_text = page.find_all('text[source=ocr]')\n\n# Find only high-confidence OCR text\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\n\n# Extract just the OCR text\nocr_content = page.find_all('text[source=ocr]').extract_text()\n\n# Search within OCR results\nnames = page.find_all('text[source=ocr]:contains(\"Smith\")', case=False)\n</code></pre>"},{"location":"ocr/#debugging-ocr-results","title":"Debugging OCR Results","text":"<p>See what the OCR engine actually found:</p> <pre><code># Run OCR first\nocr_elements = page.apply_ocr()\n\n# Color-code by confidence level\nfor element in ocr_elements:\n    if element.confidence &gt;= 0.8:\n        color = \"green\"     # High confidence\n    elif element.confidence &gt;= 0.5:\n        color = \"yellow\"    # Medium confidence  \n    else:\n        color = \"red\"       # Low confidence\n\n    element.highlight(color=color, label=f\"OCR ({element.confidence:.2f})\")\n\n# Show the results\npage.to_image()\n</code></pre> <pre><code># Or just highlight the good stuff\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nhigh_conf.highlight(color=\"green\", label=\"High Confidence OCR\")\npage.to_image()\n</code></pre>"},{"location":"ocr/#advanced-local-detection-llm-cleanup","title":"Advanced: Local Detection + LLM Cleanup","text":"<p>For really tricky documents, you can use a local model to find text regions, then send those specific regions to a language model for cleanup:</p> <pre><code>from natural_pdf.ocr.utils import direct_ocr_llm\nimport openai\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Step 1: Find text regions locally (fast, no API calls)\npage.apply_ocr('paddle', resolution=120, detect_only=True)\n\n# Step 2: Set up LLM for cleanup\nclient = openai.OpenAI(\n    base_url=\"https://api.anthropic.com/v1/\",  \n    api_key='your-api-key-here'\n)\n\nprompt = \"\"\"OCR this image. Return only the exact text from the image. \nInclude misspellings, punctuation, etc. Do not add quotes or comments.\nThe text is from a Greek spreadsheet, so expect Modern Greek or numbers.\"\"\"\n\n# Step 3: Define cleanup function\ndef correct(region):\n    return direct_ocr_llm(\n        region, \n        client, \n        prompt=prompt, \n        resolution=300, \n        model=\"claude-3-5-haiku-20241022\"\n    )\n\n# Step 4: Apply cleanup to each detected region\npage.correct_ocr(correct)\n\n# You're done! The page now has cleaned-up text\n</code></pre>"},{"location":"ocr/#interactive-ocr-correction","title":"Interactive OCR Correction","text":"<p>Natural PDF includes a web app for reviewing and correcting OCR results:</p> <ol> <li> <p>Package your PDF data: <pre><code>from natural_pdf.utils.packaging import create_correction_task_package\n\n# After running OCR on your PDF\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</code></pre></p> </li> <li> <p>Start the web app: <pre><code># Find where Natural PDF is installed\nNATURAL_PDF_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/natural_pdf\n\n# Start the web server\ncd $NATURAL_PDF_PATH/templates/spa\npython -m http.server 8000\n</code></pre></p> </li> <li> <p>Use the app:    Open <code>http://localhost:8000</code> in your browser and drag in your <code>correction_package.zip</code> file to review and edit the OCR results.</p> </li> </ol>"},{"location":"ocr/#next-steps","title":"Next Steps","text":"<p>Once you've got OCR working:</p> <ul> <li>Layout Analysis: Automatically detect document structure</li> <li>Document QA: Ask questions about your newly-readable documents</li> </ul>"},{"location":"pdf-navigation/","title":"PDF Navigation","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Open a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n</pre> from natural_pdf import PDF  # Open a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\") In\u00a0[2]: Copied! <pre># Get the total number of pages\nnum_pages = len(pdf)\nprint(f\"This PDF has {num_pages} pages\")\n\n# Get a specific page (0-indexed)\nfirst_page = pdf.pages[0]\nlast_page = pdf.pages[-1]\n\n# Iterate through the first 20 pages\nfor page in pdf.pages[:20]:\n    print(f\"Page {page.number} has {len(page.extract_text())} characters\")\n</pre> # Get the total number of pages num_pages = len(pdf) print(f\"This PDF has {num_pages} pages\")  # Get a specific page (0-indexed) first_page = pdf.pages[0] last_page = pdf.pages[-1]  # Iterate through the first 20 pages for page in pdf.pages[:20]:     print(f\"Page {page.number} has {len(page.extract_text())} characters\") <pre>This PDF has 153 pages\nPage 1 has 985 characters\nPage 2 has 778 characters\nPage 3 has 522 characters\nPage 4 has 984 characters\nPage 5 has 778 characters\nPage 6 has 523 characters\nPage 7 has 982 characters\nPage 8 has 772 characters\nPage 9 has 522 characters\nPage 10 has 1008 characters\nPage 11 has 796 characters\nPage 12 has 532 characters\nPage 13 has 986 characters\nPage 14 has 780 characters\nPage 15 has 523 characters\nPage 16 has 990 characters\nPage 17 has 782 characters\nPage 18 has 520 characters\nPage 19 has 1006 characters\nPage 20 has 795 characters\n</pre> In\u00a0[3]: Copied! <pre># Page dimensions in points (1/72 inch)\nprint(page.width, page.height)\n\n# Page number (1-indexed as shown in PDF viewers)\nprint(page.number)\n\n# Page index (0-indexed position in the PDF)\nprint(page.index)\n</pre> # Page dimensions in points (1/72 inch) print(page.width, page.height)  # Page number (1-indexed as shown in PDF viewers) print(page.number)  # Page index (0-indexed position in the PDF) print(page.index) <pre>612 792\n20\n19\n</pre> In\u00a0[4]: Copied! <pre># Extract text from all pages\nall_text = pdf.extract_text()\n\n# Find elements across all pages\nall_headings = pdf.find_all('text[size&gt;=14]:bold')\n\n# Add exclusion zones to all pages (like headers/footers)\npdf.add_exclusion(\n    lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n    label=\"header\"\n)\n</pre> # Extract text from all pages all_text = pdf.extract_text()  # Find elements across all pages all_headings = pdf.find_all('text[size&gt;=14]:bold')  # Add exclusion zones to all pages (like headers/footers) pdf.add_exclusion(     lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,     label=\"header\" ) Out[4]: <pre>&lt;natural_pdf.core.pdf.PDF at 0x109bee9b0&gt;</pre> In\u00a0[5]: Copied! <pre># Extract text from specific pages\ntext = pdf.pages[2:5].extract_text()\n\n# Find elements across specific pages\nelements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")')\n</pre> # Extract text from specific pages text = pdf.pages[2:5].extract_text()  # Find elements across specific pages elements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")') <pre>2025-05-06T15:29:28.620225Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,620] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> <pre>2025-05-06T15:29:28.631200Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,631] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> <pre>2025-05-06T15:29:28.640121Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,640] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> In\u00a0[6]: Copied! <pre># Get sections with headings as section starts\nsections = pdf.pages.get_sections(\n    start_elements='text[size&gt;=14]:bold',\n    new_section_on_page_break=False\n)\n</pre> # Get sections with headings as section starts sections = pdf.pages.get_sections(     start_elements='text[size&gt;=14]:bold',     new_section_on_page_break=False )"},{"location":"pdf-navigation/#pdf-navigation","title":"PDF Navigation\u00b6","text":"<p>This guide covers the basics of working with PDFs in Natural PDF - opening documents, accessing pages, and navigating through content.</p>"},{"location":"pdf-navigation/#opening-a-pdf","title":"Opening a PDF\u00b6","text":"<p>The main entry point to Natural PDF is the <code>PDF</code> class:</p>"},{"location":"pdf-navigation/#accessing-pages","title":"Accessing Pages\u00b6","text":"<p>Once you have a PDF object, you can access its pages:</p>"},{"location":"pdf-navigation/#page-properties","title":"Page Properties\u00b6","text":"<p>Each <code>Page</code> object has useful properties:</p>"},{"location":"pdf-navigation/#working-across-pages","title":"Working Across Pages\u00b6","text":"<p>Natural PDF makes it easy to work with content across multiple pages:</p>"},{"location":"pdf-navigation/#the-page-collection","title":"The Page Collection\u00b6","text":"<p>The <code>pdf.pages</code> object is a <code>PageCollection</code> that allows batch operations on pages:</p>"},{"location":"pdf-navigation/#document-sections-across-pages","title":"Document Sections Across Pages\u00b6","text":"<p>You can extract sections that span across multiple pages:</p>"},{"location":"pdf-navigation/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to navigate PDFs, you can:</p> <ul> <li>Find elements using selectors</li> <li>Extract text from your documents</li> <li>Work with specific regions</li> </ul>"},{"location":"process-forms-and-invoices/","title":"Extract Data from Forms and Invoices","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Extract data using a simple list that matches the inspection report columns\ndata = page.extract(schema=[\"site\", \"violation count\", \"date\", \"inspection number\", \"summary\"]).extracted()\n\n# Access the extracted information\nprint(f\"Site: {data.site}\")\nprint(f\"Violations: {data.violation_count}\")\nprint(f\"Date: {data.date}\")\nprint(f\"Inspection #: {data.inspection_number}\")\n\n# Check confidence levels\nprint(f\"Confidence \u2013 Site: {data.site_confidence:.2f}\")\nprint(f\"Confidence \u2013 Violations: {data.violation_count_confidence:.2f}\")\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Extract data using a simple list that matches the inspection report columns data = page.extract(schema=[\"site\", \"violation count\", \"date\", \"inspection number\", \"summary\"]).extracted()  # Access the extracted information print(f\"Site: {data.site}\") print(f\"Violations: {data.violation_count}\") print(f\"Date: {data.date}\") print(f\"Inspection #: {data.inspection_number}\")  # Check confidence levels print(f\"Confidence \u2013 Site: {data.site_confidence:.2f}\") print(f\"Confidence \u2013 Violations: {data.violation_count_confidence:.2f}\") <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>Device set to use mps:0\n</pre> <pre>Site: Durham\u2019s Meatpacking  \nViolations: 7\nDate: February 3, 1905\nInspection #: None\nConfidence \u2013 Site: 0.58\nConfidence \u2013 Violations: 0.41\n</pre> <p>This works completely offline using document question-answering models.</p> In\u00a0[2]: Copied! <pre>from pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Define exactly what you want to extract for the inspection report\nclass InspectionReport(BaseModel):\n    site_name: str = Field(description=\"Name of the inspection site\")\n    violation_count: int = Field(description=\"Number of violations found\")\n    inspection_date: str = Field(description=\"Inspection date in any format\")\n    inspection_number: str = Field(description=\"Inspection reference ID\")\n    summary: str = Field(description=\"Inspection summary paragraph\")\n\n# Set up LLM client (using Anthropic here)\nclient = OpenAI(\n    api_key=\"your-api-key\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\n# Extract structured data\npage.extract(schema=InspectionReport, client=client, model=\"gemini-2.0-flash\")\n\ntry:\n    report_data = page.extracted()\n    print(f\"Site: {report_data.site_name}\")\n    print(f\"Violations: {report_data.violation_count}\")\n    print(f\"Inspection #: {report_data.inspection_number}\")\nexcept Exception as e:\n    print(\"Extraction failed with error\", e)\n</pre> from pydantic import BaseModel, Field from openai import OpenAI  # Define exactly what you want to extract for the inspection report class InspectionReport(BaseModel):     site_name: str = Field(description=\"Name of the inspection site\")     violation_count: int = Field(description=\"Number of violations found\")     inspection_date: str = Field(description=\"Inspection date in any format\")     inspection_number: str = Field(description=\"Inspection reference ID\")     summary: str = Field(description=\"Inspection summary paragraph\")  # Set up LLM client (using Anthropic here) client = OpenAI(     api_key=\"your-api-key\",     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" )  # Extract structured data page.extract(schema=InspectionReport, client=client, model=\"gemini-2.0-flash\")  try:     report_data = page.extracted()     print(f\"Site: {report_data.site_name}\")     print(f\"Violations: {report_data.violation_count}\")     print(f\"Inspection #: {report_data.inspection_number}\") except Exception as e:     print(\"Extraction failed with error\", e) <pre>Extraction failed with error Stored result for 'structured' indicates a failed extraction attempt. Error: Error code: 400 - [{'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}]\n</pre> In\u00a0[3]: Copied! <pre># Sometimes data is in specific areas of the page\nheader_region = page.create_region(0, 0, page.width, page.height * 0.3)\nfooter_region = page.create_region(0, page.height * 0.7, page.width, page.height)\n\n# Extract company info from header\ncompany_data = header_region.extract(\n    schema=[\"company name\", \"address\", \"phone\"]\n).extracted()\n\n# Extract totals from footer  \ntotals_data = footer_region.extract(\n    schema=[\"subtotal\", \"tax\", \"total\"]\n).extracted()\n</pre> # Sometimes data is in specific areas of the page header_region = page.create_region(0, 0, page.width, page.height * 0.3) footer_region = page.create_region(0, page.height * 0.7, page.width, page.height)  # Extract company info from header company_data = header_region.extract(     schema=[\"company name\", \"address\", \"phone\"] ).extracted()  # Extract totals from footer   totals_data = footer_region.extract(     schema=[\"subtotal\", \"tax\", \"total\"] ).extracted() In\u00a0[4]: Copied! <pre>import os\nfrom pathlib import Path\n\n# Define your extraction schema\nclass FormData(BaseModel):\n    applicant_name: str\n    application_date: str  \n    reference_number: str\n    status: str = Field(default=\"unknown\")\n\n# Process all PDFs in a folder\nform_results = []\npdf_folder = Path(\"forms/\")\n\nfor pdf_file in pdf_folder.glob(\"*.pdf\"):\n    print(f\"Processing {pdf_file.name}...\")\n    \n    pdf = PDF(str(pdf_file))\n    page = pdf.pages[0]  # Assuming single-page forms\n    \n    # Extract data\n    page.extract(schema=FormData, client=client)\n    data = page.extracted()\n    \n    # Add filename for tracking\n    result = {\n        \"filename\": pdf_file.name,\n        \"applicant_name\": data.applicant_name,\n        \"application_date\": data.application_date,\n        \"reference_number\": data.reference_number,\n        \"status\": data.status\n    }\n    form_results.append(result)\n    \n    pdf.close()  # Clean up\n\n# Save results to CSV\nimport pandas as pd\ndf = pd.DataFrame(form_results)\ndf.to_csv(\"extracted_form_data.csv\", index=False)\nprint(f\"Processed {len(form_results)} forms\")\n</pre> import os from pathlib import Path  # Define your extraction schema class FormData(BaseModel):     applicant_name: str     application_date: str       reference_number: str     status: str = Field(default=\"unknown\")  # Process all PDFs in a folder form_results = [] pdf_folder = Path(\"forms/\")  for pdf_file in pdf_folder.glob(\"*.pdf\"):     print(f\"Processing {pdf_file.name}...\")          pdf = PDF(str(pdf_file))     page = pdf.pages[0]  # Assuming single-page forms          # Extract data     page.extract(schema=FormData, client=client)     data = page.extracted()          # Add filename for tracking     result = {         \"filename\": pdf_file.name,         \"applicant_name\": data.applicant_name,         \"application_date\": data.application_date,         \"reference_number\": data.reference_number,         \"status\": data.status     }     form_results.append(result)          pdf.close()  # Clean up  # Save results to CSV import pandas as pd df = pd.DataFrame(form_results) df.to_csv(\"extracted_form_data.csv\", index=False) print(f\"Processed {len(form_results)} forms\") <pre>Processed 0 forms\n</pre> In\u00a0[5]: Copied! <pre># Apply OCR before extraction\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Filter out low-confidence OCR text to avoid noise\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nprint(f\"Using {len(reliable_text)} high-confidence OCR elements\")\n\n# Now extract data (works on OCR'd text)\ndata = page.extract(schema=[\"invoice number\", \"total\", \"date\"]).extracted()\n</pre> # Apply OCR before extraction page.apply_ocr(engine='easyocr', languages=['en'])  # Filter out low-confidence OCR text to avoid noise reliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]') print(f\"Using {len(reliable_text)} high-confidence OCR elements\")  # Now extract data (works on OCR'd text) data = page.extract(schema=[\"invoice number\", \"total\", \"date\"]).extracted() <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> <pre>Using 29 high-confidence OCR elements\n</pre>"},{"location":"process-forms-and-invoices/#extract-data-from-forms-and-invoices","title":"Extract Data from Forms and Invoices\u00b6","text":"<p>You have a stack of invoices, forms, or structured documents where you need to pull out specific pieces of information - invoice numbers, totals, dates, names, etc. Here's how to automate that extraction.</p>"},{"location":"process-forms-and-invoices/#the-problem","title":"The Problem\u00b6","text":"<p>Manual data entry from PDFs is slow and error-prone. You need to:</p> <ul> <li>Extract the same fields from hundreds of similar documents</li> <li>Handle slight variations in layout between documents</li> <li>Get structured data you can actually work with</li> <li>Maintain accuracy while processing quickly</li> </ul>"},{"location":"process-forms-and-invoices/#quick-solution-list-the-fields-you-want","title":"Quick Solution: List the Fields You Want\u00b6","text":"<p>Don't overthink it - just tell Natural PDF what information you're looking for:</p>"},{"location":"process-forms-and-invoices/#for-complex-data-use-pydantic-schemas","title":"For Complex Data: Use Pydantic Schemas\u00b6","text":"<p>When you need more control over data types and validation:</p>"},{"location":"process-forms-and-invoices/#handle-different-document-layouts","title":"Handle Different Document Layouts\u00b6","text":"<p>For documents that vary in structure, use spatial hints:</p>"},{"location":"process-forms-and-invoices/#process-multiple-documents","title":"Process Multiple Documents\u00b6","text":"<p>Batch process a folder of similar documents:</p>"},{"location":"process-forms-and-invoices/#handle-scanned-documents","title":"Handle Scanned Documents\u00b6","text":"<p>For image-based PDFs, apply OCR first:</p>"},{"location":"process-forms-and-invoices/#common-form-patterns","title":"Common Form Patterns\u00b6","text":""},{"location":"process-forms-and-invoices/#validation-and-error-handling","title":"Validation and Error Handling\u00b6","text":"<p>Check your extracted data for common issues:</p> <pre>def validate_invoice_data(data):\n    issues = []\n    \n    # Check for missing required fields\n    if not data.invoice_number or data.invoice_number.strip() == \"\":\n        issues.append(\"Missing invoice number\")\n    \n    # Validate amounts\n    if data.total_amount &lt;= 0:\n        issues.append(\"Invalid total amount\")\n    \n    # Check date format\n    try:\n        from datetime import datetime\n        datetime.strptime(data.invoice_date, \"%Y-%m-%d\")\n    except ValueError:\n        # Try common date formats\n        common_formats = [\"%m/%d/%Y\", \"%d/%m/%Y\", \"%B %d, %Y\"]\n        date_valid = False\n        for fmt in common_formats:\n            try:\n                datetime.strptime(data.invoice_date, fmt)\n                date_valid = True\n                break\n            except ValueError:\n                continue\n        if not date_valid:\n            issues.append(f\"Invalid date format: {data.invoice_date}\")\n    \n    return issues\n\n# Validate extracted data\nvalidation_issues = validate_invoice_data(invoice_data)\nif validation_issues:\n    print(\"Data quality issues found:\")\n    for issue in validation_issues:\n        print(f\"- {issue}\")\nelse:\n    print(\"Data validation passed!\")\n</pre>"},{"location":"process-forms-and-invoices/#improve-accuracy-with-context","title":"Improve Accuracy with Context\u00b6","text":"<p>Give the AI more context for better extraction:</p> <pre># Add context about the document type\nextraction_prompt = \"\"\"\nThis is a medical insurance claim form. \nExtract the following information, paying attention to:\n- Policy numbers are usually 10-12 digits\n- Claim amounts should be in dollars\n- Dates should be in MM/DD/YYYY format\n- Provider names are usually at the top of the form\n\"\"\"\n\nclass InsuranceClaim(BaseModel):\n    policy_number: str = Field(description=\"Insurance policy number (10-12 digits)\")\n    claim_amount: float = Field(description=\"Total claim amount in USD\")\n    service_date: str = Field(description=\"Date of service in MM/DD/YYYY format\")\n    provider_name: str = Field(description=\"Healthcare provider name\")\n    patient_name: str = Field(description=\"Patient full name\")\n\n# Use custom prompt for better results\npage.extract(\n    schema=InsuranceClaim, \n    client=client,\n    prompt=extraction_prompt\n)\n</pre>"},{"location":"process-forms-and-invoices/#debug-extraction-issues","title":"Debug Extraction Issues\u00b6","text":"<p>When extraction isn't working well:</p> <pre># 1. Check what text the AI can actually see\nextracted_text = page.extract_text()\nprint(\"Available text:\")\nprint(extracted_text[:500])  # First 500 characters\n\n# 2. Try extracting with lower confidence threshold\ndata = page.extract(\n    schema=[\"invoice number\", \"total\"], \n    min_confidence=0.5  # Lower threshold\n).extracted()\n\n# 3. Check confidence scores for each field\nfor field_name in data.__fields__:\n    confidence_field = f\"{field_name}_confidence\"\n    if hasattr(data, confidence_field):\n        confidence = getattr(data, confidence_field)\n        value = getattr(data, field_name)\n        print(f\"{field_name}: '{value}' (confidence: {confidence:.2f})\")\n\n# 4. Try vision mode if text mode fails\nif any(getattr(data, f\"{field}_confidence\", 0) &lt; 0.7 for field in [\"invoice_number\", \"total\"]):\n    print(\"Low confidence detected, trying vision mode...\")\n    page.extract(schema=[\"invoice number\", \"total\"], client=client, using='vision')\n    data = page.extracted()\n</pre>"},{"location":"process-forms-and-invoices/","title":"Extract Data from Forms and Invoices","text":"<p>You have a stack of invoices, forms, or structured documents where you need to pull out specific pieces of information - invoice numbers, totals, dates, names, etc. Here's how to automate that extraction.</p>"},{"location":"process-forms-and-invoices/#the-problem","title":"The Problem","text":"<p>Manual data entry from PDFs is slow and error-prone. You need to: - Extract the same fields from hundreds of similar documents - Handle slight variations in layout between documents - Get structured data you can actually work with - Maintain accuracy while processing quickly</p>"},{"location":"process-forms-and-invoices/#quick-solution-list-the-fields-you-want","title":"Quick Solution: List the Fields You Want","text":"<p>Don't overthink it - just tell Natural PDF what information you're looking for:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Extract data using a simple list that matches the inspection report columns\ndata = page.extract(schema=[\"site\", \"violation count\", \"date\", \"inspection number\", \"summary\"]).extracted()\n\n# Access the extracted information\nprint(f\"Site: {data.site}\")\nprint(f\"Violations: {data.violation_count}\")\nprint(f\"Date: {data.date}\")\nprint(f\"Inspection #: {data.inspection_number}\")\n\n# Check confidence levels\nprint(f\"Confidence \u2013 Site: {data.site_confidence:.2f}\")\nprint(f\"Confidence \u2013 Violations: {data.violation_count_confidence:.2f}\")\n</code></pre> <p>This works completely offline using document question-answering models.</p>"},{"location":"process-forms-and-invoices/#for-complex-data-use-pydantic-schemas","title":"For Complex Data: Use Pydantic Schemas","text":"<p>When you need more control over data types and validation:</p> <pre><code>from pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Define exactly what you want to extract for the inspection report\nclass InspectionReport(BaseModel):\n    site_name: str = Field(description=\"Name of the inspection site\")\n    violation_count: int = Field(description=\"Number of violations found\")\n    inspection_date: str = Field(description=\"Inspection date in any format\")\n    inspection_number: str = Field(description=\"Inspection reference ID\")\n    summary: str = Field(description=\"Inspection summary paragraph\")\n\n# Set up LLM client (using Anthropic here)\nclient = OpenAI(\n    api_key=\"your-api-key\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\n# Extract structured data\npage.extract(schema=InspectionReport, client=client, model=\"gemini-2.0-flash\")\n\ntry:\n    report_data = page.extracted()\n    print(f\"Site: {report_data.site_name}\")\n    print(f\"Violations: {report_data.violation_count}\")\n    print(f\"Inspection #: {report_data.inspection_number}\")\nexcept Exception as e:\n    print(\"Extraction failed with error\", e)\n</code></pre>"},{"location":"process-forms-and-invoices/#handle-different-document-layouts","title":"Handle Different Document Layouts","text":"<p>For documents that vary in structure, use spatial hints:</p> <pre><code># Sometimes data is in specific areas of the page\nheader_region = page.create_region(0, 0, page.width, page.height * 0.3)\nfooter_region = page.create_region(0, page.height * 0.7, page.width, page.height)\n\n# Extract company info from header\ncompany_data = header_region.extract(\n    schema=[\"company name\", \"address\", \"phone\"]\n).extracted()\n\n# Extract totals from footer  \ntotals_data = footer_region.extract(\n    schema=[\"subtotal\", \"tax\", \"total\"]\n).extracted()\n</code></pre>"},{"location":"process-forms-and-invoices/#process-multiple-documents","title":"Process Multiple Documents","text":"<p>Batch process a folder of similar documents:</p> <pre><code>import os\nfrom pathlib import Path\n\n# Define your extraction schema\nclass FormData(BaseModel):\n    applicant_name: str\n    application_date: str  \n    reference_number: str\n    status: str = Field(default=\"unknown\")\n\n# Process all PDFs in a folder\nform_results = []\npdf_folder = Path(\"forms/\")\n\nfor pdf_file in pdf_folder.glob(\"*.pdf\"):\n    print(f\"Processing {pdf_file.name}...\")\n\n    pdf = PDF(str(pdf_file))\n    page = pdf.pages[0]  # Assuming single-page forms\n\n    # Extract data\n    page.extract(schema=FormData, client=client)\n    data = page.extracted()\n\n    # Add filename for tracking\n    result = {\n        \"filename\": pdf_file.name,\n        \"applicant_name\": data.applicant_name,\n        \"application_date\": data.application_date,\n        \"reference_number\": data.reference_number,\n        \"status\": data.status\n    }\n    form_results.append(result)\n\n    pdf.close()  # Clean up\n\n# Save results to CSV\nimport pandas as pd\ndf = pd.DataFrame(form_results)\ndf.to_csv(\"extracted_form_data.csv\", index=False)\nprint(f\"Processed {len(form_results)} forms\")\n</code></pre>"},{"location":"process-forms-and-invoices/#handle-scanned-documents","title":"Handle Scanned Documents","text":"<p>For image-based PDFs, apply OCR first:</p> <pre><code># Apply OCR before extraction\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Filter out low-confidence OCR text to avoid noise\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nprint(f\"Using {len(reliable_text)} high-confidence OCR elements\")\n\n# Now extract data (works on OCR'd text)\ndata = page.extract(schema=[\"invoice number\", \"total\", \"date\"]).extracted()\n</code></pre>"},{"location":"process-forms-and-invoices/#common-form-patterns","title":"Common Form Patterns","text":""},{"location":"process-forms-and-invoices/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>Check your extracted data for common issues:</p> <pre><code>def validate_invoice_data(data):\n    issues = []\n\n    # Check for missing required fields\n    if not data.invoice_number or data.invoice_number.strip() == \"\":\n        issues.append(\"Missing invoice number\")\n\n    # Validate amounts\n    if data.total_amount &lt;= 0:\n        issues.append(\"Invalid total amount\")\n\n    # Check date format\n    try:\n        from datetime import datetime\n        datetime.strptime(data.invoice_date, \"%Y-%m-%d\")\n    except ValueError:\n        # Try common date formats\n        common_formats = [\"%m/%d/%Y\", \"%d/%m/%Y\", \"%B %d, %Y\"]\n        date_valid = False\n        for fmt in common_formats:\n            try:\n                datetime.strptime(data.invoice_date, fmt)\n                date_valid = True\n                break\n            except ValueError:\n                continue\n        if not date_valid:\n            issues.append(f\"Invalid date format: {data.invoice_date}\")\n\n    return issues\n\n# Validate extracted data\nvalidation_issues = validate_invoice_data(invoice_data)\nif validation_issues:\n    print(\"Data quality issues found:\")\n    for issue in validation_issues:\n        print(f\"- {issue}\")\nelse:\n    print(\"Data validation passed!\")\n</code></pre>"},{"location":"process-forms-and-invoices/#improve-accuracy-with-context","title":"Improve Accuracy with Context","text":"<p>Give the AI more context for better extraction:</p> <pre><code># Add context about the document type\nextraction_prompt = \"\"\"\nThis is a medical insurance claim form. \nExtract the following information, paying attention to:\n- Policy numbers are usually 10-12 digits\n- Claim amounts should be in dollars\n- Dates should be in MM/DD/YYYY format\n- Provider names are usually at the top of the form\n\"\"\"\n\nclass InsuranceClaim(BaseModel):\n    policy_number: str = Field(description=\"Insurance policy number (10-12 digits)\")\n    claim_amount: float = Field(description=\"Total claim amount in USD\")\n    service_date: str = Field(description=\"Date of service in MM/DD/YYYY format\")\n    provider_name: str = Field(description=\"Healthcare provider name\")\n    patient_name: str = Field(description=\"Patient full name\")\n\n# Use custom prompt for better results\npage.extract(\n    schema=InsuranceClaim, \n    client=client,\n    prompt=extraction_prompt\n)\n</code></pre>"},{"location":"process-forms-and-invoices/#debug-extraction-issues","title":"Debug Extraction Issues","text":"<p>When extraction isn't working well:</p> <pre><code># 1. Check what text the AI can actually see\nextracted_text = page.extract_text()\nprint(\"Available text:\")\nprint(extracted_text[:500])  # First 500 characters\n\n# 2. Try extracting with lower confidence threshold\ndata = page.extract(\n    schema=[\"invoice number\", \"total\"], \n    min_confidence=0.5  # Lower threshold\n).extracted()\n\n# 3. Check confidence scores for each field\nfor field_name in data.__fields__:\n    confidence_field = f\"{field_name}_confidence\"\n    if hasattr(data, confidence_field):\n        confidence = getattr(data, confidence_field)\n        value = getattr(data, field_name)\n        print(f\"{field_name}: '{value}' (confidence: {confidence:.2f})\")\n\n# 4. Try vision mode if text mode fails\nif any(getattr(data, f\"{field}_confidence\", 0) &lt; 0.7 for field in [\"invoice_number\", \"total\"]):\n    print(\"Low confidence detected, trying vision mode...\")\n    page.extract(schema=[\"invoice number\", \"total\"], client=client, using='vision')\n    data = page.extracted()\n</code></pre>"},{"location":"quick-reference/","title":"Quick Reference","text":""},{"location":"quick-reference/#quick-reference","title":"Quick Reference\u00b6","text":""},{"location":"quick-reference/#essential-workflows","title":"Essential Workflows\u00b6","text":""},{"location":"quick-reference/#basic-text-extraction","title":"Basic Text Extraction\u00b6","text":"<pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\ntext = page.extract_text()\n</pre>"},{"location":"quick-reference/#find-extract-pattern","title":"Find \u2192 Extract Pattern\u00b6","text":"<pre># Find specific elements, then extract\nheading = page.find('text:contains(\"Summary\"):bold')\ncontent = heading.below().extract_text()\n</pre>"},{"location":"quick-reference/#ocr-for-scanned-documents","title":"OCR for Scanned Documents\u00b6","text":"<pre># Apply OCR first, then extract\npage.apply_ocr(engine='easyocr', languages=['en'])\ntext = page.extract_text()\n</pre>"},{"location":"quick-reference/#layout-analysis-table-extraction","title":"Layout Analysis \u2192 Table Extraction\u00b6","text":"<pre># Detect layout, then extract tables\npage.analyze_layout(engine='yolo')\ntable_region = page.find('region[type=table]')\ndata = table_region.extract_table()\n</pre>"},{"location":"quick-reference/#common-selectors","title":"Common Selectors\u00b6","text":""},{"location":"quick-reference/#text-content","title":"Text Content\u00b6","text":"<pre>page.find('text:contains(\"Invoice\")')           # Contains text\npage.find('text:contains(\"total\")', case=False) # Case insensitive\npage.find('text:contains(\"\\\\d+\")', regex=True)  # Regex pattern\n</pre>"},{"location":"quick-reference/#text-formatting","title":"Text Formatting\u00b6","text":"<pre>page.find_all('text:bold')                      # Bold text\npage.find_all('text:italic')                    # Italic text\npage.find_all('text[size&gt;=12]')                 # Large text\npage.find_all('text[fontname*=Arial]')          # Specific font\n</pre>"},{"location":"quick-reference/#spatial-relationships","title":"Spatial Relationships\u00b6","text":"<pre>page.find('text:above(\"line[width&gt;=2]\")')       # Above thick line\npage.find('text:below(\"text:contains(\"Title\")\")')  # Below title\npage.find('text:near(\"image\")')                 # Near images\n</pre>"},{"location":"quick-reference/#layout-elements","title":"Layout Elements\u00b6","text":"<pre>page.find_all('line:horizontal')                # Horizontal lines\npage.find_all('rect')                           # Rectangles\npage.find_all('region[type=table]')             # Detected tables\npage.find_all('region[type=title]')             # Detected titles\n</pre>"},{"location":"quick-reference/#ocr-and-sources","title":"OCR and Sources\u00b6","text":"<pre>page.find_all('text[source=ocr]')               # OCR-generated text\npage.find_all('text[source=pdf]')               # Original PDF text\npage.find_all('text[confidence&gt;=0.8]')          # High-confidence OCR\n</pre>"},{"location":"quick-reference/#essential-methods","title":"Essential Methods\u00b6","text":""},{"location":"quick-reference/#finding-elements","title":"Finding Elements\u00b6","text":"<pre>page.find(selector)                             # First match\npage.find_all(selector)                         # All matches\nelement.next()                                  # Next element in reading order\nelement.previous()                              # Previous element\n</pre>"},{"location":"quick-reference/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<pre>element.above(height=100)                       # Region above element\nelement.below(until='line:horizontal')          # Below until boundary\nelement.left(width=200)                         # Region to the left\nelement.right()                                 # Region to the right\n</pre>"},{"location":"quick-reference/#text-extraction","title":"Text Extraction\u00b6","text":"<pre>page.extract_text()                             # All text from page\npage.extract_text(layout=True)                  # Preserve layout\nelement.extract_text()                          # Text from specific element\nregion.extract_text()                           # Text from region\n</pre>"},{"location":"quick-reference/#table-extraction","title":"Table Extraction\u00b6","text":"<pre>page.extract_table()                            # First table on page\nregion.extract_table()                          # Table from region\nregion.extract_table(method='tatr')             # Force TATR method\nregion.extract_table(method='pdfplumber')       # Force pdfplumber method\n</pre>"},{"location":"quick-reference/#ocr","title":"OCR\u00b6","text":"<pre>page.apply_ocr()                                # Default OCR\npage.apply_ocr(engine='paddle', languages=['en', 'zh-cn'])\npage.apply_ocr(engine='easyocr', min_confidence=0.8)\nregion.apply_ocr()                              # OCR specific region\n</pre>"},{"location":"quick-reference/#layout-analysis","title":"Layout Analysis\u00b6","text":"<pre>page.analyze_layout()                           # Default YOLO\npage.analyze_layout(engine='tatr')              # Table-focused\npage.analyze_layout(engine='surya')             # High accuracy\npage.clear_detected_layout_regions()           # Clear previous results\n</pre>"},{"location":"quick-reference/#document-qa","title":"Document QA\u00b6","text":"<pre>result = page.ask(\"What is the total amount?\")\nprint(result.answer)                            # The answer\nprint(result.confidence)                        # Confidence score\nresult.show()                                   # Highlight answer location\n</pre>"},{"location":"quick-reference/#structured-data-extraction","title":"Structured Data Extraction\u00b6","text":"<pre># Simple approach\ndata = page.extract(schema=[\"company\", \"date\", \"total\"]).extracted()\n\n# With Pydantic schema\nfrom pydantic import BaseModel\nclass Invoice(BaseModel):\n    company: str\n    total: float\n    date: str\n\ndata = page.extract(schema=Invoice, client=client).extracted()\n</pre>"},{"location":"quick-reference/#visualization-debugging","title":"Visualization &amp; Debugging\u00b6","text":""},{"location":"quick-reference/#highlighting","title":"Highlighting\u00b6","text":"<pre>elements.highlight(color=\"red\")                 # Simple highlight\nelements.highlight(color=\"blue\", label=\"Headers\") # With label\nelements.highlight(group_by='type')             # Color by type\npage.clear_highlights()                         # Remove highlights\n</pre>"},{"location":"quick-reference/#viewing","title":"Viewing\u00b6","text":"<pre>page.show()                                     # Show page with highlights\nelement.show()                                  # Show specific element\npage.to_image(width=700)                        # Generate image\nregion.to_image(crop=True)                 # Crop to region only\n</pre>"},{"location":"quick-reference/#interactive-viewer","title":"Interactive Viewer\u00b6","text":"<pre>page.viewer()                                   # Launch interactive viewer (Jupyter)\n</pre>"},{"location":"quick-reference/#exclusion-zones","title":"Exclusion Zones\u00b6","text":""},{"location":"quick-reference/#page-level-exclusions","title":"Page-Level Exclusions\u00b6","text":"<pre>header = page.find('text:contains(\"CONFIDENTIAL\")').above()\npage.add_exclusion(header)                      # Exclude from extraction\npage.clear_exclusions()                         # Remove exclusions\ntext = page.extract_text(use_exclusions=False)  # Ignore exclusions\n</pre>"},{"location":"quick-reference/#pdf-level-exclusions","title":"PDF-Level Exclusions\u00b6","text":"<pre># Exclude headers from all pages\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.1),\n    label=\"Header\"\n)\n</pre>"},{"location":"quick-reference/#configuration-options","title":"Configuration Options\u00b6","text":""},{"location":"quick-reference/#ocr-engines","title":"OCR Engines\u00b6","text":"<pre>from natural_pdf.ocr import EasyOCROptions, PaddleOCROptions\n\neasy_opts = EasyOCROptions(gpu=True, paragraph=True)\npaddle_opts = PaddleOCROptions(lang='en')\n</pre>"},{"location":"quick-reference/#layout-analysis-options","title":"Layout Analysis Options\u00b6","text":"<pre>from natural_pdf.analyzers.layout import YOLOOptions\n\nyolo_opts = YOLOOptions(confidence_threshold=0.5)\npage.analyze_layout(engine='yolo', options=yolo_opts)\n</pre>"},{"location":"quick-reference/#common-patterns","title":"Common Patterns\u00b6","text":""},{"location":"quick-reference/#extract-inspection-report-data","title":"Extract Inspection Report Data\u00b6","text":"<pre># Find violation count\nviolations = page.find('text:contains(\"Violation Count\"):right(width=100)')\n\n# Get inspection number from the header box (regex search)\ninspection_num = page.find('text:contains(\"INS-[A-Z0-9]+\")', regex=True)\n\n# Extract inspection date\ninspection_date = page.find('text:contains(\"Date:\"):right(width=150)')\n\n# Get site name (text to the right of \"Site:\")\nsite_name = page.find('text:contains(\"Site:\"):right(width=300)').extract_text()\n</pre>"},{"location":"quick-reference/#process-forms","title":"Process Forms\u00b6","text":"<pre># Exclude header/footer\npage.add_exclusion(page.create_region(0, 0, page.width, 50))\npage.add_exclusion(page.create_region(0, page.height-50, page.width, page.height))\n\n# Extract form fields\nfields = page.find_all('text:bold')\nvalues = [field.right(width=300).extract_text() for field in fields]\n</pre>"},{"location":"quick-reference/#handle-scanned-documents","title":"Handle Scanned Documents\u00b6","text":"<pre># Apply OCR with high accuracy\npage.apply_ocr(engine='surya', languages=['en'])\n\n# Extract with confidence filtering\ntext_elements = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = text_elements.extract_text()\n</pre>"},{"location":"quick-reference/#troubleshooting","title":"Troubleshooting\u00b6","text":"Problem Solution No text found Try <code>page.apply_ocr()</code> first Wrong elements selected Use <code>elements.show()</code> to debug selectors Poor table extraction Try <code>page.analyze_layout(engine='tatr')</code> first Text extraction includes headers Use <code>page.add_exclusion()</code> Low OCR accuracy Try different engine or increase resolution Elements overlap multiple pages Use page-specific searches"},{"location":"quick-reference/#file-formats","title":"File Formats\u00b6","text":""},{"location":"quick-reference/#saving-results","title":"Saving Results\u00b6","text":"<pre># Save as image\npage.save_image(\"output.png\", width=700)\n\n# Save table as CSV\nimport pandas as pd\ndf = pd.DataFrame(table_data)\ndf.to_csv(\"table.csv\")\n\n# Export searchable PDF\nfrom natural_pdf.exporters import SearchablePDFExporter\nexporter = SearchablePDFExporter()\nexporter.export(pdf, \"searchable.pdf\")\n</pre>"},{"location":"quick-reference/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>New to Natural PDF? \u2192 Start with Installation</li> <li>Learning the basics? \u2192 Follow the Tutorials</li> <li>Solving specific problems? \u2192 Check the how-to guides</li> <li>Need detailed info? \u2192 See the API Reference</li> </ul>"},{"location":"quick-reference/","title":"Quick Reference","text":""},{"location":"quick-reference/#essential-workflows","title":"Essential Workflows","text":""},{"location":"quick-reference/#basic-text-extraction","title":"Basic Text Extraction","text":"<pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\ntext = page.extract_text()\n</code></pre>"},{"location":"quick-reference/#find-extract-pattern","title":"Find \u2192 Extract Pattern","text":"<pre><code># Find specific elements, then extract\nheading = page.find('text:contains(\"Summary\"):bold')\ncontent = heading.below().extract_text()\n</code></pre>"},{"location":"quick-reference/#ocr-for-scanned-documents","title":"OCR for Scanned Documents","text":"<pre><code># Apply OCR first, then extract\npage.apply_ocr(engine='easyocr', languages=['en'])\ntext = page.extract_text()\n</code></pre>"},{"location":"quick-reference/#layout-analysis-table-extraction","title":"Layout Analysis \u2192 Table Extraction","text":"<pre><code># Detect layout, then extract tables\npage.analyze_layout(engine='yolo')\ntable_region = page.find('region[type=table]')\ndata = table_region.extract_table()\n</code></pre>"},{"location":"quick-reference/#common-selectors","title":"Common Selectors","text":""},{"location":"quick-reference/#text-content","title":"Text Content","text":"<pre><code>page.find('text:contains(\"Invoice\")')           # Contains text\npage.find('text:contains(\"total\")', case=False) # Case insensitive\npage.find('text:contains(\"\\\\d+\")', regex=True)  # Regex pattern\n</code></pre>"},{"location":"quick-reference/#text-formatting","title":"Text Formatting","text":"<pre><code>page.find_all('text:bold')                      # Bold text\npage.find_all('text:italic')                    # Italic text\npage.find_all('text[size&gt;=12]')                 # Large text\npage.find_all('text[fontname*=Arial]')          # Specific font\n</code></pre>"},{"location":"quick-reference/#spatial-relationships","title":"Spatial Relationships","text":"<pre><code>page.find('text:above(\"line[width&gt;=2]\")')       # Above thick line\npage.find('text:below(\"text:contains(\"Title\")\")')  # Below title\npage.find('text:near(\"image\")')                 # Near images\n</code></pre>"},{"location":"quick-reference/#layout-elements","title":"Layout Elements","text":"<pre><code>page.find_all('line:horizontal')                # Horizontal lines\npage.find_all('rect')                           # Rectangles\npage.find_all('region[type=table]')             # Detected tables\npage.find_all('region[type=title]')             # Detected titles\n</code></pre>"},{"location":"quick-reference/#ocr-and-sources","title":"OCR and Sources","text":"<pre><code>page.find_all('text[source=ocr]')               # OCR-generated text\npage.find_all('text[source=pdf]')               # Original PDF text\npage.find_all('text[confidence&gt;=0.8]')          # High-confidence OCR\n</code></pre>"},{"location":"quick-reference/#essential-methods","title":"Essential Methods","text":""},{"location":"quick-reference/#finding-elements","title":"Finding Elements","text":"<pre><code>page.find(selector)                             # First match\npage.find_all(selector)                         # All matches\nelement.next()                                  # Next element in reading order\nelement.previous()                              # Previous element\n</code></pre>"},{"location":"quick-reference/#spatial-navigation","title":"Spatial Navigation","text":"<pre><code>element.above(height=100)                       # Region above element\nelement.below(until='line:horizontal')          # Below until boundary\nelement.left(width=200)                         # Region to the left\nelement.right()                                 # Region to the right\n</code></pre>"},{"location":"quick-reference/#text-extraction","title":"Text Extraction","text":"<pre><code>page.extract_text()                             # All text from page\npage.extract_text(layout=True)                  # Preserve layout\nelement.extract_text()                          # Text from specific element\nregion.extract_text()                           # Text from region\n</code></pre>"},{"location":"quick-reference/#table-extraction","title":"Table Extraction","text":"<pre><code>page.extract_table()                            # First table on page\nregion.extract_table()                          # Table from region\nregion.extract_table(method='tatr')             # Force TATR method\nregion.extract_table(method='pdfplumber')       # Force pdfplumber method\n</code></pre>"},{"location":"quick-reference/#ocr","title":"OCR","text":"<pre><code>page.apply_ocr()                                # Default OCR\npage.apply_ocr(engine='paddle', languages=['en', 'zh-cn'])\npage.apply_ocr(engine='easyocr', min_confidence=0.8)\nregion.apply_ocr()                              # OCR specific region\n</code></pre>"},{"location":"quick-reference/#layout-analysis","title":"Layout Analysis","text":"<pre><code>page.analyze_layout()                           # Default YOLO\npage.analyze_layout(engine='tatr')              # Table-focused\npage.analyze_layout(engine='surya')             # High accuracy\npage.clear_detected_layout_regions()           # Clear previous results\n</code></pre>"},{"location":"quick-reference/#document-qa","title":"Document QA","text":"<pre><code>result = page.ask(\"What is the total amount?\")\nprint(result.answer)                            # The answer\nprint(result.confidence)                        # Confidence score\nresult.show()                                   # Highlight answer location\n</code></pre>"},{"location":"quick-reference/#structured-data-extraction","title":"Structured Data Extraction","text":"<pre><code># Simple approach\ndata = page.extract(schema=[\"company\", \"date\", \"total\"]).extracted()\n\n# With Pydantic schema\nfrom pydantic import BaseModel\nclass Invoice(BaseModel):\n    company: str\n    total: float\n    date: str\n\ndata = page.extract(schema=Invoice, client=client).extracted()\n</code></pre>"},{"location":"quick-reference/#visualization-debugging","title":"Visualization &amp; Debugging","text":""},{"location":"quick-reference/#highlighting","title":"Highlighting","text":"<pre><code>elements.highlight(color=\"red\")                 # Simple highlight\nelements.highlight(color=\"blue\", label=\"Headers\") # With label\nelements.highlight(group_by='type')             # Color by type\npage.clear_highlights()                         # Remove highlights\n</code></pre>"},{"location":"quick-reference/#viewing","title":"Viewing","text":"<pre><code>page.show()                                     # Show page with highlights\nelement.show()                                  # Show specific element\npage.to_image(width=700)                        # Generate image\nregion.to_image(crop=True)                 # Crop to region only\n</code></pre>"},{"location":"quick-reference/#interactive-viewer","title":"Interactive Viewer","text":"<pre><code>page.viewer()                                   # Launch interactive viewer (Jupyter)\n</code></pre>"},{"location":"quick-reference/#exclusion-zones","title":"Exclusion Zones","text":""},{"location":"quick-reference/#page-level-exclusions","title":"Page-Level Exclusions","text":"<pre><code>header = page.find('text:contains(\"CONFIDENTIAL\")').above()\npage.add_exclusion(header)                      # Exclude from extraction\npage.clear_exclusions()                         # Remove exclusions\ntext = page.extract_text(use_exclusions=False)  # Ignore exclusions\n</code></pre>"},{"location":"quick-reference/#pdf-level-exclusions","title":"PDF-Level Exclusions","text":"<pre><code># Exclude headers from all pages\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.1),\n    label=\"Header\"\n)\n</code></pre>"},{"location":"quick-reference/#configuration-options","title":"Configuration Options","text":""},{"location":"quick-reference/#ocr-engines","title":"OCR Engines","text":"<pre><code>from natural_pdf.ocr import EasyOCROptions, PaddleOCROptions\n\neasy_opts = EasyOCROptions(gpu=True, paragraph=True)\npaddle_opts = PaddleOCROptions(lang='en')\n</code></pre>"},{"location":"quick-reference/#layout-analysis-options","title":"Layout Analysis Options","text":"<pre><code>from natural_pdf.analyzers.layout import YOLOOptions\n\nyolo_opts = YOLOOptions(confidence_threshold=0.5)\npage.analyze_layout(engine='yolo', options=yolo_opts)\n</code></pre>"},{"location":"quick-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"quick-reference/#extract-inspection-report-data","title":"Extract Inspection Report Data","text":"<pre><code># Find violation count\nviolations = page.find('text:contains(\"Violation Count\"):right(width=100)')\n\n# Get inspection number from the header box (regex search)\ninspection_num = page.find('text:contains(\"INS-[A-Z0-9]+\")', regex=True)\n\n# Extract inspection date\ninspection_date = page.find('text:contains(\"Date:\"):right(width=150)')\n\n# Get site name (text to the right of \"Site:\")\nsite_name = page.find('text:contains(\"Site:\"):right(width=300)').extract_text()\n</code></pre>"},{"location":"quick-reference/#process-forms","title":"Process Forms","text":"<pre><code># Exclude header/footer\npage.add_exclusion(page.create_region(0, 0, page.width, 50))\npage.add_exclusion(page.create_region(0, page.height-50, page.width, page.height))\n\n# Extract form fields\nfields = page.find_all('text:bold')\nvalues = [field.right(width=300).extract_text() for field in fields]\n</code></pre>"},{"location":"quick-reference/#handle-scanned-documents","title":"Handle Scanned Documents","text":"<pre><code># Apply OCR with high accuracy\npage.apply_ocr(engine='surya', languages=['en'])\n\n# Extract with confidence filtering\ntext_elements = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = text_elements.extract_text()\n</code></pre>"},{"location":"quick-reference/#troubleshooting","title":"Troubleshooting","text":"Problem Solution No text found Try <code>page.apply_ocr()</code> first Wrong elements selected Use <code>elements.show()</code> to debug selectors Poor table extraction Try <code>page.analyze_layout(engine='tatr')</code> first Text extraction includes headers Use <code>page.add_exclusion()</code> Low OCR accuracy Try different engine or increase resolution Elements overlap multiple pages Use page-specific searches"},{"location":"quick-reference/#file-formats","title":"File Formats","text":""},{"location":"quick-reference/#saving-results","title":"Saving Results","text":"<pre><code># Save as image\npage.save_image(\"output.png\", width=700)\n\n# Save table as CSV\nimport pandas as pd\ndf = pd.DataFrame(table_data)\ndf.to_csv(\"table.csv\")\n\n# Export searchable PDF\nfrom natural_pdf.exporters import SearchablePDFExporter\nexporter = SearchablePDFExporter()\nexporter.export(pdf, \"searchable.pdf\")\n</code></pre>"},{"location":"quick-reference/#next-steps","title":"Next Steps","text":"<ul> <li>New to Natural PDF? \u2192 Start with Installation </li> <li>Learning the basics? \u2192 Follow the Tutorials</li> <li>Solving specific problems? \u2192 Check the how-to guides</li> <li>Need detailed info? \u2192 See the API Reference </li> </ul>"},{"location":"reflowing-pages/","title":"Restructuring page content","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\nfrom natural_pdf.flows import Flow\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/multicolumn.pdf\")\npage = pdf.pages[0]\npage.to_image(width=500)\n</pre> from natural_pdf import PDF from natural_pdf.flows import Flow  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/multicolumn.pdf\") page = pdf.pages[0] page.to_image(width=500) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: <p>We can grab individual columns from it.</p> In\u00a0[2]: Copied! <pre>left = page.region(right=page.width/3)\nmid = page.region(left=page.width/3, right=page.width/3*2)\nright = page.region(left=page.width/3*2)\n\nmid.show(width=500)\n</pre> left = page.region(right=page.width/3) mid = page.region(left=page.width/3, right=page.width/3*2) right = page.region(left=page.width/3*2)  mid.show(width=500) Out[2]: In\u00a0[3]: Copied! <pre>stacked = [left, mid, right]\nflow = Flow(segments=stacked, arrangement=\"vertical\")\n</pre> stacked = [left, mid, right] flow = Flow(segments=stacked, arrangement=\"vertical\") <p>As a result, I can find text in the first column and ask it to grab what's \"below\" until it hits content in the second column.</p> In\u00a0[4]: Copied! <pre>region = (\n    flow\n    .find('text:contains(\"Table one\")')\n    .below(\n        until='text:contains(\"Table two\")',\n        include_endpoint=False\n    )\n)\nregion.show()\n</pre> region = (     flow     .find('text:contains(\"Table one\")')     .below(         until='text:contains(\"Table two\")',         include_endpoint=False     ) ) region.show() Out[4]: <p>While you can't easily extract tables yet, you can at least extract text!</p> In\u00a0[5]: Copied! <pre>print(region.extract_text())\n</pre> print(region.extract_text()) <pre>index number\n1 123\n2 456\n3 789\n4 1122\n5 1455\n6 1788\n7 2121\n8 2454\n9 2787\n10 3120\n11 3453\n12 3786\n13 4119\n14 4452\n15 4785\n16 5118\n17 5451\n18 5784\n19 6117\n20 6450\n21 6783\n22 7116\n23 7449\n24 7782\n25 8115\n26 8448\n27 8781\n28 9114\n29 9447\n30 9780\n31 10113\n32 10446\n33 10779\n34 11112\n35 11445\n36 11778\n37 12111\n38 12444\n39 12777\n</pre> In\u00a0[6]: Copied! <pre>(\n    flow\n    .find_all('text[size=12][width&gt;10]:bold')\n    .show()\n)\n</pre> (     flow     .find_all('text[size=12][width&gt;10]:bold')     .show() ) Out[6]: <p>...it's easy to extract each table that's betwen them.</p> In\u00a0[7]: Copied! <pre>regions = (\n    flow\n    .find_all('text[size=12][width&gt;10]:bold')\n    .below(\n        until='text[size=12][width&gt;10]:bold|text:contains(\"Here is a bit\")',\n        include_endpoint=False\n    )\n)\nregions.show()\n</pre> regions = (     flow     .find_all('text[size=12][width&gt;10]:bold')     .below(         until='text[size=12][width&gt;10]:bold|text:contains(\"Here is a bit\")',         include_endpoint=False     ) ) regions.show() Out[7]:"},{"location":"reflowing-pages/#restructuring-page-content","title":"Restructuring page content\u00b6","text":"<p>Flows are a way to restructure pages that are not in normal one-page reading order. This might be columnal data, tables than span pages, etc.</p>"},{"location":"reflowing-pages/#a-multi-column-pdf","title":"A multi-column PDF\u00b6","text":"<p>Here is a multi column PDF.</p>"},{"location":"reflowing-pages/#restructuring","title":"Restructuring\u00b6","text":"<p>We can use Flows to stack the three columns on top of each other.</p>"},{"location":"reflowing-pages/#find_all-and-reflows","title":"find_all and reflows\u00b6","text":"<p>Let's say we have a few headers...</p>"},{"location":"regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page page = pdf.pages[0]  # Display the page page.show(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Create a region by specifying (x0, top, x1, bottom) coordinates\n# Let's create a region in the middle of the page\nmid_region = page.create_region(\n    x0=100,         # Left edge\n    top=200,        # Top edge\n    x1=500,         # Right edge\n    bottom=400      # Bottom edge\n)\n\n# Highlight the region to see it\nmid_region.highlight(color=\"blue\").show()\n</pre> # Create a region by specifying (x0, top, x1, bottom) coordinates # Let's create a region in the middle of the page mid_region = page.create_region(     x0=100,         # Left edge     top=200,        # Top edge     x1=500,         # Right edge     bottom=400      # Bottom edge )  # Highlight the region to see it mid_region.highlight(color=\"blue\").show() Out[2]: In\u00a0[3]: Copied! <pre># Find a heading-like element\nheading = page.find('text[size&gt;=12]:bold')\n\n# Create a region below this heading element\nif heading:\n    region_below = heading.below()\n    \n    # Highlight the heading and the region below it\n    heading.highlight(color=\"red\")\n    region_below.highlight(color=\"blue\")\n    page.show()\n</pre> # Find a heading-like element heading = page.find('text[size&gt;=12]:bold')  # Create a region below this heading element if heading:     region_below = heading.below()          # Highlight the heading and the region below it     heading.highlight(color=\"red\")     region_below.highlight(color=\"blue\")     page.show() In\u00a0[4]: Copied! <pre># Create a region with height limit\nif heading:\n    # Only include 100px below the heading\n    small_region_below = heading.below(height=100)\n    \n    page.clear_highlights()\n    heading.highlight(color=\"red\")\n    small_region_below.highlight(color=\"green\")\n    page.show()\n</pre> # Create a region with height limit if heading:     # Only include 100px below the heading     small_region_below = heading.below(height=100)          page.clear_highlights()     heading.highlight(color=\"red\")     small_region_below.highlight(color=\"green\")     page.show() In\u00a0[5]: Copied! <pre># Find a line or other element to create a region above\nline = page.find('line')\nif line:\n    # Create a region above the line\n    region_above = line.above()\n    \n    page.clear_highlights()\n    line.highlight(color=\"black\")\n    region_above.highlight(color=\"purple\")\n    page.show()\n</pre> # Find a line or other element to create a region above line = page.find('line') if line:     # Create a region above the line     region_above = line.above()          page.clear_highlights()     line.highlight(color=\"black\")     region_above.highlight(color=\"purple\")     page.show() In\u00a0[6]: Copied! <pre># Find two elements to use as boundaries\nfirst_heading = page.find('text[size&gt;=11]:bold')\nnext_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None\n\nif first_heading and next_heading:\n    # Create a region from the first heading until the next heading\n    section = first_heading.below(until=next_heading, include_endpoint=False)\n    \n    # Highlight both elements and the region between them\n    page.clear_highlights()\n    first_heading.highlight(color=\"red\")\n    next_heading.highlight(color=\"red\")\n    section.highlight(color=\"yellow\")\n    page.show()\n</pre> # Find two elements to use as boundaries first_heading = page.find('text[size&gt;=11]:bold') next_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None  if first_heading and next_heading:     # Create a region from the first heading until the next heading     section = first_heading.below(until=next_heading, include_endpoint=False)          # Highlight both elements and the region between them     page.clear_highlights()     first_heading.highlight(color=\"red\")     next_heading.highlight(color=\"red\")     section.highlight(color=\"yellow\")     page.show() In\u00a0[7]: Copied! <pre># Find a region to work with (e.g., from a title to the next bold text)\ntitle = page.find('text:contains(\"Site\")')  # Adjust if needed\nif title:\n    # Create a region from title down to the next bold text\n    content_region = title.below(until='line:horizontal', include_endpoint=False)\n    \n    # Extract text from just this region\n    region_text = content_region.extract_text()\n    \n    # Show the region and the extracted text\n    page.clear_highlights()\n    content_region.highlight(color=\"green\")\n    page.show()\n    \n    # Displaying the text (first 300 chars if long)\n    print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text)\n</pre> # Find a region to work with (e.g., from a title to the next bold text) title = page.find('text:contains(\"Site\")')  # Adjust if needed if title:     # Create a region from title down to the next bold text     content_region = title.below(until='line:horizontal', include_endpoint=False)          # Extract text from just this region     region_text = content_region.extract_text()          # Show the region and the extracted text     page.clear_highlights()     content_region.highlight(color=\"green\")     page.show()          # Displaying the text (first 300 chars if long)     print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text) <pre>Date: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other me...\n</pre> In\u00a0[8]: Copied! <pre># Create a region in an interesting part of the page\ntest_region = page.create_region(\n    x0=page.width * 0.1, \n    top=page.height * 0.25, \n    x1=page.width * 0.9, \n    bottom=page.height * 0.75\n)\n\n# Find all text elements ONLY within this region\ntext_in_region = test_region.find_all('text')\n\n# Display result\npage.clear_highlights()\ntest_region.highlight(color=\"blue\")\ntext_in_region.highlight(color=\"red\")\npage.show()\n\nlen(text_in_region)  # Number of text elements found in region\n</pre> # Create a region in an interesting part of the page test_region = page.create_region(     x0=page.width * 0.1,      top=page.height * 0.25,      x1=page.width * 0.9,      bottom=page.height * 0.75 )  # Find all text elements ONLY within this region text_in_region = test_region.find_all('text')  # Display result page.clear_highlights() test_region.highlight(color=\"blue\") text_in_region.highlight(color=\"red\") page.show()  len(text_in_region)  # Number of text elements found in region Out[8]: <pre>17</pre> In\u00a0[9]: Copied! <pre># Find a specific region to capture\n# (Could be a table, figure, or any significant area)\nregion_for_image = page.create_region(\n    x0=100, \n    top=150,\n    x1=page.width - 100,\n    bottom=300\n)\n\n# Generate an image of just this region\nregion_for_image.to_image(crop=True)  # Shows just the region\n</pre> # Find a specific region to capture # (Could be a table, figure, or any significant area) region_for_image = page.create_region(     x0=100,      top=150,     x1=page.width - 100,     bottom=300 )  # Generate an image of just this region region_for_image.to_image(crop=True)  # Shows just the region Out[9]: In\u00a0[10]: Copied! <pre># Take an existing region and expand it\nregion_a = page.create_region(200, 200, 400, 400)\n\n# Expand by a certain number of points in each direction\nexpanded = region_a.expand(left=20, right=20, top=20, bottom=20)\n\n# Visualize original and expanded regions\npage.clear_highlights()\nregion_a.highlight(color=\"blue\", label=\"Original\")\nexpanded.highlight(color=\"red\", label=\"Expanded\")\npage.to_image()\n</pre> # Take an existing region and expand it region_a = page.create_region(200, 200, 400, 400)  # Expand by a certain number of points in each direction expanded = region_a.expand(left=20, right=20, top=20, bottom=20)  # Visualize original and expanded regions page.clear_highlights() region_a.highlight(color=\"blue\", label=\"Original\") expanded.highlight(color=\"red\", label=\"Expanded\") page.to_image() Out[10]: In\u00a0[11]: Copied! <pre># Create a region for the whole page\nfull_page_region = page.create_region(0, 0, page.width, page.height)\n\n# Extract text without exclusions as baseline\nfull_text = full_page_region.extract_text()\nprint(f\"Full page text length: {len(full_text)} characters\")\n</pre> # Create a region for the whole page full_page_region = page.create_region(0, 0, page.width, page.height)  # Extract text without exclusions as baseline full_text = full_page_region.extract_text() print(f\"Full page text length: {len(full_text)} characters\") <pre>Full page text length: 1255 characters\n</pre> In\u00a0[12]: Copied! <pre># Define an area we want to exclude (like a header)\n# Let's exclude the top 10% of the page\nheader_zone = page.create_region(0, 0, page.width, page.height * 0.1)\n\n# Add this as an exclusion for the page\npage.add_exclusion(header_zone)\n\n# Visualize the exclusion\npage.clear_highlights()\nheader_zone.highlight(color=\"red\", label=\"Excluded\")\npage.show()\n</pre> # Define an area we want to exclude (like a header) # Let's exclude the top 10% of the page header_zone = page.create_region(0, 0, page.width, page.height * 0.1)  # Add this as an exclusion for the page page.add_exclusion(header_zone)  # Visualize the exclusion page.clear_highlights() header_zone.highlight(color=\"red\", label=\"Excluded\") page.show() Out[12]: In\u00a0[13]: Copied! <pre># Now extract text again - the header should be excluded\ntext_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default\n\n# Compare text lengths\nprint(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\")\nprint(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\")\n</pre> # Now extract text again - the header should be excluded text_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default  # Compare text lengths print(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\") print(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\") <pre>Original text: 1255 chars\nText with exclusion: 1193 chars\nDifference: 62 chars excluded\n</pre> In\u00a0[14]: Copied! <pre># When done with this page, clear exclusions\npage.clear_exclusions()\n</pre> # When done with this page, clear exclusions page.clear_exclusions() Out[14]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[15]: Copied! <pre># Define a PDF-level exclusion for headers\n# This will exclude the top 30% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.3),\n    label=\"Header zone\"\n)\n\n# Define a PDF-level exclusion for footers\n# This will exclude the bottom 20% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),\n    label=\"Footer zone\"\n)\n\n# PDF-level exclusions are used whenever you extract text\n# Let's try on the first three pages\nfor page in pdf.pages[:3]:\n    text = page.extract_text()\n    text_original = page.extract_text(use_exclusions=False)\n    print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\")\n</pre> # Define a PDF-level exclusion for headers # This will exclude the top 30% of every page pdf.add_exclusion(     lambda p: p.create_region(0, 0, p.width, p.height * 0.3),     label=\"Header zone\" )  # Define a PDF-level exclusion for footers # This will exclude the bottom 20% of every page pdf.add_exclusion(     lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),     label=\"Footer zone\" )  # PDF-level exclusions are used whenever you extract text # Let's try on the first three pages for page in pdf.pages[:3]:     text = page.extract_text()     text_original = page.extract_text(use_exclusions=False)     print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\") <pre>Page 1 \u2013 Before: 456 After: 456\n</pre> In\u00a0[16]: Copied! <pre># Clear PDF-level exclusions when done\npdf.clear_exclusions()\nprint(\"Cleared all PDF-level exclusions\")\n</pre> # Clear PDF-level exclusions when done pdf.clear_exclusions() print(\"Cleared all PDF-level exclusions\") <pre>Cleared all PDF-level exclusions\n</pre> In\u00a0[17]: Copied! <pre># First, run layout analysis to detect regions\npage.analyze_layout()  # Uses 'yolo' engine by default\n\n# Find all detected regions\ndetected_regions = page.find_all('region')\nprint(f\"Found {len(detected_regions)} layout regions\")\n</pre> # First, run layout analysis to detect regions page.analyze_layout()  # Uses 'yolo' engine by default  # Find all detected regions detected_regions = page.find_all('region') print(f\"Found {len(detected_regions)} layout regions\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5my64pv4/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 949.4ms\n</pre> <pre>Speed: 5.9ms preprocess, 949.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Found 7 layout regions\n</pre> In\u00a0[18]: Copied! <pre># Highlight all detected regions by type\ndetected_regions.highlight(group_by='region_type').show()\n</pre> # Highlight all detected regions by type detected_regions.highlight(group_by='region_type').show() Out[18]: In\u00a0[19]: Copied! <pre># Extract text from a specific region type (e.g., title)\ntitle_regions = page.find_all('region[type=title]')\nif title_regions:\n    titles_text = title_regions.extract_text()\n    print(f\"Title text: {titles_text}\")\n</pre> # Extract text from a specific region type (e.g., title) title_regions = page.find_all('region[type=title]') if title_regions:     titles_text = title_regions.extract_text()     print(f\"Title text: {titles_text}\") <pre>Title text: \n</pre>"},{"location":"regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that define boundaries for operations like text extraction, element finding, or visualization. They're one of Natural PDF's most powerful features for working with specific parts of a document.</p>"},{"location":"regions/#setup","title":"Setup\u00b6","text":"<p>Let's set up a PDF to experiment with regions.</p>"},{"location":"regions/#creating-regions","title":"Creating Regions\u00b6","text":"<p>There are several ways to create regions in Natural PDF.</p>"},{"location":"regions/#using-create_region-with-coordinates","title":"Using <code>create_region()</code> with Coordinates\u00b6","text":"<p>This is the most direct method - provide the coordinates directly.</p>"},{"location":"regions/#using-element-methods-above-below-left-right","title":"Using Element Methods: <code>above()</code>, <code>below()</code>, <code>left()</code>, <code>right()</code>\u00b6","text":"<p>You can create regions relative to existing elements.</p>"},{"location":"regions/#creating-a-region-between-elements-with-until","title":"Creating a Region Between Elements with <code>until()</code>\u00b6","text":""},{"location":"regions/#using-regions","title":"Using Regions\u00b6","text":"<p>Once you have a region, here's what you can do with it.</p>"},{"location":"regions/#extract-text-from-a-region","title":"Extract Text from a Region\u00b6","text":""},{"location":"regions/#find-elements-within-a-region","title":"Find Elements Within a Region\u00b6","text":"<p>You can use a region as a \"filter\" to only find elements within its boundaries.</p>"},{"location":"regions/#generate-an-image-of-a-region","title":"Generate an Image of a Region\u00b6","text":""},{"location":"regions/#adjust-and-expand-regions","title":"Adjust and Expand Regions\u00b6","text":""},{"location":"regions/#using-exclusion-zones-with-regions","title":"Using Exclusion Zones with Regions\u00b6","text":"<p>Exclusion zones are regions that you want to ignore during operations like text extraction.</p>"},{"location":"regions/#document-level-exclusions","title":"Document-Level Exclusions\u00b6","text":"<p>PDF-level exclusions apply to all pages and use functions to adapt to each page.</p>"},{"location":"regions/#working-with-layout-analysis-regions","title":"Working with Layout Analysis Regions\u00b6","text":"<p>When you run layout analysis, the detected regions (tables, titles, etc.) are also Region objects.</p>"},{"location":"regions/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you understand regions, you can:</p> <ul> <li>Extract tables from table regions</li> <li>Ask questions about specific regions</li> <li>Exclude content from extraction</li> </ul>"},{"location":"tables/","title":"Getting Tables Out of PDFs","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Try to extract the first table found on the page\n# This uses pdfplumber behind the scenes\ntable_data = page.extract_table() # Returns a list of lists\ntable_data\n</pre> # Try to extract the first table found on the page # This uses pdfplumber behind the scenes table_data = page.extract_table() # Returns a list of lists table_data Out[2]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>This might work great, or it might give you garbage. Tables are tricky.</p> In\u00a0[3]: Copied! <pre># Use YOLO to find table regions\npage.analyze_layout(engine='yolo')\n\n# Find what it thinks are tables\ntable_regions_yolo = page.find_all('region[type=table][model=yolo]')\ntable_regions_yolo.show()\n</pre> # Use YOLO to find table regions page.analyze_layout(engine='yolo')  # Find what it thinks are tables table_regions_yolo = page.find_all('region[type=table][model=yolo]') table_regions_yolo.show() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpy_y9uk6k/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1619.9ms\n</pre> <pre>Speed: 8.5ms preprocess, 1619.9ms inference, 3.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[3]: In\u00a0[4]: Copied! <pre># Extract data from the detected table\ntable_regions_yolo[0].extract_table()\n</pre> # Extract data from the detected table table_regions_yolo[0].extract_table() Out[4]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[5]: Copied! <pre># Clear previous results and try TATR\npage.clear_detected_layout_regions() \npage.analyze_layout(engine='tatr')\n</pre> # Clear previous results and try TATR page.clear_detected_layout_regions()  page.analyze_layout(engine='tatr') Out[5]: <pre>&lt;ElementCollection[Region](count=15)&gt;</pre> In\u00a0[6]: Copied! <pre># Find the table that TATR detected\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.show()\n</pre> # Find the table that TATR detected tatr_table = page.find('region[type=table][model=tatr]') tatr_table.show() Out[6]: In\u00a0[7]: Copied! <pre># TATR finds the internal structure too\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\nf\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\"\n</pre> # TATR finds the internal structure too rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]') f\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\" Out[7]: <pre>'TATR found: 8 rows, 4 columns, 1 headers'</pre> In\u00a0[8]: Copied! <pre>tatr_table = page.find('region[type=table][model=tatr]')\n# Use TATR's smart extraction\ntatr_table.extract_table(method='tatr')\n</pre> tatr_table = page.find('region[type=table][model=tatr]') # Use TATR's smart extraction tatr_table.extract_table(method='tatr') Out[8]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[9]: Copied! <pre># Or force it to use pdfplumber instead (maybe for comparison)\ntatr_table.extract_table(method='pdfplumber')\n</pre> # Or force it to use pdfplumber instead (maybe for comparison) tatr_table.extract_table(method='pdfplumber') Out[9]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre> In\u00a0[10]: Copied! <pre># Custom settings for tricky tables\ntable_settings = {\n    \"vertical_strategy\": \"text\",      # Use text alignment instead of lines\n    \"horizontal_strategy\": \"lines\",   # Still use lines for rows\n    \"intersection_x_tolerance\": 5,    # Be more forgiving about line intersections\n}\n\nresults = page.extract_table(table_settings=table_settings)\n</pre> # Custom settings for tricky tables table_settings = {     \"vertical_strategy\": \"text\",      # Use text alignment instead of lines     \"horizontal_strategy\": \"lines\",   # Still use lines for rows     \"intersection_x_tolerance\": 5,    # Be more forgiving about line intersections }  results = page.extract_table(table_settings=table_settings) In\u00a0[11]: Copied! <pre>import pandas as pd\n\n# Convert to a pandas DataFrame for easy manipulation\ndf = pd.DataFrame(page.extract_table())\ndf\n</pre> import pandas as pd  # Convert to a pandas DataFrame for easy manipulation df = pd.DataFrame(page.extract_table()) df Out[11]: 0 1 2 3 0 Statute Description Level Repeat? 1 4.12.7 Unsanitary Working Conditions. Critical 2 5.8.3 Inadequate Protective Equipment. Serious 3 6.3.9 Ineffective Injury Prevention. Serious 4 7.1.5 Failure to Properly Store Hazardous Materials. Critical 5 8.9.2 Lack of Adequate Fire Safety Measures. Serious 6 9.6.4 Inadequate Ventilation Systems. Serious 7 10.2.7 Insufficient Employee Training for Safe Work P... Serious In\u00a0[12]: Copied! <pre># This should work but doesn't quite yet - we're working on it!\n# tatr_table.cells\n</pre> # This should work but doesn't quite yet - we're working on it! # tatr_table.cells"},{"location":"tables/#getting-tables-out-of-pdfs","title":"Getting Tables Out of PDFs\u00b6","text":"<p>Tables in PDFs can be a real pain. Sometimes they're perfectly formatted with nice lines, other times they're just text floating around that vaguely looks like a table. Natural PDF gives you several different approaches to tackle whatever table nightmare you're dealing with.</p>"},{"location":"tables/#setup","title":"Setup\u00b6","text":"<p>Let's start with a PDF that has some tables to work with.</p>"},{"location":"tables/#the-quick-and-dirty-approach","title":"The Quick and Dirty Approach\u00b6","text":"<p>If you know there's a table somewhere and just want to try extracting it, start simple:</p>"},{"location":"tables/#the-smart-way-detect-first-then-extract","title":"The Smart Way: Detect First, Then Extract\u00b6","text":"<p>A better approach is to first find where the tables actually are, then extract them properly.</p>"},{"location":"tables/#finding-tables-with-yolo-fast-and-pretty-good","title":"Finding Tables with YOLO (Fast and Pretty Good)\u00b6","text":"<p>The YOLO model is good at spotting table-shaped areas on a page.</p>"},{"location":"tables/#finding-tables-with-tatr-slow-but-very-smart","title":"Finding Tables with TATR (Slow but Very Smart)\u00b6","text":"<p>The TATR model actually understands table structure - it can tell you where rows, columns, and headers are.</p>"},{"location":"tables/#choosing-your-extraction-method","title":"Choosing Your Extraction Method\u00b6","text":"<p>When you call <code>extract_table()</code> on a detected region, Natural PDF picks the extraction method automatically:</p> <ul> <li>YOLO-detected regions \u2192 uses <code>pdfplumber</code> (looks for lines and text alignment)</li> <li>TATR-detected regions \u2192 uses the smart <code>tatr</code> method (uses the detected structure)</li> </ul> <p>You can override this if needed:</p>"},{"location":"tables/#when-to-use-which","title":"When to Use Which?\u00b6","text":"<ul> <li><code>pdfplumber</code>: Great for clean tables with visible grid lines. Fast and reliable.</li> <li><code>tatr</code>: Better for messy tables, tables without lines, or tables with merged cells. Slower but smarter.</li> </ul>"},{"location":"tables/#when-tables-dont-cooperate","title":"When Tables Don't Cooperate\u00b6","text":"<p>Sometimes the automatic detection doesn't work well. You can tweak pdfplumber's settings:</p>"},{"location":"tables/#saving-your-results","title":"Saving Your Results\u00b6","text":"<p>Once you've got your table data, you'll probably want to do something useful with it:</p>"},{"location":"tables/#working-with-tatr-cell-structure","title":"Working with TATR Cell Structure\u00b6","text":"<p>TATR is smart enough to create individual cell regions, but accessing them directly is still a work in progress:</p>"},{"location":"tables/#next-steps","title":"Next Steps\u00b6","text":"<p>Tables are just one part of document structure. Once you've got table extraction working:</p> <ul> <li>Layout Analysis: See how table detection fits into understanding the whole document</li> <li>Working with Regions: Manually define table areas when automatic detection fails</li> </ul>"},{"location":"text-analysis/","title":"Text Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0] In\u00a0[2]: Copied! <pre># Find the first word element\nword = page.find('word') \n\nprint(f\"Text:\", word.text)\nprint(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name\nprint(f\"Size:\", word.size)\nprint(f\"Color:\", word.color) # Non-stroking color\nprint(f\"Is Bold:\", word.bold)\nprint(f\"Is Italic:\", word.italic)\n</pre> # Find the first word element word = page.find('word')   print(f\"Text:\", word.text) print(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name print(f\"Size:\", word.size) print(f\"Color:\", word.color) # Non-stroking color print(f\"Is Bold:\", word.bold) print(f\"Is Italic:\", word.italic) <pre>Text: Jungle Health and Safety Inspection Service\nFont Name: Helvetica\nSize: 8.0\nColor: (0, 0, 0)\nIs Bold: False\nIs Italic: False\n</pre> <ul> <li><code>fontname</code>: Often an internal reference (like 'F1', 'F2') or a basic name.</li> <li><code>size</code>: Font size in points.</li> <li><code>color</code>: The non-stroking color, typically a tuple representing RGB or Grayscale values (e.g., <code>(0.0, 0.0, 0.0)</code> for black).</li> <li><code>bold</code>, <code>italic</code>: Boolean flags indicating if the font style is bold or italic (heuristically determined based on font name conventions).</li> </ul> In\u00a0[3]: Copied! <pre># Find all bold text elements\nbold_text = page.find_all('text:bold')\n\n# Find all italic text elements\nitalic_text = page.find_all('text:italic')\n\n# Find text that is both bold and larger than 12pt\nbold_headings = page.find_all('text:bold[size&gt;=12]')\n\nprint(f\"Found {len(bold_text)} bold elements.\")\nprint(f\"Found {len(italic_text)} italic elements.\")\nprint(f\"Found {len(bold_headings)} bold headings.\")\n</pre> # Find all bold text elements bold_text = page.find_all('text:bold')  # Find all italic text elements italic_text = page.find_all('text:italic')  # Find text that is both bold and larger than 12pt bold_headings = page.find_all('text:bold[size&gt;=12]')  print(f\"Found {len(bold_text)} bold elements.\") print(f\"Found {len(italic_text)} italic elements.\") print(f\"Found {len(bold_headings)} bold headings.\") <pre>Found 9 bold elements.\nFound 0 italic elements.\nFound 1 bold headings.\n</pre> In\u00a0[4]: Copied! <pre>page.analyze_text_styles()\npage.text_style_labels\n</pre> page.analyze_text_styles() page.text_style_labels Out[4]: <pre>['10.0pt Bold Helvetica',\n '10.0pt Helvetica',\n '12.0pt Bold Helvetica',\n '8.0pt Helvetica']</pre> <p>One they're assigned, you can filter based on <code>style_label</code> instead of going bit-by-bit.</p> In\u00a0[5]: Copied! <pre>page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]')\n</pre> page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]') Out[5]: <pre>&lt;ElementCollection[TextElement](count=8)&gt;</pre> In\u00a0[6]: Copied! <pre>page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700)\n</pre> page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700) Out[6]: <p>This allows you to quickly see patterns in font usage across the page layout.</p> In\u00a0[7]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Select the first page page = pdf.pages[0] page.to_image(width=700) Out[7]: <p>Look!</p> In\u00a0[8]: Copied! <pre>page.find_all('text')[0].fontname\n</pre> page.find_all('text')[0].fontname Out[8]: <pre>'AAAAAB+font000000002a8d158a'</pre> <p>The part before the <code>+</code> is the variant \u2013 bold, italic, etc \u2013 while the part after it is the \"real\" font name.</p>"},{"location":"text-analysis/#text-analysis","title":"Text Analysis\u00b6","text":"<p>Analyzing the properties of text elements, such as their font, size, style, and color, can be crucial for understanding document structure and extracting specific information. Natural PDF provides tools to access and analyze these properties.</p>"},{"location":"text-analysis/#introduction","title":"Introduction\u00b6","text":"<p>Beyond just the sequence of characters, the style of text carries significant meaning. Headings are often larger and bolder, important terms might be italicized, and different sections might use distinct fonts. This page covers how to access and utilize this stylistic information.</p>"},{"location":"text-analysis/#accessing-font-information","title":"Accessing Font Information\u00b6","text":"<p>Every <code>TextElement</code> (representing characters or words) holds information about its font properties.</p>"},{"location":"text-analysis/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>You can directly select text based on its style using pseudo-classes in selectors:</p>"},{"location":"text-analysis/#analyzing-fonts-on-a-page","title":"Analyzing Fonts on a Page\u00b6","text":"<p>You can use <code>analyze_text_styles</code> to assign labels to text based on font sizes, bold/italic and font names.</p>"},{"location":"text-analysis/#visualizing-text-properties","title":"Visualizing Text Properties\u00b6","text":"<p>Use highlighting to visually inspect text properties. Grouping by attributes like <code>fontname</code> or <code>size</code> can be very insightful. In the example below we go right to grouping by the <code>style_label</code>, which combines font name, size and variant.</p>"},{"location":"text-analysis/#weird-font-names","title":"Weird font names\u00b6","text":"<p>Oftentimes font names aren't what you're used to \u2013 Arial, Helvetica, etc \u2013 the PDF has given them weird, weird names. Relax, it's okay, they're normal fonts.</p>"},{"location":"text-extraction/","title":"Text Extraction Guide","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page for initial examples\npage = pdf.pages[0]\n\n# Display the first page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page for initial examples page = pdf.pages[0]  # Display the first page page.show(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Extract all text from the first page\n# Displaying first 500 characters\nprint(page.extract_text()[:500])\n</pre> # Extract all text from the first page # Displaying first 500 characters print(page.extract_text()[:500]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the lev\n</pre> <p>You can also preserve layout with <code>layout=True</code>.</p> In\u00a0[3]: Copied! <pre># Extract text from the entire document (may take time)\n# Uncomment to run:\nprint(page.extract_text(layout=True)[:2000])\n</pre> # Extract text from the entire document (may take time) # Uncomment to run: print(page.extract_text(layout=True)[:2000]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[4]: Copied! <pre># Find a single element, e.g., a title containing \"Summary\"\n# Adjust selector as needed\ndate_element = page.find('text:contains(\"Site\")')\ndate_element # Display the found element object\n</pre> # Find a single element, e.g., a title containing \"Summary\" # Adjust selector as needed date_element = page.find('text:contains(\"Site\")') date_element # Display the found element object Out[4]: <pre>&lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;</pre> In\u00a0[5]: Copied! <pre>date_element.show()\n</pre> date_element.show() Out[5]: In\u00a0[6]: Copied! <pre>date_element.text\n</pre> date_element.text Out[6]: <pre>'Site: '</pre> In\u00a0[7]: Copied! <pre># Find multiple elements, e.g., bold headings (size &gt;= 8)\nheading_elements = page.find_all('text[size&gt;=8]:bold')\nheading_elements \n</pre> # Find multiple elements, e.g., bold headings (size &gt;= 8) heading_elements = page.find_all('text[size&gt;=8]:bold') heading_elements  Out[7]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[8]: Copied! <pre>page.find_all('text[size&gt;=8]:bold').show()\n</pre> page.find_all('text[size&gt;=8]:bold').show() Out[8]: In\u00a0[9]: Copied! <pre># Pull out all of their text (why? I don't know!)\nprint(heading_elements.extract_text())\n</pre> # Pull out all of their text (why? I don't know!) print(heading_elements.extract_text()) <pre>Site: Date:  Violation Count: Summary: ViolationsStatuteDescriptionLevelRepeat?\n</pre> In\u00a0[10]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"Hazardous Materials\")').text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"Hazardous Materials\")').text Out[10]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[11]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text Out[11]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[12]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\nregex = \"\\d+, \\d{4}\"\npage.find(f'text:contains(\"{regex}\")', regex=True)\n</pre> # Regular expression (e.g., \"YYYY Report\") regex = \"\\d+, \\d{4}\" page.find(f'text:contains(\"{regex}\")', regex=True) Out[12]: <pre>&lt;TextElement text='February 3...' font='Helvetica' size=10.0 bbox=(80.56, 104.07000000000005, 156.71000000000004, 114.07000000000005)&gt;</pre> In\u00a0[13]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\npage.find_all('text[fontname=\"Helvetica\"][size=10]')\n</pre> # Regular expression (e.g., \"YYYY Report\") page.find_all('text[fontname=\"Helvetica\"][size=10]') Out[13]: <pre>&lt;ElementCollection[TextElement](count=32)&gt;</pre> In\u00a0[14]: Copied! <pre># Region below an element (e.g., below \"Introduction\")\n# Adjust selector as needed\npage.find('text:contains(\"Summary\")').below(include_source=True).show()\n</pre> # Region below an element (e.g., below \"Introduction\") # Adjust selector as needed page.find('text:contains(\"Summary\")').below(include_source=True).show() Out[14]: In\u00a0[15]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_source=True)\n    .extract_text()\n    [:500]\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_source=True)     .extract_text()     [:500] ) Out[15]: <pre>'Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\ninto the vats; and when they were fished out, there was never enough of them left to b'</pre> In\u00a0[16]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_source=True, until='line:horizontal')\n    .show()\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_source=True, until='line:horizontal')     .show() ) Out[16]: In\u00a0[17]: Copied! <pre># Manually defined region via coordinates (x0, top, x1, bottom)\nmanual_region = page.create_region(30, 60, 600, 300)\nmanual_region.show()\n</pre> # Manually defined region via coordinates (x0, top, x1, bottom) manual_region = page.create_region(30, 60, 600, 300) manual_region.show() Out[17]: In\u00a0[18]: Copied! <pre># Extract text from the manual region\nmanual_region.extract_text()[:500]\n</pre> # Extract text from the manual region manual_region.extract_text()[:500] Out[18]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\nint'</pre> In\u00a0[19]: Copied! <pre>header_content = page.find('rect')\nfooter_content = page.find_all('line')[-1].below()\n\nheader_content.highlight()\nfooter_content.highlight()\npage.to_image()\n</pre> header_content = page.find('rect') footer_content = page.find_all('line')[-1].below()  header_content.highlight() footer_content.highlight() page.to_image() Out[19]: In\u00a0[20]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[20]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the lev'</pre> In\u00a0[21]: Copied! <pre>page.add_exclusion(header_content)\npage.add_exclusion(footer_content)\n</pre> page.add_exclusion(header_content) page.add_exclusion(footer_content) Out[21]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[22]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[22]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\nint'</pre> In\u00a0[23]: Copied! <pre>full_text_no_exclusions = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text()\nf\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\"\n</pre> full_text_no_exclusions = page.extract_text(use_exclusions=False) clean_text = page.extract_text() f\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\" Out[23]: <pre>'Original length: 1149, Excluded length: 1149'</pre> In\u00a0[24]: Copied! <pre>page.clear_exclusions()\n</pre> page.clear_exclusions() Out[24]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>Exclusions can also be defined globally at the PDF level using <code>pdf.add_exclusion()</code> with a function.</p> In\u00a0[25]: Copied! <pre>print(page.extract_text())\n</pre> print(page.extract_text()) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[26]: Copied! <pre>print(page.extract_text(use_exclusions=False, layout=True))\n</pre> print(page.extract_text(use_exclusions=False, layout=True)) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[27]: Copied! <pre># Find the first text element on the page\nfirst_text = page.find_all('text')[1]\nfirst_text # Display basic info\n</pre> # Find the first text element on the page first_text = page.find_all('text')[1] first_text # Display basic info Out[27]: <pre>&lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;</pre> In\u00a0[28]: Copied! <pre># Highlight the first text element\nfirst_text.show()\n</pre> # Highlight the first text element first_text.show() Out[28]: In\u00a0[29]: Copied! <pre># Get detailed font properties dictionary\nfirst_text.font_info()\n</pre> # Get detailed font properties dictionary first_text.font_info() Out[29]: <pre>{'text': 'INS-UP70N51NCL41R',\n 'fontname': 'Helvetica',\n 'font_family': 'Helvetica',\n 'font_variant': '',\n 'size': 8.0,\n 'bold': False,\n 'italic': False,\n 'color': (1, 0, 0)}</pre> In\u00a0[30]: Copied! <pre># Check specific style properties directly\nf\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\"\n</pre> # Check specific style properties directly f\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\" Out[30]: <pre>'Is Bold: False, Is Italic: False, Font: Helvetica, Size: 8.0'</pre> In\u00a0[31]: Copied! <pre># Find elements by font attributes (adjust selectors)\n# Example: Find Arial fonts\narial_text = page.find_all('text[fontname*=Helvetica]')\narial_text # Display list of found elements\n</pre> # Find elements by font attributes (adjust selectors) # Example: Find Arial fonts arial_text = page.find_all('text[fontname*=Helvetica]') arial_text # Display list of found elements Out[31]: <pre>&lt;ElementCollection[TextElement](count=44)&gt;</pre> In\u00a0[32]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nlarge_text = page.find_all('text[size&gt;=12]')\nlarge_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) large_text = page.find_all('text[size&gt;=12]') large_text Out[32]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[33]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nbold_text = page.find_all('text:bold')\nbold_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) bold_text = page.find_all('text:bold') bold_text Out[33]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[34]: Copied! <pre># Analyze styles on the page\n# This returns a dictionary mapping style names to ElementList objects\npage.analyze_text_styles()\npage.text_style_labels\n</pre> # Analyze styles on the page # This returns a dictionary mapping style names to ElementList objects page.analyze_text_styles() page.text_style_labels Out[34]: <pre>['10.0pt Bold Helvetica (medium)',\n '10.0pt Helvetica (medium)',\n '12.0pt Bold Helvetica (large)',\n '8.0pt Helvetica (small)']</pre> In\u00a0[35]: Copied! <pre>page.find_all('text').show(group_by='style_label')\n</pre> page.find_all('text').show(group_by='style_label') Out[35]: In\u00a0[36]: Copied! <pre>page.find_all('text[style_label=\"8.0pt Helvetica (small)\"]')\n</pre> page.find_all('text[style_label=\"8.0pt Helvetica (small)\"]') Out[36]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> In\u00a0[37]: Copied! <pre>page.find_all('text[fontname=\"Helvetica\"][size=8]')\n</pre> page.find_all('text[fontname=\"Helvetica\"][size=8]') Out[37]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> <p>Font variants (e.g., <code>AAAAAB+FontName</code>) are also accessible via the <code>font-variant</code> attribute selector: <code>page.find_all('text[font-variant=\"AAAAAB\"]')</code>.</p> In\u00a0[38]: Copied! <pre># Get first 5 text elements in reading order\nelements_in_order = page.find_all('text')\nelements_in_order[:5]\n</pre> # Get first 5 text elements in reading order elements_in_order = page.find_all('text') elements_in_order[:5] Out[38]: <pre>[&lt;TextElement text='Jungle Hea...' font='Helvetica' size=8.0 bbox=(385.0, 35.65599999999995, 541.9680000000001, 43.65599999999995)&gt;,\n &lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;,\n &lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;,\n &lt;TextElement text='Durham\u2019s M...' font='Helvetica' size=10.0 bbox=(74.45, 84.07000000000005, 182.26000000000002, 94.07000000000005)&gt;,\n &lt;TextElement text='Chicago, I...' font='Helvetica' size=10.0 bbox=(182.26000000000002, 84.07000000000005, 234.50000000000003, 94.07000000000005)&gt;]</pre> In\u00a0[39]: Copied! <pre># Text extracted via page.extract_text() respects this order automatically\n# (Result already shown in Basic Text Extraction section)\npage.extract_text()[:100]\n</pre> # Text extracted via page.extract_text() respects this order automatically # (Result already shown in Basic Text Extraction section) page.extract_text()[:100] Out[39]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Il'</pre> In\u00a0[40]: Copied! <pre>page.clear_highlights()\n\nstart = page.find('text:contains(\"Date\")')\nstart.highlight(label='Date label')\nstart.next().highlight(label='Maybe the date', color='green')\nstart.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')\n\npage.to_image()\n</pre> page.clear_highlights()  start = page.find('text:contains(\"Date\")') start.highlight(label='Date label') start.next().highlight(label='Maybe the date', color='green') start.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')  page.to_image() Out[40]:"},{"location":"text-extraction/#text-extraction-guide","title":"Text Extraction Guide\u00b6","text":"<p>This guide demonstrates various ways to extract text from PDFs using Natural PDF, from simple page dumps to targeted extraction based on elements, regions, and styles.</p>"},{"location":"text-extraction/#setup","title":"Setup\u00b6","text":"<p>First, let's import necessary libraries and load a sample PDF. We'll use <code>example.pdf</code> from the tutorials' <code>pdfs</code> directory. Adjust the path if your setup differs.</p>"},{"location":"text-extraction/#basic-text-extraction","title":"Basic Text Extraction\u00b6","text":"<p>Get all text from a page or the entire document.</p>"},{"location":"text-extraction/#extracting-text-from-specific-elements","title":"Extracting Text from Specific Elements\u00b6","text":"<p>Use selectors with <code>find()</code> or <code>find_all()</code> to target specific elements. Selectors like <code>:contains(\"Summary\")</code> are examples; adapt them to your PDF.</p>"},{"location":"text-extraction/#advanced-text-searches","title":"Advanced text searches\u00b6","text":""},{"location":"text-extraction/#regions","title":"Regions\u00b6","text":""},{"location":"text-extraction/#filtering-out-headers-and-footers","title":"Filtering Out Headers and Footers\u00b6","text":"<p>Use Exclusion Zones to remove unwanted content before extraction. Adjust selectors for typical header/footer content.</p>"},{"location":"text-extraction/#controlling-whitespace","title":"Controlling Whitespace\u00b6","text":"<p>Manage how spaces and blank lines are handled during extraction using <code>layout</code>.</p>"},{"location":"text-extraction/#font-information-access","title":"Font Information Access\u00b6","text":"<p>Inspect font details of text elements.</p>"},{"location":"text-extraction/#working-with-font-styles","title":"Working with Font Styles\u00b6","text":"<p>Analyze and group text elements by their computed font style, which combines attributes like font name, size, boldness, etc., into logical groups.</p>"},{"location":"text-extraction/#reading-order","title":"Reading Order\u00b6","text":"<p>Text extraction respects a pathetic attempt at natural reading order (top-to-bottom, left-to-right by default). <code>page.find_all('text')</code> returns elements already sorted this way.</p>"},{"location":"text-extraction/#element-navigation","title":"Element Navigation\u00b6","text":"<p>Move between elements sequentially based on reading order using <code>.next()</code> and <code>.previous()</code>.</p>"},{"location":"text-extraction/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to extract text, you might want to explore:</p> <ul> <li>Working with regions for more precise extraction</li> <li>OCR capabilities for scanned documents</li> <li>Document layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"},{"location":"tutorials/01-loading-and-extraction/","title":"Loading and Basic Text Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" <p>In this tutorial, we'll learn how to:</p> <ol> <li>Load a PDF document</li> <li>Extract text from pages</li> <li>Extract specific elements</li> </ol> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\nimport os\n\n# Load a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Basic info about the document\n{\n    \"Filename\": os.path.basename(pdf.path),\n    \"Pages\": len(pdf.pages),\n    \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),\n    \"Author\": pdf.metadata.get(\"Author\", \"N/A\")\n}\n</pre> from natural_pdf import PDF import os  # Load a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Basic info about the document {     \"Filename\": os.path.basename(pdf.path),     \"Pages\": len(pdf.pages),     \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),     \"Author\": pdf.metadata.get(\"Author\", \"N/A\") } Out[2]: <pre>{'Filename': '01-practice.pdf', 'Pages': 1, 'Title': 'N/A', 'Author': 'N/A'}</pre> In\u00a0[3]: Copied! <pre># Get the first page\npage = pdf.pages[0]\n\n# Extract text from the page\ntext = page.extract_text()\n\n# Show the first 200 characters of the text\nprint(text[:200])\n</pre> # Get the first page page = pdf.pages[0]  # Extract text from the page text = page.extract_text()  # Show the first 200 characters of the text print(text[:200]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men\n</pre> In\u00a0[4]: Copied! <pre># Find text elements containing specific words\nelements = page.find_all('text:contains(\"Inadequate\")')\n\n# Show these elements on the page\nelements.show()\n</pre> # Find text elements containing specific words elements = page.find_all('text:contains(\"Inadequate\")')  # Show these elements on the page elements.show() Out[4]: In\u00a0[5]: Copied! <pre># Analyze the page layout\npage.analyze_layout(engine='yolo')\n\n# Find and highlight all detected regions\npage.find_all('region').show(group_by='type')\n</pre> # Analyze the page layout page.analyze_layout(engine='yolo')  # Find and highlight all detected regions page.find_all('region').show(group_by='type') <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpeusn1c6d/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1303.0ms\n</pre> <pre>Speed: 6.0ms preprocess, 1303.0ms inference, 1.7ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[5]: In\u00a0[6]: Copied! <pre># Process all pages\nfor page in pdf.pages:\n    page_text = page.extract_text()\n    print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page\n</pre> # Process all pages for page in pdf.pages:     page_text = page.extract_text()     print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page <pre>Page 1 Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Il\n</pre> <p>This tutorial covered the basics of loading PDFs and extracting text. In the next tutorials, we'll explore more advanced features like searching for specific elements, extracting structured content, and working with tables.</p>"},{"location":"tutorials/01-loading-and-extraction/#loading-and-basic-text-extraction","title":"Loading and Basic Text Extraction\u00b6","text":""},{"location":"tutorials/01-loading-and-extraction/#loading-a-pdf","title":"Loading a PDF\u00b6","text":"<p>Let's start by loading a PDF file:</p>"},{"location":"tutorials/01-loading-and-extraction/#extracting-text","title":"Extracting Text\u00b6","text":"<p>Now that we have loaded the PDF, let's extract the text from the first page:</p>"},{"location":"tutorials/01-loading-and-extraction/#finding-and-extracting-specific-elements","title":"Finding and Extracting Specific Elements\u00b6","text":"<p>We can find specific elements using spatial queries and text content:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>We can analyze the layout of the page to identify different regions:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>You can also work with multiple pages:</p>"},{"location":"tutorials/02-finding-elements/","title":"Finding Specific Elements","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page (index 0)\npage = pdf.pages[0]\n\n# Find the text element containing \"Site:\"\n# The ':contains()' pseudo-class looks for text content.\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Find the text element containing \"Date:\"\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Visualize the found elements\nsite_label.highlight(color=\"red\", label=\"Site\")\ndate_label.highlight(color=\"blue\", label=\"Date\")\n\n# Access the text content directly\n{\n    \"Site Label\": site_label.text,\n    \"Date Label\": date_label.text\n}\n\n# Display page with both highlights\npage.to_image()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page (index 0) page = pdf.pages[0]  # Find the text element containing \"Site:\" # The ':contains()' pseudo-class looks for text content. site_label = page.find('text:contains(\"Site:\")')  # Find the text element containing \"Date:\" date_label = page.find('text:contains(\"Date:\")')  # Visualize the found elements site_label.highlight(color=\"red\", label=\"Site\") date_label.highlight(color=\"blue\", label=\"Date\")  # Access the text content directly {     \"Site Label\": site_label.text,     \"Date Label\": date_label.text }  # Display page with both highlights page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Find text elements that are red\nred_text = page.find('text[color~=red]')\nprint(f\"Found red text: {red_text.text}\")\nred_text.show()\n\n# Find elements with specific RGB colors\nblue_text = page.find('text[color=rgb(0,0,255)]')\n</pre> # Find text elements that are red red_text = page.find('text[color~=red]') print(f\"Found red text: {red_text.text}\") red_text.show()  # Find elements with specific RGB colors blue_text = page.find('text[color=rgb(0,0,255)]') <pre>Found red text: INS-UP70N51NCL41R\n</pre> In\u00a0[4]: Copied! <pre># Find horizontal lines\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Find thick lines (width &gt;= 2)\nthick_lines = page.find_all('line[width&gt;=2]')\n\n# Find rectangles\nrectangles = page.find_all('rect')\n\n# Visualize what we found\nhorizontal_lines.highlight(color=\"blue\", label=\"Horizontal\")\nthick_lines.highlight(color=\"red\", label=\"Thick\")\nrectangles.highlight(color=\"green\", label=\"Rectangles\")\n\n# Display page with all shapes highlighted\npage.to_image()\n</pre> # Find horizontal lines horizontal_lines = page.find_all('line:horizontal')  # Find thick lines (width &gt;= 2) thick_lines = page.find_all('line[width&gt;=2]')  # Find rectangles rectangles = page.find_all('rect')  # Visualize what we found horizontal_lines.highlight(color=\"blue\", label=\"Horizontal\") thick_lines.highlight(color=\"red\", label=\"Thick\") rectangles.highlight(color=\"green\", label=\"Rectangles\")  # Display page with all shapes highlighted page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Find text with specific font properties\nbold_text = page.find_all('text:bold')\nlarge_text = page.find_all('text[size&gt;=12]')\n\n# Find text with specific font names\nhelvetica_text = page.find_all('text[fontname=Helvetica]')\n</pre> # Find text with specific font properties bold_text = page.find_all('text:bold') large_text = page.find_all('text[size&gt;=12]')  # Find text with specific font names helvetica_text = page.find_all('text[fontname=Helvetica]') In\u00a0[6]: Copied! <pre># Find text above a specific element\nabove_text = page.find('line[width=2]').above().extract_text()\n\n# Find text below a specific element\nbelow_text = page.find('text:contains(\"Summary\")').below().extract_text()\n\n# Find text to the right of a specific element\nnearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text()\n</pre> # Find text above a specific element above_text = page.find('line[width=2]').above().extract_text()  # Find text below a specific element below_text = page.find('text:contains(\"Summary\")').below().extract_text()  # Find text to the right of a specific element nearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text() In\u00a0[7]: Copied! <pre># Find large, bold text that contains specific words\nimportant_text = page.find_all('text[size&gt;=12]:bold:contains(\"Critical\")')\n\n# Find red text inside a rectangle\nhighlighted_text = page.find('rect').find_all('text[color~=red]')\n</pre> # Find large, bold text that contains specific words important_text = page.find_all('text[size&gt;=12]:bold:contains(\"Critical\")')  # Find red text inside a rectangle highlighted_text = page.find('rect').find_all('text[color~=red]') <p>Handling Missing Elements</p> <pre><code>In these examples, we know certain elements exist in the PDF. In real-world scenarios, `page.find()` might not find a match and would return `None`. Production code should check for this:\n\n```py\nsite_label = page.find('text:contains(\"Site:\")')\nif site_label:\n    # Found it! Proceed...\n    print(site_label.extract_text())\nelse:\n    # Didn't find it, handle appropriately...\n    \"Warning: 'Site:' label not found.\"\n```</code></pre> <p>Visual Debugging</p> <pre><code>When working with complex selectors, it's helpful to visualize what you're finding:\n\n```py\nelements = page.find_all('text[color~=red]')\nelements.show()\n```</code></pre>"},{"location":"tutorials/02-finding-elements/#finding-specific-elements","title":"Finding Specific Elements\u00b6","text":"<p>Extracting all the text is useful, but often you need specific pieces of information. <code>natural-pdf</code> lets you find elements using selectors, similar to CSS.</p> <p>Let's find the \"Site\" and \"Date\" information from our <code>01-practice.pdf</code>:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-color","title":"Finding Elements by Color\u00b6","text":"<p>You can find elements based on their color:</p>"},{"location":"tutorials/02-finding-elements/#finding-lines-and-shapes","title":"Finding Lines and Shapes\u00b6","text":"<p>Find lines and rectangles based on their properties:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-font-properties","title":"Finding Elements by Font Properties\u00b6","text":""},{"location":"tutorials/02-finding-elements/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>You can find elements based on their position relative to other elements:</p>"},{"location":"tutorials/02-finding-elements/#combining-selectors","title":"Combining Selectors\u00b6","text":"<p>You can combine multiple conditions to find exactly what you need:</p>"},{"location":"tutorials/03-extracting-blocks/","title":"Extracting Text Blocks","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the starting element (\"Summary:\")\nstart_marker = page.find('text:contains(\"Summary:\")')\n\n# Select elements below the start_marker, stopping *before*\n# the thick horizontal line (a line with height &gt; 1).\nsummary_elements = start_marker.below(\n    include_source=True, # Include the \"Summary:\" text itself\n    until=\"line[height &gt; 1]\"\n)\n\n# Visualize the elements found in this block\nsummary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")\n\n# Extract and display the text from the collection of summary elements\nsummary_elements.extract_text()\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the starting element (\"Summary:\") start_marker = page.find('text:contains(\"Summary:\")')  # Select elements below the start_marker, stopping *before* # the thick horizontal line (a line with height &gt; 1). summary_elements = start_marker.below(     include_source=True, # Include the \"Summary:\" text itself     until=\"line[height &gt; 1]\" )  # Visualize the elements found in this block summary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")  # Extract and display the text from the collection of summary elements summary_elements.extract_text()  Out[2]: <pre>'Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\ninto the vats; and when they were fished out, there was never enough of them left to be worth\\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\\nto the world as Durham\u2019s Pure Leaf Lard!\\nViolations\\nStatute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[3]: Copied! <pre># Display the page image to see the visualization\npage.to_image()\n</pre> # Display the page image to see the visualization page.to_image() Out[3]: <p>This selects the elements using <code>.below(until=...)</code> and extracts their text. The second code block displays the page image with the visualized section.</p> <p>Selector Specificity</p> <pre><code>We used `line[height &gt; 1]` to find the thick horizontal line. You might need to adjust selectors based on the specific PDF structure. Inspecting element properties can help you find reliable start and end markers.</code></pre>"},{"location":"tutorials/03-extracting-blocks/#extracting-text-blocks","title":"Extracting Text Blocks\u00b6","text":"<p>Often, you need a specific section, like a paragraph between two headings. You can find a starting element and select everything below it until an ending element.</p> <p>Let's extract the \"Summary\" section from <code>01-practice.pdf</code>. It starts after \"Summary:\" and ends before the thick horizontal line.</p>"},{"location":"tutorials/04-table-extraction/","title":"Basic Table Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf  # core install already includes pdfplumber\n</pre> #%pip install natural-pdf  # core install already includes pdfplumber In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# For a single table, extract_table returns list-of-lists\ntable = page.extract_table(method=\"pdfplumber\")\ntable  # List-of-lists of cell text\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # For a single table, extract_table returns list-of-lists table = page.extract_table(method=\"pdfplumber\") table  # List-of-lists of cell text Out[2]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p><code>extract_table()</code> defaults to the plumber backend, so the explicit <code>method</code> is optional\u2014but it clarifies what's happening.</p> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Detect layout with Table Transformer\npage.analyze_layout(engine=\"tatr\")\n\n# Grab the first detected table region\ntable_region = page.find('region[type=table]')\n\ntable_region.show(label=\"TATR Table\", color=\"purple\")\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Detect layout with Table Transformer page.analyze_layout(engine=\"tatr\")  # Grab the first detected table region table_region = page.find('region[type=table]')  table_region.show(label=\"TATR Table\", color=\"purple\") Out[3]: In\u00a0[4]: Copied! <pre>tatr_rows = table_region.extract_table()  # Uses TATR backend implicitly\n</pre> tatr_rows = table_region.extract_table()  # Uses TATR backend implicitly In\u00a0[5]: Copied! <pre>page.clear_detected_layout_regions()\npage.analyze_layout(engine=\"paddle\", confidence=0.3)\n\npaddle_table = page.find('region[type=table]')\nif paddle_table:\n    paddle_table.show(color=\"green\", label=\"Paddle Table\")\n    paddle_rows = paddle_table.extract_table(method=\"pdfplumber\")  # fall back to ruling-line extraction inside the region\n</pre> page.clear_detected_layout_regions() page.analyze_layout(engine=\"paddle\", confidence=0.3)  paddle_table = page.find('region[type=table]') if paddle_table:     paddle_table.show(color=\"green\", label=\"Paddle Table\")     paddle_rows = paddle_table.extract_table(method=\"pdfplumber\")  # fall back to ruling-line extraction inside the region <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre>"},{"location":"tutorials/04-table-extraction/#basic-table-extraction","title":"Basic Table Extraction\u00b6","text":"<p>PDFs often contain tables, and <code>natural-pdf</code> provides methods to extract their data. The key is to first triangulate where your table is on the page, then use powerful extraction tools on that specific region.</p> <p>Let's extract the \"Violations\" table from our practice PDF.</p>"},{"location":"tutorials/04-table-extraction/#method-1-pdfplumber-default","title":"Method 1 \u2013 pdfplumber (default)\u00b6","text":""},{"location":"tutorials/04-table-extraction/#method-2-tatr-based-extraction","title":"Method 2 \u2013 TATR-based extraction\u00b6","text":"<p>When you do a TATR layout analysis, it detects tables, rows and cells with a LayoutLM model. Once a region has <code>source=\"detected\"</code> and <code>type=\"table\"</code>, calling <code>extract_table()</code> on that region uses the tatr backend automatically.</p>"},{"location":"tutorials/04-table-extraction/#method-3-paddleocr-layout","title":"Method 3 \u2013 PaddleOCR Layout\u00b6","text":"<p>You can also try PaddleOCR's layout detector to locate tables:</p>"},{"location":"tutorials/04-table-extraction/#choosing-the-right-backend","title":"Choosing the right backend\u00b6","text":"<ul> <li>plumber \u2013 fastest; needs rule lines or tidy whitespace.</li> <li>tatr \u2013 robust to missing lines; slower; requires AI extra.</li> <li>text \u2013 whitespace clustering; fallback when lines + models fail.</li> </ul> <p>You can call <code>page.extract_table(method=\"text\")</code> or on a <code>Region</code> as well.</p> <p>The general workflow is: try different layout analyzers to locate your table, then extract from the specific region. Keep trying options until one works for your particular PDF!</p> <p>For complex grids where even models struggle, see Tutorial 11 (enhanced table processing) for a lines-first workflow.</p>"},{"location":"tutorials/04-table-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Compare accuracy/time of the three methods on the sample PDF.</li> <li>Show how to call <code>page.extract_table(method=\"text\")</code> as a no-dependency fallback.</li> <li>Add snippet exporting <code>rows</code> to pandas DataFrame.</li> <li>Demonstrate cell post-processing (strip %, cast numbers).</li> </ul>"},{"location":"tutorials/05-excluding-content/","title":"Excluding Content (Headers/Footers)","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\n\n# Load the PDF\npdf = PDF(pdf_url)\npage = pdf.pages[0]\n\n# Let's see the bottom part of the text WITHOUT exclusions\n# It likely contains page numbers or other footer info.\nfull_text_unfiltered = page.extract_text()\n\n# Show the last 200 characters (likely containing footer text)\nfull_text_unfiltered[-200:]\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"  # Load the PDF pdf = PDF(pdf_url) page = pdf.pages[0]  # Let's see the bottom part of the text WITHOUT exclusions # It likely contains page numbers or other footer info. full_text_unfiltered = page.extract_text()  # Show the last 200 characters (likely containing footer text) full_text_unfiltered[-200:] Out[2]: <pre>' C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0\\nWrite-In Totals 0 0 0 0\\nPrecinct Summary - 11/06/2024 12:22 AM Page 1 of 387\\nReport generated with Electionware Copyright \u00a9 2007-2020'</pre> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\n\n# Define the exclusion region on every page using a lambda function\nfooter_height = 200\npdf.add_exclusion(\n    lambda page: page.region(top=page.height - footer_height),\n    label=\"Bottom 200pt Footer\"\n)\n\n# Now extract text from the first page again, exclusions are active by default\npage = pdf.pages[0]\n\n# Visualize the excluded area\nfooter_region_viz = page.region(top=page.height - footer_height)\nfooter_region_viz.show(label=\"Excluded Footer Area\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url)  # Define the exclusion region on every page using a lambda function footer_height = 200 pdf.add_exclusion(     lambda page: page.region(top=page.height - footer_height),     label=\"Bottom 200pt Footer\" )  # Now extract text from the first page again, exclusions are active by default page = pdf.pages[0]  # Visualize the excluded area footer_region_viz = page.region(top=page.height - footer_height) footer_region_viz.show(label=\"Excluded Footer Area\") page.to_image() Out[3]: In\u00a0[4]: Copied! <pre>filtered_text = page.extract_text() # use_exclusions=True is default\n\n# Show the last 200 chars with footer area excluded\nfiltered_text[-200:]\n</pre> filtered_text = page.extract_text() # use_exclusions=True is default  # Show the last 200 chars with footer area excluded filtered_text[-200:] Out[4]: <pre>'TOR\\nVote For 1\\nElection Provisional\\nTOTAL Mail Votes\\nDay Votes\\nDEM ROBERT P CASEY JR 99 70 29 0\\nREP DAVE MCCORMICK 79 69 10 0\\nLIB JOHN C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0'</pre> <p>This method is simple but might cut off content if the footer height varies or content extends lower on some pages.</p> In\u00a0[5]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\npage = pdf.pages[0] # Get page for finding elements\n\n# Find the last horizontal line on the first page\n# We'll use this logic to define our exclusion for all pages\nlast_line = page.find_all('line')[-1]\n\n# Define the exclusion function using a lambda\n# This finds the last line on *each* page and excludes below it\npdf.add_exclusion(\n    lambda p: p.find_all('line')[-1].below(),\n    label=\"Element-Based Footer\"\n)\n\n# Extract text again, with the element-based exclusion active\nfiltered_text_element = page.extract_text()\n\n# Show the last 200 chars with element-based footer exclusion\n\"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]\n\n# Visualize the element-based exclusion area\npage.clear_highlights()\n# Need to find the region again for visualization\nfooter_boundary = page.find_all('line')[-1]\nfooter_region_element = footer_boundary.below()\nfooter_region_element.show(label=\"Excluded Footer Area (Element)\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url) page = pdf.pages[0] # Get page for finding elements  # Find the last horizontal line on the first page # We'll use this logic to define our exclusion for all pages last_line = page.find_all('line')[-1]  # Define the exclusion function using a lambda # This finds the last line on *each* page and excludes below it pdf.add_exclusion(     lambda p: p.find_all('line')[-1].below(),     label=\"Element-Based Footer\" )  # Extract text again, with the element-based exclusion active filtered_text_element = page.extract_text()  # Show the last 200 chars with element-based footer exclusion \"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]  # Visualize the element-based exclusion area page.clear_highlights() # Need to find the region again for visualization footer_boundary = page.find_all('line')[-1] footer_region_element = footer_boundary.below() footer_region_element.show(label=\"Excluded Footer Area (Element)\") page.to_image() Out[5]: <p>This element-based approach is usually more reliable as it adapts to the content's position, but it depends on finding consistent boundary elements (like lines or specific text markers).</p>"},{"location":"tutorials/05-excluding-content/#excluding-content-headersfooters","title":"Excluding Content (Headers/Footers)\u00b6","text":"<p>Often, PDFs have repeating headers or footers on every page that you want to ignore when extracting the main content. <code>natural-pdf</code> allows you to define exclusion regions.</p> <p>We'll use a different PDF for this example, which has a distinct header and footer section: <code>0500000US42007.pdf</code>.</p>"},{"location":"tutorials/05-excluding-content/#approach-1-excluding-a-fixed-area","title":"Approach 1: Excluding a Fixed Area\u00b6","text":"<p>A simple way to exclude headers or footers is to define a fixed region based on page coordinates. Let's exclude the bottom 200 pixels of the page.</p>"},{"location":"tutorials/05-excluding-content/#approach-2-excluding-based-on-elements","title":"Approach 2: Excluding Based on Elements\u00b6","text":"<p>A more robust way is to find specific elements that reliably mark the start of the footer (or end of the header) and exclude everything below (or above) them. In <code>Examples.md</code>, the footer was defined as everything below the last horizontal line.</p>"},{"location":"tutorials/05-excluding-content/#todo","title":"TODO\u00b6","text":"<ul> <li>Show a text-based exclusion: <code>pdf.add_exclusion(lambda p: p.find('text:contains(\"Page \")').below())</code> for dynamic page numbers.</li> <li>Demonstrate stacking multiple exclusions (e.g., header + footer) and the order they are applied.</li> <li>Provide an example disabling exclusions temporarily with <code>extract_text(use_exclusions=False)</code>.</li> <li>Include a multi-page preview that outlines exclusions on every page.</li> </ul> <p>Applying Exclusions</p> <pre><code>*   `pdf.add_exclusion(func)` applies the exclusion function (which takes a page and returns a region) to *all* pages in the PDF.\n*   `page.add_exclusion(region)` adds an exclusion region only to that specific page.\n*   `extract_text(use_exclusions=False)` can be used to temporarily disable exclusions.</code></pre>"},{"location":"tutorials/06-document-qa/","title":"Document Question Answering (QA)","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"  # DocumentQA relies on torch + transformers\n</pre> #%pip install \"natural-pdf[ai]\"  # DocumentQA relies on torch + transformers In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Ask about the date\nquestion_1 = \"What is the inspection date?\"\nanswer_1 = page.ask(question_1)\n\n# The result dictionary always contains:\n#   answer      \u2013 extracted span (string, may be empty)\n#   confidence  \u2013 model score 0\u20131\n#   start / end \u2013 indices into page.words\n#   found       \u2013 False if confidence &lt; min_confidence\nanswer_1\n# \u279c {'answer': 'July 31, 2023', 'confidence': 0.82, 'start': 33, 'end': 36, 'found': True}\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Ask about the date question_1 = \"What is the inspection date?\" answer_1 = page.ask(question_1)  # The result dictionary always contains: #   answer      \u2013 extracted span (string, may be empty) #   confidence  \u2013 model score 0\u20131 #   start / end \u2013 indices into page.words #   found       \u2013 False if confidence &lt; min_confidence answer_1 # \u279c {'answer': 'July 31, 2023', 'confidence': 0.82, 'start': 33, 'end': 36, 'found': True} <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n</pre> Out[2]: <pre>{'answer': 'February 3, 1905',\n 'confidence': 0.9979940056800842,\n 'start': 6,\n 'end': 6,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre># Ask about the company name\nquestion_2 = \"What company was inspected?\"\nanswer_2 = page.ask(question_2)\n\n# Display the answer dictionary\nanswer_2\n</pre> # Ask about the company name question_2 = \"What company was inspected?\" answer_2 = page.ask(question_2)  # Display the answer dictionary answer_2 Out[3]: <pre>{'answer': 'Jungle Health and Safety Inspection Service',\n 'confidence': 0.9988948106765747,\n 'start': 0,\n 'end': 0,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre># Ask about specific content from the table\nquestion_3 = \"What is statute 5.8.3 about?\"\nanswer_3 = page.ask(question_3)\n\n# Display the answer\nanswer_3\n</pre> # Ask about specific content from the table question_3 = \"What is statute 5.8.3 about?\" answer_3 = page.ask(question_3)  # Display the answer answer_3 Out[4]: <pre>{'answer': 'Inadequate Protective Equipment.',\n 'confidence': 0.9997999668121338,\n 'start': 26,\n 'end': 26,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> <p>The results include the extracted <code>answer</code>, a <code>confidence</code> score (useful for filtering uncertain answers), the <code>page_num</code>, and the <code>source_elements</code>.</p> In\u00a0[5]: Copied! <pre>from natural_pdf.elements.collections import ElementCollection\n\npage.clear_highlights()\n\nif answer_1[\"found\"]:\n    words = ElementCollection(page.words[answer_1[\"start\"] : answer_1[\"end\"] + 1])\n    words.show(color=\"yellow\", label=question_1)\n\npage.to_image()\n</pre> from natural_pdf.elements.collections import ElementCollection  page.clear_highlights()  if answer_1[\"found\"]:     words = ElementCollection(page.words[answer_1[\"start\"] : answer_1[\"end\"] + 1])     words.show(color=\"yellow\", label=question_1)  page.to_image() Out[5]: In\u00a0[6]: Copied! <pre>from natural_pdf import PDF\nimport pandas as pd\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# List of questions to ask\nquestions = [\n    \"What is the inspection date?\",\n    \"What company was inspected?\",\n    \"What is statute 5.8.3 about?\",\n    \"How many violations were there in total?\" # This might be less reliable\n]\n\n# Collect answers for each question\nresults = []\nfor q in questions:\n    ans = page.ask(q, min_confidence=0.2)\n    ans[\"question\"] = q\n    results.append(ans)\n\ncols = [\"question\", \"answer\", \"confidence\", \"found\"]\nqa_df = pd.DataFrame(results)[cols]\nqa_df\n</pre> from natural_pdf import PDF import pandas as pd  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # List of questions to ask questions = [     \"What is the inspection date?\",     \"What company was inspected?\",     \"What is statute 5.8.3 about?\",     \"How many violations were there in total?\" # This might be less reliable ]  # Collect answers for each question results = [] for q in questions:     ans = page.ask(q, min_confidence=0.2)     ans[\"question\"] = q     results.append(ans)  cols = [\"question\", \"answer\", \"confidence\", \"found\"] qa_df = pd.DataFrame(results)[cols] qa_df Out[6]: question answer confidence found 0 What is the inspection date? February 3, 1905 0.997994 True 1 What company was inspected? Jungle Health and Safety Inspection Service 0.998895 True 2 What is statute 5.8.3 about? Inadequate Protective Equipment. 0.999800 True 3 How many violations were there in total? 4.12.7 0.662557 True <p>This shows how you can iterate through questions, collect the answer dictionaries, and then create a structured DataFrame, making it easy to review questions, answers, and their confidence levels together.</p>"},{"location":"tutorials/06-document-qa/#document-question-answering-qa","title":"Document Question Answering (QA)\u00b6","text":"<p>Sometimes, instead of searching for specific text patterns, you just want to ask the document a question directly. <code>natural-pdf</code> includes an extractive Question Answering feature.</p> <p>\"Extractive\" means it finds the literal answer text within the document, rather than generating a new answer or summarizing.</p> <p>Let's ask our <code>01-practice.pdf</code> a few questions.</p>"},{"location":"tutorials/06-document-qa/#visualising-where-the-answer-came-from","title":"Visualising Where the Answer Came From\u00b6","text":""},{"location":"tutorials/06-document-qa/#collecting-results-into-a-dataframe","title":"Collecting Results into a DataFrame\u00b6","text":"<p>If you're asking multiple questions, it's often useful to collect the results into a pandas DataFrame for easier analysis.</p>"},{"location":"tutorials/06-document-qa/#todo","title":"TODO\u00b6","text":"<ul> <li>Demonstrate passing <code>model=\"impira/layoutlm-document-qa\"</code> to switch models.</li> <li>Show multi-page QA: iterate over <code>pdf.pages</code> and add <code>page</code> column to the results.</li> <li>Add batch helper (<code>pdf.ask_many(questions)</code>) once implemented.</li> </ul>"},{"location":"tutorials/06-document-qa/#wish-list","title":"Wish List\u00b6","text":"<ul> <li>Support for highlighting answer automatically via a <code>show_answer()</code> helper.</li> <li>Option to return bounding box coordinates directly (<code>bbox</code>) in the answer dict.</li> <li>Add <code>ElementCollection.to_dataframe()</code> for one-call DataFrame creation.</li> </ul> <p>QA Model and Limitations</p> <pre><code>*   The QA system relies on underlying transformer models. Performance and confidence scores vary.\n*   It works best for questions where the answer is explicitly stated. It cannot synthesize information or perform calculations (e.g., counting items might fail or return text containing a number rather than the count itself).\n*   You can potentially specify different QA models via the `model=` argument in `page.ask()` if others are configured.</code></pre>"},{"location":"tutorials/07-layout-analysis/","title":"Layout Analysis","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Analyze the layout using the default model\n# This adds 'detected' Region objects to the page\n# It returns an ElementCollection of the detected regions\npage.analyze_layout()\ndetected_regions = page.find_all('region[source=\"detected\"]')\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Analyze the layout using the default model # This adds 'detected' Region objects to the page # It returns an ElementCollection of the detected regions page.analyze_layout() detected_regions = page.find_all('region[source=\"detected\"]') <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp9p_u03be/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 3721.5ms\n</pre> <pre>Speed: 12.4ms preprocess, 3721.5ms inference, 2.3ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[3]: Copied! <pre># Visualize all detected regions, using default colors based on type\ndetected_regions.show(group_by='type', include_attrs=['confidence'])\n</pre> # Visualize all detected regions, using default colors based on type detected_regions.show(group_by='type', include_attrs=['confidence']) Out[3]: In\u00a0[4]: Copied! <pre># Find and visualize only the detected table region(s)\ntables = page.find_all('region[type=table]')\ntables.show(color='lightgreen', label='Detected Table')\n</pre> # Find and visualize only the detected table region(s) tables = page.find_all('region[type=table]') tables.show(color='lightgreen', label='Detected Table') Out[4]: In\u00a0[5]: Copied! <pre># Extract text specifically from the detected table region\ntable_region = tables.first # Assuming only one table was detected\n# Extract text preserving layout\ntable_text_layout = table_region.extract_text(layout=True)\ntable_text_layout\n</pre> # Extract text specifically from the detected table region table_region = tables.first # Assuming only one table was detected # Extract text preserving layout table_text_layout = table_region.extract_text(layout=True) table_text_layout Out[5]: <pre>'Statute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[6]: Copied! <pre># Layout-detected regions can also be used for table extraction\n# This can be more robust than the basic page.extract_tables()\n# especially for tables without clear lines.\ntable_data = table_region.extract_table()\ntable_data\n</pre> # Layout-detected regions can also be used for table extraction # This can be more robust than the basic page.extract_tables() # especially for tables without clear lines. table_data = table_region.extract_table() table_data Out[6]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[7]: Copied! <pre># Re-run layout with PaddleOCR detector\npage.clear_detected_layout_regions()\n\npaddle_regions = page.analyze_layout(engine=\"paddle\", confidence=0.3)\n#paddle_regions.show(group_by=\"type\")\n\n# Only keep detections the model tagged as \"table\" or \"figure\"\ntables_and_figs = paddle_regions.filter(lambda r: r.normalized_type in {\"table\", \"figure\"})\n#tables_and_figs.show(label_format=\"{normalized_type} ({confidence:.2f})\")\n</pre> # Re-run layout with PaddleOCR detector page.clear_detected_layout_regions()  paddle_regions = page.analyze_layout(engine=\"paddle\", confidence=0.3) #paddle_regions.show(group_by=\"type\")  # Only keep detections the model tagged as \"table\" or \"figure\" tables_and_figs = paddle_regions.filter(lambda r: r.normalized_type in {\"table\", \"figure\"}) #tables_and_figs.show(label_format=\"{normalized_type} ({confidence:.2f})\") <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <p>The helper accepts these common kwargs (see <code>LayoutOptions</code> subclasses for full list):</p> <ul> <li><code>confidence</code> \u2013 minimum score for retaining a prediction.</li> <li><code>classes</code> / <code>exclude_classes</code> \u2013 whitelist or blacklist region types.</li> <li><code>device</code> \u2013 \"cuda\" or \"cpu\"; defaults to GPU if available.</li> </ul> <p>Each engine also exposes its own options class (e.g., <code>YOLOLayoutOptions</code>) for fine control over NMS thresholds, model sizes, etc. Pass an instance via the <code>options=</code> param.</p> <p>Layout analysis provides structured <code>Region</code> objects. You can filter these regions by their predicted <code>type</code> and then perform actions like visualization or extracting text/tables specifically from those regions.</p>"},{"location":"tutorials/07-layout-analysis/#layout-analysis","title":"Layout Analysis\u00b6","text":"<p>Beyond simple text and lines, <code>natural-pdf</code> can use layout analysis models (like YOLO or DETR) to identify semantic regions within a page, such as paragraphs, tables, figures, headers, etc. This provides a higher-level understanding of the document structure.</p>"},{"location":"tutorials/07-layout-analysis/#available-layout-engines","title":"Available Layout Engines\u00b6","text":"<ul> <li>yolo \u2013 YOLOv5 model trained on DocLayNet; fast and good at classic page objects (paragraph, table, figure, heading).  Install via <code>npdf install yolo</code>.</li> <li>tatr \u2013 Microsoft's Table Transformer (LayoutLM) specialised in tables; already included in the ai extra.</li> <li>paddle \u2013 PaddleOCR`s layout detector; lightweight and CPU-friendly.</li> <li>surya \u2013 Surya Layout Parser (DETR backbone) tuned for invoices and forms.</li> <li>docling \u2013 YOLOX model published by DocLING researchers; performs well on historical documents.</li> <li>gemini \u2013 Calls Google's Vision Gemini API (experimental, requires <code>OPENAI_API_KEY</code>).</li> </ul> <p><code>page.analyze_layout()</code> defaults to the first available engine (search order <code>yolo \u2192 paddle \u2192 tatr</code>), but you can pick one explicitly with <code>engine=\"...\"</code>.</p> <p>Let's analyze the layout of our <code>01-practice.pdf</code>.</p>"},{"location":"tutorials/07-layout-analysis/#switching-engines-and-tuning-thresholds","title":"Switching Engines and Tuning Thresholds\u00b6","text":""},{"location":"tutorials/07-layout-analysis/#todo","title":"TODO\u00b6","text":"<ul> <li>Add a speed/accuracy comparison snippet looping over all installed engines.</li> <li>Demonstrate multi-page batch: <code>pdf.pages[::2].analyze_layout(engine=\"yolo\")</code>.</li> <li>Show <code>page.get_sections(start_elements=page.find_all('region[type=heading]'))</code> to split by detected headings.</li> <li>Include an example of exporting regions to COCO JSON for custom model fine-tuning.</li> <li>Document how to override the model path via <code>model_name</code> and how to plug a remote inference client (<code>client=</code>).</li> </ul>"},{"location":"tutorials/07-layout-analysis/#wish-list-future-enhancements","title":"Wish List (Future Enhancements)\u00b6","text":"<ul> <li>Confidence palette \u2013 Allow <code>show(color_by=\"confidence\")</code> to auto-map scores to a red\u2013green gradient.</li> <li><code>ElementCollection.to_json()</code> \u2013 one-liner export of detected regions (and optionally <code>to_dataframe()</code>).</li> <li>Model cache override \u2013 honor an env variable like <code>NATPDF_MODEL_DIR</code> so enterprises can redirect weight downloads.</li> <li>Remote inference support \u2013 make the <code>client=</code> hook forward images to a custom REST or gRPC service.</li> </ul>"},{"location":"tutorials/07-working-with-regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Create a region in the top portion of the page\ntop_region = page.create_region(\n    50,          # x0 (left)\n    100,          # y0 (top)\n    page.width - 50,  # x1 (right)\n    200          # y1 (bottom)\n)\n\n# Visualize the region\ntop_region.show(color=\"blue\")\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Create a region in the top portion of the page top_region = page.create_region(     50,          # x0 (left)     100,          # y0 (top)     page.width - 50,  # x1 (right)     200          # y1 (bottom) )  # Visualize the region top_region.show(color=\"blue\") Out[2]: In\u00a0[3]: Copied! <pre># Extract text from this region\ntop_region.extract_text()\n</pre> # Extract text from this region top_region.extract_text() Out[3]: <pre>'Date: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'</pre> In\u00a0[4]: Copied! <pre># Find an element to create regions around\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Create regions relative to this element\nbelow_title = title.below(height=100)\nright_of_title = title.right(width=200) \nabove_title = title.above(height=50)\n\n# Visualize these regions\npage.clear_highlights()\nbelow_title.highlight(color=\"green\", label=\"Below\")\nright_of_title.highlight(color=\"red\", label=\"Right\")\nabove_title.highlight(color=\"orange\", label=\"Above\")\n\npage.to_image()\n</pre> # Find an element to create regions around title = page.find('text:contains(\"Jungle Health\")')  # Create regions relative to this element below_title = title.below(height=100) right_of_title = title.right(width=200)  above_title = title.above(height=50)  # Visualize these regions page.clear_highlights() below_title.highlight(color=\"green\", label=\"Below\") right_of_title.highlight(color=\"red\", label=\"Right\") above_title.highlight(color=\"orange\", label=\"Above\")  page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Extract text from the region below the title\nbelow_title.extract_text()\n</pre> # Extract text from the region below the title below_title.extract_text() Out[5]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[6]: Copied! <pre># Create a region for a specific document section\nform_region = page.create_region(50, 100, page.width - 50, 300)\n\n# Find elements only within this region\nlabels = form_region.find_all('text:contains(\":\")') \n\n# Visualize the region and the elements found\nform_region.show(\n    color=(0, 0, 1, 0.2),\n    label=\"Form Region\"\n)\nlabels.show(color=\"purple\", label=\"Labels\")\n</pre> # Create a region for a specific document section form_region = page.create_region(50, 100, page.width - 50, 300)  # Find elements only within this region labels = form_region.find_all('text:contains(\":\")')   # Visualize the region and the elements found form_region.show(     color=(0, 0, 1, 0.2),     label=\"Form Region\" ) labels.show(color=\"purple\", label=\"Labels\") Out[6]: In\u00a0[7]: Copied! <pre># Find an element to work with\nelement = page.find('text:contains(\"Summary:\")')\n\n# Create a tight region around the element\ntight_region = element.expand(0, 0, 0, 0)\n\n# Expand it to include surrounding content\nexpanded_region = tight_region.expand(\n    left=10,       # Expand 10 points to the left\n    right=200,     # Expand 200 points to the right\n    top=5,  # Expand 5 points above\n    bottom=100  # Expand 100 points below\n)\n\n# Visualize both regions\npage.clear_highlights()\ntight_region.highlight(color=\"red\", label=\"Original\")\nexpanded_region.highlight(color=\"blue\", label=\"Expanded\")\n\npage.to_image()\n</pre> # Find an element to work with element = page.find('text:contains(\"Summary:\")')  # Create a tight region around the element tight_region = element.expand(0, 0, 0, 0)  # Expand it to include surrounding content expanded_region = tight_region.expand(     left=10,       # Expand 10 points to the left     right=200,     # Expand 200 points to the right     top=5,  # Expand 5 points above     bottom=100  # Expand 100 points below )  # Visualize both regions page.clear_highlights() tight_region.highlight(color=\"red\", label=\"Original\") expanded_region.highlight(color=\"blue\", label=\"Expanded\")  page.to_image() Out[7]: In\u00a0[8]: Copied! <pre># Find two elements to serve as boundaries\nstart_elem = page.find('text:contains(\"Summary:\")')\nend_elem = page.find('text:contains(\"Violations\")')\n\n# Create a region from start to end element\nbounded_region = start_elem.until(end_elem)\n\n# Visualize the bounded region\nbounded_region.show(color=\"green\", label=\"Bounded Region\")\n\n# Extract text from this bounded region\nbounded_region.extract_text()[:200] + \"...\"\n</pre> # Find two elements to serve as boundaries start_elem = page.find('text:contains(\"Summary:\")') end_elem = page.find('text:contains(\"Violations\")')  # Create a region from start to end element bounded_region = start_elem.until(end_elem)  # Visualize the bounded region bounded_region.show(color=\"green\", label=\"Bounded Region\")  # Extract text from this bounded region bounded_region.extract_text()[:200] + \"...\" Out[8]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men...'</pre> In\u00a0[9]: Copied! <pre># Define multiple regions to extract different parts of the document\nheader_region = page.create_region(0, 0, page.width, 100)\nmain_region = page.create_region(100, 100, page.width - 100, page.height - 150)\nfooter_region = page.create_region(0, page.height - 50, page.width, page.height)\n\n# Visualize all regions\nheader_region.show(color=\"blue\", label=\"Header\")\nmain_region.show(color=\"green\", label=\"Main Content\")\nfooter_region.show(color=\"red\", label=\"Footer\")\n\n# Extract content from each region\ndocument_parts = {\n    \"header\": header_region.extract_text(),\n    \"main\": main_region.extract_text()[:100] + \"...\",\n    \"footer\": footer_region.extract_text()\n}\n\n# Show what we extracted\ndocument_parts\n</pre> # Define multiple regions to extract different parts of the document header_region = page.create_region(0, 0, page.width, 100) main_region = page.create_region(100, 100, page.width - 100, page.height - 150) footer_region = page.create_region(0, page.height - 50, page.width, page.height)  # Visualize all regions header_region.show(color=\"blue\", label=\"Header\") main_region.show(color=\"green\", label=\"Main Content\") footer_region.show(color=\"red\", label=\"Footer\")  # Extract content from each region document_parts = {     \"header\": header_region.extract_text(),     \"main\": main_region.extract_text()[:100] + \"...\",     \"footer\": footer_region.extract_text() }  # Show what we extracted document_parts Out[9]: <pre>{'header': 'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.',\n 'main': 'ruary 3, 1905\\nCount: 7\\nWorst of any, however, were the fertilizer men, and those who served in the c...',\n 'footer': 'Jungle Health and Safety Inspection Service'}</pre> In\u00a0[10]: Copied! <pre># Find a region of interest\ntable_header = page.find('text:contains(\"Statute\")')\ntable_region = table_header.below(height=100)\n\n# Visualize the region\ntable_region.show(color=\"purple\", label=\"Table Region\")\n\n# Create an image of just this region\ntable_region.to_image(resolution=150)\n</pre> # Find a region of interest table_header = page.find('text:contains(\"Statute\")') table_region = table_header.below(height=100)  # Visualize the region table_region.show(color=\"purple\", label=\"Table Region\")  # Create an image of just this region table_region.to_image(resolution=150) Out[10]: <p>Regions allow you to precisely target specific parts of a document for extraction and analysis. They're essential for handling complex document layouts and isolating the exact content you need.</p>"},{"location":"tutorials/07-working-with-regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that let you focus on specific parts of a document. They're perfect for extracting text from defined areas, finding elements within certain boundaries, and working with document sections.</p>"},{"location":"tutorials/07-working-with-regions/#creating-regions-from-elements","title":"Creating Regions from Elements\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#finding-elements-within-regions","title":"Finding Elements Within Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#expanding-and-adjusting-regions","title":"Expanding and Adjusting Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-bounded-regions","title":"Creating Bounded Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#working-with-multiple-regions","title":"Working with Multiple Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-an-image-of-a-region","title":"Creating an Image of a Region\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/","title":"Spatial Navigation","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the title of the document\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Visualize our starting point\ntitle.show(color=\"red\", label=\"Document Title\")\n\n# Display the title text\ntitle.text\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the title of the document title = page.find('text:contains(\"Jungle Health\")')  # Visualize our starting point title.show(color=\"red\", label=\"Document Title\")  # Display the title text title.text Out[2]: <pre>'Jungle Health and Safety Inspection Service'</pre> In\u00a0[3]: Copied! <pre># Create a region below the title\nregion_below = title.below(height=100)\n\n# Visualize the region\nregion_below.show(color=\"blue\", label=\"Below Title\")\n\n# Find and extract text from this region\ntext_below = region_below.extract_text()\ntext_below\n</pre> # Create a region below the title region_below = title.below(height=100)  # Visualize the region region_below.show(color=\"blue\", label=\"Below Title\")  # Find and extract text from this region text_below = region_below.extract_text() text_below Out[3]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[4]: Copied! <pre># Find two labels to serve as boundaries\nsite_label = page.find('text:contains(\"Site:\")')\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Get the region between these labels\nbetween_region = site_label.below(\n    include_source=True,     # Include starting element\n    until='text:contains(\"Date:\")',  # Stop at this element\n    include_endpoint=False    # Don't include ending element\n)\n\n# Visualize the region between labels\nbetween_region.show(color=\"green\", label=\"Between\")\n\n# Extract text from this bounded area\nbetween_region.extract_text()\n</pre> # Find two labels to serve as boundaries site_label = page.find('text:contains(\"Site:\")') date_label = page.find('text:contains(\"Date:\")')  # Get the region between these labels between_region = site_label.below(     include_source=True,     # Include starting element     until='text:contains(\"Date:\")',  # Stop at this element     include_endpoint=False    # Don't include ending element )  # Visualize the region between labels between_region.show(color=\"green\", label=\"Between\")  # Extract text from this bounded area between_region.extract_text() Out[4]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.'</pre> In\u00a0[5]: Copied! <pre># Find a field label\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Get the content to the right (the field value)\nvalue_region = site_label.right(width=200)\n\n# Visualize the label and value regions\nsite_label.show(color=\"red\", label=\"Label\")\nvalue_region.show(color=\"blue\", label=\"Value\")\n\n# Extract just the value text\nvalue_region.extract_text()\n</pre> # Find a field label site_label = page.find('text:contains(\"Site:\")')  # Get the content to the right (the field value) value_region = site_label.right(width=200)  # Visualize the label and value regions site_label.show(color=\"red\", label=\"Label\") value_region.show(color=\"blue\", label=\"Value\")  # Extract just the value text value_region.extract_text() Out[5]: <pre>'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt'</pre> In\u00a0[6]: Copied! <pre># Start with a label element\nlabel = page.find('text:contains(\"Site:\")')\n\n# Find the next and previous elements in reading order\nnext_elem = label.next()\nprev_elem = label.prev()\n\n# Visualize all three elements\nlabel.show(color=\"red\", label=\"Current\")\nnext_elem.show(color=\"green\", label=\"Next\") if next_elem else None\nprev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None\n\n# Show the text of adjacent elements\n{\n    \"current\": label.text,\n    \"next\": next_elem.text if next_elem else \"None\",\n    \"previous\": prev_elem.text if prev_elem else \"None\"\n}\n</pre> # Start with a label element label = page.find('text:contains(\"Site:\")')  # Find the next and previous elements in reading order next_elem = label.next() prev_elem = label.prev()  # Visualize all three elements label.show(color=\"red\", label=\"Current\") next_elem.show(color=\"green\", label=\"Next\") if next_elem else None prev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None  # Show the text of adjacent elements {     \"current\": label.text,     \"next\": next_elem.text if next_elem else \"None\",     \"previous\": prev_elem.text if prev_elem else \"None\" } Out[6]: <pre>{'current': 'Site: ', 'next': 'i', 'previous': 'S'}</pre> In\u00a0[7]: Copied! <pre># Find a section label\nsummary = page.find('text:contains(\"Summary:\")')\n\n# Find the next bold text element\nnext_bold = summary.next('text:bold', limit=20)\n\n# Find the nearest line element\nnearest_line = summary.nearest('line')\n\n# Visualize what we found\nsummary.show(color=\"red\", label=\"Summary\")\nnext_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None\nnearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None\n\n# Show the content we found\n{\n    \"summary\": summary.text,\n    \"next_bold\": next_bold.text if next_bold else \"None found\",\n    \"nearest_line\": nearest_line if nearest_line else \"None found\"\n}\n</pre> # Find a section label summary = page.find('text:contains(\"Summary:\")')  # Find the next bold text element next_bold = summary.next('text:bold', limit=20)  # Find the nearest line element nearest_line = summary.nearest('line')  # Visualize what we found summary.show(color=\"red\", label=\"Summary\") next_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None nearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None  # Show the content we found {     \"summary\": summary.text,     \"next_bold\": next_bold.text if next_bold else \"None found\",     \"nearest_line\": nearest_line if nearest_line else \"None found\" } Out[7]: <pre>{'summary': 'Summary: ',\n 'next_bold': 'u',\n 'nearest_line': &lt;LineElement type=horizontal width=2.0 bbox=(50.0, 352.0, 550.0, 352.0)&gt;}</pre> In\u00a0[8]: Copied! <pre># Find a table heading\ntable_heading = page.find('text:contains(\"Statute\")')\ntable_heading.show(color=\"purple\", label=\"Table Header\")\n\n# Extract table rows using spatial navigation\nrows = []\ncurrent = table_heading\n\n# Get the next 4 rows\nfor i in range(4):\n    # Find the next row below the current one\n    next_row = current.below(height=15)\n    \n    if next_row:\n        rows.append(next_row)\n        current = next_row  # Move to the next row\n    else:\n        break\n\n# Visualize all found rows\npage.clear_highlights()\nfor i, row in enumerate(rows):\n    row.highlight(label=f\"Row {i+1}\", use_color_cycling=True)\n</pre> # Find a table heading table_heading = page.find('text:contains(\"Statute\")') table_heading.show(color=\"purple\", label=\"Table Header\")  # Extract table rows using spatial navigation rows = [] current = table_heading  # Get the next 4 rows for i in range(4):     # Find the next row below the current one     next_row = current.below(height=15)          if next_row:         rows.append(next_row)         current = next_row  # Move to the next row     else:         break  # Visualize all found rows page.clear_highlights() for i, row in enumerate(rows):     row.highlight(label=f\"Row {i+1}\", use_color_cycling=True) In\u00a0[9]: Copied! <pre># Extract text from each row\n[row.extract_text() for row in rows]\n</pre> # Extract text from each row [row.extract_text() for row in rows] Out[9]: <pre>['4.12.7 Unsanitary Working Conditions. Critical',\n '4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious',\n '5.8.3 Inadequate Protective Equipment. Serious',\n '6.3.9 Ineffective Injury Prevention. Serious']</pre> In\u00a0[10]: Copied! <pre># Find all potential field labels (text with a colon)\nlabels = page.find_all('text:contains(\":\")') \n\n# Visualize the labels\nlabels.show(color=\"blue\", label=\"Labels\")\n\n# Extract key-value pairs\nfield_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    key = label.text.strip().rstrip(':')\n    \n    # Skip if not a proper label\n    if not key:\n        continue\n    \n    # Get the value to the right\n    value = label.right(width=200).extract_text().strip()\n    \n    # Add to our collection\n    field_data[key] = value\n\n# Show the extracted data\nfield_data\n</pre> # Find all potential field labels (text with a colon) labels = page.find_all('text:contains(\":\")')   # Visualize the labels labels.show(color=\"blue\", label=\"Labels\")  # Extract key-value pairs field_data = {}  for label in labels:     # Clean up the label text     key = label.text.strip().rstrip(':')          # Skip if not a proper label     if not key:         continue          # Get the value to the right     value = label.right(width=200).extract_text().strip()          # Add to our collection     field_data[key] = value  # Show the extracted data field_data Out[10]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> <p>Spatial navigation mimics how humans read documents, letting you navigate content based on physical relationships between elements. It's especially useful for extracting structured data from forms, tables, and formatted documents.</p> In\u00a0[11]: Copied! <pre># Step 1 \u2013 find the heading text\nheading = page.find('text:contains(\"Summary:\")')\n\n# Step 2 \u2013 get the first bold word after that heading (skip up to 30 elements)\nvalue_label = heading.next('text:bold', limit=30)\n\n# Step 3 \u2013 grab the value region to the right of that bold word\nvalue_region = value_label.right(until='line')  # Extend until the boundary line\n\nvalue_region.show(color=\"orange\", label=\"Summary Value\")\nvalue_region.extract_text()\n</pre> # Step 1 \u2013 find the heading text heading = page.find('text:contains(\"Summary:\")')  # Step 2 \u2013 get the first bold word after that heading (skip up to 30 elements) value_label = heading.next('text:bold', limit=30)  # Step 3 \u2013 grab the value region to the right of that bold word value_region = value_label.right(until='line')  # Extend until the boundary line  value_region.show(color=\"orange\", label=\"Summary Value\") value_region.extract_text() Out[11]: <pre>'e: Durha\\nte: Febr\\nolation C\\nmmary:\\nese peop\\nitor at a h\\nme of wh\\no the vats\\nhibiting -\\nthe world\\nolation\\ntatute\\n12.7\\n8.3\\n3.9\\n1.5\\n9.2\\n6.4\\n0.2.7'</pre> In\u00a0[12]: Copied! <pre>inspection_date_value = (\n    page.find('text:startswith(\"Date:\")')\n        .right(width=500, height='element')            # Move right to get the date value region\n        .find('text')                # Narrow to text elements only\n)\n</pre> inspection_date_value = (     page.find('text:startswith(\"Date:\")')         .right(width=500, height='element')            # Move right to get the date value region         .find('text')                # Narrow to text elements only ) <p>Because each call returns an element, you never lose the spatial context \u2013 you can always add another <code>.below()</code> or <code>.nearest()</code> later.</p>"},{"location":"tutorials/08-spatial-navigation/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>Spatial navigation lets you work with PDF content based on the physical layout of elements on the page. It's perfect for finding elements relative to each other and extracting information in context.</p>"},{"location":"tutorials/08-spatial-navigation/#finding-elements-above-and-below","title":"Finding Elements Above and Below\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-content-between-elements","title":"Finding Content Between Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#navigating-left-and-right","title":"Navigating Left and Right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-adjacent-elements","title":"Finding Adjacent Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#combining-with-element-selectors","title":"Combining with Element Selectors\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-table-rows-with-spatial-navigation","title":"Extracting Table Rows with Spatial Navigation\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-key-value-pairs","title":"Extracting Key-Value Pairs\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#todo","title":"TODO\u00b6","text":"<ul> <li>Add examples for navigating across multiple pages using <code>pdf.pages</code> slicing and <code>below(..., until=...)</code> that spans pages.</li> <li>Show how to chain selectors, e.g., <code>page.find('text:bold').below().right()</code> for complex paths.</li> <li>Include a sidebar on performance when many spatial calls are chained and how to cache intermediate regions.</li> <li>Add examples using <code>.until()</code> for one-liner \"from here until X\" extractions.</li> <li>Show using <code>width=\"element\"</code> vs <code>\"full\"</code> in <code>.below()</code> and <code>.above()</code> to restrict horizontal span.</li> <li>Demonstrate attribute selectors (e.g., <code>line[width&gt;2]</code>) and <code>:not()</code> pseudo-class for exclusion in spatial chains.</li> <li>Briefly introduce <code>.expand()</code> for fine-tuning region size after spatial selection.</li> </ul>"},{"location":"tutorials/08-spatial-navigation/#chaining-spatial-calls","title":"Chaining Spatial Calls\u00b6","text":"<p>Spatial helpers like <code>.below()</code>, <code>.right()</code>, <code>.nearest()</code> and friends return Element or Region objects, so you can keep chaining operations just like you would with jQuery or BeautifulSoup.</p> <ol> <li>Start with a selector (string or Element).</li> <li>Apply a spatial function.</li> <li>Optionally, add another selector to narrow the result.</li> <li>Repeat!</li> </ol>"},{"location":"tutorials/08-spatial-navigation/#example-1-heading-next-bold-word-value-to-its-right","title":"Example 1 \u2013 Heading \u2192 next bold word \u2192 value to its right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#example-2-find-a-label-anywhere-on-the-document-and-walk-to-its-value-in-one-chain","title":"Example 2 \u2013 Find a label anywhere on the document and walk to its value in one chain\u00b6","text":""},{"location":"tutorials/09-section-extraction/","title":"Section Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF using the relative path\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\n# Identify horizontal rules that look like section dividers\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Visualize the potential section boundaries (single element type \u279c use .show())\nhorizontal_lines.show(color=\"red\", label=\"Section Boundaries\")\npage.to_image()\n</pre> from natural_pdf import PDF  # Load the PDF using the relative path pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  # Identify horizontal rules that look like section dividers horizontal_lines = page.find_all('line:horizontal')  # Visualize the potential section boundaries (single element type \u279c use .show()) horizontal_lines.show(color=\"red\", label=\"Section Boundaries\") page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Count what we found\nlen(horizontal_lines)\n</pre> # Count what we found len(horizontal_lines) Out[3]: <pre>9</pre> In\u00a0[4]: Copied! <pre># Extract sections based on horizontal lines\n# Each section starts at a horizontal line and ends at the next one\nbook_sections = page.get_sections(\n    start_elements=horizontal_lines,\n    boundary_inclusion='start'  # Include the boundary in the section\n)\n\n# Visualize each section\npage.clear_highlights()\nfor section in book_sections:\n    section.show()\npage.to_image()\n</pre> # Extract sections based on horizontal lines # Each section starts at a horizontal line and ends at the next one book_sections = page.get_sections(     start_elements=horizontal_lines,     boundary_inclusion='start'  # Include the boundary in the section )  # Visualize each section page.clear_highlights() for section in book_sections:     section.show() page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Display section count and preview the first section\n{\n    \"total_sections\": len(book_sections),\n    \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\"\n}\n</pre> # Display section count and preview the first section {     \"total_sections\": len(book_sections),     \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\" } Out[5]: <pre>{'total_sections': 9,\n 'first_section_text': '6/12/2023 - Copies Removed: 2\\nTristan Strong punches a hole in the sky (Removed: 1)\\nAuthor: Mbalia, ...'}</pre> In\u00a0[6]: Copied! <pre># Extract and display content from the first few book entries\nbook_entries = []\n\nfor i, section in enumerate(book_sections[:5]):\n    # Extract the section text\n    text = section.extract_text().strip()\n    \n    # Try to parse book information\n    title = \"\"\n    author = \"\"\n    isbn = \"\"\n    \n    # Extract title (typically the first line)\n    title_match = section.find('text:contains(\"Title:\")')\n    if title_match:\n        title_value = title_match.right(width=400).extract_text()\n        title = title_value.strip()\n    \n    # Extract author\n    author_match = section.find('text:contains(\"Author:\")')\n    if author_match:\n        author_value = author_match.right(width=400).extract_text()\n        author = author_value.strip()\n    \n    # Extract ISBN\n    isbn_match = section.find('text:contains(\"ISBN:\")')\n    if isbn_match:\n        isbn_value = isbn_match.right(width=400).extract_text()\n        isbn = isbn_value.strip()\n    \n    # Add to our collection\n    book_entries.append({\n        \"number\": i + 1,\n        \"title\": title,\n        \"author\": author,\n        \"isbn\": isbn,\n        \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text\n    })\n\n# Display the structured book entries\nimport pandas as pd\npd.DataFrame(book_entries)\n</pre> # Extract and display content from the first few book entries book_entries = []  for i, section in enumerate(book_sections[:5]):     # Extract the section text     text = section.extract_text().strip()          # Try to parse book information     title = \"\"     author = \"\"     isbn = \"\"          # Extract title (typically the first line)     title_match = section.find('text:contains(\"Title:\")')     if title_match:         title_value = title_match.right(width=400).extract_text()         title = title_value.strip()          # Extract author     author_match = section.find('text:contains(\"Author:\")')     if author_match:         author_value = author_match.right(width=400).extract_text()         author = author_value.strip()          # Extract ISBN     isbn_match = section.find('text:contains(\"ISBN:\")')     if isbn_match:         isbn_value = isbn_match.right(width=400).extract_text()         isbn = isbn_value.strip()          # Add to our collection     book_entries.append({         \"number\": i + 1,         \"title\": title,         \"author\": author,         \"isbn\": isbn,         \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text     })  # Display the structured book entries import pandas as pd pd.DataFrame(book_entries) Out[6]: number title author isbn preview 0 1 Log Atlanta Public S\\n023\\nemoved: 2\\na hole i... Atlanta Public Schools\\nPublished: 2019\\nAcqui... 6/12/2023 - Copies Removed: 2\\nTristan Strong ... 1 2 6/7/2023 - Copies Removed: 2 2 3 Atlanta Public School\\nved: 2\\nin the sky (Rem... Atlanta Public Schools\\n93-2 Published: 2019\\n... Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Ba... 3 4 Atlanta Public Schools\\nd: 2\\nn the sky (Remov... Atlanta Public Schools\\nPublished: 2019\\nAcqui... Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book ... 4 5 6/6/2023 - Copies Removed: 130 In\u00a0[7]: Copied! <pre>page.viewer()\n</pre> page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[7]: <pre>InteractiveViewerWidget()</pre> In\u00a0[8]: Copied! <pre># Find title elements with specific selectors\ntitle_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\ntitle_elements.show()\n</pre> # Find title elements with specific selectors title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]') title_elements.show() Out[8]: In\u00a0[9]: Copied! <pre># Extract sections starting from each title\n# This now directly returns an ElementCollection\ntitle_sections = page.get_sections(\n    start_elements=title_elements,\n    boundary_inclusion='start'\n)\n\n# Show the title-based sections\npage.clear_highlights()\ntitle_sections.show()\npage.to_image()\n</pre> # Extract sections starting from each title # This now directly returns an ElementCollection title_sections = page.get_sections(     start_elements=title_elements,     boundary_inclusion='start' )  # Show the title-based sections page.clear_highlights() title_sections.show() page.to_image() Out[9]: In\u00a0[10]: Copied! <pre># Count the sections found\nlen(title_sections)\n</pre> # Count the sections found len(title_sections) Out[10]: <pre>7</pre> In\u00a0[11]: Copied! <pre># Use horizontal line elements as section dividers\ndividers = page.find_all('line:horizontal')\n\n# Compare the different boundary inclusion options\ninclusion_options = {\n    'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),\n    'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),\n    'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),\n    'both': page.get_sections(start_elements=dividers, boundary_inclusion='both')\n}\n\n# Count sections with each option\nsection_counts = {option: len(sections) for option, sections in inclusion_options.items()}\nsection_counts\n</pre> # Use horizontal line elements as section dividers dividers = page.find_all('line:horizontal')  # Compare the different boundary inclusion options inclusion_options = {     'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),     'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),     'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),     'both': page.get_sections(start_elements=dividers, boundary_inclusion='both') }  # Count sections with each option section_counts = {option: len(sections) for option, sections in inclusion_options.items()} section_counts Out[11]: <pre>{'none': 9, 'start': 9, 'end': 9, 'both': 9}</pre> In\u00a0[12]: Copied! <pre># Define specific start and end points - let's extract just one book entry\n# We'll look for the first and second horizontal lines\npage.clear_highlights()\n\nstart_point = title_elements[0]\nend_point = title_elements[1]\n\n# Extract the section between these points\nsingle_book_entry = page.get_sections(\n    start_elements=[start_point],\n    end_elements=[end_point],\n    boundary_inclusion='start'  # Include the start but not the end\n)\n    \n# Visualize the custom section\nsingle_book_entry.show(color=\"green\", label=\"Single Book Entry\")\n    \nprint(single_book_entry[0].extract_text())\n\npage.to_image()\n</pre> # Define specific start and end points - let's extract just one book entry # We'll look for the first and second horizontal lines page.clear_highlights()  start_point = title_elements[0] end_point = title_elements[1]  # Extract the section between these points single_book_entry = page.get_sections(     start_elements=[start_point],     end_elements=[end_point],     boundary_inclusion='start'  # Include the start but not the end )      # Visualize the custom section single_book_entry.show(color=\"green\", label=\"Single Book Entry\")      print(single_book_entry[0].extract_text())  page.to_image() <pre>Tristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-2 Published: 2019\nSite Barcode Price Acquired Removed By\nJoseph Humphries 32441014018707 6/11/2021 113396-42441\nElementary School\nWas Available -- Weeded\nUpside down in the middle of nowhere (Removed: 1)\n</pre> Out[12]: In\u00a0[13]: Copied! <pre># Get sections across the first two pages\nmulti_page_sections = [] # Initialize as a list\n\nfor page_num in range(min(2, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page (returns ElementCollection)\n    page_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Add elements from the collection to our list\n    multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection\n\n# Display info about each section (showing first 3)\n[{\n    \"page\": section.page.number + 1,  # 1-indexed page number for display\n    \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text()\n} for section in multi_page_sections]\n</pre> # Get sections across the first two pages multi_page_sections = [] # Initialize as a list  for page_num in range(min(2, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page (returns ElementCollection)     page_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Add elements from the collection to our list     multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection  # Display info about each section (showing first 3) [{     \"page\": section.page.number + 1,  # 1-indexed page number for display     \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text() } for section in multi_page_sections] Out[13]: <pre>[{'page': 2, 'text': 'Tristan Strong punches a hole in the sky (Removed:...'},\n {'page': 2, 'text': 'Upside down in the middle of nowhere (Removed: 1)\\n...'},\n {'page': 2, 'text': 'Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Bazaz. ...'},\n {'page': 2, 'text': 'Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book edito...'},\n {'page': 2, 'text': 'The Abenaki (Removed: 1)\\nAuthor: Landau, Elaine. I...'},\n {'page': 2, 'text': 'Afghanistan (Removed: 1)\\nAuthor: Milivojevic, Jova...'},\n {'page': 2, 'text': 'Alexander the Great rocks the world (Removed: 1)\\nA...'},\n {'page': 3, 'text': 'The Anasazi (Removed: 1)\\nAuthor: Petersen, David. ...'},\n {'page': 3, 'text': 'And then what happened, Paul Revere? (Removed: 1)\\n...'},\n {'page': 3, 'text': 'The assassination of Martin Luther King Jr (Remove...'},\n {'page': 3, 'text': 'Barbara Jordan. (Removed: 1)\\nAuthor: Wexler, Diane...'},\n {'page': 3, 'text': 'Bedtime for Batman (Removed: 1)\\nAuthor: Dahl, Mich...'},\n {'page': 3, 'text': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskeg...'},\n {'page': 3, 'text': 'Bigfoot Wallace (Removed: 1)\\nAuthor: Harper,Jo. IS...'},\n {'page': 3, 'text': 'The blaze engulfs : January 1939 to December 1941 ...'}]</pre> In\u00a0[14]: Copied! <pre># Extract all book entries across multiple pages\nbook_database = []\n\n# Process first 3 pages (or fewer if the document is shorter)\nfor page_num in range(min(3, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page\n    book_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Process each book section\n    for section in book_sections:\n        # Skip sections that are too short (might be headers/footers)\n        if len(section.extract_text()) &lt; 50:\n            continue\n            \n        # Extract book information\n        book_info = {\"page\": page_num + 1}\n        \n        for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.strip(':').lower()\n                field_value = field_element.extract_text().replace(field, '').strip()\n                book_info[field_name] = field_value\n\n        # Below the field name\n        for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.lower()\n                field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()\n                book_info[field_name] = field_value\n\n        book_database.append(book_info)\n\n# Display sample entries (first 3)\nimport pandas as pd\n\ndf = pd.json_normalize(book_database)\ndf.head()\n</pre> # Extract all book entries across multiple pages book_database = []  # Process first 3 pages (or fewer if the document is shorter) for page_num in range(min(3, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page     book_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Process each book section     for section in book_sections:         # Skip sections that are too short (might be headers/footers)         if len(section.extract_text()) &lt; 50:             continue                      # Extract book information         book_info = {\"page\": page_num + 1}                  for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.strip(':').lower()                 field_value = field_element.extract_text().replace(field, '').strip()                 book_info[field_name] = field_value          # Below the field name         for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.lower()                 field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()                 book_info[field_name] = field_value          book_database.append(book_info)  # Display sample entries (first 3) import pandas as pd  df = pd.json_normalize(book_database) df.head() Out[14]: page author isbn price acquired barcode removed by 0 1 Mbalia, Kwame. 978-1-36803993-2 6/11/2021 11 32441014018707 113396-42441 1 1 Lamana, Julie T. 978-1-45212456-8 (alk. $15.00 6/12/2023 11 32441012580849 113396-42441 2 1 Wangu, Madhu Bazaz. 0-8160-2442-1 $10.00 4/19/2018 ch 33343000017835 christen.mcclain 3 1 Kelly Wand, book editor. 0-7377-1314-3 (lib.) $19.95 3/21/2006 ch *3431000028742 christen.mcclain 4 1 Landau, Elaine. 0-531-20227-5 $16.50 2/21/2000 33 33170000506628 33554-43170 <p>Section extraction lets you break down documents into logical parts, making it easier to generate summaries, extract specific content, and create structured data from semi-structured documents. In this example, we've shown how to convert a PDF library catalog into a structured book database.</p>"},{"location":"tutorials/09-section-extraction/#section-extraction","title":"Section Extraction\u00b6","text":"<p>Documents are often organized into logical sections like chapters, articles, or content blocks. This tutorial shows how to extract these sections using natural-pdf, using a library weeding log as an example.</p>"},{"location":"tutorials/09-section-extraction/#basic-section-extraction","title":"Basic Section Extraction\u00b6","text":""},{"location":"tutorials/09-section-extraction/#working-with-section-content","title":"Working with Section Content\u00b6","text":""},{"location":"tutorials/09-section-extraction/#using-different-section-boundaries","title":"Using Different Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#section-boundary-inclusion-options","title":"Section Boundary Inclusion Options\u00b6","text":""},{"location":"tutorials/09-section-extraction/#custom-section-boundaries","title":"Custom Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#multi-page-sections","title":"Multi-page Sections\u00b6","text":""},{"location":"tutorials/09-section-extraction/#building-a-book-database","title":"Building a Book Database\u00b6","text":""},{"location":"tutorials/09-section-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Demonstrate using <code>page.init_search()</code> to pre-index all pages and retrieve section headings quickly.</li> <li>Add an example that merges multi-page sections by passing <code>new_section_on_page_break=False</code>.</li> <li>Include tips for detecting numbered headings (\"1.\", \"2.\") when ruling lines are absent.</li> <li>Provide a performance note on large PDFs and how to stream through pages lazily.</li> </ul>"},{"location":"tutorials/10-form-field-extraction/","title":"Form Field Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"\n</pre> #%pip install \"natural-pdf[ai]\" <p>If you already have the core library, simply run <code>npdf install ai</code> to add the extra ML packages.</p> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find fields with labels ending in colon\nlabels = page.find_all('text:contains(\":\")')\n\n# Visualize the found labels\nlabels.show(color=\"blue\", label=\"Field Labels\")\n\n# Count how many potential fields we found\nlen(labels)\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find fields with labels ending in colon labels = page.find_all('text:contains(\":\")')  # Visualize the found labels labels.show(color=\"blue\", label=\"Field Labels\")  # Count how many potential fields we found len(labels) Out[2]: <pre>4</pre> In\u00a0[3]: Copied! <pre># Extract the value for each field label\nform_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    field_name = label.text.strip().rstrip(':')\n    \n    # Find the value to the right of the label\n    value_region = label.right(width=200)\n    value = value_region.extract_text().strip()\n    \n    # Store in our dictionary\n    form_data[field_name] = value\n\n# Display the extracted data\nform_data\n</pre> # Extract the value for each field label form_data = {}  for label in labels:     # Clean up the label text     field_name = label.text.strip().rstrip(':')          # Find the value to the right of the label     value_region = label.right(width=200)     value = value_region.extract_text().strip()          # Store in our dictionary     form_data[field_name] = value  # Display the extracted data form_data Out[3]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> In\u00a0[4]: Copied! <pre># Clear previous highlights\npage.clear_highlights()\n\n# Highlight both labels and their values\nfor label in labels:\n    # Highlight the label in red\n    label.show(color=\"red\", label=\"Label\")\n    \n    # Highlight the value area in blue\n    label.right(width=200).show(color=\"blue\", label=\"Value\")\n\n# Show the page image with highlighted elements\npage.to_image()\n</pre> # Clear previous highlights page.clear_highlights()  # Highlight both labels and their values for label in labels:     # Highlight the label in red     label.show(color=\"red\", label=\"Label\")          # Highlight the value area in blue     label.right(width=200).show(color=\"blue\", label=\"Value\")  # Show the page image with highlighted elements page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Extract values that might span multiple lines\nmulti_line_data = {}\n\nfor label in labels:\n    # Get the field name\n    field_name = label.text.strip().rstrip(':')\n    \n    # Look both to the right and below\n    right_value = label.right(width=200).extract_text().strip()\n    below_value = label.below(height=50).extract_text().strip()\n    \n    # Combine the values if they're different\n    if right_value in below_value:\n        value = below_value\n    else:\n        value = f\"{right_value} {below_value}\".strip()\n    \n    # Add to results\n    multi_line_data[field_name] = value\n\n# Show fields with potential multi-line values\nmulti_line_data\n</pre> # Extract values that might span multiple lines multi_line_data = {}  for label in labels:     # Get the field name     field_name = label.text.strip().rstrip(':')          # Look both to the right and below     right_value = label.right(width=200).extract_text().strip()     below_value = label.below(height=50).extract_text().strip()          # Combine the values if they're different     if right_value in below_value:         value = below_value     else:         value = f\"{right_value} {below_value}\".strip()          # Add to results     multi_line_data[field_name] = value  # Show fields with potential multi-line values multi_line_data Out[5]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt Date: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health Violation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'}</pre> In\u00a0[6]: Copied! <pre>import re\n\n# Find dates in the format July 31, YYY\ndate_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'\n\n# Search all text elements for dates\ntext_elements = page.find_all('text')\nprint([elem.text for elem in text_elements])\ndates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))\n\n# Visualize the date fields\ndates.show(color=\"green\", label=\"Date\")\n\n# Extract just the date values\ndate_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates]\ndate_texts\n</pre> import re  # Find dates in the format July 31, YYY date_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'  # Search all text elements for dates text_elements = page.find_all('text') print([elem.text for elem in text_elements]) dates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))  # Visualize the date fields dates.show(color=\"green\", label=\"Date\")  # Extract just the date values date_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates] date_texts <pre>['Jungle Health and Safety Inspection Service', 'INS-UP70N51NCL41R', 'Site: ', 'Durham\u2019s Meatpacking  ', 'Chicago, Ill.', 'Date:  ', 'February 3, 1905', 'Violation Count: ', '7', 'Summary: ', 'Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.', 'These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary ', 'visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in ', 'some of which there were open vats near the level of the floor, their peculiar trouble was that they fell', 'into the vats; and when they were fished out, there was never enough of them left to be worth ', 'exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out', 'to the world as Durham\u2019s Pure Leaf Lard!', 'Violations', 'Statute', 'Description', 'Level', 'Repeat?', '4.12.7', 'Unsanitary Working Conditions.', 'Critical', '5.8.3', 'Inadequate Protective Equipment.', 'Serious', '6.3.9', 'Ineffective Injury Prevention.', 'Serious', '7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', '8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', '9.6.4', 'Inadequate Ventilation Systems.', 'Serious', '10.2.7', 'Insufficient Employee Training for Safe Work Practices.', 'Serious', 'Jungle Health and Safety Inspection Service']\n</pre> Out[6]: <pre>['February 3, 1905']</pre> In\u00a0[7]: Copied! <pre># Run layout analysis to find table structures\npage.analyze_layout()\n\n# Find possible form tables\ntables = page.find_all('region[type=table]')\n\nif tables:\n    # Visualize the tables\n    tables.show(color=\"purple\", label=\"Form Table\")\n    \n    # Extract data from the first table\n    first_table = tables[0]\n    table_data = first_table.extract_table()\n    table_data\nelse:\n    # Try to find form-like structure using text alignment\n    # Create a region where a form might be\n    form_region = page.create_region(50, 200, page.width - 50, 500)\n    \n    # Group text by vertical position\n    rows = {}\n    text_elements = form_region.find_all('text')\n    \n    for elem in text_elements:\n        # Round y-position to group elements in the same row\n        row_pos = round(elem.top / 5) * 5\n        if row_pos not in rows:\n            rows[row_pos] = []\n        rows[row_pos].append(elem)\n    \n    # Extract data from rows (first 5 rows)\n    row_data = []\n    for y in sorted(rows.keys())[:5]:\n        # Sort elements by x-position (left to right)\n        elements = sorted(rows[y], key=lambda e: e.x0)\n        \n        # Show the row\n        row_box = form_region.create_region(\n            min(e.x0 for e in elements), \n            min(e.top for e in elements),\n            max(e.x1 for e in elements),\n            max(e.bottom for e in elements)\n        )\n        row_box.show(color=None, use_color_cycling=True)\n        \n        # Extract text from row\n        row_text = [e.text for e in elements]\n        row_data.append(row_text)\n    \n    # Show the extracted rows\n    row_data\n</pre> # Run layout analysis to find table structures page.analyze_layout()  # Find possible form tables tables = page.find_all('region[type=table]')  if tables:     # Visualize the tables     tables.show(color=\"purple\", label=\"Form Table\")          # Extract data from the first table     first_table = tables[0]     table_data = first_table.extract_table()     table_data else:     # Try to find form-like structure using text alignment     # Create a region where a form might be     form_region = page.create_region(50, 200, page.width - 50, 500)          # Group text by vertical position     rows = {}     text_elements = form_region.find_all('text')          for elem in text_elements:         # Round y-position to group elements in the same row         row_pos = round(elem.top / 5) * 5         if row_pos not in rows:             rows[row_pos] = []         rows[row_pos].append(elem)          # Extract data from rows (first 5 rows)     row_data = []     for y in sorted(rows.keys())[:5]:         # Sort elements by x-position (left to right)         elements = sorted(rows[y], key=lambda e: e.x0)                  # Show the row         row_box = form_region.create_region(             min(e.x0 for e in elements),              min(e.top for e in elements),             max(e.x1 for e in elements),             max(e.bottom for e in elements)         )         row_box.show(color=None, use_color_cycling=True)                  # Extract text from row         row_text = [e.text for e in elements]         row_data.append(row_text)          # Show the extracted rows     row_data <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpibivw_3_/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1573.9ms\n</pre> <pre>Speed: 6.3ms preprocess, 1573.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[8]: Copied! <pre># Combine label-based and pattern-based extraction\nall_fields = {}\n\n# 1. First get fields with explicit labels\nfor label in labels:\n    field_name = label.text.strip().rstrip(':')\n    value = label.right(width=200).extract_text().strip()\n    all_fields[field_name] = value\n\n# 2. Add date fields that we found with pattern matching\nfor date_elem in dates:\n    # Find the nearest label\n    nearby_label = date_elem.nearest('text:contains(\":\")')\n    \n    if nearby_label:\n        # Extract the label text\n        label_text = nearby_label.text.strip().rstrip(':')\n        \n        # Get the date value\n        date_value = re.search(date_pattern, date_elem.text).group(0)\n        \n        # Add to our results if not already present\n        if label_text not in all_fields:\n            all_fields[label_text] = date_value\n\n# Show all extracted fields\nall_fields\n</pre> # Combine label-based and pattern-based extraction all_fields = {}  # 1. First get fields with explicit labels for label in labels:     field_name = label.text.strip().rstrip(':')     value = label.right(width=200).extract_text().strip()     all_fields[field_name] = value  # 2. Add date fields that we found with pattern matching for date_elem in dates:     # Find the nearest label     nearby_label = date_elem.nearest('text:contains(\":\")')          if nearby_label:         # Extract the label text         label_text = nearby_label.text.strip().rstrip(':')                  # Get the date value         date_value = re.search(date_pattern, date_elem.text).group(0)                  # Add to our results if not already present         if label_text not in all_fields:             all_fields[label_text] = date_value  # Show all extracted fields all_fields Out[8]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> In\u00a0[9]: Copied! <pre>answer = page.ask(\"What is the invoice total?\")\n</pre> answer = page.ask(\"What is the invoice total?\") <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n</pre> <p><code>answer['answer']</code> is the literal text found on the page.</p> <p>For a deep dive into Question Answering\u2014including confidence tuning, batching, and answer-span highlighting\u2014see Tutorial 06: Document Question Answering.</p> <p>Form field extraction enables you to automate data entry and document processing. By combining different techniques like label detection, spatial navigation, and pattern matching, you can handle a wide variety of form layouts.</p>"},{"location":"tutorials/10-form-field-extraction/#form-field-extraction","title":"Form Field Extraction\u00b6","text":"<p>Extracting key-value pairs from documents can be tackled in two complementary ways:</p> <ul> <li>Rule-based / spatial heuristics \u2013 look for label text, navigate rightward or downward, group elements into rows, etc.</li> <li>Extractive Document QA \u2013 feed the page image and its words to a fine-tuned LayoutLM model and ask natural-language questions such as \"What is the invoice total?\". The model returns the answer span exactly as it appears in the document along with a confidence score.</li> </ul> <p>This tutorial starts with classical heuristics and then upgrades to the LayoutLM-based DocumentQA engine built into <code>natural-pdf</code>. Because DocumentQA relies on <code>torch</code>, <code>transformers</code>, and <code>vision</code> extras, install the AI optional dependencies first:</p>"},{"location":"tutorials/10-form-field-extraction/#extracting-field-values","title":"Extracting Field Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#visualizing-labels-and-values","title":"Visualizing Labels and Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#handling-multi-line-values","title":"Handling Multi-line Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#finding-pattern-based-fields","title":"Finding Pattern-Based Fields\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#working-with-form-tables","title":"Working with Form Tables\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#combining-different-extraction-techniques","title":"Combining Different Extraction Techniques\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#asking-questions-with-layoutlm-documentqa","title":"Asking Questions with LayoutLM (DocumentQA)\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#optional-one-liner-qa","title":"Optional one-liner QA\u00b6","text":"<p>Need a single field but can't locate the right label?  You can fall back to <code>page.ask()</code> which runs the LayoutLM extractive QA model:</p>"},{"location":"tutorials/10-form-field-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Showcase the new <code>init_search</code> workflow for quickly locating form labels across multi-page documents.</li> <li>Compare heuristics for multi-col forms (e.g., left/right alignment vs. table structures) and when to switch strategies.</li> <li>Demonstrate embedding page classification (e.g., \"invoice\" vs \"purchase order\") before field extraction to route documents to the correct template.</li> <li>Provide an end-to-end example saving the extracted dictionary to JSON and a searchable PDF via <code>pdf.save_searchable()</code>.</li> <li>Add a sidebar contrasting extractive QA with generative LLM approaches and notes on when to choose each.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/","title":"Enhanced Table Processing","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Optional fine-tuning for pdfplumber.  Typical tweaks are vertical/horizontal strategies.\nsettings = {\n    \"vertical_strategy\": \"lines\",\n    \"horizontal_strategy\": \"lines\",\n    \"intersection_tolerance\": 3,\n}\n\nrows = page.extract_table(method=\"pdfplumber\", table_settings=settings)\nrows  # \u25b6\ufe0e returns a list of lists\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Optional fine-tuning for pdfplumber.  Typical tweaks are vertical/horizontal strategies. settings = {     \"vertical_strategy\": \"lines\",     \"horizontal_strategy\": \"lines\",     \"intersection_tolerance\": 3, }  rows = page.extract_table(method=\"pdfplumber\", table_settings=settings) rows  # \u25b6\ufe0e returns a list of lists Out[1]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>Expected output: a small list of rows containing the text exactly as it appears in the digital table.</p> In\u00a0[2]: Copied! <pre>settings_text = {\n    \"vertical_strategy\": \"text\",   # look for whitespace gutters\n    \"horizontal_strategy\": \"text\", # group into rows by vertical gaps\n    \"text_x_tolerance\": 2,          # tune for narrow columns\n    \"text_y_tolerance\": 2,\n}\n\nrows_text = page.extract_table(method=\"pdfplumber\", table_settings=settings_text)\n</pre> settings_text = {     \"vertical_strategy\": \"text\",   # look for whitespace gutters     \"horizontal_strategy\": \"text\", # group into rows by vertical gaps     \"text_x_tolerance\": 2,          # tune for narrow columns     \"text_y_tolerance\": 2, }  rows_text = page.extract_table(method=\"pdfplumber\", table_settings=settings_text) <p>Compare <code>rows_text</code> with the earlier <code>rows</code> list\u2014if your PDF omits the grid, the whitespace strategy will usually outperform line-based detection.</p> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# If the page is scanned, run OCR first so each cell has text\npage.apply_ocr(engine=\"easyocr\", languages=[\"en\"], resolution=200)\n\n# Table Transformer needs the layout model; specify device if you have GPU\nrows = page.extract_table(method=\"tatr\")\nrows\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # If the page is scanned, run OCR first so each cell has text page.apply_ocr(engine=\"easyocr\", languages=[\"en\"], resolution=200)  # Table Transformer needs the layout model; specify device if you have GPU rows = page.extract_table(method=\"tatr\") rows <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> Out[3]: <pre>[]</pre> <p>Expected output: the table rows\u2014even when the grid is just implied\u2014arrive with text already OCR-corrected.</p> In\u00a0[4]: Copied! <pre># from natural_pdf import PDF\n\n# pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/whitespace-table.pdf\")\n# page = pdf.pages[0]\n\n# rows = page.extract_table(method=\"text\", table_settings={\"min_words_horizontal\": 2})\n# for row in rows:\n#     print(row)\n</pre> # from natural_pdf import PDF  # pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/whitespace-table.pdf\") # page = pdf.pages[0]  # rows = page.extract_table(method=\"text\", table_settings={\"min_words_horizontal\": 2}) # for row in rows: #     print(row) <p>Expected output: printed rows that roughly match the visual columns; best effort on ragged layouts.</p> In\u00a0[5]: Copied! <pre>page.detect_lines(resolution=200, source_label=\"detected\", horizontal=True, vertical=True)\n\n# (Optional) visual check\npage.find_all(\"line[source=detected]\").show(group_by=\"orientation\")\n\n# Convert lines \u2192 regions\npage.detect_table_structure_from_lines(source_label=\"detected\", cell_padding=0.5)\n\ntable = page.find(\"region[type='table']\")\n</pre> page.detect_lines(resolution=200, source_label=\"detected\", horizontal=True, vertical=True)  # (Optional) visual check page.find_all(\"line[source=detected]\").show(group_by=\"orientation\")  # Convert lines \u2192 regions page.detect_table_structure_from_lines(source_label=\"detected\", cell_padding=0.5)  table = page.find(\"region[type='table']\")"},{"location":"tutorials/11-enhanced-table-processing/#enhanced-table-processing","title":"Enhanced Table Processing\u00b6","text":"<p>Tables can appear in PDFs in wildly different ways\u2014cleanly tagged in the PDF structure, drawn with ruling lines, or simply implied by visual spacing.  <code>natural-pdf</code> exposes several back-ends under the single method <code>extract_table()</code> so you can choose the strategy that matches your document.</p> <p>Below we walk through the three main options, when to reach for each one, and sample code you can adapt (replace the example PDF URLs with your own files).</p>"},{"location":"tutorials/11-enhanced-table-processing/#1-methodpdfplumber-default","title":"1. <code>method=\"pdfplumber\"</code>  (default)\u00b6","text":"<ul> <li>How it works \u2013 delegates to pdfplumber's ruling-line heuristics; looks for vertical/horizontal lines and whitespace gutters.</li> <li>Best for \u2013 digitally-born PDFs where the table grid is drawn or where columns have consistent whitespace.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example-a-grid-based-line-detection","title":"Example A \u2013 Grid-based (line) detection\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#example-b-whitespace-driven-detection","title":"Example B \u2013 Whitespace-driven detection\u00b6","text":"<p>Sometimes a table is drawn without ruling lines (or the PDF stores them as thick rectangles so the line detector ignores them).  In that case you can switch both strategies to <code>\"text\"</code> so pdfplumber clusters by the gaps between words rather than relying on graphics commands:</p>"},{"location":"tutorials/11-enhanced-table-processing/#2-methodtatr-table-transformer","title":"2. <code>method=\"tatr\"</code>  (Table Transformer)\u00b6","text":"<ul> <li>How it works \u2013 runs Microsoft's Table Transformer (LayoutLM-based) to detect tables, rows and cells visually, then reads the text inside each cell.</li> <li>Best for \u2013 scanned or camera-based documents, or born-digital files where ruling lines are missing/irregular.</li> <li>Dependencies \u2013 requires the AI extra (<code>pip install \"natural-pdf[ai]\"</code>) because it needs <code>torch</code>, <code>transformers</code>, and <code>torchvision</code>.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example","title":"Example\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#3-methodtext-whitespace-heuristic","title":"3. <code>method=\"text\"</code>  (Whitespace heuristic)\u00b6","text":"<ul> <li>How it works \u2013 groups words into lines, then uses whitespace clustering (Jenks breaks) to infer columns; no layout model.</li> <li>Best for \u2013 simple, left-aligned tables with consistent columns but no ruling lines; fastest option.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example","title":"Example\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#4-lines-first-workflow-when-pdfplumber-misses-rowscols","title":"4. Lines-first workflow (when pdfplumber misses rows/cols)\u00b6","text":"<p>If <code>method=\"pdfplumber\"</code> cannot find the grid, detect lines explicitly and build the table structure yourself.</p>"},{"location":"tutorials/11-enhanced-table-processing/#todo","title":"TODO\u00b6","text":"<ul> <li>Provide a benchmark matrix of speed vs. accuracy for the three methods.</li> <li>Add a snippet showing how to export cell regions directly to a pandas DataFrame.</li> <li>Document edge-cases: rotated tables, merged cells, or header repetition across pages.</li> <li>Include guidance on mixing methods\u2014e.g., run <code>detect_lines</code> first, fall back to <code>text</code> for cells lacking grid.</li> </ul>"},{"location":"tutorials/12-ocr-integration/","title":"OCR Integration for Scanned Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Try extracting text without OCR\ntext_without_ocr = page.extract_text()\nf\"Without OCR: {len(text_without_ocr)} characters extracted\"\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # Try extracting text without OCR text_without_ocr = page.extract_text() f\"Without OCR: {len(text_without_ocr)} characters extracted\" Out[2]: <pre>'Without OCR: 0 characters extracted'</pre> In\u00a0[3]: Copied! <pre># Apply OCR using the default engine (EasyOCR) for English\npage.apply_ocr(languages=['en'])\n\n# Select all text pieces found by OCR\ntext_elements = page.find_all('text[source=ocr]')\nprint(f\"Found {len(text_elements)} text elements using default OCR\")\n\n# Visualize the elements\ntext_elements.show()\n</pre> # Apply OCR using the default engine (EasyOCR) for English page.apply_ocr(languages=['en'])  # Select all text pieces found by OCR text_elements = page.find_all('text[source=ocr]') print(f\"Found {len(text_elements)} text elements using default OCR\")  # Visualize the elements text_elements.show() <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> <pre>Found 44 text elements using default OCR\n</pre> Out[3]: In\u00a0[4]: Copied! <pre># Apply OCR using PaddleOCR for English\npage.apply_ocr(engine='paddle', languages=['en'])\nprint(f\"Found {len(page.find_all('text[source=ocr]'))} elements after English OCR.\")\n\n# Apply OCR using PaddleOCR for Chinese\npage.apply_ocr(engine='paddle', languages=['ch'])\nprint(f\"Found {len(page.find_all('text[source=ocr]'))} elements after Chinese OCR.\")\n\ntext_with_ocr = page.extract_text()\nprint(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\")\n</pre> # Apply OCR using PaddleOCR for English page.apply_ocr(engine='paddle', languages=['en']) print(f\"Found {len(page.find_all('text[source=ocr]'))} elements after English OCR.\")  # Apply OCR using PaddleOCR for Chinese page.apply_ocr(engine='paddle', languages=['ch']) print(f\"Found {len(page.find_all('text[source=ocr]'))} elements after Chinese OCR.\")  text_with_ocr = page.extract_text() print(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\") <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Found 43 elements after English OCR.\n</pre> <pre>Found 43 elements after Chinese OCR.\n\nExtracted text after OCR:\nRed (RGB tuple)\nJungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham's Meatpacking Chicago, .\nDate:February 3.1905\nViolation Cou...\n</pre> <p>You can also use <code>.describe()</code> to see a summary of the OCR outcome...</p> In\u00a0[5]: Copied! <pre>page.describe()\n</pre> page.describe() Out[5]: <p>...or <code>.inspect()</code> on the text elements for individual details.</p> In\u00a0[6]: Copied! <pre>page.find_all('text').inspect()\n</pre> page.find_all('text').inspect() Out[6]: In\u00a0[7]: Copied! <pre>import natural_pdf as npdf\n\n# Set global OCR defaults\nnpdf.options.ocr.engine = 'surya'          # Default OCR engine\nnpdf.options.ocr.min_confidence = 0.7      # Default confidence threshold\n\n# Now all OCR calls use these defaults\npdf = npdf.PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npdf.pages[0].apply_ocr()  # Uses: engine='surya', languages=['en', 'es'], min_confidence=0.7\n\n# You can still override defaults for specific calls\npdf.pages[0].apply_ocr(engine='easyocr', languages=['fr'])  # Override engine and languages\n</pre> import natural_pdf as npdf  # Set global OCR defaults npdf.options.ocr.engine = 'surya'          # Default OCR engine npdf.options.ocr.min_confidence = 0.7      # Default confidence threshold  # Now all OCR calls use these defaults pdf = npdf.PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") pdf.pages[0].apply_ocr()  # Uses: engine='surya', languages=['en', 'es'], min_confidence=0.7  # You can still override defaults for specific calls pdf.pages[0].apply_ocr(engine='easyocr', languages=['fr'])  # Override engine and languages <pre>Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n</pre> <pre>Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n</pre> <pre>\rDetecting bboxes:   0%|                                                  | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.15it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.15it/s]</pre> <pre>\n</pre> <pre>\rRecognizing Text:   0%|                                                  | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:13&lt;00:00, 13.77s/it]</pre> <pre>\rRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:13&lt;00:00, 13.77s/it]</pre> <pre>\n</pre> <pre>[2025-06-18 17:40:38,350] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> Out[7]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>This is especially useful when processing many documents with the same OCR settings, as you don't need to specify the parameters repeatedly.</p> In\u00a0[8]: Copied! <pre>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\n\n# Re-apply OCR using EasyOCR with specific options\neasy_opts = EasyOCROptions(\n    paragraph=False,\n)\npage.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)\n\npaddle_opts = PaddleOCROptions()\npage.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)\n\nsurya_opts = SuryaOCROptions()\npage.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts)\n</pre> from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions  # Re-apply OCR using EasyOCR with specific options easy_opts = EasyOCROptions(     paragraph=False, ) page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)  paddle_opts = PaddleOCROptions() page.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)  surya_opts = SuryaOCROptions() page.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts) <pre>Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n</pre> <pre>Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n</pre> <pre>\rDetecting bboxes:   0%|                                                  | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.60it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.60it/s]</pre> <pre>\n</pre> Out[8]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[9]: Copied! <pre># Process all pages in the document\n\n# Apply OCR to all pages (example using EasyOCR)\npdf.apply_ocr(engine='easyocr', languages=['en'])\nprint(f\"Applied OCR to {len(pdf.pages)} pages.\")\n\n# Or apply layout analysis to all pages (example using Paddle)\n# pdf.apply_layout(engine='paddle')\n# print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")\n\n# Extract text from all pages (uses OCR results if available)\nall_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")\n\nprint(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\")\n</pre> # Process all pages in the document  # Apply OCR to all pages (example using EasyOCR) pdf.apply_ocr(engine='easyocr', languages=['en']) print(f\"Applied OCR to {len(pdf.pages)} pages.\")  # Or apply layout analysis to all pages (example using Paddle) # pdf.apply_layout(engine='paddle') # print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")  # Extract text from all pages (uses OCR results if available) all_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")  print(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\") <pre>[2025-06-18 17:41:16,862] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'page_separator'\n</pre> <pre>Applied OCR to 1 pages.\n\nCombined text from all pages:\nRed (RGB tuple )\nJungle Health and Safety Inspection Service\nDate: February 3, 1905\nViolation Count:\nSummary: Worst of any, however; were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor\nsome of which there were open vats near the level of the floor; their peculiar trouble was that they fell\nexhibitingsometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham's Pure Leaf Lardl\nViolat...\n</pre>"},{"location":"tutorials/12-ocr-integration/#ocr-integration-for-scanned-documents","title":"OCR Integration for Scanned Documents\u00b6","text":"<p>Optical Character Recognition (OCR) allows you to extract text from scanned documents where the text isn't embedded in the PDF. This tutorial demonstrates how to work with scanned documents.</p>"},{"location":"tutorials/12-ocr-integration/#applying-ocr-and-finding-elements","title":"Applying OCR and Finding Elements\u00b6","text":"<p>The core method is <code>page.apply_ocr()</code>. This runs the OCR process and adds <code>TextElement</code> objects to the page. You can specify the engine and languages.</p> <p>Note: Re-applying OCR to the same page or region will automatically remove any previously generated OCR elements for that area before adding the new ones.</p>"},{"location":"tutorials/12-ocr-integration/#page-1-summary","title":"Page 1 Summary\u00b6","text":"<p>Elements:</p> <ul> <li>text: 43 elements</li> </ul> <p>Text Analysis:</p> <ul> <li>typography:<ul> <li>fonts:<ul> <li>OCR: 43</li> </ul> </li> <li>sizes:<ul> <li>11.0pt: 13</li> <li>12.0pt: 10</li> <li>13.0pt: 7</li> <li>15.0pt: 6</li> <li>14.0pt: 4</li> <li>10.0pt: 1</li> <li>7.0pt: 1</li> <li>8.0pt: 1</li> </ul> </li> </ul> </li> <li>ocr quality:<ul> <li>confidence stats:<ul> <li>mean: 0.95</li> <li>min: 0.64</li> <li>max: 1.00</li> </ul> </li> <li>quality distribution:<ul> <li>99%+ (21/43) 49%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591</code></li> <li>95%+ (32/43) 74%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591</code></li> <li>90%+ (38/43) 88%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591</code></li> </ul> </li> <li>lowest scoring:<ul> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> <li></li> </ul> </li> </ul> </li> </ul>"},{"location":"tutorials/12-ocr-integration/#1-064","title":"1: 0.64: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#2-066","title":"2: 0.66: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#3-068","title":"3: 0.68: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#4-070","title":"4: 0.70: \u25a1\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#5-089-some-of-which-there-were-open-vats-near-the-level-of-thefloo","title":"5: 0.89: some of which there were open vats near the level of thefloo...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#6-094-insufficient-employee-training-for-safe-work-practices","title":"6: 0.94: Insufficient Employee Training for Safe Work Practices.\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#7-094-these-people-could-not-be-shown-to-the-visitor-for-the-odo","title":"7: 0.94: These people could not be shown to the visitor - for the odo...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#8-094-summary-worst-of-any-however-were-the-fertilizer-men-and","title":"8: 0.94: Summary: Worst of any, however, were the fertilizer men, and...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#9-094-visitor-at-a-hundred-yardsand-as-for-the-other-men-who-wor","title":"9: 0.94: visitor at a hundred yards,and as for the other men, who wor...\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#10-095-jungle-health-and-safety-inspection-service","title":"10: 0.95: Jungle Health and Safety Inspection Service\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#collection-inspection-43-elements","title":"Collection Inspection (43 elements)\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#word-elements","title":"Word Elements\u00b6","text":"text x0 top x1 bottom font_family size bold italic source confidence color Red (RGB tuple) 541 57 583 68 OCR 11 False False ocr 0.98 #000000 Jungle Health and Safety Inspection Service 329 83 467 94 OCR 11 False False ocr 0.95 #000000 INS-UP70N51NCL41R 328 92 402 101 OCR 10 False False ocr 1.00 #000000 Site: Durham's Meatpacking Chicago, . 41 125 201 136 OCR 11 False False ocr 0.95 #000000 Date:February 3.1905 40 141 134 152 OCR 11 False False ocr 0.95 #000000 Violation Count: 7 40 159 117 170 OCR 11 False False ocr 0.99 #000000 Summary: Worst of any, however, were the fertilize... 41 177 434 187 OCR 11 False False ocr 0.94 #000000 These people could not be shown to the visitor - f... 41 190 438 201 OCR 11 False False ocr 0.94 #000000 visitor at a hundred yards,and as for the other me... 40 204 420 215 OCR 11 False False ocr 0.94 #000000 some of which there were open vats near the level ... 40 218 427 229 OCR 11 False False ocr 0.89 #000000 into the yats: and when they were fished out. ther... 43 234 397 241 OCR 7 False False ocr 0.95 #000000 exhibiting - sometimes they would be overlooked fo... 43 248 423 255 OCR 8 False False ocr 0.96 #000000 to the world as Durham's Pure Leaf Lard! 41 260 200 270 OCR 11 False False ocr 0.97 #000000 Violations 40 372 91 384 OCR 12 False False ocr 1.00 #000000 Statute 44 393 80 408 OCR 15 False False ocr 1.00 #000000 Repeat? 430 393 471 408 OCR 15 False False ocr 1.00 #000000 Description 87 394 138 406 OCR 12 False False ocr 1.00 #000000 Level 389 394 415 407 OCR 13 False False ocr 1.00 #000000 4.12.7 45 411 74 425 OCR 14 False False ocr 1.00 #000000 Unsanitary Working Conditions. 88 411 211 425 OCR 14 False False ocr 0.99 #000000 Critical 389 412 419 424 OCR 12 False False ocr 1.00 #000000 5.8.3 45 428 68 441 OCR 13 False False ocr 1.00 #000000 Inadequate Protective Equipment. 88 428 221 442 OCR 14 False False ocr 0.96 #000000 Serious 389 429 421 441 OCR 12 False False ocr 1.00 #000000 \u25a1 442 444 458 460 OCR 15 False False ocr 0.70 #000000 6.3.9 45 446 68 459 OCR 13 False False ocr 1.00 #000000 Ineffective Injury Prevention. 88 446 200 460 OCR 14 False False ocr 0.97 #000000 Serious 389 447 421 459 OCR 12 False False ocr 1.00 #000000 \u25a1 442 461 458 477 OCR 15 False False ocr 0.68 #000000 7.1.5 44 463 68 475 OCR 12 False False ocr 1.00 #000000 <p>Showing 30 of 43 elements (pass limit= to see more)</p>"},{"location":"tutorials/12-ocr-integration/#setting-default-ocr-options","title":"Setting Default OCR Options\u00b6","text":"<p>You can set global default OCR options using <code>natural_pdf.options</code>. These defaults will be used automatically when you call <code>apply_ocr()</code> without specifying parameters.</p>"},{"location":"tutorials/12-ocr-integration/#advanced-ocr-configuration","title":"Advanced OCR Configuration\u00b6","text":"<p>For more control, import and use the specific <code>Options</code> class for your chosen engine within the <code>apply_ocr</code> call.</p>"},{"location":"tutorials/12-ocr-integration/#interactive-ocr-correction-debugging","title":"Interactive OCR Correction / Debugging\u00b6","text":"<p>If OCR results aren't perfect, you can use the bundled interactive web application (SPA) to review and correct them.</p> <ol> <li><p>Package the data: After running <code>apply_ocr</code> (or <code>apply_layout</code>), use <code>create_correction_task_package</code> to create a zip file containing the PDF images and detected elements.</p> <pre>from natural_pdf.utils.packaging import create_correction_task_package\n\npage.apply_ocr()\n\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</pre> </li> <li><p>Run the SPA: Navigate to the SPA directory within the installed <code>natural_pdf</code> library in your terminal and start a simple web server.</p> </li> <li><p>Use the SPA: Open <code>http://localhost:8000</code> in your browser. Drag the <code>correction_package.zip</code> file onto the page to load the document. You can then click on text elements to correct the OCR results.</p> </li> </ol>"},{"location":"tutorials/12-ocr-integration/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>Apply OCR or layout analysis to all pages using the <code>PDF</code> object.</p>"},{"location":"tutorials/12-ocr-integration/#saving-pdfs-with-searchable-text","title":"Saving PDFs with Searchable Text\u00b6","text":"<p>After applying OCR to a PDF, you can save a new version of the PDF where the recognized text is embedded as an invisible layer. This makes the text searchable and copyable in standard PDF viewers.</p> <p>Use the <code>save_searchable()</code> method on the <code>PDF</code></p>"},{"location":"tutorials/12-ocr-integration/#todo","title":"TODO\u00b6","text":"<ul> <li>Add guidance on installing only the OCR engines you need (e.g. <code>pip install \"natural-pdf[ai] easyocr\"</code>) instead of the heavy <code>[all]</code> extra.</li> <li>Show how to use <code>detect_only=True</code> to combine OCR detection with external recognition for higher accuracy (ties into fine-tuning tutorial).</li> <li>Include an example of saving a searchable PDF via <code>pdf.save_searchable(\"output.pdf\")</code> after OCR.</li> <li>Mention <code>resolution</code> parameter trade-offs (speed vs accuracy) when calling <code>apply_ocr</code>.</li> <li>Provide a quick snippet demonstrating <code>.viewer()</code> for interactive visual QC of OCR results.</li> </ul>"},{"location":"tutorials/13-semantic-search/","title":"Semantic Search Across Multiple Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[search]\"\n</pre> #%pip install \"natural-pdf[search]\" In\u00a0[2]: Copied! <pre>import natural_pdf\n\n# Define the paths to your PDF files\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n]\n\n# Or use glob patterns\n# collection = natural_pdf.PDFCollection(\"pdfs/*.pdf\")\n\n# Create a PDFCollection\ncollection = natural_pdf.PDFCollection(pdf_paths)\nprint(f\"Created collection with {len(collection.pdfs)} PDFs.\")\n</pre> import natural_pdf  # Define the paths to your PDF files pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\" ]  # Or use glob patterns # collection = natural_pdf.PDFCollection(\"pdfs/*.pdf\")  # Create a PDFCollection collection = natural_pdf.PDFCollection(pdf_paths) print(f\"Created collection with {len(collection.pdfs)} PDFs.\") <pre>Created collection with 2 PDFs.\n</pre> In\u00a0[3]: Copied! <pre># Initialize search.\n# index=True will build the serachable database immediately\n# persist=True will save it so you don't need to do it every time\ncollection.init_search(index=True)\nprint(\"Search index initialized.\")\n</pre> # Initialize search. # index=True will build the serachable database immediately # persist=True will save it so you don't need to do it every time collection.init_search(index=True) print(\"Search index initialized.\") <pre>/Users/soma/Development/natural-pdf/.nox/tutorials/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n</pre> <pre>Search index initialized.\n</pre> In\u00a0[4]: Copied! <pre># Perform a search query\nquery = \"american president\"\nresults = collection.find_relevant(query)\n\nprint(f\"Found {len(results)} results for '{query}':\")\n</pre> # Perform a search query query = \"american president\" results = collection.find_relevant(query)  print(f\"Found {len(results)} results for '{query}':\") <pre>Found 6 results for 'american president':\n</pre> In\u00a0[5]: Copied! <pre># Process and display the results\nif results:\n    for i, result in enumerate(results):\n        print(f\"  {i+1}. PDF: {result['pdf_path']}\")\n        print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")\n        # Display a snippet of the content\n        snippet = result.get('content_snippet', '')\n        print(f\"     Snippet: {snippet}...\") \nelse:\n    print(\"  No relevant results found.\")\n\n# You can access the full content if needed via the result object, \n# though 'content_snippet' is usually sufficient for display.\n</pre> # Process and display the results if results:     for i, result in enumerate(results):         print(f\"  {i+1}. PDF: {result['pdf_path']}\")         print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")         # Display a snippet of the content         snippet = result.get('content_snippet', '')         print(f\"     Snippet: {snippet}...\")  else:     print(\"  No relevant results found.\")  # You can access the full content if needed via the result object,  # though 'content_snippet' is usually sufficient for display. <pre>  1. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 2 (Score: -0.8584)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nThe Anasazi (Removed: 1)\nAuthor: Petersen, David. ISBN: 0-516-01121-9 (trade) Published: 1991\nSit...\n  2. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 5 (Score: -0.8661)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000562167 $13.10 11/5/1999 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  3. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\n     Page: 1 (Score: -1.0080)\n     Snippet: Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men...\n  4. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 4 (Score: -1.0489)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nChildren of the Philippines (Removed: 1)\nAuthor: Kinkade, Sheila, 1962- ISBN: 0-87614-993-X Publi...\n  5. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 3 (Score: -1.0890)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000507600 $19.45 2/21/2000 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  6. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 1 (Score: -1.0946)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/12/2023 - Copies Removed: 2\nTristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-...\n</pre> <p>Semantic search allows you to efficiently query large sets of documents to find the most relevant information without needing exact keyword matches, leveraging the meaning and context of your query.</p>"},{"location":"tutorials/13-semantic-search/#semantic-search-across-multiple-documents","title":"Semantic Search Across Multiple Documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to find information relevant to a specific query across all documents, not just within a single one. This tutorial demonstrates how to perform semantic search over a <code>PDFCollection</code>.</p> <p>You can do semantic search with the default install, but for increased performance with LanceDB I recommend installing the search extension.</p>"},{"location":"tutorials/13-semantic-search/#initializing-the-search-index","title":"Initializing the Search Index\u00b6","text":"<p>Before performing a search, you need to initialize the search capabilities for the collection. This involves processing the documents and building an index.</p>"},{"location":"tutorials/13-semantic-search/#performing-a-semantic-search","title":"Performing a Semantic Search\u00b6","text":"<p>Once the index is ready, you can use the <code>find_relevant()</code> method to search for content semantically related to your query.</p>"},{"location":"tutorials/13-semantic-search/#understanding-search-results","title":"Understanding Search Results\u00b6","text":"<p>The <code>find_relevant()</code> method returns a list of dictionaries, each representing a relevant text chunk found in one of the PDFs. Each result includes:</p> <ul> <li><code>pdf_path</code>: The path to the PDF document where the result was found.</li> <li><code>page_number</code>: The page number within the PDF.</li> <li><code>score</code>: A relevance score (higher means more relevant).</li> <li><code>content_snippet</code>: A snippet of the text chunk that matched the query.</li> </ul> <p>In the future we should be able to easily look at the PDF!</p>"},{"location":"tutorials/13-semantic-search/#todo","title":"TODO\u00b6","text":"<ul> <li>Add example for using <code>persist=True</code> and <code>collection_name</code> in <code>init_search</code> to create a persistent on-disk index.</li> <li>Show how to override the embedding model (e.g. <code>embedding_model=\"all-MiniLM-L12-v2\"</code>).</li> <li>Mention <code>top_k</code> and filtering options available through <code>SearchOptions</code> when calling <code>find_relevant</code>.</li> <li>Provide a short snippet on visualising matched pages/elements once highlighting support lands (future feature).</li> <li>Clarify that installing the AI stack (<code>natural-pdf[ai]</code>) also pulls in <code>sentence-transformers</code>, which is needed for in-memory NumPy fallback.</li> </ul>"},{"location":"tutorials/14-categorizing-documents/","title":"Categorizing documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"\n</pre> #%pip install \"natural-pdf[ai]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/cia-doc.pdf\")\npdf.pages.to_image(cols=6)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/cia-doc.pdf\") pdf.pages.to_image(cols=6) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[2]: In\u00a0[3]: Copied! <pre>pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='vision')\n\nfor page in pdf.pages:\n    print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\")\n</pre> pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='vision')  for page in pdf.pages:     print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\") <pre>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n</pre> <pre>Device set to use mps:0\n</pre> <pre>Page 1 is text - 0.633\nPage 2 is text - 0.957\nPage 3 is text - 0.921\nPage 4 is diagram - 0.895\nPage 5 is diagram - 0.891\nPage 6 is invoice - 0.919\nPage 7 is text - 0.834\nPage 8 is invoice - 0.594\nPage 9 is invoice - 0.971\nPage 10 is invoice - 0.987\nPage 11 is invoice - 0.994\nPage 12 is invoice - 0.992\nPage 13 is text - 0.822\nPage 14 is text - 0.936\nPage 15 is diagram - 0.913\nPage 16 is text - 0.617\nPage 17 is invoice - 0.868\n</pre> <p>How did it do?</p> In\u00a0[4]: Copied! <pre>(\n    pdf.pages\n    .filter(lambda page: page.category == 'diagram')\n    .to_image(show_category=True)\n)\n</pre> (     pdf.pages     .filter(lambda page: page.category == 'diagram')     .to_image(show_category=True) ) Out[4]: <p>Looks great! Note that I had to play around with the categories a bit before I got something that worked. Using \"blank\" doesn't ever show up, \"invoice\" did a lot better than \"form,\" etc etc. It's pretty quick and easy to sanity check so you shouldn't have to suffer too much.</p> <p>I can also save just those pages into a new PDF document.</p> In\u00a0[\u00a0]: skip-execution Copied! <pre>(\n    pdf.pages\n    .filter(lambda page: page.category == 'diagram')\n    .save_pdf(\"output.pdf\", original=True)\n)\n</pre> (     pdf.pages     .filter(lambda page: page.category == 'diagram')     .save_pdf(\"output.pdf\", original=True) ) In\u00a0[5]: Copied! <pre>pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='text')\n\nfor page in pdf.pages:\n    print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\")\n</pre> pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='text')  for page in pdf.pages:     print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\") <pre>Device set to use mps:0\n</pre> <pre>Page 1 is text - 0.514\nPage 2 is text - 0.587\nPage 3 is invoice - 0.603\nPage 4 is diagram - 0.65\nPage 5 is diagram - 0.567\nPage 6 is text - 0.654\nPage 7 is diagram - 0.466\nPage 8 is text - 0.626\nPage 9 is text - 0.513\nPage 10 is text - 0.542\nPage 11 is invoice - 0.506\nPage 12 is text - 0.78\nPage 13 is text - 0.456\nPage 14 is diagram - 0.721\nPage 15 is diagram - 0.8\nPage 16 is text - 0.499\nPage 17 is text - 0.78\n</pre> <p>How does it compare to our vision option?</p> In\u00a0[6]: Copied! <pre>pdf.pages.filter(lambda page: page.category == 'diagram').to_image(show_category=True)\n</pre> pdf.pages.filter(lambda page: page.category == 'diagram').to_image(show_category=True) Out[6]: <p>Yes, you can notice that it's wrong, but more importantly look at the confidence scores. Low scores are your best clue that something might not be perfect (beyond manually checking things, of course).</p> <p>If you're processing documents that are text-heavy you'll have much better luck with a text model as compared to a vision one.</p> In\u00a0[7]: Copied! <pre>import natural_pdf\n\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n]\n\n# Import your PDFs\npdfs = natural_pdf.PDFCollection(pdf_paths)\n\n# Run your classification\npdfs.classify_all(['school', 'business'], using='text')\n</pre> import natural_pdf  pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\" ]  # Import your PDFs pdfs = natural_pdf.PDFCollection(pdf_paths)  # Run your classification pdfs.classify_all(['school', 'business'], using='text') <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[7]: <pre>&lt;PDFCollection(count=2)&gt;</pre> <p>What's the first PDF?</p> In\u00a0[8]: Copied! <pre>print(f\"{pdfs[0].category} - confidence of {pdfs[0].category_confidence:0.3}\")\n\n# Look at the first page\npdfs[0].pages[0].to_image(width=500)\n</pre> print(f\"{pdfs[0].category} - confidence of {pdfs[0].category_confidence:0.3}\")  # Look at the first page pdfs[0].pages[0].to_image(width=500) <pre>business - confidence of 0.837\n</pre> Out[8]: <p>How about the second?</p> In\u00a0[9]: Copied! <pre>print(f\"{pdfs[1].category} - confidence of {pdfs[1].category_confidence:0.3}\")\n\n# Look at the first page\npdfs[1].pages[0].to_image(width=500)\n</pre> print(f\"{pdfs[1].category} - confidence of {pdfs[1].category_confidence:0.3}\")  # Look at the first page pdfs[1].pages[0].to_image(width=500) <pre>school - confidence of 0.569\n</pre> Out[9]:"},{"location":"tutorials/14-categorizing-documents/#categorizing-documents","title":"Categorizing documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to automatically categorize pages of PDFs or entire collections of PDFs.</p>"},{"location":"tutorials/14-categorizing-documents/#vision-classification","title":"Vision classification\u00b6","text":"<p>These pages are easily differentiable based on how they look, so we can most likely use a vision model to tell them apart.</p>"},{"location":"tutorials/14-categorizing-documents/#text-classification-default","title":"Text classification (default)\u00b6","text":"<p>By default the search is done using text. It takes the text on the page and feeds it to the classifier along with the categories. Note that you might need to OCR your content first!</p>"},{"location":"tutorials/14-categorizing-documents/#pdf-classification","title":"PDF classification\u00b6","text":"<p>If you want to classify entire PDFs, the process is similar. The only gotcha is you can't use <code>using=\"vision\"</code> with multi-page PDFs (yet?).</p>"},{"location":"tutorials/14-categorizing-documents/#todo","title":"TODO\u00b6","text":"<ul> <li>Document advanced parameters for classification helpers (<code>min_confidence</code>, <code>multi_label</code>, <code>analysis_key</code>) so users can fine-tune behaviour or store multiple result sets.</li> <li>Add an example that passes an explicit Hugging Face model ID (e.g. <code>model=\"openai/clip-vit-base-patch16\"</code>) for reproducibility.</li> <li>Note that vision classification only works for single-page PDFs or per-page classification, not whole multi-page PDFs.</li> <li>Remind readers to install the AI stack if they skipped the earlier magic: <code>pip install \"natural-pdf[ai]\"</code>.</li> <li>Suggest using <code>pdf.pages.to_image(show_category=True)</code> to visually QC an entire document after classification.</li> </ul>"},{"location":"visual-debugging/","title":"Visual Debugging","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# View a few elements that contain the word \"Summary\"\npage.find_all('text:contains(\"Summary\")').show()\n\n# Crop-only display of the region below the heading\nheading = page.find('text:bold[size&gt;=12]')\nregion_below = heading.below(height=250)\nregion_below.show(crop=True)\n\n# Colour by element type\npage.find_all('text, rect, line').show(group_by='type')\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # View a few elements that contain the word \"Summary\" page.find_all('text:contains(\"Summary\")').show()  # Crop-only display of the region below the heading heading = page.find('text:bold[size&gt;=12]') region_below = heading.below(height=250) region_below.show(crop=True)  # Colour by element type page.find_all('text, rect, line').show(group_by='type') <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: <p>Need a prettier image for an article or report? <code>.show()</code> accepts the same keyword arguments as <code>.highlight()</code> (<code>color</code>, <code>label</code>, <code>group_by</code>, <code>include_attrs</code>, \u2026) so you can style the output exactly the way you like without leaving any permanent marks on the page.</p> <p>Tip \u2013 <code>crop=True</code></p> <p>Pass <code>crop=True</code> to <code>.show()</code> (or <code>.to_image()</code>) when you want the smallest image that still contains all of the selected elements/region. This works for both <code>Region</code> objects and regular <code>ElementCollection</code>s \u2013 perfect for quickly zooming into the exact area you're debugging.</p> In\u00a0[2]: Copied! <pre># Zoom into just the bold text you found\npage.find_all('text:bold').show(crop=True)\n</pre> # Zoom into just the bold text you found page.find_all('text:bold').show(crop=True) Out[2]: In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find a specific element and add a persistent highlight\npage.find_all('text:contains(\"Summary\")').highlight()\npage.find_all('text:contains(\"Date\")').highlight()\npage.find_all('line').highlight()\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find a specific element and add a persistent highlight page.find_all('text:contains(\"Summary\")').highlight() page.find_all('text:contains(\"Date\")').highlight() page.find_all('line').highlight() page.to_image(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[3]: In\u00a0[4]: Copied! <pre>page.clear_highlights()\n\ntitle = page.find('text:bold[size&gt;=12]')\n\n# Highlight with a specific color (string name, hex, or RGB/RGBA tuple)\n# title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity\n# title.highlight(color=\"#FF0000\")        # Hex color\ntitle.highlight(color=\"red\")           # Color name\n\ntext = page.find('text:contains(\"Critical\")')\n\n# Add a label to the highlight (appears in legend)\ntext.highlight(label=\"Critical\")\n\n# Combine color and label\nrect = page.find('rect')\nrect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")\n\npage.to_image(width=700)\n</pre> page.clear_highlights()  title = page.find('text:bold[size&gt;=12]')  # Highlight with a specific color (string name, hex, or RGB/RGBA tuple) # title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity # title.highlight(color=\"#FF0000\")        # Hex color title.highlight(color=\"red\")           # Color name  text = page.find('text:contains(\"Critical\")')  # Add a label to the highlight (appears in legend) text.highlight(label=\"Critical\")  # Combine color and label rect = page.find('rect') rect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")  page.to_image(width=700) Out[4]: In\u00a0[5]: Copied! <pre># Find and highlight all headings with a single color/label\nheadings = page.find_all('text[size&gt;=14]:bold')\nheadings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")\n\n# Find and highlight all tables\ntables = page.find_all('region[type=table]')\ntables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")\n\n# View the result\npage.viewer()\n</pre> # Find and highlight all headings with a single color/label headings = page.find_all('text[size&gt;=14]:bold') headings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")  # Find and highlight all tables tables = page.find_all('region[type=table]') tables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")  # View the result page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[5]: <pre>InteractiveViewerWidget()</pre> In\u00a0[6]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Highlight the region\ncontent.show()\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Highlight the region content.show() Out[6]: <p>Or look at just the region by itself</p> In\u00a0[7]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Crop to the region\ncontent.to_image(crop=True, include_highlights=False)\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Crop to the region content.to_image(crop=True, include_highlights=False) Out[7]: In\u00a0[8]: Copied! <pre># Analyze and visualize text styles\npage.clear_highlights()\n\npage.analyze_text_styles()\npage.find_all('text').show(group_by='style_label')\n\npage.to_image(width=700)\n</pre> # Analyze and visualize text styles page.clear_highlights()  page.analyze_text_styles() page.find_all('text').show(group_by='style_label')  page.to_image(width=700) Out[8]: In\u00a0[9]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\ntext = page.find_all('line')\ntext.highlight(include_attrs=['width', 'color'])\n\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  text = page.find_all('line') text.highlight(include_attrs=['width', 'color'])  page.to_image(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[9]: <p>Does it get busy? YES.</p> In\u00a0[10]: Copied! <pre># Clear all highlights on the page\npage.clear_highlights()\n\n# Apply new highlights\npage.find_all('text:bold').highlight(label=\"Bold Text\")\npage.viewer()\n</pre> # Clear all highlights on the page page.clear_highlights()  # Apply new highlights page.find_all('text:bold').highlight(label=\"Bold Text\") page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[10]: <pre>InteractiveViewerWidget()</pre> In\u00a0[11]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\")\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\") page = pdf.pages[0] page.to_image(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[11]: In\u00a0[12]: Copied! <pre>response = page.ask(\"How many votes did Kamala Harris get on Election Day?\")\nresponse\n</pre> response = page.ask(\"How many votes did Kamala Harris get on Election Day?\") response <pre>Device set to use mps:0\n</pre> Out[12]: <pre>{'answer': '60',\n 'confidence': 0.31857365369796753,\n 'start': 31,\n 'end': 31,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[13]: Copied! <pre>response['source_elements'].show()\n</pre> response['source_elements'].show() Out[13]:"},{"location":"visual-debugging/#visual-debugging","title":"Visual Debugging\u00b6","text":"<p>Sometimes it's hard to understand what's happening when working with PDFs. Natural PDF provides powerful visual debugging tools to help you see what you're extracting.</p>"},{"location":"visual-debugging/#quick-visualization-with-show","title":"Quick Visualization with <code>.show()</code>\u00b6","text":"<p>The fastest way to see what you have selected is to call <code>.show()</code> on the element (or collection/region) that you receive. Unlike <code>.highlight()</code> this does not persist anything on the page \u2013 it simply returns a <code>PIL.Image</code> with temporary highlights that you can display right away.</p>"},{"location":"visual-debugging/#adding-persistent-highlights","title":"Adding Persistent Highlights\u00b6","text":"<p>Use the <code>.highlight()</code> method on <code>Element</code> or <code>ElementCollection</code> objects to add persistent highlights to a page. These highlights are stored and will appear when viewing the page later.</p>"},{"location":"visual-debugging/#customizing-persistent-highlights","title":"Customizing Persistent Highlights\u00b6","text":"<p>Customize the appearance of persistent highlights added with <code>.highlight()</code>:</p>"},{"location":"visual-debugging/#highlighting-multiple-elements","title":"Highlighting Multiple Elements\u00b6","text":"<p>Highlighting an <code>ElementCollection</code> applies the highlight to all elements within it. By default, all elements in the collection get the same color and a label based on their type.</p>"},{"location":"visual-debugging/#viewing-regions","title":"Viewing Regions\u00b6","text":"<p>You can visualise regions to see the exact area you're working with:</p>"},{"location":"visual-debugging/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>Visualize text styles to understand the document structure:</p>"},{"location":"visual-debugging/#displaying-attributes","title":"Displaying Attributes\u00b6","text":"<p>You can display element attributes directly on the highlights:</p>"},{"location":"visual-debugging/#clearing-highlights","title":"Clearing Highlights\u00b6","text":"<p>You can clear persistent highlights from a page:</p>"},{"location":"visual-debugging/#document-qa-visualization","title":"Document QA Visualization\u00b6","text":"<p>Visualize document QA results:</p>"},{"location":"visual-debugging/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to visualize PDF content, you might want to explore:</p> <ul> <li>OCR capabilities for working with scanned documents</li> <li>Layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"}]}