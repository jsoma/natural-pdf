{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Natural PDF","text":"<p>A friendly library for working with PDFs, built on top of pdfplumber.</p> <p>Natural PDF lets you find and extract content from PDFs using simple code that makes sense.</p> <ul> <li>Live demo here</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install natural_pdf\n# All the extras\npip install \"natural_pdf[all]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from natural_pdf import PDF\n\npdf = PDF('document.pdf')\npage = pdf.pages[0]\n\n# Find the title and get content below it\ntitle = page.find('text:contains(\"Summary\"):bold')\ncontent = title.below().extract_text()\n\n# Exclude everything above 'CONFIDENTIAL' and below last line on page\npage.add_exclusion(page.find('text:contains(\"CONFIDENTIAL\")').above())\npage.add_exclusion(page.find_all('line')[-1].below())\n\n# Get the clean text without header/footer\nclean_text = page.extract_text()\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<p>Here are a few highlights of what you can do:</p>"},{"location":"#find-elements-with-selectors","title":"Find Elements with Selectors","text":"<p>Use CSS-like selectors to find text, shapes, and more.</p> <pre><code># Find bold text containing \"Revenue\"\npage.find('text:contains(\"Revenue\"):bold').extract_text()\n\n# Find all large text\npage.find_all('text[size&gt;=12]').extract_text()\n</code></pre> <p>Learn more about selectors \u2192</p>"},{"location":"#navigate-spatially","title":"Navigate Spatially","text":"<p>Move around the page relative to elements, not just coordinates.</p> <pre><code># Extract text below a specific heading\nintro_text = page.find('text:contains(\"Introduction\")').below().extract_text()\n\n# Extract text from one heading to the next\nmethods_text = page.find('text:contains(\"Methods\")').below(\n    until='text:contains(\"Results\")'\n).extract_text()\n</code></pre> <p>Explore more navigation methods \u2192</p>"},{"location":"#extract-clean-text","title":"Extract Clean Text","text":"<p>Easily extract text content, automatically handling common page elements like headers and footers (if exclusions are set).</p> <pre><code># Extract all text from the page (respecting exclusions)\npage_text = page.extract_text()\n\n# Extract text from a specific region\nsome_region = page.find(...)\nregion_text = some_region.extract_text()\n</code></pre> <p>Learn about text extraction \u2192 Learn about exclusion zones \u2192</p>"},{"location":"#apply-ocr","title":"Apply OCR","text":"<p>Extract text from scanned documents using various OCR engines.</p> <pre><code># Apply OCR using the default engine\nocr_elements = page.apply_ocr()\n\n# Extract text (will use OCR results if available)\ntext = page.extract_text()\n</code></pre> <p>Explore OCR options \u2192</p>"},{"location":"#analyze-document-layout","title":"Analyze Document Layout","text":"<p>Use AI models to detect document structures like titles, paragraphs, and tables.</p> <pre><code># Detect document structure\npage.analyze_layout()\n\n# Highlight titles and tables\npage.find_all('region[type=title]').highlight(color=\"purple\")\npage.find_all('region[type=table]').highlight(color=\"blue\")\n\n# Extract data from the first table\ntable_data = page.find('region[type=table]').extract_table()\n</code></pre> <p>Learn about layout models \u2192 Working with tables? \u2192</p>"},{"location":"#document-question-answering","title":"Document Question Answering","text":"<p>Ask natural language questions directly to your documents.</p> <pre><code># Ask a question\nresult = pdf.ask(\"What was the company's revenue in 2022?\")\nif result.get(\"found\", False):\n    print(f\"Answer: {result['answer']}\")\n</code></pre> <p>Learn about Document QA \u2192</p>"},{"location":"#classify-pages-and-regions","title":"Classify Pages and Regions","text":"<p>Categorize pages or specific regions based on their content using text or vision models.</p> <p>Note: Requires <code>pip install \"natural-pdf[classification]\"</code></p> <pre><code># Classify a page based on text\ncategories = [\"invoice\", \"scientific article\", \"presentation\"]\npage.classify(categories=categories, model=\"text\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n\n\n# Classify a page based on what it looks like\ncategories = [\"invoice\", \"scientific article\", \"presentation\"]\npage.classify(categories=categories, model=\"vision\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n</code></pre>"},{"location":"#visualize-your-work","title":"Visualize Your Work","text":"<p>Debug and understand your extractions visually.</p> <pre><code># Highlight headings\npage.find_all('text[size&gt;=14]').highlight(color=\"red\", label=\"Headings\")\n\n# Launch the interactive viewer (Jupyter)\n# Requires: pip install natural-pdf[interactive]\npage.viewer()\n\n# Or save an image\n# page.save_image(\"highlighted.png\")\n</code></pre> <p>See more visualization options \u2192</p>"},{"location":"#documentation-topics","title":"Documentation Topics","text":"<p>Choose what you want to learn about:</p>"},{"location":"#task-based-guides","title":"Task-based Guides","text":"<ul> <li>Getting Started: Install the library and run your first extraction</li> <li>PDF Navigation: Open PDFs and work with pages</li> <li>Element Selection: Find text and other elements using selectors</li> <li>Text Extraction: Extract clean text from documents</li> <li>Regions: Work with specific areas of a page</li> <li>Visual Debugging: See what you're extracting</li> <li>OCR: Extract text from scanned documents</li> <li>Layout Analysis: Detect document structure</li> <li>Tables: Extract tabular data</li> <li>Document QA: Ask questions to your documents</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference: Complete library reference</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all the classes and methods in Natural PDF.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#pdf-class","title":"PDF Class","text":"<p>The main entry point for working with PDFs.</p> <pre><code>class PDF:\n    \"\"\"\n    The main entry point for working with PDFs.\n\n    Parameters:\n        path (str): Path to the PDF file.\n        password (str, optional): Password for encrypted PDFs. Default: None\n        reading_order (bool, optional): Sort elements in reading order. Default: True\n        keep_spaces (bool, optional): Keep spaces in word elements. Default: True\n        font_attrs (list, optional): Font attributes to use for text grouping. \n                                    Default: ['fontname', 'size']\n        ocr (bool/dict/str, optional): OCR configuration. Default: False\n        ocr_engine (str/Engine, optional): OCR engine to use. Default: \"easyocr\"\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>pages</code> Access pages in the document N/A (property) <code>PageCollection</code> <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>add_exclusion(func, label=None)</code> Add a document-wide exclusion zone <code>func</code>: Function taking a page and returning region<code>label</code>: Optional label for the exclusion <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections across all pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries ('start', 'end', 'both', 'none') <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None)</code> Ask a question about the document content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path <code>dict</code>: Result with answer and metadata"},{"location":"api/#page-class","title":"Page Class","text":"<p>Represents a single page in a PDF document.</p> <pre><code>class Page:\n    \"\"\"\n    Represents a single page in a PDF document.\n\n    Properties:\n        page_number (int): 1-indexed page number\n        page_index (int): 0-indexed page position\n        width (float): Page width in points\n        height (float): Page height in points\n        pdf (PDF): Parent PDF object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the page <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>create_region(x0, top, x1, bottom)</code> Create a region at specific coordinates <code>x0</code>: Left coordinate<code>top</code>: Top coordinate<code>x1</code>: Right coordinate<code>bottom</code>: Bottom coordinate <code>Region</code> <code>highlight(elements, color=None, label=None)</code> Highlight elements on the page <code>elements</code>: Elements to highlight<code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight <code>Page</code> (self) <code>highlight_all(include_types=None, include_text_styles=False, include_layout_regions=False)</code> Highlight all elements on the page <code>include_types</code>: Element types to include<code>include_text_styles</code>: Whether to include text styles<code>include_layout_regions</code>: Whether to include layout regions <code>Page</code> (self) <code>save_image(path, resolution=72, labels=True)</code> Save an image of the page with highlights <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>None</code> <code>to_image(resolution=72, labels=True)</code> Get a PIL Image of the page with highlights <code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>PIL.Image</code> <code>analyze_text_styles()</code> Group text by visual style properties None <code>dict</code>: Mapping of style name to elements <code>analyze_layout(engine=\"yolo\", confidence=0.2, existing=\"replace\")</code> Detect layout regions using ML models <code>model</code>: Model to use (\"yolo\", \"tatr\")<code>confidence</code>: Confidence threshold<code>existing</code>: How to handle existing regions <code>ElementCollection</code>: Detected regions <code>add_exclusion(region, label=None)</code> Add an exclusion zone to the page <code>region</code>: Region to exclude<code>label</code>: Optional label for the exclusion <code>Region</code>: The exclusion region <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections from the page <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the page content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>apply_ocr(languages=None, min_confidence=0.0, **kwargs)</code> Apply OCR to the page <code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold<code>**kwargs</code>: Additional OCR engine parameters <code>ElementCollection</code>: OCR text elements"},{"location":"api/#region-class","title":"Region Class","text":"<p>Represents a rectangular area on a page.</p> <pre><code>class Region:\n    \"\"\"\n    Represents a rectangular area on a page.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the region\n        height (float): Height of the region\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the region <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>expand(left=0, top=0, right=0, bottom=0, width_factor=1.0, height_factor=1.0)</code> Expand the region in specified directions <code>left/top/right/bottom</code>: Points to expand in each direction<code>width_factor/height_factor</code>: Scale width/height by this factor <code>Region</code>: Expanded region <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight the region <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Region attributes to display <code>Region</code> (self) <code>to_image(resolution=72, crop_only=False)</code> Get a PIL Image of just the region <code>resolution</code>: Image resolution in DPI<code>crop_only</code>: Whether to exclude border <code>PIL.Image</code> <code>save_image(path, resolution=72, crop_only=False)</code> Save an image of just the region <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>crop_only</code>: Whether to exclude border <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections within the region <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the region content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>extract_table(method=None, table_settings=None, use_ocr=False)</code> Extract table data from the region <code>method</code>: Extraction method (\"plumber\", \"tatr\")<code>table_settings</code>: Custom settings for extraction<code>use_ocr</code>: Whether to use OCR text <code>list</code>: Table data as rows and columns <code>intersects(other)</code> Check if this region intersects with another <code>other</code>: Another region <code>bool</code>: True if regions intersect <code>contains(x, y)</code> Check if a point is within the region <code>x</code>: X coordinate<code>y</code>: Y coordinate <code>bool</code>: True if point is in region"},{"location":"api/#element-types","title":"Element Types","text":""},{"location":"api/#element-base-class","title":"Element Base Class","text":"<p>The base class for all PDF elements.</p> <pre><code>class Element:\n    \"\"\"\n    Base class for all PDF elements.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the element\n        height (float): Height of the element\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>above(height=None, full_width=True, until=None, include_until=True)</code> Create a region above the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>below(height=None, full_width=True, until=None, include_until=True)</code> Create a region below the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>select_until(selector, include_endpoint=True, full_width=True)</code> Create a region from this element to another <code>selector</code>: Selector for endpoint<code>include_endpoint</code>: Whether to include endpoint<code>full_width</code>: Whether to span page width <code>Region</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight this element <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Element attributes to display <code>Element</code> (self) <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from this element <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>next(selector=None, limit=None, apply_exclusions=True)</code> Get the next element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>prev(selector=None, limit=None, apply_exclusions=True)</code> Get the previous element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>nearest(selector, max_distance=None, apply_exclusions=True)</code> Get the nearest element matching selector <code>selector</code>: Selector for elements<code>max_distance</code>: Maximum distance in points<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code>"},{"location":"api/#textelement","title":"TextElement","text":"<p>Represents text elements in the PDF.</p> <pre><code>class TextElement(Element):\n    \"\"\"\n    Represents text elements in the PDF.\n\n    Additional Properties:\n        text (str): The text content\n        fontname (str): The font name\n        size (float): The font size\n        bold (bool): Whether the text is bold\n        italic (bool): Whether the text is italic\n        color (tuple): The text color as RGB tuple\n        confidence (float): OCR confidence (for OCR text)\n        source (str): 'pdf' or 'ocr'\n    \"\"\"\n</code></pre> <p>Main Properties</p> Property Type Description <code>text</code> <code>str</code> The text content <code>fontname</code> <code>str</code> The font name <code>size</code> <code>float</code> The font size <code>bold</code> <code>bool</code> Whether the text is bold <code>italic</code> <code>bool</code> Whether the text is italic <code>color</code> <code>tuple</code> The text color as RGB tuple <code>confidence</code> <code>float</code> OCR confidence (for OCR text) <code>source</code> <code>str</code> 'pdf' or 'ocr' <code>font_variant</code> <code>str</code> Font variant identifier (e.g., 'AAAAAB+') <p>Additional Methods</p> Method Description Parameters Returns <code>font_info()</code> Get detailed font information None <code>dict</code>: Font properties"},{"location":"api/#collections","title":"Collections","text":""},{"location":"api/#elementcollection","title":"ElementCollection","text":"<p>A collection of elements with batch operations.</p> <pre><code>class ElementCollection:\n    \"\"\"\n    A collection of elements with batch operations.\n\n    This class provides operations that can be applied to multiple elements at once.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all elements <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>filter(selector)</code> Filter elements by selector <code>selector</code>: CSS-like selector string <code>ElementCollection</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight all elements <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Attributes to display <code>ElementCollection</code> (self) <code>first</code> Get the first element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>last</code> Get the last element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>highest()</code> Get the highest element on the page None <code>Element</code> or <code>None</code> <code>lowest()</code> Get the lowest element on the page None <code>Element</code> or <code>None</code> <code>leftmost()</code> Get the leftmost element on the page None <code>Element</code> or <code>None</code> <code>rightmost()</code> Get the rightmost element on the page None <code>Element</code> or <code>None</code> <code>__len__()</code> Get the number of elements None <code>int</code> <code>__getitem__(index)</code> Get an element by index <code>index</code>: Index or slice <code>Element</code> or <code>ElementCollection</code>"},{"location":"api/#pagecollection","title":"PageCollection","text":"<p>A collection of pages with cross-page operations.</p> <pre><code>class PageCollection:\n    \"\"\"\n    A collection of pages with cross-page operations.\n\n    This class provides operations that can be applied across multiple pages.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start', new_section_on_page_break=False)</code> Get sections spanning multiple pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries<code>new_section_on_page_break</code>: Whether to start new sections at page breaks <code>list[Region]</code> <code>__len__()</code> Get the number of pages None <code>int</code> <code>__getitem__(index)</code> Get a page by index <code>index</code>: Index or slice <code>Page</code> or <code>PageCollection</code>"},{"location":"api/#ocr-classes","title":"OCR Classes","text":""},{"location":"api/#ocrengine","title":"OCREngine","text":"<p>Base class for OCR engines.</p> <pre><code>class OCREngine:\n    \"\"\"\n    Base class for OCR engines.\n\n    This class provides the interface for OCR engines.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>process_image(image, languages=None, min_confidence=0.0, **kwargs)</code> Process an image with OCR <code>image</code>: PIL Image<code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold <code>list</code>: OCR results"},{"location":"api/#easyocrengine","title":"EasyOCREngine","text":"<p>OCR engine using EasyOCR.</p> <pre><code>class EasyOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using EasyOCR.\n\n    Parameters:\n        model_dir (str, optional): Directory for models. Default: None\n    \"\"\"\n</code></pre>"},{"location":"api/#paddleocrengine","title":"PaddleOCREngine","text":"<p>OCR engine using PaddleOCR.</p> <pre><code>class PaddleOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using PaddleOCR.\n\n    Parameters:\n        use_angle_cls (bool, optional): Use text direction classification. Default: False\n        lang (str, optional): Language code. Default: \"en\"\n        det (bool, optional): Use text detection. Default: True\n        rec (bool, optional): Use text recognition. Default: True\n        cls (bool, optional): Use text direction classification. Default: False\n        det_model_dir (str, optional): Detection model directory. Default: None\n        rec_model_dir (str, optional): Recognition model directory. Default: None\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre>"},{"location":"api/#document-qa-classes","title":"Document QA Classes","text":""},{"location":"api/#documentqa","title":"DocumentQA","text":"<p>Class for document question answering.</p> <pre><code>class DocumentQA:\n    \"\"\"\n    Class for document question answering.\n\n    Parameters:\n        model (str, optional): Model name or path. Default: \"microsoft/layoutlmv3-base\"\n        device (str, optional): Device to use. Default: \"cpu\"\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>ask(question, image, word_boxes, min_confidence=0.0, max_answer_length=None, language=None)</code> Ask a question about a document <code>question</code>: Question to ask<code>image</code>: Document image<code>word_boxes</code>: Text positions<code>min_confidence</code>: Minimum confidence threshold<code>max_answer_length</code>: Maximum answer length<code>language</code>: Language code <code>dict</code>: Result with answer and metadata"},{"location":"api/#selector-syntax","title":"Selector Syntax","text":"<p>Natural PDF uses a CSS-like selector syntax to find elements in PDFs.</p>"},{"location":"api/#basic-selectors","title":"Basic Selectors","text":"Selector Description Example <code>element_type</code> Match elements of this type <code>text</code>, <code>rect</code>, <code>line</code> <code>[attribute=value]</code> Match elements with this attribute value <code>[fontname=Arial]</code>, <code>[size=12]</code> <code>[attribute&gt;=value]</code> Match elements with attribute &gt;= value <code>[size&gt;=12]</code> <code>[attribute&lt;=value]</code> Match elements with attribute &lt;= value <code>[size&lt;=10]</code> <code>[attribute~=value]</code> Match elements with attribute approximately equal <code>[color~=red]</code>, <code>[color~=(1,0,0)]</code> <code>[attribute*=value]</code> Match elements with attribute containing value <code>[fontname*=Arial]</code>"},{"location":"api/#pseudo-classes","title":"Pseudo-Classes","text":"Pseudo-Class Description Example <code>:contains(\"text\")</code> Match elements containing text <code>text:contains(\"Summary\")</code> <code>:starts-with(\"text\")</code> Match elements starting with text <code>text:starts-with(\"Summary\")</code> <code>:ends-with(\"text\")</code> Match elements ending with text <code>text:ends-with(\"2023\")</code> <code>:bold</code> Match bold text <code>text:bold</code> <code>:italic</code> Match italic text <code>text:italic</code>"},{"location":"api/#attribute-names","title":"Attribute Names","text":"Attribute Element Types Description <code>fontname</code> text Font name <code>size</code> text Font size <code>color</code> text, rect, line Color <code>width</code> rect, line Width <code>height</code> rect Height <code>confidence</code> text (OCR) OCR confidence score <code>source</code> text Source ('pdf' or 'ocr') <code>type</code> region Region type (e.g., 'table', 'title') <code>model</code> region Layout model that detected the region <code>font-variant</code> text Font variant identifier"},{"location":"api/#constants-and-configuration","title":"Constants and Configuration","text":""},{"location":"api/#color-names","title":"Color Names","text":"<p>Natural PDF supports color names in selectors.</p> Color Name RGB Value Example <code>red</code> (1, 0, 0) <code>[color~=red]</code> <code>green</code> (0, 1, 0) <code>[color~=green]</code> <code>blue</code> (0, 0, 1) <code>[color~=blue]</code> <code>black</code> (0, 0, 0) <code>[color~=black]</code> <code>white</code> (1, 1, 1) <code>[color~=white]</code>"},{"location":"api/#region-types","title":"Region Types","text":"<p>Layout analysis models detect the following region types:</p> Model Region Types YOLO <code>title</code>, <code>plain-text</code>, <code>table</code>, <code>figure</code>, <code>figure_caption</code>, <code>table_caption</code>, <code>table_footnote</code>, <code>isolate_formula</code>, <code>formula_caption</code>, <code>abandon</code> TATR <code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>"},{"location":"categorizing-documents/","title":"Categorizing Pages and Regions","text":"<p>Natural PDF allows you to automatically categorize pages or specific regions within a page using machine learning models. This is incredibly useful for filtering large collections of documents or understanding the structure and content of individual PDFs.</p>"},{"location":"categorizing-documents/#installation","title":"Installation","text":"<p>To use the classification features, you need to install the optional dependencies:</p> <pre><code>pip install \"natural-pdf[classification]\"\n</code></pre> <p>This installs necessary libraries like <code>torch</code>, <code>transformers</code>, and others.</p>"},{"location":"categorizing-documents/#core-concept-the-classify-method","title":"Core Concept: The <code>.classify()</code> Method","text":"<p>The primary way to perform categorization is using the <code>.classify()</code> method available on <code>Page</code> and <code>Region</code> objects.</p> <pre><code>from natural_pdf import PDF\n\n# Example: Classify a Page\npdf = PDF(\"pdfs/01-practice.pdf\")\npage = pdf.pages[0]\ncategories = [\"invoice\", \"letter\", \"report cover\", \"data table\"]\nresults = page.classify(categories=categories, model=\"text\")\n\n# Access the top result\nprint(f\"Top Category: {page.category}\")\nprint(f\"Confidence: {page.category_confidence:.3f}\")\n\n# Access all results\n# print(page.classification_results)\n</code></pre> <p>Key Arguments:</p> <ul> <li><code>categories</code> (required): A list of strings representing the potential categories you want to classify the item into.</li> <li><code>model</code> (optional): Specifies which classification model or strategy to use. Defaults to <code>\"text\"</code>.<ul> <li><code>\"text\"</code>: Uses a text-based model (default: <code>facebook/bart-large-mnli</code>) suitable for classifying based on language content.</li> <li><code>\"vision\"</code>: Uses a vision-based model (default: <code>openai/clip-vit-base-patch32</code>) suitable for classifying based on visual layout and appearance.</li> <li>Specific Model ID: You can provide a Hugging Face model ID (e.g., <code>\"google/siglip-base-patch16-224\"</code>, <code>\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"</code>) compatible with zero-shot text or image classification. The library attempts to infer whether it's text or vision, but you might need <code>using</code>.</li> </ul> </li> <li><code>using</code> (optional): Explicitly set to <code>\"text\"</code> or <code>\"vision\"</code> if the automatic inference based on the <code>model</code> ID fails or is ambiguous.</li> <li><code>min_confidence</code> (optional): A float between 0.0 and 1.0. Only categories with a confidence score greater than or equal to this threshold will be included in the results (default: 0.0).</li> </ul>"},{"location":"categorizing-documents/#text-vs-vision-classification","title":"Text vs. Vision Classification","text":"<p>Choosing the right model type depends on your goal:</p>"},{"location":"categorizing-documents/#text-classification-modeltext","title":"Text Classification (<code>model=\"text\"</code>)","text":"<ul> <li>How it works: Extracts the text from the page or region and analyzes the language content.</li> <li>Best for:<ul> <li>Topic Identification: Determining what a page or section is about (e.g., \"budget discussion,\" \"environmental impact,\" \"legal terms\").</li> <li>Content-Driven Document Types: Identifying document types primarily defined by their text (e.g., emails, meeting minutes, news articles, reports).</li> </ul> </li> <li>Data Journalism Example: You have thousands of pages of government reports. You can use text classification to find all pages discussing \"public health funding\" or classify paragraphs within environmental impact statements to find mentions of specific endangered species.</li> </ul> <pre><code># Find pages related to finance\nfinancial_categories = [\"budget\", \"revenue\", \"expenditure\", \"forecast\"]\npdf.classify_pages(categories=financial_categories, model=\"text\")\nbudget_pages = [p for p in pdf.pages if p.category == \"budget\"]\n</code></pre>"},{"location":"categorizing-documents/#vision-classification-modelvision","title":"Vision Classification (<code>model=\"vision\"</code>)","text":"<ul> <li>How it works: Renders the page or region as an image and analyzes its visual layout, structure, and appearance.</li> <li>Best for:<ul> <li>Layout-Driven Document Types: Identifying documents recognizable by their structure (e.g., invoices, receipts, forms, presentation slides, title pages).</li> <li>Identifying Visual Elements: Distinguishing between pages dominated by text, tables, charts, or images.</li> </ul> </li> <li>Data Journalism Example: You have a scanned archive of campaign finance filings containing various document types. You can use vision classification to quickly isolate all the pages that look like donation receipts or expenditure forms, even if the OCR quality is poor.</li> </ul> <pre><code># Find pages that look like invoices or receipts\nvisual_categories = [\"invoice\", \"receipt\", \"letter\", \"form\"]\npage.classify(categories=visual_categories, model=\"vision\")\nif page.category in [\"invoice\", \"receipt\"]:\n    print(f\"Page {page.number} looks like an invoice or receipt.\")\n</code></pre>"},{"location":"categorizing-documents/#classifying-specific-objects","title":"Classifying Specific Objects","text":""},{"location":"categorizing-documents/#pages-pageclassify","title":"Pages (<code>page.classify(...)</code>)","text":"<p>Classifying a whole page is useful for sorting documents or identifying the overall purpose of a page within a larger document.</p> <pre><code># Classify the first page\npage = pdf.pages[0]\npage_types = [\"cover page\", \"table of contents\", \"chapter start\", \"appendix\"]\npage.classify(categories=page_types, model=\"vision\") # Vision often good for page structure\nprint(f\"Page 1 Type: {page.category}\")\n</code></pre>"},{"location":"categorizing-documents/#regions-regionclassify","title":"Regions (<code>region.classify(...)</code>)","text":"<p>Classifying a specific region allows for more granular analysis within a page. You might first detect regions using Layout Analysis and then classify those regions.</p> <pre><code># Assume layout analysis has run, find paragraphs\nparagraphs = page.find_all(\"region[type=paragraph]\")\nif paragraphs:\n    # Classify the topic of the first paragraph\n    topic_categories = [\"introduction\", \"methodology\", \"results\", \"conclusion\"]\n    # Use text model for topic\n    paragraphs[0].classify(categories=topic_categories, model=\"text\")\n    print(f\"First paragraph category: {paragraphs[0].category}\")\n</code></pre>"},{"location":"categorizing-documents/#accessing-classification-results","title":"Accessing Classification Results","text":"<p>After running <code>.classify()</code>, you can access the results:</p> <ul> <li><code>page.category</code> or <code>region.category</code>: Returns the string label of the category with the highest confidence score from the last classification run. Returns <code>None</code> if no classification has been run or no category met the threshold.</li> <li><code>page.category_confidence</code> or <code>region.category_confidence</code>: Returns the float confidence score (0.0-1.0) for the top category. Returns <code>None</code> otherwise.</li> <li><code>page.classification_results</code> or <code>region.classification_results</code>: Returns the full result dictionary stored in the object's <code>.metadata['classification']</code>, containing the model used, engine type, categories provided, timestamp, and a list of all scores above the threshold sorted by confidence. Returns <code>None</code> if no classification has been run.</li> </ul> <pre><code>results = page.classify(categories=[\"invoice\", \"letter\"], model=\"text\", min_confidence=0.5)\n\nif page.category == \"invoice\":\n    print(f\"Found an invoice with confidence {page.category_confidence:.2f}\")\n\n# See all results above the threshold\n# print(page.classification_results['scores'])\n</code></pre>"},{"location":"categorizing-documents/#classifying-collections","title":"Classifying Collections","text":"<p>For batch processing, use the <code>.classify_all()</code> method on <code>PDFCollection</code> or <code>ElementCollection</code> objects. This displays a progress bar tracking individual items (pages or elements).</p>"},{"location":"categorizing-documents/#pdfcollection-collectionclassify_all","title":"PDFCollection (<code>collection.classify_all(...)</code>)","text":"<p>Classifies pages across all PDFs in the collection. Use <code>max_workers</code> for parallel processing across different PDF files.</p> <pre><code>collection = natural_pdf.PDFCollection.from_directory(\"./documents/\")\ncategories = [\"form\", \"datasheet\", \"image\", \"text document\"]\n\n# Classify all pages using vision model, processing 4 PDFs concurrently\ncollection.classify_all(categories=categories, model=\"vision\", max_workers=4)\n\n# Filter PDFs containing forms\nform_pdfs = []\nfor pdf in collection:\n    if any(p.category == \"form\" for p in pdf.pages if p.category):\n        form_pdfs.append(pdf.path)\n    pdf.close() # Remember to close PDFs\n\nprint(f\"Found forms in: {form_pdfs}\")\n</code></pre>"},{"location":"categorizing-documents/#elementcollection-element_collectionclassify_all","title":"ElementCollection (<code>element_collection.classify_all(...)</code>)","text":"<p>Classifies all classifiable elements (currently <code>Page</code> and <code>Region</code>) within the collection.</p> <pre><code># Assume 'pdf' is loaded and 'layout_regions' is an ElementCollection of Regions\nlayout_regions = pdf.find_all(\"region\")\nregion_types = [\"paragraph\", \"list\", \"table\", \"figure\", \"caption\"]\n\n# Classify all detected regions based on vision\nlayout_regions.classify_all(categories=region_types, model=\"vision\")\n\n# Count table regions\ntable_count = sum(1 for r in layout_regions if r.category == \"table\")\nprint(f\"Found {table_count} regions classified as tables.\")\n</code></pre>"},{"location":"data-extraction/","title":"Structured Data Extraction","text":"<p>Extracting specific, structured information (like invoice numbers, dates, or addresses) from documents often requires more than simple text extraction. Natural PDF integrates with Large Language Models (LLMs) via Pydantic schemas to achieve this.</p>"},{"location":"data-extraction/#introduction","title":"Introduction","text":"<p>This feature allows you to define the exact data structure you want using a Pydantic model and then instruct an LLM to populate that structure based on the content of a PDF element (like a <code>Page</code> or <code>Region</code>).</p>"},{"location":"data-extraction/#basic-extraction","title":"Basic Extraction","text":"<ol> <li>Define a Schema: Create a Pydantic model for your desired data.</li> <li>Extract: Use the <code>.extract()</code> method on a <code>PDF</code>, <code>Page</code>, or <code>Region</code> object.</li> <li>Access: Use the <code>.extracted()</code> method to retrieve the results.</li> </ol> <pre><code>from natural_pdf import PDF\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI # Example client\n\n# Example: Initialize your LLM client\nclient = OpenAI() \n\n# Load the PDF\npdf = PDF(\"path/to/your/document.pdf\")\npage = pdf.pages[0]\n\n# 1. Define your schema\nclass InvoiceInfo(BaseModel):\n    invoice_number: str = Field(description=\"The main invoice identifier\")\n    total_amount: float = Field(description=\"The final amount due\")\n    company_name: Optional[str] = Field(None, description=\"The name of the issuing company\")\n\n# 2. Extract data (using default analysis_key=\"default-structured\")\npage.extract(schema=InvoiceInfo, client=client) \n\n# 3. Access the results\n# Access the full result object\nfull_data = page.extracted() \nprint(full_data) \n\n# Access a single field\ninv_num = page.extracted('invoice_number')\nprint(f\"Invoice Number: {inv_num}\") \n</code></pre>"},{"location":"data-extraction/#keys-and-overwriting","title":"Keys and Overwriting","text":"<ul> <li>By default, results are stored under the key <code>\"default-structured\"</code> in the element's <code>.analyses</code> dictionary.</li> <li>Use the <code>analysis_key</code> parameter in <code>.extract()</code> to store results under a different name (e.g., <code>analysis_key=\"customer_details\"</code>).</li> <li>Attempting to extract using an existing <code>analysis_key</code> will raise an error unless <code>overwrite=True</code> is specified.</li> </ul> <pre><code># Extract using a specific key\npage.extract(InvoiceInfo, client, analysis_key=\"invoice_header\")\n\n# Access using the specific key\nheader_data = page.extracted(analysis_key=\"invoice_header\") \ncompany = page.extracted('company_name', analysis_key=\"invoice_header\")\n</code></pre>"},{"location":"data-extraction/#applying-to-regions-and-collections","title":"Applying to Regions and Collections","text":"<p>The <code>.extract()</code> and <code>.extracted()</code> methods work identically on <code>Region</code> objects, allowing you to target specific areas of a page for structured data extraction.</p> <pre><code># Assuming 'header_region' is a Region object you defined\nheader_region.extract(InvoiceInfo, client)\ncompany = header_region.extracted('company_name')\n</code></pre> <p>Furthermore, you can apply extraction to collections of elements (like <code>pdf.pages</code>, or the result of <code>pdf.find_all(...)</code>) using the <code>.apply()</code> method. This iterates through the collection and calls <code>.extract()</code> on each item.</p> <pre><code># Example: Extract InvoiceInfo from the first 5 pages\nresults = pdf.pages[:5].apply(\n    'extract', \n    schema=InvoiceInfo, \n    client=client, \n    analysis_key=\"page_invoice_info\", # Use a specific key for batch results\n    overwrite=True # Allow overwriting if run multiple times\n)\n\n# Access results for the first page in the collection\nfirst_page_company = results[0].extracted('company_name', analysis_key=\"page_invoice_info\")\n</code></pre> <p>This provides a powerful way to turn unstructured PDF content into structured, usable data.</p>"},{"location":"document-qa/","title":"Document Question Answering","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n\n# Display the first page \npage = pdf.pages[0]\npage.show()\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")  # Display the first page  page = pdf.pages[0] page.show() Out[1]: In\u00a0[2]: Copied! <pre># Ask a question about the entire document\npage.ask(\"How many votes did Harris and Waltz get?\")\n</pre> # Ask a question about the entire document page.ask(\"How many votes did Harris and Waltz get?\") <pre>Device set to use cpu\n</pre> Out[2]: <pre>{'answer': '148',\n 'confidence': 0.9995507001876831,\n 'start': 20,\n 'end': 20,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre>page.ask(\"Who got the most votes for Attorney General?\")\n</pre> page.ask(\"Who got the most votes for Attorney General?\") Out[3]: <pre>{'answer': 'DEM EUGENE DEPASQUALE',\n 'confidence': 0.9180715084075928,\n 'start': 63,\n 'end': 63,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre>page.ask(\"Who was the Republican candidate for Attorney General?\")\n</pre> page.ask(\"Who was the Republican candidate for Attorney General?\") Out[4]: <pre>{'answer': 'LIB ROBERT COWBURN',\n 'confidence': 0.2159084975719452,\n 'start': 67,\n 'end': 67,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[5]: Copied! <pre># Get a specific page\nregion = page.find('text:contains(\"Attorney General\")').below()\nregion.show()\n</pre> # Get a specific page region = page.find('text:contains(\"Attorney General\")').below() region.show() Out[5]: In\u00a0[6]: Copied! <pre>region.ask(\"How many write-in votes were cast?\")\n</pre> region.ask(\"How many write-in votes were cast?\") Out[6]: <pre>{'answer': '498',\n 'confidence': 0.9988918304443359,\n 'start': 17,\n 'end': 17,\n 'found': True,\n 'region': &lt;natural_pdf.elements.region.Region at 0x37f5846d0&gt;,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\n\nquestions = [\n    \"How many votes did Harris and Walz get?\",\n    \"How many votes did Trump get?\",\n    \"How many votes did Natural PDF get?\",\n    \"What was the date of this form?\"\n]\n\n# You can actually do this but with multiple questions\n# in the model itself buuuut Natural PDF can'd do it yet\nresults = [page.ask(q) for q in questions]\n\ndf = pd.json_normalize(results)\ndf.insert(0, 'question', questions)\ndf\n</pre> import pandas as pd  questions = [     \"How many votes did Harris and Walz get?\",     \"How many votes did Trump get?\",     \"How many votes did Natural PDF get?\",     \"What was the date of this form?\" ]  # You can actually do this but with multiple questions # in the model itself buuuut Natural PDF can'd do it yet results = [page.ask(q) for q in questions]  df = pd.json_normalize(results) df.insert(0, 'question', questions) df Out[7]: question answer confidence start end found page_num source_elements 0 How many votes did Harris and Walz get? 148 0.999671 20 20 True 0 (&lt;TextElement text='148' font='Helvetica' size... 1 How many votes did Trump get? 348 0.310207 22 22 True 0 (&lt;TextElement text='348' font='Helvetica' size... 2 How many votes did Natural PDF get? November 5, 2024 0.237137 3 3 True 0 (&lt;TextElement text='November 5...' font='Helve... 3 What was the date of this form? November 5, 2024 0.792704 3 3 True 0 (&lt;TextElement text='November 5...' font='Helve..."},{"location":"document-qa/#document-question-answering","title":"Document Question Answering\u00b6","text":"<p>Natural PDF includes document QA functionality that allows you to ask natural language questions about your PDFs and get relevant answers. This feature uses LayoutLM models to understand both the text content and the visual layout of your documents.</p>"},{"location":"document-qa/#setup","title":"Setup\u00b6","text":"<p>Let's start by loading a sample PDF to experiment with question answering.</p>"},{"location":"document-qa/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here's how to ask questions to a PDF page:</p>"},{"location":"document-qa/#asking-questions-to-part-of-a-page-questions","title":"Asking questions to part of a page questions\u00b6","text":"<p>You can also ask questions to a specific region of a page*:</p>"},{"location":"document-qa/#asking-multiple-questions","title":"Asking multiple questions\u00b6","text":""},{"location":"document-qa/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you've learned about document QA, explore:</p> <ul> <li>Element Selection: Find specific elements to focus your questions.</li> <li>Layout Analysis: Automatically detect document structure.</li> <li>Working with Regions: Define custom areas for targeted questioning.</li> <li>Text Extraction: Extract and preprocess text before QA.</li> </ul>"},{"location":"element-selection/","title":"Finding Elements with Selectors","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() Out[1]: In\u00a0[2]: Copied! <pre># Find the first text element containing \"Summary\"\nsummary_text = page.find('text:contains(\"Summary\")')\nsummary_text\n</pre> # Find the first text element containing \"Summary\" summary_text = page.find('text:contains(\"Summary\")') summary_text Out[2]: <pre>&lt;TextElement text='Summary: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 144.07000000000005, 101.68, 154.07000000000005)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all text elements containing \"Inadequate\"\ncontains_inadequate = page.find_all('text:contains(\"Inadequate\")')\nlen(contains_inadequate)\n</pre> # Find all text elements containing \"Inadequate\" contains_inadequate = page.find_all('text:contains(\"Inadequate\")') len(contains_inadequate) Out[3]: <pre>2</pre> In\u00a0[4]: Copied! <pre>summary_text.highlight(label='summary')\ncontains_inadequate.highlight(label=\"inadequate\")\npage.to_image(width=700)\n</pre> summary_text.highlight(label='summary') contains_inadequate.highlight(label=\"inadequate\") page.to_image(width=700) Out[4]: In\u00a0[5]: Copied! <pre># Find all text elements\nall_text = page.find_all('text')\nlen(all_text)\n</pre> # Find all text elements all_text = page.find_all('text') len(all_text) Out[5]: <pre>44</pre> In\u00a0[6]: Copied! <pre># Find all rectangle elements\nall_rects = page.find_all('rect')\nlen(all_rects)\n</pre> # Find all rectangle elements all_rects = page.find_all('rect') len(all_rects) Out[6]: <pre>8</pre> In\u00a0[7]: Copied! <pre># Find all line elements\nall_lines = page.find_all('line')\nlen(all_lines)\n</pre> # Find all line elements all_lines = page.find_all('line') len(all_lines) Out[7]: <pre>21</pre> In\u00a0[8]: Copied! <pre>page.find_all('line').show()\n</pre> page.find_all('line').show() Out[8]: In\u00a0[9]: Copied! <pre># Find large text (size &gt;= 11 points)\npage.find_all('text[size&gt;=11]')\n</pre> # Find large text (size &gt;= 11 points) page.find_all('text[size&gt;=11]') Out[9]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[10]: Copied! <pre># Find text with 'Helvetica' in the font name\npage.find_all('text[fontname*=Helvetica]')\n</pre> # Find text with 'Helvetica' in the font name page.find_all('text[fontname*=Helvetica]') Out[10]: <pre>&lt;ElementCollection[TextElement](count=44)&gt;</pre> In\u00a0[11]: Copied! <pre># Find red text (using approximate color match)\n# This PDF has text with color (0.8, 0.0, 0.0)\nred_text = page.find_all('text[color~=red]')\n</pre> # Find red text (using approximate color match) # This PDF has text with color (0.8, 0.0, 0.0) red_text = page.find_all('text[color~=red]') <pre>2025-04-25T16:22:29.260713Z [warning  ] Unsupported operator '~=' encountered during filter building for attribute 'color' lineno=377 module=natural_pdf.selectors.parser\n</pre> <pre>[2025-04-25 12:22:29,260] [ WARNING] parser.py:377 - Unsupported operator '~=' encountered during filter building for attribute 'color'\n</pre> In\u00a0[12]: Copied! <pre># Highlight the red text (ignoring existing highlights)\nred_text.show()\n</pre> # Highlight the red text (ignoring existing highlights) red_text.show() Out[12]: In\u00a0[13]: Copied! <pre># Find thick lines (width &gt;= 2)\npage.find_all('line[width&gt;=2]')\n</pre> # Find thick lines (width &gt;= 2) page.find_all('line[width&gt;=2]') Out[13]: <pre>&lt;ElementCollection[LineElement](count=1)&gt;</pre> In\u00a0[14]: Copied! <pre># Find bold text\npage.find_all('text:bold').show()\n</pre> # Find bold text page.find_all('text:bold').show() Out[14]: In\u00a0[15]: Copied! <pre># Combine attribute and pseudo-class: bold text size &gt;= 11\npage.find_all('text[size&gt;=11]:bold')\n</pre> # Combine attribute and pseudo-class: bold text size &gt;= 11 page.find_all('text[size&gt;=11]:bold') Out[15]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[16]: Copied! <pre># Find all text elements that are NOT bold\nnon_bold_text = page.find_all('text:not(:bold)')\n\n# Find all elements that are NOT regions of type 'table'\nnot_tables = page.find_all(':not(region[type=table])')\n\n# Find text elements that do not contain \"Total\" (case-insensitive)\nrelevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)\n\n# Find text elements that are not empty\nnon_empty_text = page.find_all('text:not(:empty)')\n</pre> # Find all text elements that are NOT bold non_bold_text = page.find_all('text:not(:bold)')  # Find all elements that are NOT regions of type 'table' not_tables = page.find_all(':not(region[type=table])')  # Find text elements that do not contain \"Total\" (case-insensitive) relevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)  # Find text elements that are not empty non_empty_text = page.find_all('text:not(:empty)') <p>Note: The selector inside <code>:not()</code> follows the same rules as regular selectors but currently does not support combinators (like <code>&gt;</code>, <code>+</code>, <code>~</code>, or descendant space) within <code>:not()</code>. You can nest basic type, attribute, and other pseudo-class selectors.</p> In\u00a0[17]: Copied! <pre># Find the thick horizontal line first\nref_line = page.find('line[width&gt;=2]')\n\n# Find text elements strictly above that line\ntext_above_line = page.find_all('text:above(\"line[width&gt;=2]\")')\ntext_above_line\n</pre> # Find the thick horizontal line first ref_line = page.find('line[width&gt;=2]')  # Find text elements strictly above that line text_above_line = page.find_all('text:above(\"line[width&gt;=2]\")') text_above_line Out[17]: <pre>&lt;ElementCollection[TextElement](count=17)&gt;</pre> In\u00a0[18]: Copied! <pre># Case-insensitive search for \"summary\"\npage.find_all('text:contains(\"summary\")', case=False)\n</pre> # Case-insensitive search for \"summary\" page.find_all('text:contains(\"summary\")', case=False) Out[18]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[19]: Copied! <pre># Regular expression search for the inspection ID (e.g., INS-XXX...)\n# The ID is in the red text we found earlier\npage.find_all('text:contains(\"INS-\\\\w+\")', regex=True)\n</pre> # Regular expression search for the inspection ID (e.g., INS-XXX...) # The ID is in the red text we found earlier page.find_all('text:contains(\"INS-\\\\w+\")', regex=True) Out[19]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[20]: Copied! <pre># Combine regex and case-insensitivity\npage.find_all('text:contains(\"jungle health\")', regex=True, case=False)\n</pre> # Combine regex and case-insensitivity page.find_all('text:contains(\"jungle health\")', regex=True, case=False) Out[20]: <pre>&lt;ElementCollection[TextElement](count=2)&gt;</pre> In\u00a0[21]: Copied! <pre># Get all headings (using a selector for large, bold text)\nheadings = page.find_all('text[size&gt;=11]:bold')\nheadings\n</pre> # Get all headings (using a selector for large, bold text) headings = page.find_all('text[size&gt;=11]:bold') headings Out[21]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[22]: Copied! <pre># Get the first and last heading in reading order\nfirst = headings.first\nlast = headings.last\n(first, last)\n</pre> # Get the first and last heading in reading order first = headings.first last = headings.last (first, last) Out[22]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[23]: Copied! <pre># Get the physically highest/lowest element in the collection\nhighest = headings.highest()\nlowest = headings.lowest()\n(highest, lowest)\n</pre> # Get the physically highest/lowest element in the collection highest = headings.highest() lowest = headings.lowest() (highest, lowest) Out[23]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[24]: Copied! <pre># Filter the collection further: headings containing \"Service\"\nservice_headings = headings.find_all('text:contains(\"Service\")')\nservice_headings\n</pre> # Filter the collection further: headings containing \"Service\" service_headings = headings.find_all('text:contains(\"Service\")') service_headings Out[24]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre> In\u00a0[25]: Copied! <pre># Extract text from all elements in the collection\nheadings.extract_text()\n</pre> # Extract text from all elements in the collection headings.extract_text() Out[25]: <pre>'Violations'</pre> <p>Remember: <code>.highest()</code>, <code>.lowest()</code>, <code>.leftmost()</code>, <code>.rightmost()</code> raise errors if the collection spans multiple pages.</p> In\u00a0[26]: Copied! <pre># Find text elements with a specific font variant prefix (if any exist)\n# This example PDF doesn't use variants, but the selector works like this:\npage.find_all('text[font-variant=AAAAAB]')\n</pre> # Find text elements with a specific font variant prefix (if any exist) # This example PDF doesn't use variants, but the selector works like this: page.find_all('text[font-variant=AAAAAB]') Out[26]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre>"},{"location":"element-selection/#finding-elements-with-selectors","title":"Finding Elements with Selectors\u00b6","text":"<p>Natural PDF uses CSS-like selectors to find elements (text, lines, images, etc.) within a PDF page or document. This guide demonstrates how to use these selectors effectively.</p>"},{"location":"element-selection/#setup","title":"Setup\u00b6","text":"<p>Let's load a sample PDF to work with. We'll use <code>01-practice.pdf</code> which has various elements.</p>"},{"location":"element-selection/#basic-element-finding","title":"Basic Element Finding\u00b6","text":"<p>The core methods are <code>find()</code> (returns the first match) and <code>find_all()</code> (returns all matches as an <code>ElementCollection</code>).</p> <p>The basic selector structure is <code>element_type[attribute_filter]:pseudo_class</code>.</p>"},{"location":"element-selection/#finding-text-by-content","title":"Finding Text by Content\u00b6","text":""},{"location":"element-selection/#selecting-by-element-type","title":"Selecting by Element Type\u00b6","text":"<p>You can select specific types of elements found in PDFs.</p>"},{"location":"element-selection/#filtering-by-attributes","title":"Filtering by Attributes\u00b6","text":"<p>Use square brackets <code>[]</code> to filter elements by their properties (attributes).</p>"},{"location":"element-selection/#common-attributes-operators","title":"Common Attributes &amp; Operators\u00b6","text":"Attribute Example Usage Operators Notes <code>size</code> (text) <code>text[size&gt;=12]</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Font size in points <code>fontname</code> <code>text[fontname*=Bold]</code> <code>=</code>, <code>*=</code> <code>*=</code> for contains substring <code>color</code> (text) <code>text[color~=red]</code> <code>~=</code> Approx. match (name, rgb, hex) <code>width</code> (line) <code>line[width&gt;1]</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Line thickness <code>source</code> <code>text[source=ocr]</code> <code>=</code> <code>pdf</code>, <code>ocr</code>, <code>detected</code> <code>type</code> (region) <code>region[type=table]</code> <code>=</code> Layout analysis region type"},{"location":"element-selection/#using-pseudo-classes","title":"Using Pseudo-Classes\u00b6","text":"<p>Use colons <code>:</code> for special conditions (pseudo-classes).</p>"},{"location":"element-selection/#common-pseudo-classes","title":"Common Pseudo-Classes\u00b6","text":"Pseudo-Class Example Usage Notes <code>:contains('text')</code> <code>text:contains('Report')</code> Finds elements containing specific text <code>:bold</code> <code>text:bold</code> Finds text heuristically identified as bold <code>:italic</code> <code>text:italic</code> Finds text heuristically identified as italic <code>:below(selector)</code> <code>text:below('line[width&gt;=2]')</code> Finds elements physically below the reference element <code>:above(selector)</code> <code>text:above('text:contains(\"Summary\")')</code> Finds elements physically above the reference element <code>:left-of(selector)</code> <code>line:left-of('rect')</code> Finds elements physically left of the reference element <code>:right-of(selector)</code> <code>text:right-of('rect')</code> Finds elements physically right of the reference element <code>:near(selector)</code> <code>text:near('image')</code> Finds elements physically near the reference element <p>Note: Spatial pseudo-classes like <code>:below</code>, <code>:above</code> identify elements based on bounding box positions relative to the first element matched by the inner selector.</p>"},{"location":"element-selection/#negation-pseudo-class-not","title":"Negation Pseudo-class (<code>:not()</code>)\u00b6","text":"<p>You can exclude elements that match a certain selector using the <code>:not()</code> pseudo-class. It takes another simple selector as its argument.</p>"},{"location":"element-selection/#spatial-pseudo-classes-examples","title":"Spatial Pseudo-Classes Examples\u00b6","text":""},{"location":"element-selection/#advanced-text-searching-options","title":"Advanced Text Searching Options\u00b6","text":"<p>Pass options to <code>find()</code> or <code>find_all()</code> for more control over text matching.</p>"},{"location":"element-selection/#working-with-elementcollections","title":"Working with ElementCollections\u00b6","text":"<p><code>find_all()</code> returns an <code>ElementCollection</code>, which is like a list but with extra PDF-specific methods.</p>"},{"location":"element-selection/#font-variants","title":"Font Variants\u00b6","text":"<p>Sometimes PDFs use font variants (prefixes like <code>AAAAAB+</code>) which can be useful for selection.</p>"},{"location":"element-selection/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you can find elements, explore:</p> <ul> <li>Text Extraction: Get text content from found elements.</li> <li>Spatial Navigation: Use found elements as anchors to navigate (<code>.above()</code>, <code>.below()</code>, etc.).</li> <li>Working with Regions: Define areas based on found elements.</li> <li>Visual Debugging: Techniques for highlighting and visualizing elements.</li> </ul>"},{"location":"finetuning/","title":"OCR Fine-tuning","text":"<p>While the built-in OCR engines (EasyOCR, PaddleOCR, Surya) offer good general performance, you might encounter situations where their accuracy isn't sufficient for your specific needs. This is often the case with:</p> <ul> <li>Unique Fonts: Documents using unusual or stylized fonts.</li> <li>Specific Languages: Languages or scripts not perfectly covered by the default models.</li> <li>Low Quality Scans: Noisy or degraded document images.</li> <li>Specialized Layouts: Text within complex tables, forms, or unusual arrangements.</li> </ul> <p>Fine-tuning allows you to adapt a pre-trained OCR recognition model to your specific data, significantly improving its accuracy on documents similar to those used for training.</p>"},{"location":"finetuning/#why-fine-tune","title":"Why Fine-tune?","text":"<ul> <li>Higher Accuracy: Achieve better text extraction results on your specific document types.</li> <li>Adaptability: Train the model to recognize domain-specific terms, symbols, or layouts.</li> <li>Reduced Errors: Minimize downstream errors in data extraction and processing pipelines.</li> </ul>"},{"location":"finetuning/#strategy-detect-llm-correct-export","title":"Strategy: Detect + LLM Correct + Export","text":"<p>Training an OCR model requires accurate ground truth: images of text snippets paired with their correct transcriptions. Manually creating this data is tedious. A powerful alternative leverages the strengths of different models:</p> <ol> <li>Detect Text Regions: Use a robust local OCR engine (like Surya or PaddleOCR) primarily for its detection capabilities (<code>detect_only=True</code>). This identifies the locations of text on the page, even if the initial recognition isn't perfect. You can combine this with layout analysis or region selections (<code>.region()</code>, <code>.below()</code>, <code>.add_exclusion()</code>) to focus on the specific areas you care about.</li> <li>Correct with LLM: For each detected text region, send the image snippet to a powerful Large Language Model (LLM) with multimodal capabilities (like GPT-4o, Claude 3.5 Sonnet/Haiku) using the <code>direct_ocr_llm</code> utility. The LLM performs high-accuracy OCR on the snippet, providing a \"ground truth\" transcription.</li> <li>Export for Fine-tuning: Use the <code>PaddleOCRRecognitionExporter</code> to package the original image snippets (from step 1) along with their corresponding LLM-generated text labels (from step 2) into the specific format required by PaddleOCR for fine-tuning its recognition model.</li> </ol> <p>This approach combines the efficient spatial detection of local models with the superior text recognition of large generative models to create a high-quality fine-tuning dataset with minimal manual effort.</p>"},{"location":"finetuning/#example-fine-tuning-for-greek-spreadsheet-text","title":"Example: Fine-tuning for Greek Spreadsheet Text","text":"<p>Let's walk through an example of preparing data to fine-tune PaddleOCR for text from a scanned Greek spreadsheet, adapting the process described above.</p> <pre><code># --- 1. Setup and Load PDF ---\nfrom natural_pdf import PDF\nfrom natural_pdf.ocr.utils import direct_ocr_llm\nfrom natural_pdf.exporters import PaddleOCRRecognitionExporter\nimport openai # Or your preferred LLM client library\nimport os\n\n# Ensure your LLM API key is set (using environment variables is recommended)\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" \n# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" \n\n# pdf_path = \"path/to/your/document.pdf\" \npdf_path = \"pdfs/hidden/the-bad-one.pdf\" # Replace with your PDF path\npdf = PDF(pdf_path)\n\n# --- 2. (Optional) Exclude Irrelevant Areas ---\n# If the document has consistent headers, footers, or margins you want to ignore\n# Use exclusions *before* detection\npdf.add_exclusion(lambda page: page.region(right=45)) # Exclude left margin/line numbers\npdf.add_exclusion(lambda page: page.region(left=500)) # Exclude right margin\n\n# --- 3. Detect Text Regions ---\n# Use a good detection engine. Surya is often robust for line detection.\n# We only want the bounding boxes, not the initial (potentially inaccurate) OCR text.\nprint(\"Detecting text regions...\")\n# Process only a subset of pages for demonstration if needed\nfor page in pdf.pages[:10]:\n    # Use a moderate resolution for detection; higher res used for LLM correction later\n    page.apply_ocr(engine='surya', resolution=120, detect_only=True) \nprint(f\"Detection complete for {num_pages_to_process} pages.\")\n\n# (Optional) Visualize detected boxes on a sample page\n# pdf.pages[9].find_all('text[source=ocr]').show() \n\n# --- 4. Correct with LLM ---\n# Configure your LLM client (example using OpenAI client, adaptable for others)\n# For Anthropic: client = openai.OpenAI(base_url=\"https://api.anthropic.com/v1/\", api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\nclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) \n\n# Craft a clear prompt for the LLM\n# Be as specific as possible! If it's in a specific language, what kinds\n# of characters, etc.\nprompt = \"\"\"OCR this image patch. Return only the exact text content visible in the image. \nPreserve original spelling, capitalization, punctuation, and symbols. \nDo not add any explanatory text, translations, comments, or quotation marks around the result.\nThe text is likely from a Greek document, potentially a spreadsheet, containing Modern Greek words or numbers.\"\"\"\n\n# Define the correction function using direct_ocr_llm\ndef correct_text_region(region):\n    # Use a high resolution for the LLM call for best accuracy\n    return direct_ocr_llm(\n        region, \n        client, \n        prompt=prompt, \n        resolution=300, \n        # model=\"claude-3-5-sonnet-20240620\" # Example Anthropic model\n        model=\"gpt-4o-mini\" # Example OpenAI model\n    )\n\n# Apply the correction function to the detected text regions\nprint(\"Applying LLM correction to detected regions...\")\nfor page in pdf.pages[:num_pages_to_process]:\n    # This finds elements added by apply_ocr and passes their regions to 'correct_text_region'\n    # The returned text from the LLM replaces the original OCR text for these elements\n    # The source attribute is updated (e.g., to 'ocr-llm-corrected')\n    page.correct_ocr(correct_text_region) \nprint(\"LLM correction complete.\")\n\n# --- 5. Export for PaddleOCR Fine-tuning ---\nprint(\"Configuring exporter...\")\nexporter = PaddleOCRRecognitionExporter(\n    # Select all of the non-blank OCR text\n    # Hopefully it's all been LLM-corrected! \n    selector=\"text[source^=ocr][text!='']\", \n    resolution=300,     # Resolution for the exported image crops\n    padding=2,          # Add slight padding around text boxes\n    split_ratio=0.9,    # 90% for training, 10% for validation\n    random_seed=42,     # For reproducible train/val split\n    include_guide=True  # Include the Colab fine-tuning notebook\n)\n\n# Define the output directory\noutput_directory = \"./my_paddleocr_finetune_data\"\nprint(f\"Exporting data to {output_directory}...\")\n\n# Run the export process\nexporter.export(pdf, output_directory)\n\nprint(\"Export complete.\")\nprint(f\"Dataset ready for fine-tuning in: {output_directory}\")\nprint(f\"Next step: Upload '{os.path.join(output_directory, 'fine_tune_paddleocr.ipynb')}' and the rest of the contents to Google Colab.\")\n\n# --- Cleanup ---\npdf.close() \n</code></pre>"},{"location":"finetuning/#running-the-fine-tuning","title":"Running the Fine-tuning","text":"<p>The <code>PaddleOCRRecognitionExporter</code> automatically includes a Jupyter Notebook (<code>fine_tune_paddleocr.ipynb</code>) in the output directory. This notebook is pre-configured to guide you through the fine-tuning process on Google Colab (which offers free GPU access):</p> <ol> <li>Upload: Upload the entire output directory (e.g., <code>my_paddleocr_finetune_data</code>) to your Google Drive or directly to your Colab instance.</li> <li>Open Notebook: Open the <code>fine_tune_paddleocr.ipynb</code> notebook in Google Colab.</li> <li>Set Runtime: Ensure the Colab runtime is set to use a GPU (Runtime -&gt; Change runtime type -&gt; GPU).</li> <li>Run Cells: Execute the cells in the notebook sequentially. It will:<ul> <li>Install necessary libraries (PaddlePaddle, PaddleOCR).</li> <li>Point the training configuration to your uploaded dataset (<code>images/</code>, <code>train.txt</code>, <code>val.txt</code>, <code>dict.txt</code>).</li> <li>Download a pre-trained PaddleOCR model (usually a multilingual one).</li> <li>Start the fine-tuning process using your data.</li> <li>Save the fine-tuned model checkpoints.</li> <li>Export the best model into an \"inference format\" suitable for use with <code>natural-pdf</code>.</li> </ul> </li> <li>Download Model: Download the resulting <code>inference_model</code> directory from Colab.</li> </ol>"},{"location":"finetuning/#using-the-fine-tuned-model","title":"Using the Fine-tuned Model","text":"<p>Once you have the <code>inference_model</code> directory, you can instruct <code>natural-pdf</code> to use it for OCR:</p> <pre><code>from natural_pdf import PDF\nfrom natural_pdf.ocr import PaddleOCROptions\n\n# Path to the directory you downloaded from Colab\nfinetuned_model_dir = \"/path/to/your/downloaded/inference_model\" \n\n# Specify the path in PaddleOCROptions\npaddle_opts = PaddleOCROptions(\n    rec_model_dir=finetuned_model_dir,\n    rec_char_dict_path=os.path.join(finetuned_model_dir, 'your_dict.txt') # Or wherever your dict is\n    use_gpu=True # If using GPU locally\n)\n\npdf = PDF(\"another-similar-document.pdf\")\npage = pdf.pages[0]\n\n# Apply OCR using your fine-tuned model\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# Extract text using the improved results\ntext = page.extract_text() \nprint(text)\n\npdf.close()\n</code></pre> <p>By following this process, you can significantly enhance OCR performance on your specific documents using the power of fine-tuning. </p>"},{"location":"installation/","title":"Getting Started with Natural PDF","text":"<p>Let's get Natural PDF installed and run your first extraction.</p>"},{"location":"installation/#installation","title":"Installation","text":"<p>The base installation includes the core library and necessary AI dependencies (like PyTorch and Transformers):</p> <pre><code>pip install natural-pdf\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Natural PDF has modular dependencies for different features. Install them based on your needs:</p> <pre><code># --- OCR Engines ---\n# Install support for EasyOCR\npip install natural-pdf[easyocr]\n\n# Install support for PaddleOCR (requires paddlepaddle)\npip install natural-pdf[paddle]\n\n# Install support for Surya OCR\npip install natural-pdf[surya]\n\n# --- Layout Detection ---\n# Install support for YOLO layout model\npip install natural-pdf[layout_yolo]\n\n# --- Interactive Widget ---\n# Install support for the interactive .viewer() widget in Jupyter\npip install natural-pdf[interactive]\n\n# --- All Features ---\n# Install all optional dependencies\npip install natural-pdf[all]\n</code></pre>"},{"location":"installation/#your-first-pdf-extraction","title":"Your First PDF Extraction","text":"<p>Here's a quick example to make sure everything is working:</p> <pre><code>from natural_pdf import PDF\n\n# Open a PDF\npdf = PDF('your_document.pdf')\n\n# Get the first page\npage = pdf.pages[0]\n\n# Extract all text\ntext = page.extract_text()\nprint(text)\n\n# Find something specific\ntitle = page.find('text:bold')\nprint(f\"Found title: {title.text}\")\n</code></pre>"},{"location":"installation/#whats-next","title":"What's Next?","text":"<p>Now that you have Natural PDF installed, you can:</p> <ul> <li>Learn to navigate PDFs</li> <li>Explore how to select elements</li> <li>See how to extract text</li> </ul>"},{"location":"interactive-widget/","title":"Interactive widget","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.viewer()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[1]:"},{"location":"interactive-widget/#interactive-widget","title":"Interactive widget\u00b6","text":"<p>This is the best possible way, in all of history, to explore a PDF.</p>"},{"location":"layout-analysis/","title":"Document Layout Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.to_image(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Analyze the layout using the default engine (YOLO)\n# This adds 'region' elements to the page\npage.analyze_layout()\n</pre> # Analyze the layout using the default engine (YOLO) # This adds 'region' elements to the page page.analyze_layout() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmprdeq3z9_/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 3113.1ms\n</pre> <pre>Speed: 5.5ms preprocess, 3113.1ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[2]: <pre>&lt;ElementCollection[Region](count=8)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all detected regions\nregions = page.find_all('region')\nlen(regions) # Show how many regions were detected\n</pre> # Find all detected regions regions = page.find_all('region') len(regions) # Show how many regions were detected Out[3]: <pre>8</pre> In\u00a0[4]: Copied! <pre>first_region = regions[0]\nf\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\"\n</pre> first_region = regions[0] f\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\" Out[4]: <pre>\"First region: type='abandon', confidence=0.74\"</pre> In\u00a0[5]: Copied! <pre># Highlight all detected regions, colored by type\nregions.highlight(group_by='type')\npage.to_image(width=700)\n</pre> # Highlight all detected regions, colored by type regions.highlight(group_by='type') page.to_image(width=700) Out[5]: In\u00a0[6]: Copied! <pre># Find all detected titles\ntitles = page.find_all('region[type=title]')\ntitles\n</pre> # Find all detected titles titles = page.find_all('region[type=title]') titles Out[6]: <pre>&lt;ElementCollection[Region](count=2)&gt;</pre> In\u00a0[7]: Copied! <pre>titles.show()\n</pre> titles.show() Out[7]: In\u00a0[8]: Copied! <pre>page.find_all('region[type=table]').show()\n</pre> page.find_all('region[type=table]').show() Out[8]: In\u00a0[9]: Copied! <pre>page.find('region[type=table]').extract_text(layout=True)\n</pre> page.find('region[type=table]').extract_text(layout=True) Out[9]: <pre>'Statute Description Level Repeat? 4.12.7 Unsanitary Working Conditions. Critical 5.8.3 Inadequate Protective Equipment. Serious 6.3.9 Ineffective Injury Prevention. Serious 7.1.5 Failure to Properly Store Hazardous Materials. Critical 8.9.2 Lack of Adequate Fire Safety Measures. Serious 9.6.4 Inadequate Ventilation Systems. Serious 10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[10]: Copied! <pre>page.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"paddle\")\npage.find_all('region[model=paddle]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"paddle\") page.find_all('region[model=paddle]').highlight(group_by='region_type') page.to_image(width=700) Out[10]: In\u00a0[11]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[11]: In\u00a0[12]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"docling\")\npage.find_all('region[model=docling]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"docling\") page.find_all('region[model=docling]').highlight(group_by='region_type') page.to_image(width=700) Out[12]: In\u00a0[13]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"surya\")\npage.find_all('region[model=surya]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"surya\") page.find_all('region[model=surya]').highlight(group_by='region_type') page.to_image(width=700) <pre>Loaded layout model s3://layout/2025_02_18 on device cpu with dtype torch.float16\n</pre> <pre>\rRecognizing layout:   0%|                                                   | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:08&lt;00:00,  8.59s/it]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:08&lt;00:00,  8.59s/it]</pre> <pre>\n</pre> Out[13]: <p>Note: Calling <code>analyze_layout</code> multiple times (even with the same engine) can add duplicate regions. You might want to use <code>page.clear_detected_layout_regions()</code> first, or filter by model using <code>region[model=yolo]</code>.</p> In\u00a0[14]: Copied! <pre># Re-run YOLO analysis (clearing previous results might be good practice)\npage.clear_detected_layout_regions()\npage.analyze_layout(engine=\"yolo\")\n\n# Find only high-confidence regions (e.g., &gt;= 0.8)\nhigh_conf_regions = page.find_all('region[confidence&gt;=0.8]')\nlen(high_conf_regions)\n</pre> # Re-run YOLO analysis (clearing previous results might be good practice) page.clear_detected_layout_regions() page.analyze_layout(engine=\"yolo\")  # Find only high-confidence regions (e.g., &gt;= 0.8) high_conf_regions = page.find_all('region[confidence&gt;=0.8]') len(high_conf_regions) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpgr5id04j/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1478.1ms\n</pre> <pre>Speed: 4.7ms preprocess, 1478.1ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre># Ensure TATR analysis has been run\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Ensure TATR analysis has been run page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[15]: In\u00a0[16]: Copied! <pre># Find different structural elements from TATR\ntables = page.find_all('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\n\nf\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\"\n</pre> # Find different structural elements from TATR tables = page.find_all('region[type=table][model=tatr]') rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]')  f\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\" Out[16]: <pre>'Found: 2 tables, 8 rows, 4 columns, 1 headers (from TATR)'</pre> In\u00a0[17]: Copied! <pre># Find the TATR table region again\ntatr_table = page.find('region[type=table][model=tatr]')\n\n# This extraction uses the detected rows/columns\ntatr_table.extract_table()\n</pre> # Find the TATR table region again tatr_table = page.find('region[type=table][model=tatr]')  # This extraction uses the detected rows/columns tatr_table.extract_table() Out[17]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>if you'd like the normal approach instead of the \"intelligent\" one, you can ask for pdfplumber.</p> In\u00a0[18]: Copied! <pre># This extraction uses the detected rows/columns\ntatr_table.extract_table(method='pdfplumber')\n</pre> # This extraction uses the detected rows/columns tatr_table.extract_table(method='pdfplumber') Out[18]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre>"},{"location":"layout-analysis/#document-layout-analysis","title":"Document Layout Analysis\u00b6","text":"<p>Natural PDF can automatically detect the structure of a document (titles, paragraphs, tables, figures) using layout analysis models. This guide shows how to use this feature.</p>"},{"location":"layout-analysis/#setup","title":"Setup\u00b6","text":"<p>We'll use a sample PDF that includes various layout elements.</p>"},{"location":"layout-analysis/#running-basic-layout-analysis","title":"Running Basic Layout Analysis\u00b6","text":"<p>Use the <code>analyze_layout()</code> method. By default, it uses the YOLO model.</p>"},{"location":"layout-analysis/#visualizing-detected-layout","title":"Visualizing Detected Layout\u00b6","text":"<p>Use <code>highlight()</code> or <code>show()</code> on the detected regions.</p>"},{"location":"layout-analysis/#finding-specific-region-types","title":"Finding Specific Region Types\u00b6","text":"<p>Use attribute selectors to find regions of a specific type.</p>"},{"location":"layout-analysis/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>Detected regions are like any other <code>Region</code> object. You can extract text, find elements within them, etc.</p>"},{"location":"layout-analysis/#using-different-layout-models","title":"Using Different Layout Models\u00b6","text":"<p>Natural PDF supports multiple engines (<code>yolo</code>, <code>paddle</code>, <code>tatr</code>). Specify the engine when calling <code>analyze_layout</code>.</p> <p>Note: Using different engines requires installing the corresponding extras (e.g., <code>natural-pdf[layout_paddle]</code>). <code>yolo</code> is the default.</p>"},{"location":"layout-analysis/#controlling-confidence-threshold","title":"Controlling Confidence Threshold\u00b6","text":"<p>Filter detections by their confidence score.</p>"},{"location":"layout-analysis/#table-structure-with-tatr","title":"Table Structure with TATR\u00b6","text":"<p>The TATR engine provides detailed table structure elements (<code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>). This is very useful for precise table extraction.</p>"},{"location":"layout-analysis/#enhanced-table-extraction-with-tatr","title":"Enhanced Table Extraction with TATR\u00b6","text":"<p>When a <code>region[type=table]</code> comes from the TATR model, <code>extract_table()</code> can use the underlying row/column structure for more robust extraction.</p>"},{"location":"layout-analysis/#next-steps","title":"Next Steps\u00b6","text":"<p>Layout analysis provides regions that you can use for:</p> <ul> <li>Table Extraction: Especially powerful with TATR regions.</li> <li>Text Extraction: Extract text only from specific region types (e.g., paragraphs).</li> <li>Document QA: Focus question answering on specific detected regions.</li> </ul>"},{"location":"ocr/","title":"OCR Integration","text":"<p>Natural PDF includes OCR (Optical Character Recognition) to extract text from scanned documents or images embedded in PDFs.</p>"},{"location":"ocr/#ocr-engine-comparison","title":"OCR Engine Comparison","text":"<p>Natural PDF supports multiple OCR engines:</p> Feature EasyOCR PaddleOCR Surya OCR Gemini (Layout + potential OCR) Installation <code>natural-pdf[easyocr]</code> <code>natural-pdf[paddle]</code> <code>natural-pdf[surya]</code> <code>natural-pdf[gemini]</code> Primary Strength Good general performance, simpler Excellent Asian language, speed High accuracy, multilingual lines Advanced layout analysis (via API) Speed Moderate Fast Moderate (GPU recommended) API Latency Memory Usage Higher Efficient Higher (GPU recommended) N/A (API) Paragraph Detect Yes (via option) No No (focuses on lines) Yes (Layout model) Handwritten Better support Limited Limited Potentially (API model dependent) Small Text Moderate Good Good Potentially (API model dependent) When to Use General documents, handwritten text Asian languages, speed-critical tasks Highest accuracy needed, line-level Complex layouts, API integration"},{"location":"ocr/#basic-ocr-usage","title":"Basic OCR Usage","text":"<p>Apply OCR directly to a page or region:</p> <pre><code>from natural_pdf import PDF\n\n# Assume 'page' is a Page object from a PDF\npage = pdf.pages[0]\n\n# Apply OCR using the default engine (or specify one)\nocr_elements = page.apply_ocr(languages=['en'])\n\n# Extract text (will use the results from apply_ocr if run previously)\ntext = page.extract_text()\nprint(text)\n</code></pre>"},{"location":"ocr/#configuring-ocr","title":"Configuring OCR","text":"<p>Specify the engine and basic options directly:</p>"},{"location":"ocr/#ocr-configuration","title":"OCR Configuration","text":"<pre><code># Use PaddleOCR for Chinese and English\nocr_elements = page.apply_ocr(engine='paddle', languages=['zh-cn', 'en'])\n\n# Use EasyOCR with a lower confidence threshold\nocr_elements = page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.3)\n</code></pre> <p>For advanced, engine-specific settings, use the Options classes:</p> <pre><code>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\nfrom natural_pdf.analyzers.layout import GeminiOptions # Note: Gemini is primarily layout\n\n# --- Configure PaddleOCR ---\npaddle_opts = PaddleOCROptions(\n    languages=['en', 'zh-cn'],\n    use_gpu=True,         # Explicitly enable GPU if available\n    use_angle_cls=False,  # Disable text direction classification (if text is upright)\n    det_db_thresh=0.25,   # Lower detection threshold (more boxes, potentially noisy)\n    rec_batch_num=16      # Increase recognition batch size for potential speedup on GPU\n    # rec_char_dict_path='/path/to/custom_dict.txt' # Optional: Path to a custom character dictionary\n    # See PaddleOCROptions documentation or source code for all parameters\n )\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# --- Configure EasyOCR ---\neasy_opts = EasyOCROptions(\n    languages=['en', 'fr'],\n    gpu=True,            # Explicitly enable GPU if available\n    paragraph=True,      # Group results into paragraphs (if structure is clear)\n    detail=1,            # Ensure bounding boxes are returned (required)\n    text_threshold=0.6,  # Confidence threshold for text detection (adjust based on tuning table)\n    link_threshold=0.4,  # Standard EasyOCR param, uncomment if confirmed in wrapper\n    low_text=0.4,        # Standard EasyOCR param, uncomment if confirmed in wrapper\n    batch_size=8         # Processing batch size (adjust based on memory)\n    # See EasyOCROptions documentation or source code for all parameters\n )\nocr_elements = page.apply_ocr(engine='easyocr', options=easy_opts)\n\n# --- Configure Surya OCR ---\n# Surya focuses on line detection and recognition\nsurya_opts = SuryaOCROptions(\n    languages=['en', 'de'], # Specify languages for recognition\n    # device='cuda',       # Use GPU ('cuda') or CPU ('cpu') &lt;-- Set via env var TORCH_DEVICE\n    min_confidence=0.4   # Example: Adjust minimum confidence for results\n    # Core Surya options like device, batch size, and thresholds are typically\n    # set via environment variables (see note below).\n)\nocr_elements = page.apply_ocr(engine='surya', options=surya_opts)\n\n# --- Configure Gemini (as layout analyzer, can be used with OCR) ---\n# Gemini requires API key (GOOGLE_API_KEY environment variable)\n# Note: Gemini is used via apply_layout, but its options can influence OCR if used together\ngemini_opts = GeminiOptions(\n    prompt=\"Extract text content and identify document elements.\",\n    # model_name=\"gemini-1.5-flash-latest\" # Specify a model if needed\n    # See GeminiOptions documentation for more parameters\n)\n# Typically used like this (layout first, then potentially OCR on regions)\nlayout_elements = page.apply_layout(engine='gemini', options=gemini_opts)\n# If Gemini also performed OCR or you want to OCR layout regions:\n# ocr_elements = some_region.apply_ocr(...)\n\n# It can sometimes be used directly if the model supports it, but less common:\n# try:\n#     ocr_elements = page.apply_ocr(engine='gemini', options=gemini_opts)\n# except Exception as e:\n#     print(f\"Gemini might not be configured for direct OCR via apply_ocr: {e}\")\n</code></pre>"},{"location":"ocr/#applying-ocr-directly","title":"Applying OCR Directly","text":"<p>The <code>page.apply_ocr(...)</code> and <code>region.apply_ocr(...)</code> methods are the primary way to run OCR:</p> <pre><code># Apply OCR to a page and get the OCR elements\nocr_elements = page.apply_ocr(engine='easyocr')\nprint(f\"Found {len(ocr_elements)} text elements via OCR\")\n\n# Apply OCR to a specific region\ntitle = page.find('text:contains(\"Title\")')\ncontent_region = title.below(height=300)\nregion_ocr_elements = content_region.apply_ocr(engine='paddle', languages=['en'])\n\n# Note: Re-applying OCR to the same page or region will remove any\n# previously generated OCR elements for that area before adding the new ones.\n</code></pre>"},{"location":"ocr/#ocr-engines","title":"OCR Engines","text":"<p>Choose the engine best suited for your document and language requirements using the <code>engine</code> parameter in <code>apply_ocr</code>.</p>"},{"location":"ocr/#finding-and-working-with-ocr-text","title":"Finding and Working with OCR Text","text":"<p>After applying OCR, work with the text just like regular text:</p> <pre><code># Find all OCR text elements\nocr_text = page.find_all('text[source=ocr]')\n\n# Find high-confidence OCR text\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\n\n# Extract text only from OCR elements\nocr_text_content = page.find_all('text[source=ocr]').extract_text()\n\n# Filter OCR text by content\nnames = page.find_all('text[source=ocr]:contains(\"Smith\")', case=False)\n</code></pre>"},{"location":"ocr/#visualizing-ocr-results","title":"Visualizing OCR Results","text":"<p>See OCR results to help debug issues:</p> <pre><code># Apply OCR \nocr_elements = page.apply_ocr()\n\n# Highlight all OCR elements\nfor element in ocr_elements:\n    # Color based on confidence\n    if element.confidence &gt;= 0.8:\n        color = \"green\"  # High confidence\n    elif element.confidence &gt;= 0.5:\n        color = \"yellow\"  # Medium confidence\n    else:\n        color = \"red\"  # Low confidence\n\n    element.highlight(color=color, label=f\"OCR ({element.confidence:.2f})\")\n\n# Get the visualization as an image\nimage = page.to_image(labels=True)\n# Just return the image in a Jupyter cell\nimage\n\n# Highlight only high-confidence elements\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nhigh_conf.highlight(color=\"green\", label=\"High Confidence OCR\")\n</code></pre>"},{"location":"ocr/#detect-llm-ocr","title":"Detect + LLM OCR","text":"<p>Sometimes you have a difficult piece of content where you need to use a local model to identify the content, then send it off in pieces to be identified by the LLM. You can do this with Natural PDF!</p> <pre><code>from natural_pdf import PDF\nfrom natural_pdf.ocr.utils import direct_ocr_llm\nimport openai\n\npdf = PDF(\"needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Detect\npage.apply_ocr('paddle', resolution=120, detect_only=True)\n\n# Build the framework\nclient = openai.OpenAI(base_url=\"https://api.anthropic.com/v1/\",  api_key='sk-XXXXX')\nprompt = \"\"\"OCR this image. Return only the exact text from the image. Include misspellings,\npunctuation, etc. Do not surround it with quotation marks. Do not include translations or comments.\nThe text is from a Greek spreadsheet, so most likely content is Modern Greek or numeric.\"\"\"\n\n# This returns the cleaned-up text\ndef correct(region):\n    return direct_ocr_llm(region, client, prompt=prompt, resolution=300, model=\"claude-3-5-haiku-20241022\")\n\n# Run 'correct' on each text element\npage.correct_ocr(correct)\n\n# You're done!\n</code></pre>"},{"location":"ocr/#interactive-ocr-correction-debugging","title":"Interactive OCR Correction / Debugging","text":"<p>Natural PDF includes a utility to package a PDF and its detected elements, along with an interactive web application (SPA) for reviewing and correcting OCR results.</p> <ol> <li> <p>Package the data:     Use the <code>create_correction_task_package</code> function to create a zip file containing the necessary data for the SPA.</p> <pre><code>from natural_pdf.utils.packaging import create_correction_task_package\n\n# Assuming 'pdf' is your loaded PDF object after running apply_ocr or apply_layout\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</code></pre> </li> <li> <p>Run the SPA:     The correction SPA is bundled with the library. You need to run a simple web server from the directory containing the SPA's files. The location of these files might depend on your installation, but you can typically find them within the installed <code>natural_pdf</code> package directory under <code>templates/spa</code>.</p> <p>Example using Python's built-in server (run from your terminal):</p> <pre><code># Find the path to the installed natural_pdf package\n# (This command might vary depending on your environment)\nNATURAL_PDF_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/natural_pdf\n\n# Navigate to the SPA directory\ncd $NATURAL_PDF_PATH/templates/spa\n\n# Start the web server (e.g., on port 8000)\npython -m http.server 8000\n</code></pre> </li> <li> <p>Use the SPA:     Open your web browser to <code>http://localhost:8000</code>. The SPA should load, allowing you to drag and drop the <code>correction_package.zip</code> file you created into the application to view and edit the OCR results.</p> </li> </ol>"},{"location":"ocr/#next-steps","title":"Next Steps","text":"<p>With OCR capabilities, you can explore:</p> <ul> <li>Layout Analysis for automatically detecting document structure</li> <li>Document QA for asking questions about your documents</li> <li>Visual Debugging for visualizing OCR results</li> </ul>"},{"location":"pdf-navigation/","title":"PDF Navigation","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Open a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n</pre> from natural_pdf import PDF  # Open a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\") In\u00a0[2]: Copied! <pre># Get the total number of pages\nnum_pages = len(pdf)\nprint(f\"This PDF has {num_pages} pages\")\n\n# Get a specific page (0-indexed)\nfirst_page = pdf.pages[0]\nlast_page = pdf.pages[-1]\n\n# Iterate through the first 20 pages\nfor page in pdf.pages[:20]:\n    print(f\"Page {page.number} has {len(page.extract_text())} characters\")\n</pre> # Get the total number of pages num_pages = len(pdf) print(f\"This PDF has {num_pages} pages\")  # Get a specific page (0-indexed) first_page = pdf.pages[0] last_page = pdf.pages[-1]  # Iterate through the first 20 pages for page in pdf.pages[:20]:     print(f\"Page {page.number} has {len(page.extract_text())} characters\") <pre>This PDF has 153 pages\nPage 1 has 985 characters\nPage 2 has 778 characters\nPage 3 has 522 characters\nPage 4 has 984 characters\nPage 5 has 778 characters\nPage 6 has 523 characters\n</pre> <pre>Page 7 has 982 characters\nPage 8 has 772 characters\nPage 9 has 522 characters\nPage 10 has 1008 characters\n</pre> <pre>Page 11 has 796 characters\nPage 12 has 532 characters\nPage 13 has 986 characters\nPage 14 has 780 characters\nPage 15 has 523 characters\nPage 16 has 990 characters\nPage 17 has 782 characters\n</pre> <pre>Page 18 has 520 characters\nPage 19 has 1006 characters\nPage 20 has 795 characters\n</pre> In\u00a0[3]: Copied! <pre># Page dimensions in points (1/72 inch)\nprint(page.width, page.height)\n\n# Page number (1-indexed as shown in PDF viewers)\nprint(page.number)\n\n# Page index (0-indexed position in the PDF)\nprint(page.index)\n</pre> # Page dimensions in points (1/72 inch) print(page.width, page.height)  # Page number (1-indexed as shown in PDF viewers) print(page.number)  # Page index (0-indexed position in the PDF) print(page.index) <pre>612 792\n20\n19\n</pre> In\u00a0[4]: Copied! <pre># Extract text from all pages\nall_text = pdf.extract_text()\n\n# Find elements across all pages\nall_headings = pdf.find_all('text[size&gt;=14]:bold')\n\n# Add exclusion zones to all pages (like headers/footers)\npdf.add_exclusion(\n    lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n    label=\"header\"\n)\n</pre> # Extract text from all pages all_text = pdf.extract_text()  # Find elements across all pages all_headings = pdf.find_all('text[size&gt;=14]:bold')  # Add exclusion zones to all pages (like headers/footers) pdf.add_exclusion(     lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,     label=\"header\" ) Out[4]: <pre>&lt;natural_pdf.core.pdf.PDF at 0x1045224d0&gt;</pre> In\u00a0[5]: Copied! <pre># Extract text from specific pages\ntext = pdf.pages[2:5].extract_text()\n\n# Find elements across specific pages\nelements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")')\n</pre> # Extract text from specific pages text = pdf.pages[2:5].extract_text()  # Find elements across specific pages elements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")') In\u00a0[6]: Copied! <pre># Get sections with headings as section starts\nsections = pdf.pages.get_sections(\n    start_elements='text[size&gt;=14]:bold',\n    new_section_on_page_break=False\n)\n</pre> # Get sections with headings as section starts sections = pdf.pages.get_sections(     start_elements='text[size&gt;=14]:bold',     new_section_on_page_break=False )"},{"location":"pdf-navigation/#pdf-navigation","title":"PDF Navigation\u00b6","text":"<p>This guide covers the basics of working with PDFs in Natural PDF - opening documents, accessing pages, and navigating through content.</p>"},{"location":"pdf-navigation/#opening-a-pdf","title":"Opening a PDF\u00b6","text":"<p>The main entry point to Natural PDF is the <code>PDF</code> class:</p>"},{"location":"pdf-navigation/#accessing-pages","title":"Accessing Pages\u00b6","text":"<p>Once you have a PDF object, you can access its pages:</p>"},{"location":"pdf-navigation/#page-properties","title":"Page Properties\u00b6","text":"<p>Each <code>Page</code> object has useful properties:</p>"},{"location":"pdf-navigation/#working-across-pages","title":"Working Across Pages\u00b6","text":"<p>Natural PDF makes it easy to work with content across multiple pages:</p>"},{"location":"pdf-navigation/#the-page-collection","title":"The Page Collection\u00b6","text":"<p>The <code>pdf.pages</code> object is a <code>PageCollection</code> that allows batch operations on pages:</p>"},{"location":"pdf-navigation/#document-sections-across-pages","title":"Document Sections Across Pages\u00b6","text":"<p>You can extract sections that span across multiple pages:</p>"},{"location":"pdf-navigation/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to navigate PDFs, you can:</p> <ul> <li>Find elements using selectors</li> <li>Extract text from your documents</li> <li>Work with specific regions</li> </ul>"},{"location":"regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page page = pdf.pages[0]  # Display the page page.show(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Create a region by specifying (x0, top, x1, bottom) coordinates\n# Let's create a region in the middle of the page\nmid_region = page.create_region(\n    x0=100,         # Left edge\n    top=200,        # Top edge\n    x1=500,         # Right edge\n    bottom=400      # Bottom edge\n)\n\n# Highlight the region to see it\nmid_region.highlight(color=\"blue\").show()\n</pre> # Create a region by specifying (x0, top, x1, bottom) coordinates # Let's create a region in the middle of the page mid_region = page.create_region(     x0=100,         # Left edge     top=200,        # Top edge     x1=500,         # Right edge     bottom=400      # Bottom edge )  # Highlight the region to see it mid_region.highlight(color=\"blue\").show() Out[2]: In\u00a0[3]: Copied! <pre># Find a heading-like element\nheading = page.find('text[size&gt;=12]:bold')\n\n# Create a region below this heading element\nif heading:\n    region_below = heading.below()\n    \n    # Highlight the heading and the region below it\n    heading.highlight(color=\"red\")\n    region_below.highlight(color=\"blue\")\n    page.show()\n</pre> # Find a heading-like element heading = page.find('text[size&gt;=12]:bold')  # Create a region below this heading element if heading:     region_below = heading.below()          # Highlight the heading and the region below it     heading.highlight(color=\"red\")     region_below.highlight(color=\"blue\")     page.show() In\u00a0[4]: Copied! <pre># Create a region with height limit\nif heading:\n    # Only include 100px below the heading\n    small_region_below = heading.below(height=100)\n    \n    page.clear_highlights()\n    heading.highlight(color=\"red\")\n    small_region_below.highlight(color=\"green\")\n    page.show()\n</pre> # Create a region with height limit if heading:     # Only include 100px below the heading     small_region_below = heading.below(height=100)          page.clear_highlights()     heading.highlight(color=\"red\")     small_region_below.highlight(color=\"green\")     page.show() In\u00a0[5]: Copied! <pre># Find a line or other element to create a region above\nline = page.find('line')\nif line:\n    # Create a region above the line\n    region_above = line.above()\n    \n    page.clear_highlights()\n    line.highlight(color=\"black\")\n    region_above.highlight(color=\"purple\")\n    page.show()\n</pre> # Find a line or other element to create a region above line = page.find('line') if line:     # Create a region above the line     region_above = line.above()          page.clear_highlights()     line.highlight(color=\"black\")     region_above.highlight(color=\"purple\")     page.show() In\u00a0[6]: Copied! <pre># Find two elements to use as boundaries\nfirst_heading = page.find('text[size&gt;=11]:bold')\nnext_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None\n\nif first_heading and next_heading:\n    # Create a region from the first heading until the next heading\n    section = first_heading.below(until=next_heading, include_endpoint=False)\n    \n    # Highlight both elements and the region between them\n    page.clear_highlights()\n    first_heading.highlight(color=\"red\")\n    next_heading.highlight(color=\"red\")\n    section.highlight(color=\"yellow\")\n    page.show()\n</pre> # Find two elements to use as boundaries first_heading = page.find('text[size&gt;=11]:bold') next_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None  if first_heading and next_heading:     # Create a region from the first heading until the next heading     section = first_heading.below(until=next_heading, include_endpoint=False)          # Highlight both elements and the region between them     page.clear_highlights()     first_heading.highlight(color=\"red\")     next_heading.highlight(color=\"red\")     section.highlight(color=\"yellow\")     page.show() In\u00a0[7]: Copied! <pre># Find a region to work with (e.g., from a title to the next bold text)\ntitle = page.find('text:contains(\"Site\")')  # Adjust if needed\nif title:\n    # Create a region from title down to the next bold text\n    content_region = title.below(until='line:horizontal', include_endpoint=False)\n    \n    # Extract text from just this region\n    region_text = content_region.extract_text()\n    \n    # Show the region and the extracted text\n    page.clear_highlights()\n    content_region.highlight(color=\"green\")\n    page.show()\n    \n    # Displaying the text (first 300 chars if long)\n    print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text)\n</pre> # Find a region to work with (e.g., from a title to the next bold text) title = page.find('text:contains(\"Site\")')  # Adjust if needed if title:     # Create a region from title down to the next bold text     content_region = title.below(until='line:horizontal', include_endpoint=False)          # Extract text from just this region     region_text = content_region.extract_text()          # Show the region and the extracted text     page.clear_highlights()     content_region.highlight(color=\"green\")     page.show()          # Displaying the text (first 300 chars if long)     print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text) <pre>Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the othe...\n</pre> In\u00a0[8]: Copied! <pre># Create a region in an interesting part of the page\ntest_region = page.create_region(\n    x0=page.width * 0.1, \n    top=page.height * 0.25, \n    x1=page.width * 0.9, \n    bottom=page.height * 0.75\n)\n\n# Find all text elements ONLY within this region\ntext_in_region = test_region.find_all('text')\n\n# Display result\npage.clear_highlights()\ntest_region.highlight(color=\"blue\")\ntext_in_region.highlight(color=\"red\")\npage.show()\n\nlen(text_in_region)  # Number of text elements found in region\n</pre> # Create a region in an interesting part of the page test_region = page.create_region(     x0=page.width * 0.1,      top=page.height * 0.25,      x1=page.width * 0.9,      bottom=page.height * 0.75 )  # Find all text elements ONLY within this region text_in_region = test_region.find_all('text')  # Display result page.clear_highlights() test_region.highlight(color=\"blue\") text_in_region.highlight(color=\"red\") page.show()  len(text_in_region)  # Number of text elements found in region Out[8]: <pre>29</pre> In\u00a0[9]: Copied! <pre># Find a specific region to capture\n# (Could be a table, figure, or any significant area)\nregion_for_image = page.create_region(\n    x0=100, \n    top=150,\n    x1=page.width - 100,\n    bottom=300\n)\n\n# Generate an image of just this region\nregion_for_image.to_image(crop_only=True)  # Shows just the region\n</pre> # Find a specific region to capture # (Could be a table, figure, or any significant area) region_for_image = page.create_region(     x0=100,      top=150,     x1=page.width - 100,     bottom=300 )  # Generate an image of just this region region_for_image.to_image(crop_only=True)  # Shows just the region Out[9]: In\u00a0[10]: Copied! <pre># Take an existing region and expand it\nregion_a = page.create_region(200, 200, 400, 400)\n\n# Expand by a certain number of points in each direction\nexpanded = region_a.expand(left=20, right=20, top=20, bottom=20)\n\n# Visualize original and expanded regions\npage.clear_highlights()\nregion_a.highlight(color=\"blue\", label=\"Original\")\nexpanded.highlight(color=\"red\", label=\"Expanded\")\npage.to_image()\n</pre> # Take an existing region and expand it region_a = page.create_region(200, 200, 400, 400)  # Expand by a certain number of points in each direction expanded = region_a.expand(left=20, right=20, top=20, bottom=20)  # Visualize original and expanded regions page.clear_highlights() region_a.highlight(color=\"blue\", label=\"Original\") expanded.highlight(color=\"red\", label=\"Expanded\") page.to_image() Out[10]: In\u00a0[11]: Copied! <pre># Create a region for the whole page\nfull_page_region = page.create_region(0, 0, page.width, page.height)\n\n# Extract text without exclusions as baseline\nfull_text = full_page_region.extract_text()\nprint(f\"Full page text length: {len(full_text)} characters\")\n</pre> # Create a region for the whole page full_page_region = page.create_region(0, 0, page.width, page.height)  # Extract text without exclusions as baseline full_text = full_page_region.extract_text() print(f\"Full page text length: {len(full_text)} characters\") <pre>Full page text length: 1262 characters\n</pre> In\u00a0[12]: Copied! <pre># Define an area we want to exclude (like a header)\n# Let's exclude the top 10% of the page\nheader_zone = page.create_region(0, 0, page.width, page.height * 0.1)\n\n# Add this as an exclusion for the page\npage.add_exclusion(header_zone)\n\n# Visualize the exclusion\npage.clear_highlights()\nheader_zone.highlight(color=\"red\", label=\"Excluded\")\npage.show()\n</pre> # Define an area we want to exclude (like a header) # Let's exclude the top 10% of the page header_zone = page.create_region(0, 0, page.width, page.height * 0.1)  # Add this as an exclusion for the page page.add_exclusion(header_zone)  # Visualize the exclusion page.clear_highlights() header_zone.highlight(color=\"red\", label=\"Excluded\") page.show() Out[12]: In\u00a0[13]: Copied! <pre># Now extract text again - the header should be excluded\ntext_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default\n\n# Compare text lengths\nprint(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\")\nprint(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\")\n</pre> # Now extract text again - the header should be excluded text_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default  # Compare text lengths print(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\") print(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\") <pre>Original text: 1262 chars\nText with exclusion: 1198 chars\nDifference: 64 chars excluded\n</pre> In\u00a0[14]: Copied! <pre># When done with this page, clear exclusions\npage.clear_exclusions()\n</pre> # When done with this page, clear exclusions page.clear_exclusions() Out[14]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[15]: Copied! <pre># Define a PDF-level exclusion for headers\n# This will exclude the top 30% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.3),\n    label=\"Header zone\"\n)\n\n# Define a PDF-level exclusion for footers\n# This will exclude the bottom 20% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),\n    label=\"Footer zone\"\n)\n\n# PDF-level exclusions are used whenever you extract text\n# Let's try on the first three pages\nfor page in pdf.pages[:3]:\n    text = page.extract_text()\n    text_original = page.extract_text(use_exclusions=False)\n    print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\")\n</pre> # Define a PDF-level exclusion for headers # This will exclude the top 30% of every page pdf.add_exclusion(     lambda p: p.create_region(0, 0, p.width, p.height * 0.3),     label=\"Header zone\" )  # Define a PDF-level exclusion for footers # This will exclude the bottom 20% of every page pdf.add_exclusion(     lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),     label=\"Footer zone\" )  # PDF-level exclusions are used whenever you extract text # Let's try on the first three pages for page in pdf.pages[:3]:     text = page.extract_text()     text_original = page.extract_text(use_exclusions=False)     print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\") <pre>Page 1 \u2013 Before: 1260 After: 456\n</pre> In\u00a0[16]: Copied! <pre># Clear PDF-level exclusions when done\npdf.clear_exclusions()\nprint(\"Cleared all PDF-level exclusions\")\n</pre> # Clear PDF-level exclusions when done pdf.clear_exclusions() print(\"Cleared all PDF-level exclusions\") <pre>Cleared all PDF-level exclusions\n</pre> In\u00a0[17]: Copied! <pre># First, run layout analysis to detect regions\npage.analyze_layout()  # Uses 'yolo' engine by default\n\n# Find all detected regions\ndetected_regions = page.find_all('region')\nprint(f\"Found {len(detected_regions)} layout regions\")\n</pre> # First, run layout analysis to detect regions page.analyze_layout()  # Uses 'yolo' engine by default  # Find all detected regions detected_regions = page.find_all('region') print(f\"Found {len(detected_regions)} layout regions\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpw8v45ahq/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1574.9ms\n</pre> <pre>Speed: 4.1ms preprocess, 1574.9ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Found 8 layout regions\n</pre> In\u00a0[18]: Copied! <pre># Highlight all detected regions by type\ndetected_regions.highlight(group_by='region_type').show()\n</pre> # Highlight all detected regions by type detected_regions.highlight(group_by='region_type').show() Out[18]: In\u00a0[19]: Copied! <pre># Extract text from a specific region type (e.g., title)\ntitle_regions = page.find_all('region[type=title]')\nif title_regions:\n    titles_text = title_regions.extract_text()\n    print(f\"Title text: {titles_text}\")\n</pre> # Extract text from a specific region type (e.g., title) title_regions = page.find_all('region[type=title]') if title_regions:     titles_text = title_regions.extract_text()     print(f\"Title text: {titles_text}\") <pre>Title text: Violation Count:  7 Violations\n</pre>"},{"location":"regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that define boundaries for operations like text extraction, element finding, or visualization. They're one of Natural PDF's most powerful features for working with specific parts of a document.</p>"},{"location":"regions/#setup","title":"Setup\u00b6","text":"<p>Let's set up a PDF to experiment with regions.</p>"},{"location":"regions/#creating-regions","title":"Creating Regions\u00b6","text":"<p>There are several ways to create regions in Natural PDF.</p>"},{"location":"regions/#using-create_region-with-coordinates","title":"Using <code>create_region()</code> with Coordinates\u00b6","text":"<p>This is the most direct method - provide the coordinates directly.</p>"},{"location":"regions/#using-element-methods-above-below-left-right","title":"Using Element Methods: <code>above()</code>, <code>below()</code>, <code>left()</code>, <code>right()</code>\u00b6","text":"<p>You can create regions relative to existing elements.</p>"},{"location":"regions/#creating-a-region-between-elements-with-until","title":"Creating a Region Between Elements with <code>until()</code>\u00b6","text":""},{"location":"regions/#using-regions","title":"Using Regions\u00b6","text":"<p>Once you have a region, here's what you can do with it.</p>"},{"location":"regions/#extract-text-from-a-region","title":"Extract Text from a Region\u00b6","text":""},{"location":"regions/#find-elements-within-a-region","title":"Find Elements Within a Region\u00b6","text":"<p>You can use a region as a \"filter\" to only find elements within its boundaries.</p>"},{"location":"regions/#generate-an-image-of-a-region","title":"Generate an Image of a Region\u00b6","text":""},{"location":"regions/#adjust-and-expand-regions","title":"Adjust and Expand Regions\u00b6","text":""},{"location":"regions/#using-exclusion-zones-with-regions","title":"Using Exclusion Zones with Regions\u00b6","text":"<p>Exclusion zones are regions that you want to ignore during operations like text extraction.</p>"},{"location":"regions/#document-level-exclusions","title":"Document-Level Exclusions\u00b6","text":"<p>PDF-level exclusions apply to all pages and use functions to adapt to each page.</p>"},{"location":"regions/#working-with-layout-analysis-regions","title":"Working with Layout Analysis Regions\u00b6","text":"<p>When you run layout analysis, the detected regions (tables, titles, etc.) are also Region objects.</p>"},{"location":"regions/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you understand regions, you can:</p> <ul> <li>Extract tables from table regions</li> <li>Ask questions about specific regions</li> <li>Exclude content from extraction</li> </ul>"},{"location":"tables/","title":"Table Extraction","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() Out[1]: In\u00a0[2]: Copied! <pre># Extract the first table found on the page using pdfplumber\n# This works best for simple tables with clear lines\ntable_data = page.extract_table() # Returns a list of lists\ntable_data\n</pre> # Extract the first table found on the page using pdfplumber # This works best for simple tables with clear lines table_data = page.extract_table() # Returns a list of lists table_data Out[2]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>This might fail or give poor results if there are multiple tables or the table structure is complex.</p> In\u00a0[3]: Copied! <pre># Detect layout elements using YOLO (default)\npage.analyze_layout(engine='yolo')\n\n# Find regions detected as tables\ntable_regions_yolo = page.find_all('region[type=table][model=yolo]')\ntable_regions_yolo.show()\n</pre> # Detect layout elements using YOLO (default) page.analyze_layout(engine='yolo')  # Find regions detected as tables table_regions_yolo = page.find_all('region[type=table][model=yolo]') table_regions_yolo.show() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmptrpq_4cf/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 2941.3ms\n</pre> <pre>Speed: 7.3ms preprocess, 2941.3ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[3]: In\u00a0[4]: Copied! <pre>table_regions_yolo[0].extract_table()\n</pre> table_regions_yolo[0].extract_table() Out[4]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[5]: Copied! <pre>page.clear_detected_layout_regions() # Clear previous YOLO regions for clarity\npage.analyze_layout(engine='tatr')\n</pre> page.clear_detected_layout_regions() # Clear previous YOLO regions for clarity page.analyze_layout(engine='tatr') Out[5]: <pre>&lt;ElementCollection[Region](count=15)&gt;</pre> In\u00a0[6]: Copied! <pre># Find the main table region(s) detected by TATR\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.show()\n</pre> # Find the main table region(s) detected by TATR tatr_table = page.find('region[type=table][model=tatr]') tatr_table.show() Out[6]: In\u00a0[7]: Copied! <pre># Find rows, columns, headers detected by TATR\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\nf\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\"\n</pre> # Find rows, columns, headers detected by TATR rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]') f\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\" Out[7]: <pre>'TATR found: 8 rows, 4 columns, 1 headers'</pre> In\u00a0[8]: Copied! <pre>tatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.extract_table(method='tatr')\n</pre> tatr_table = page.find('region[type=table][model=tatr]') tatr_table.extract_table(method='tatr') Out[8]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[9]: Copied! <pre># Force using pdfplumber even on a TATR-detected region\n# (Might be useful for comparison or if TATR structure is flawed)\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.extract_table(method='pdfplumber')\n</pre> # Force using pdfplumber even on a TATR-detected region # (Might be useful for comparison or if TATR structure is flawed) tatr_table = page.find('region[type=table][model=tatr]') tatr_table.extract_table(method='pdfplumber') Out[9]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre> In\u00a0[10]: Copied! <pre># Example: Use text alignment for vertical lines, explicit lines for horizontal\n# See pdfplumber documentation for all settings\ntable_settings = {\n    \"vertical_strategy\": \"text\",\n    \"horizontal_strategy\": \"lines\",\n    \"intersection_x_tolerance\": 5, # Increase tolerance for intersections\n}\n\nresults = page.extract_table(\n    table_settings=table_settings\n)\n</pre> # Example: Use text alignment for vertical lines, explicit lines for horizontal # See pdfplumber documentation for all settings table_settings = {     \"vertical_strategy\": \"text\",     \"horizontal_strategy\": \"lines\",     \"intersection_x_tolerance\": 5, # Increase tolerance for intersections }  results = page.extract_table(     table_settings=table_settings ) In\u00a0[11]: Copied! <pre>import pandas as pd\n\npd.DataFrame(page.extract_table())\n</pre> import pandas as pd  pd.DataFrame(page.extract_table()) Out[11]: 0 1 2 3 0 Statute Description Level Repeat? 1 4.12.7 Unsanitary Working Conditions. Critical 2 5.8.3 Inadequate Protective Equipment. Serious 3 6.3.9 Ineffective Injury Prevention. Serious 4 7.1.5 Failure to Properly Store Hazardous Materials. Critical 5 8.9.2 Lack of Adequate Fire Safety Measures. Serious 6 9.6.4 Inadequate Ventilation Systems. Serious 7 10.2.7 Insufficient Employee Training for Safe Work P... Serious In\u00a0[12]: Copied! <pre># This doesn't work! I forget why, I should troubleshoot later.\n# tatr_table.cells\n</pre> # This doesn't work! I forget why, I should troubleshoot later. # tatr_table.cells"},{"location":"tables/#table-extraction","title":"Table Extraction\u00b6","text":"<p>Extracting tables from PDFs can range from straightforward to complex. Natural PDF provides several tools and methods to handle different scenarios, leveraging both rule-based (<code>pdfplumber</code>) and model-based (<code>TATR</code>) approaches.</p>"},{"location":"tables/#setup","title":"Setup\u00b6","text":"<p>Let's load a PDF containing tables.</p>"},{"location":"tables/#basic-table-extraction-no-detection","title":"Basic Table Extraction (No Detection)\u00b6","text":"<p>If you know a table exists, you can try <code>extract_table()</code> directly on the page or a region. This uses <code>pdfplumber</code> behind the scenes.</p>"},{"location":"tables/#layout-analysis-for-table-detection","title":"Layout Analysis for Table Detection\u00b6","text":"<p>A more robust approach can be to first detect the table boundaries using layout analysis.</p>"},{"location":"tables/#using-yolo-default","title":"Using YOLO (Default)\u00b6","text":"<p>The default YOLO model finds the overall bounding box of tables.</p>"},{"location":"tables/#using-tatr-table-transformer","title":"Using TATR (Table Transformer)\u00b6","text":"<p>The TATR model provides detailed table structure (rows, columns, headers).</p>"},{"location":"tables/#controlling-extraction-method-plumber-vs-tatr","title":"Controlling Extraction Method (<code>plumber</code> vs <code>tatr</code>)\u00b6","text":"<p>When you call <code>extract_table()</code> on a region:</p> <ul> <li>If the region was detected by YOLO (or not detected at all), it uses the <code>plumber</code> method.</li> <li>If the region was detected by TATR, it defaults to the <code>tatr</code> method, which uses the detected row/column structure.</li> </ul> <p>You can override this using the <code>method</code> argument.</p>"},{"location":"tables/#when-to-use-which-method","title":"When to Use Which Method?\u00b6","text":"<ul> <li><code>pdfplumber</code>: Good for simple tables with clear grid lines. Faster.</li> <li><code>tatr</code>: Better for tables without clear lines, complex cell merging, or irregular layouts. Leverages the model's understanding of rows and columns.</li> </ul>"},{"location":"tables/#customizing-pdfplumber-settings","title":"Customizing <code>pdfplumber</code> Settings\u00b6","text":"<p>If using the <code>pdfplumber</code> method (explicitly or implicitly), you can pass <code>pdfplumber</code> settings via <code>table_settings</code>.</p>"},{"location":"tables/#saving-extracted-tables","title":"Saving Extracted Tables\u00b6","text":"<p>You can easily save the extracted data (list of lists) to common formats.</p>"},{"location":"tables/#working-directly-with-tatr-cells","title":"Working Directly with TATR Cells\u00b6","text":"<p>The TATR engine implicitly creates cell regions at the intersection of detected rows and columns. You can access these for fine-grained control.</p>"},{"location":"tables/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Layout Analysis: Understand how table detection fits into overall document structure analysis.</li> <li>Working with Regions: Manually define table areas if detection fails.</li> </ul>"},{"location":"text-analysis/","title":"Text Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0] In\u00a0[2]: Copied! <pre># Find the first word element\nword = page.find('word') \n\nprint(f\"Text:\", word.text)\nprint(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name\nprint(f\"Size:\", word.size)\nprint(f\"Color:\", word.color) # Non-stroking color\nprint(f\"Is Bold:\", word.bold)\nprint(f\"Is Italic:\", word.italic)\n</pre> # Find the first word element word = page.find('word')   print(f\"Text:\", word.text) print(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name print(f\"Size:\", word.size) print(f\"Color:\", word.color) # Non-stroking color print(f\"Is Bold:\", word.bold) print(f\"Is Italic:\", word.italic) <pre>Text: Jungle Health and Safety Inspection Service\nFont Name: Helvetica\nSize: 8.0\nColor: (0, 0, 0)\nIs Bold: False\nIs Italic: False\n</pre> <ul> <li><code>fontname</code>: Often an internal reference (like 'F1', 'F2') or a basic name.</li> <li><code>size</code>: Font size in points.</li> <li><code>color</code>: The non-stroking color, typically a tuple representing RGB or Grayscale values (e.g., <code>(0.0, 0.0, 0.0)</code> for black).</li> <li><code>bold</code>, <code>italic</code>: Boolean flags indicating if the font style is bold or italic (heuristically determined based on font name conventions).</li> </ul> In\u00a0[3]: Copied! <pre># Find all bold text elements\nbold_text = page.find_all('text:bold')\n\n# Find all italic text elements\nitalic_text = page.find_all('text:italic')\n\n# Find text that is both bold and larger than 12pt\nbold_headings = page.find_all('text:bold[size&gt;=12]')\n\nprint(f\"Found {len(bold_text)} bold elements.\")\nprint(f\"Found {len(italic_text)} italic elements.\")\nprint(f\"Found {len(bold_headings)} bold headings.\")\n</pre> # Find all bold text elements bold_text = page.find_all('text:bold')  # Find all italic text elements italic_text = page.find_all('text:italic')  # Find text that is both bold and larger than 12pt bold_headings = page.find_all('text:bold[size&gt;=12]')  print(f\"Found {len(bold_text)} bold elements.\") print(f\"Found {len(italic_text)} italic elements.\") print(f\"Found {len(bold_headings)} bold headings.\") <pre>Found 9 bold elements.\nFound 0 italic elements.\nFound 1 bold headings.\n</pre> In\u00a0[4]: Copied! <pre>page.analyze_text_styles()\npage.text_style_labels\n</pre> page.analyze_text_styles() page.text_style_labels Out[4]: <pre>['10.0pt Bold Helvetica',\n '10.0pt Helvetica',\n '12.0pt Bold Helvetica',\n '8.0pt Helvetica']</pre> <p>One they're assigned, you can filter based on <code>style_label</code> instead of going bit-by-bit.</p> In\u00a0[5]: Copied! <pre>page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]')\n</pre> page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]') Out[5]: <pre>&lt;ElementCollection[TextElement](count=8)&gt;</pre> In\u00a0[6]: Copied! <pre>page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700)\n</pre> page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700) Out[6]: <p>This allows you to quickly see patterns in font usage across the page layout.</p> In\u00a0[7]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Select the first page page = pdf.pages[0] page.to_image(width=700) Out[7]: <p>Look!</p> In\u00a0[8]: Copied! <pre>page.find_all('text')[0].fontname\n</pre> page.find_all('text')[0].fontname Out[8]: <pre>'AAAAAB+font000000002a8d158a'</pre> <p>The part before the <code>+</code> is the variant \u2013 bold, italic, etc \u2013 while the part after it is the \"real\" font name.</p>"},{"location":"text-analysis/#text-analysis","title":"Text Analysis\u00b6","text":"<p>Analyzing the properties of text elements, such as their font, size, style, and color, can be crucial for understanding document structure and extracting specific information. Natural PDF provides tools to access and analyze these properties.</p>"},{"location":"text-analysis/#introduction","title":"Introduction\u00b6","text":"<p>Beyond just the sequence of characters, the style of text carries significant meaning. Headings are often larger and bolder, important terms might be italicized, and different sections might use distinct fonts. This page covers how to access and utilize this stylistic information.</p>"},{"location":"text-analysis/#accessing-font-information","title":"Accessing Font Information\u00b6","text":"<p>Every <code>TextElement</code> (representing characters or words) holds information about its font properties.</p>"},{"location":"text-analysis/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>You can directly select text based on its style using pseudo-classes in selectors:</p>"},{"location":"text-analysis/#analyzing-fonts-on-a-page","title":"Analyzing Fonts on a Page\u00b6","text":"<p>You can use <code>analyze_text_styles</code> to assign labels to text based on font sizes, bold/italic and font names.</p>"},{"location":"text-analysis/#visualizing-text-properties","title":"Visualizing Text Properties\u00b6","text":"<p>Use highlighting to visually inspect text properties. Grouping by attributes like <code>fontname</code> or <code>size</code> can be very insightful. In the example below we go right to grouping by the <code>style_label</code>, which combines font name, size and variant.</p>"},{"location":"text-analysis/#weird-font-names","title":"Weird font names\u00b6","text":"<p>Oftentimes font names aren't what you're used to \u2013 Arial, Helvetica, etc \u2013 the PDF has given them weird, weird names. Relax, it's okay, they're normal fonts.</p>"},{"location":"text-extraction/","title":"Text Extraction Guide","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page for initial examples\npage = pdf.pages[0]\n\n# Display the first page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page for initial examples page = pdf.pages[0]  # Display the first page page.show(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Extract all text from the first page\n# Displaying first 500 characters\nprint(page.extract_text()[:500])\n</pre> # Extract all text from the first page # Displaying first 500 characters print(page.extract_text()[:500]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\nDate:  February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \nsome of which there were open vats near the\n</pre> <p>You can also preserve layout with <code>layout=True</code>.</p> In\u00a0[3]: Copied! <pre># Extract text from the entire document (may take time)\n# Uncomment to run:\nprint(page.extract_text(layout=True)[:2000])\n</pre> # Extract text from the entire document (may take time) # Uncomment to run: print(page.extract_text(layout=True)[:2000]) <pre>                                                                                    \n                                                                                    \n                                                                                    \n                                                     Jungle Health and Safety Inspection Service\n                                                     INS-UP70N51NCL41R              \n                                                                                    \n       Site: Durham\u2019s Meatpacking  Chicago, Ill.                                    \n                                                                                    \n       Date:  February 3, 1905                                                      \n                                                                                    \n       Violation Count: 7                                                           \n       Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\n       These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \n                                                                                    \n       visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \n       some of which there were open vats near the level of the floor, their peculiar trouble was that they fell\n       into the vats; and when they were fished out, there was never enough of them left to be worth \n       exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\n       to the world as Durham\u2019s Pure Leaf Lard!                                     \n                                                                                    \n                                                                                    \n                                                          \n</pre> In\u00a0[4]: Copied! <pre># Find a single element, e.g., a title containing \"Summary\"\n# Adjust selector as needed\ndate_element = page.find('text:contains(\"Site\")')\ndate_element # Display the found element object\n</pre> # Find a single element, e.g., a title containing \"Summary\" # Adjust selector as needed date_element = page.find('text:contains(\"Site\")') date_element # Display the found element object Out[4]: <pre>&lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;</pre> In\u00a0[5]: Copied! <pre>date_element.show()\n</pre> date_element.show() Out[5]: In\u00a0[6]: Copied! <pre>date_element.text\n</pre> date_element.text Out[6]: <pre>'Site: '</pre> In\u00a0[7]: Copied! <pre># Find multiple elements, e.g., bold headings (size &gt;= 8)\nheading_elements = page.find_all('text[size&gt;=8]:bold')\nheading_elements \n</pre> # Find multiple elements, e.g., bold headings (size &gt;= 8) heading_elements = page.find_all('text[size&gt;=8]:bold') heading_elements  Out[7]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[8]: Copied! <pre>page.find_all('text[size&gt;=8]:bold').show()\n</pre> page.find_all('text[size&gt;=8]:bold').show() Out[8]: In\u00a0[9]: Copied! <pre># Pull out all of their text (why? I don't know!)\nprint(heading_elements.extract_text())\n</pre> # Pull out all of their text (why? I don't know!) print(heading_elements.extract_text()) <pre>Site:  Date:  Violation Count:  Summary:  Violations Statute Description Level Repeat?\n</pre> In\u00a0[10]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"Hazardous Materials\")').text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"Hazardous Materials\")').text Out[10]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[11]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text Out[11]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[12]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\nregex = \"\\d+, \\d{4}\"\npage.find(f'text:contains(\"{regex}\")', regex=True)\n</pre> # Regular expression (e.g., \"YYYY Report\") regex = \"\\d+, \\d{4}\" page.find(f'text:contains(\"{regex}\")', regex=True) Out[12]: <pre>&lt;TextElement text='February 3...' font='Helvetica' size=10.0 bbox=(80.56, 104.07000000000005, 156.71000000000004, 114.07000000000005)&gt;</pre> In\u00a0[13]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\npage.find_all('text[fontname=\"Helvetica\"][size=10]')\n</pre> # Regular expression (e.g., \"YYYY Report\") page.find_all('text[fontname=\"Helvetica\"][size=10]') Out[13]: <pre>&lt;ElementCollection[TextElement](count=31)&gt;</pre> In\u00a0[14]: Copied! <pre># Region below an element (e.g., below \"Introduction\")\n# Adjust selector as needed\npage.find('text:contains(\"Summary\")').below(include_element=True).show()\n</pre> # Region below an element (e.g., below \"Introduction\") # Adjust selector as needed page.find('text:contains(\"Summary\")').below(include_element=True).show() Out[14]: In\u00a0[15]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_element=True)\n    .extract_text()\n    [:500]\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_element=True)     .extract_text()     [:500] ) Out[15]: <pre>'Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left t'</pre> In\u00a0[16]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_element=True, until='line:horizontal')\n    .show()\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_element=True, until='line:horizontal')     .show() ) Out[16]: In\u00a0[17]: Copied! <pre># Manually defined region via coordinates (x0, top, x1, bottom)\nmanual_region = page.create_region(30, 60, 600, 300)\nmanual_region.show()\n</pre> # Manually defined region via coordinates (x0, top, x1, bottom) manual_region = page.create_region(30, 60, 600, 300) manual_region.show() Out[17]: In\u00a0[18]: Copied! <pre># Extract text from the manual region\nmanual_region.extract_text()[:500]\n</pre> # Extract text from the manual region manual_region.extract_text()[:500] Out[18]: <pre>'Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fe'</pre> In\u00a0[19]: Copied! <pre>header_content = page.find('rect')\nfooter_content = page.find_all('line')[-1].below()\n\nheader_content.highlight()\nfooter_content.highlight()\npage.to_image()\n</pre> header_content = page.find('rect') footer_content = page.find_all('line')[-1].below()  header_content.highlight() footer_content.highlight() page.to_image() Out[19]: In\u00a0[20]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[20]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\\nDate:  February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \\nsome of which there were open vats near the'</pre> In\u00a0[21]: Copied! <pre>page.add_exclusion(header_content)\npage.add_exclusion(footer_content)\n</pre> page.add_exclusion(header_content) page.add_exclusion(footer_content) Out[21]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[22]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[22]: <pre>'Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fe'</pre> In\u00a0[23]: Copied! <pre>full_text_no_exclusions = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text()\nf\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\"\n</pre> full_text_no_exclusions = page.extract_text(use_exclusions=False) clean_text = page.extract_text() f\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\" Out[23]: <pre>'Original length: 1260, Excluded length: 1156'</pre> In\u00a0[24]: Copied! <pre>page.clear_exclusions()\n</pre> page.clear_exclusions() Out[24]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>Exclusions can also be defined globally at the PDF level using <code>pdf.add_exclusion()</code> with a function.</p> In\u00a0[25]: Copied! <pre>print(page.extract_text())\n</pre> print(page.extract_text()) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\nDate:  February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth \nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[26]: Copied! <pre>print(page.extract_text(use_exclusions=False, layout=True))\n</pre> print(page.extract_text(use_exclusions=False, layout=True)) <pre>                                                                                    \n                                                                                    \n                                                                                    \n                                                     Jungle Health and Safety Inspection Service\n                                                     INS-UP70N51NCL41R              \n                                                                                    \n       Site: Durham\u2019s Meatpacking  Chicago, Ill.                                    \n                                                                                    \n       Date:  February 3, 1905                                                      \n                                                                                    \n       Violation Count: 7                                                           \n       Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\n       These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \n                                                                                    \n       visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \n       some of which there were open vats near the level of the floor, their peculiar trouble was that they fell\n       into the vats; and when they were fished out, there was never enough of them left to be worth \n       exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\n       to the world as Durham\u2019s Pure Leaf Lard!                                     \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n       Violations                                                                   \n                                                                                    \n        Statute Description                                    Level  Repeat?       \n        4.12.7 Unsanitary Working Conditions.                  Critical             \n                                                                                    \n        5.8.3 Inadequate Protective Equipment.                 Serious              \n        6.3.9 Ineffective Injury Prevention.                   Serious              \n                                                                                    \n        7.1.5 Failure to Properly Store Hazardous Materials.   Critical             \n        8.9.2 Lack of Adequate Fire Safety Measures.           Serious              \n                                                                                    \n        9.6.4 Inadequate Ventilation Systems.                  Serious              \n        10.2.7 Insufficient Employee Training for Safe Work Practices. Serious      \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                Jungle Health and Safety Inspection Service         \n</pre> In\u00a0[27]: Copied! <pre># Find the first text element on the page\nfirst_text = page.find_all('text')[1]\nfirst_text # Display basic info\n</pre> # Find the first text element on the page first_text = page.find_all('text')[1] first_text # Display basic info Out[27]: <pre>&lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;</pre> In\u00a0[28]: Copied! <pre># Highlight the first text element\nfirst_text.show()\n</pre> # Highlight the first text element first_text.show() Out[28]: In\u00a0[29]: Copied! <pre># Get detailed font properties dictionary\nfirst_text.font_info()\n</pre> # Get detailed font properties dictionary first_text.font_info() Out[29]: <pre>{'text': 'INS-UP70N51NCL41R',\n 'fontname': 'Helvetica',\n 'font_family': 'Helvetica',\n 'font_variant': '',\n 'size': 8.0,\n 'bold': False,\n 'italic': False,\n 'color': (1, 0, 0)}</pre> In\u00a0[30]: Copied! <pre># Check specific style properties directly\nf\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\"\n</pre> # Check specific style properties directly f\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\" Out[30]: <pre>'Is Bold: False, Is Italic: False, Font: Helvetica, Size: 8.0'</pre> In\u00a0[31]: Copied! <pre># Find elements by font attributes (adjust selectors)\n# Example: Find Arial fonts\narial_text = page.find_all('text[fontname*=Helvetica]')\narial_text # Display list of found elements\n</pre> # Find elements by font attributes (adjust selectors) # Example: Find Arial fonts arial_text = page.find_all('text[fontname*=Helvetica]') arial_text # Display list of found elements Out[31]: <pre>&lt;ElementCollection[TextElement](count=43)&gt;</pre> In\u00a0[32]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nlarge_text = page.find_all('text[size&gt;=12]')\nlarge_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) large_text = page.find_all('text[size&gt;=12]') large_text Out[32]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[33]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nbold_text = page.find_all('text:bold')\nbold_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) bold_text = page.find_all('text:bold') bold_text Out[33]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[34]: Copied! <pre># Analyze styles on the page\n# This returns a dictionary mapping style names to ElementList objects\npage.analyze_text_styles()\npage.text_style_labels\n</pre> # Analyze styles on the page # This returns a dictionary mapping style names to ElementList objects page.analyze_text_styles() page.text_style_labels Out[34]: <pre>['10.0pt Bold Helvetica',\n '10.0pt Helvetica',\n '12.0pt Bold Helvetica',\n '8.0pt Helvetica']</pre> In\u00a0[35]: Copied! <pre>page.find_all('text').highlight(group_by='style_label').to_image()\n</pre> page.find_all('text').highlight(group_by='style_label').to_image() Out[35]: In\u00a0[36]: Copied! <pre>page.find_all('text[style_label=\"8.0pt Helvetica\"]')\n</pre> page.find_all('text[style_label=\"8.0pt Helvetica\"]') Out[36]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> In\u00a0[37]: Copied! <pre>page.find_all('text[fontname=\"Helvetica\"][size=8]')\n</pre> page.find_all('text[fontname=\"Helvetica\"][size=8]') Out[37]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> <p>Font variants (e.g., <code>AAAAAB+FontName</code>) are also accessible via the <code>font-variant</code> attribute selector: <code>page.find_all('text[font-variant=\"AAAAAB\"]')</code>.</p> In\u00a0[38]: Copied! <pre># Get first 5 text elements in reading order\nelements_in_order = page.find_all('text')\nelements_in_order[:5]\n</pre> # Get first 5 text elements in reading order elements_in_order = page.find_all('text') elements_in_order[:5] Out[38]: <pre>[&lt;TextElement text='Jungle Hea...' font='Helvetica' size=8.0 bbox=(385.0, 35.65599999999995, 541.9680000000001, 43.65599999999995)&gt;,\n &lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;,\n &lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;,\n &lt;TextElement text='Durham\u2019s M...' font='Helvetica' size=10.0 bbox=(74.45, 84.07000000000005, 234.50000000000003, 94.07000000000005)&gt;,\n &lt;TextElement text='Date: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 104.07000000000005, 80.56, 114.07000000000005)&gt;]</pre> In\u00a0[39]: Copied! <pre># Text extracted via page.extract_text() respects this order automatically\n# (Result already shown in Basic Text Extraction section)\npage.extract_text()[:100]\n</pre> # Text extracted via page.extract_text() respects this order automatically # (Result already shown in Basic Text Extraction section) page.extract_text()[:100] Out[39]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking  Chicago, I'</pre> In\u00a0[40]: Copied! <pre>page.clear_highlights()\n\nstart = page.find('text:contains(\"Date\")')\nstart.highlight(label='Date label')\nstart.next().highlight(label='Maybe the date', color='green')\nstart.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')\n\npage.to_image()\n</pre> page.clear_highlights()  start = page.find('text:contains(\"Date\")') start.highlight(label='Date label') start.next().highlight(label='Maybe the date', color='green') start.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')  page.to_image() Out[40]:"},{"location":"text-extraction/#text-extraction-guide","title":"Text Extraction Guide\u00b6","text":"<p>This guide demonstrates various ways to extract text from PDFs using Natural PDF, from simple page dumps to targeted extraction based on elements, regions, and styles.</p>"},{"location":"text-extraction/#setup","title":"Setup\u00b6","text":"<p>First, let's import necessary libraries and load a sample PDF. We'll use <code>example.pdf</code> from the tutorials' <code>pdfs</code> directory. Adjust the path if your setup differs.</p>"},{"location":"text-extraction/#basic-text-extraction","title":"Basic Text Extraction\u00b6","text":"<p>Get all text from a page or the entire document.</p>"},{"location":"text-extraction/#extracting-text-from-specific-elements","title":"Extracting Text from Specific Elements\u00b6","text":"<p>Use selectors with <code>find()</code> or <code>find_all()</code> to target specific elements. Selectors like <code>:contains(\"Summary\")</code> are examples; adapt them to your PDF.</p>"},{"location":"text-extraction/#advanced-text-searches","title":"Advanced text searches\u00b6","text":""},{"location":"text-extraction/#regions","title":"Regions\u00b6","text":""},{"location":"text-extraction/#filtering-out-headers-and-footers","title":"Filtering Out Headers and Footers\u00b6","text":"<p>Use Exclusion Zones to remove unwanted content before extraction. Adjust selectors for typical header/footer content.</p>"},{"location":"text-extraction/#controlling-whitespace","title":"Controlling Whitespace\u00b6","text":"<p>Manage how spaces and blank lines are handled during extraction using <code>layout</code>.</p>"},{"location":"text-extraction/#font-information-access","title":"Font Information Access\u00b6","text":"<p>Inspect font details of text elements.</p>"},{"location":"text-extraction/#working-with-font-styles","title":"Working with Font Styles\u00b6","text":"<p>Analyze and group text elements by their computed font style, which combines attributes like font name, size, boldness, etc., into logical groups.</p>"},{"location":"text-extraction/#reading-order","title":"Reading Order\u00b6","text":"<p>Text extraction respects a pathetic attempt at natural reading order (top-to-bottom, left-to-right by default). <code>page.find_all('text')</code> returns elements already sorted this way.</p>"},{"location":"text-extraction/#element-navigation","title":"Element Navigation\u00b6","text":"<p>Move between elements sequentially based on reading order using <code>.next()</code> and <code>.previous()</code>.</p>"},{"location":"text-extraction/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to extract text, you might want to explore:</p> <ul> <li>Working with regions for more precise extraction</li> <li>OCR capabilities for scanned documents</li> <li>Document layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"},{"location":"tutorials/01-loading-and-extraction/","title":"Loading and Basic Text Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" <p>In this tutorial, we'll learn how to:</p> <ol> <li>Load a PDF document</li> <li>Extract text from pages</li> <li>Extract specific elements</li> </ol> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\nimport os\n\n# Load a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Basic info about the document\n{\n    \"Filename\": os.path.basename(pdf.path),\n    \"Pages\": len(pdf.pages),\n    \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),\n    \"Author\": pdf.metadata.get(\"Author\", \"N/A\")\n}\n</pre> from natural_pdf import PDF import os  # Load a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Basic info about the document {     \"Filename\": os.path.basename(pdf.path),     \"Pages\": len(pdf.pages),     \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),     \"Author\": pdf.metadata.get(\"Author\", \"N/A\") } Out[2]: <pre>{'Filename': 'tmps_4_pao2.pdf', 'Pages': 1, 'Title': 'N/A', 'Author': 'N/A'}</pre> In\u00a0[3]: Copied! <pre># Get the first page\npage = pdf.pages[0]\n\n# Extract text from the page\ntext = page.extract_text()\n\n# Show the first 200 characters of the text\nprint(text[:200])\n</pre> # Get the first page page = pdf.pages[0]  # Extract text from the page text = page.extract_text()  # Show the first 200 characters of the text print(text[:200]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men\n</pre> In\u00a0[4]: Copied! <pre># Find text elements containing specific words\nelements = page.find_all('text:contains(\"Inadequate\")')\n\n# Show these elements on the page\npage.clear_highlights()\nelements.highlight(color=\"red\", label=\"Contains 'Inadequate'\")\n\n# Display the page to see them\npage.to_image(width=700)\n</pre> # Find text elements containing specific words elements = page.find_all('text:contains(\"Inadequate\")')  # Show these elements on the page page.clear_highlights() elements.highlight(color=\"red\", label=\"Contains 'Inadequate'\")  # Display the page to see them page.to_image(width=700) Out[4]: In\u00a0[5]: Copied! <pre># Analyze the page layout\npage.analyze_layout(engine='yolo')\n\n# Find and highlight all detected regions\npage.clear_highlights()\npage.find_all('region').highlight(group_by='type')\n\n# Display the page to see the regions\npage.to_image(width=900)\n</pre> # Analyze the page layout page.analyze_layout(engine='yolo')  # Find and highlight all detected regions page.clear_highlights() page.find_all('region').highlight(group_by='type')  # Display the page to see the regions page.to_image(width=900) <pre>2025-04-27T16:30:46.354730Z [warning  ] GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available. lineno=76 module=natural_pdf.analyzers.layout.gemini\n</pre> <pre>[2025-04-27 12:30:46,354] [ WARNING] gemini.py:76 - GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available.\n</pre> <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpo0qkek9z/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1453.3ms\n</pre> <pre>Speed: 4.7ms preprocess, 1453.3ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[5]: In\u00a0[6]: Copied! <pre># Process all pages\nfor page in pdf.pages:\n    page_text = page.extract_text()\n    print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page\n</pre> # Process all pages for page in pdf.pages:     page_text = page.extract_text()     print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page <pre>Page 1 Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Il\n</pre> <p>This tutorial covered the basics of loading PDFs and extracting text. In the next tutorials, we'll explore more advanced features like searching for specific elements, extracting structured content, and working with tables.</p> In\u00a0[7]: Copied! <pre>%%bash\npip install \"natural-pdf[all]\"\n</pre> %%bash pip install \"natural-pdf[all]\" <pre>Requirement already satisfied: natural-pdf[all] in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (0.1.8.dev3+g25114887.d20250426)\n</pre> <pre>Requirement already satisfied: pdfplumber in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (0.11.6)\n</pre> <pre>Requirement already satisfied: Pillow in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (10.4.0)\n</pre> <pre>Requirement already satisfied: colour in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (0.1.5)\n</pre> <pre>Requirement already satisfied: numpy in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (2.2.4)\n</pre> <pre>Requirement already satisfied: urllib3 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (2.4.0)\n</pre> <pre>Requirement already satisfied: tqdm in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (4.67.1)\n</pre> <pre>Requirement already satisfied: sentence-transformers in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (4.1.0)\n</pre> <pre>Requirement already satisfied: timm in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (1.0.15)\n</pre> <pre>Requirement already satisfied: docling in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (2.30.0)\n</pre> <pre>Requirement already satisfied: easyocr in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (1.7.2)\n</pre> <pre>Requirement already satisfied: haystack-ai in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (2.12.2)\n</pre> <pre>Requirement already satisfied: chroma-haystack in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (3.1.0)\n</pre> <pre>Requirement already satisfied: ipywidgets&lt;9.0.0,&gt;=7.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (8.1.6)\n</pre> <pre>Requirement already satisfied: doclayout_yolo in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (0.0.3)\n</pre> <pre>Requirement already satisfied: openai&gt;=1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (1.74.0)\n</pre> <pre>Requirement already satisfied: pydantic in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (2.11.3)\n</pre> <pre>Requirement already satisfied: ocrmypdf in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (16.10.0)\n</pre> <pre>Requirement already satisfied: pikepdf in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (9.7.0)\n</pre> <pre>Requirement already satisfied: paddlepaddle in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (3.0.0)\n</pre> <pre>Requirement already satisfied: paddleocr in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (2.10.0)\n</pre> <pre>Requirement already satisfied: surya-ocr in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (0.13.1)\n</pre> <pre>Requirement already satisfied: pytest in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (8.3.5)\n</pre> <pre>Requirement already satisfied: pdfminer.six==20250327 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pdfplumber-&gt;natural-pdf[all]) (20250327)\n</pre> <pre>Requirement already satisfied: pypdfium2&gt;=4.18.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pdfplumber-&gt;natural-pdf[all]) (4.30.0)\n</pre> <pre>Requirement already satisfied: charset-normalizer&gt;=2.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pdfminer.six==20250327-&gt;pdfplumber-&gt;natural-pdf[all]) (3.4.1)\n</pre> <pre>Requirement already satisfied: cryptography&gt;=36.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pdfminer.six==20250327-&gt;pdfplumber-&gt;natural-pdf[all]) (44.0.2)\n</pre> <pre>Requirement already satisfied: comm&gt;=0.1.3 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.2.2)\n</pre> <pre>Requirement already satisfied: ipython&gt;=6.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (9.1.0)\n</pre> <pre>Requirement already satisfied: traitlets&gt;=4.3.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (5.14.3)\n</pre> <pre>Requirement already satisfied: widgetsnbextension~=4.0.14 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (4.0.14)\n</pre> <pre>Requirement already satisfied: jupyterlab_widgets~=3.0.14 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (3.0.14)\n</pre> <pre>Requirement already satisfied: anyio&lt;5,&gt;=3.5.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from openai&gt;=1.0-&gt;natural-pdf[all]) (4.9.0)\n</pre> <pre>Requirement already satisfied: distro&lt;2,&gt;=1.7.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from openai&gt;=1.0-&gt;natural-pdf[all]) (1.9.0)\n</pre> <pre>Requirement already satisfied: httpx&lt;1,&gt;=0.23.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from openai&gt;=1.0-&gt;natural-pdf[all]) (0.28.1)\n</pre> <pre>Requirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from openai&gt;=1.0-&gt;natural-pdf[all]) (0.9.0)\n</pre> <pre>Requirement already satisfied: sniffio in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from openai&gt;=1.0-&gt;natural-pdf[all]) (1.3.1)\n</pre> <pre>Requirement already satisfied: typing-extensions&lt;5,&gt;=4.11 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from openai&gt;=1.0-&gt;natural-pdf[all]) (4.13.2)\n</pre> <pre>Requirement already satisfied: annotated-types&gt;=0.6.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pydantic-&gt;natural-pdf[all]) (0.7.0)\n</pre> <pre>Requirement already satisfied: pydantic-core==2.33.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pydantic-&gt;natural-pdf[all]) (2.33.1)\n</pre> <pre>Requirement already satisfied: typing-inspection&gt;=0.4.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pydantic-&gt;natural-pdf[all]) (0.4.0)\n</pre> <pre>Requirement already satisfied: chromadb&gt;=1.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chroma-haystack-&gt;natural-pdf[all]) (1.0.4)\n</pre> <pre>Requirement already satisfied: haystack-experimental in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (0.9.0)\n</pre> <pre>Requirement already satisfied: jinja2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (3.1.6)\n</pre> <pre>Requirement already satisfied: jsonschema in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (4.23.0)\n</pre> <pre>Requirement already satisfied: lazy-imports in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (0.4.0)\n</pre> <pre>Requirement already satisfied: more-itertools in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (10.6.0)\n</pre> <pre>Requirement already satisfied: networkx in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (3.4.2)\n</pre> <pre>Requirement already satisfied: posthog!=3.12.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (3.25.0)\n</pre> <pre>Requirement already satisfied: python-dateutil in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (2.9.0.post0)\n</pre> <pre>Requirement already satisfied: pyyaml in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (6.0.2)\n</pre> <pre>Requirement already satisfied: requests in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (2.32.3)\n</pre> <pre>Requirement already satisfied: tenacity!=8.4.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from haystack-ai-&gt;natural-pdf[all]) (9.1.2)\n</pre> <pre>Requirement already satisfied: matplotlib&gt;=3.3.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (3.10.1)\n</pre> <pre>Requirement already satisfied: opencv-python&gt;=4.6.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (4.11.0.86)\n</pre> <pre>Requirement already satisfied: scipy&gt;=1.4.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (1.15.2)\n</pre> <pre>Requirement already satisfied: torch&gt;=2.0.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (2.6.0)\n</pre> <pre>Requirement already satisfied: torchvision&gt;=0.15.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (0.21.0)\n</pre> <pre>Requirement already satisfied: psutil in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (7.0.0)\n</pre> <pre>Requirement already satisfied: py-cpuinfo in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (9.0.0)\n</pre> <pre>Requirement already satisfied: thop&gt;=0.1.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (0.1.1.post2209072238)\n</pre> <pre>Requirement already satisfied: pandas&gt;=1.1.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (2.2.3)\n</pre> <pre>Requirement already satisfied: seaborn&gt;=0.11.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (0.13.2)\n</pre> <pre>Requirement already satisfied: albumentations&gt;=1.4.11 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (2.0.5)\n</pre> <pre>Requirement already satisfied: beautifulsoup4&lt;5.0.0,&gt;=4.12.3 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (4.13.4)\n</pre> <pre>Requirement already satisfied: certifi&gt;=2024.7.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (2025.1.31)\n</pre> <pre>Requirement already satisfied: docling-core&lt;3.0.0,&gt;=2.26.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (2.27.0)\n</pre> <pre>Requirement already satisfied: docling-ibm-models&lt;4.0.0,&gt;=3.4.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (3.4.1)\n</pre> <pre>Requirement already satisfied: docling-parse&lt;5.0.0,&gt;=4.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (4.0.1)\n</pre> <pre>Requirement already satisfied: filetype&lt;2.0.0,&gt;=1.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (1.2.0)\n</pre> <pre>Requirement already satisfied: huggingface_hub&lt;1,&gt;=0.23 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (0.30.2)\n</pre> <pre>Requirement already satisfied: lxml&lt;6.0.0,&gt;=4.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (5.3.2)\n</pre> <pre>Requirement already satisfied: marko&lt;3.0.0,&gt;=2.1.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (2.1.3)\n</pre> <pre>Requirement already satisfied: openpyxl&lt;4.0.0,&gt;=3.1.5 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (3.1.5)\n</pre> <pre>Requirement already satisfied: pluggy&lt;2.0.0,&gt;=1.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (1.5.0)\n</pre> <pre>Requirement already satisfied: pydantic-settings&lt;3.0.0,&gt;=2.3.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (2.8.1)\n</pre> <pre>Requirement already satisfied: pylatexenc&lt;3.0,&gt;=2.10 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (2.10)\n</pre> <pre>Requirement already satisfied: python-docx&lt;2.0.0,&gt;=1.1.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (1.1.2)\n</pre> <pre>Requirement already satisfied: python-pptx&lt;2.0.0,&gt;=1.0.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (1.0.2)\n</pre> <pre>Requirement already satisfied: rtree&lt;2.0.0,&gt;=1.3.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (1.4.0)\n</pre> <pre>Requirement already satisfied: typer&lt;0.16.0,&gt;=0.12.5 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-&gt;natural-pdf[all]) (0.15.2)\n</pre> <pre>Requirement already satisfied: opencv-python-headless in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from easyocr-&gt;natural-pdf[all]) (4.11.0.86)\n</pre> <pre>Requirement already satisfied: scikit-image in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from easyocr-&gt;natural-pdf[all]) (0.25.2)\n</pre> <pre>Requirement already satisfied: python-bidi in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from easyocr-&gt;natural-pdf[all]) (0.6.6)\n</pre> <pre>Requirement already satisfied: Shapely in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from easyocr-&gt;natural-pdf[all]) (2.1.0)\n</pre> <pre>Requirement already satisfied: pyclipper in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from easyocr-&gt;natural-pdf[all]) (1.3.0.post6)\n</pre> <pre>Requirement already satisfied: ninja in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from easyocr-&gt;natural-pdf[all]) (1.11.1.4)\n</pre> <pre>Requirement already satisfied: transformers[sentencepiece] in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from natural-pdf[all]) (4.51.3)\n</pre> <pre>Requirement already satisfied: deprecation&gt;=2.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (2.1.0)\n</pre> <pre>Requirement already satisfied: img2pdf&gt;=0.5 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (0.6.0)\n</pre> <pre>Requirement already satisfied: packaging&gt;=20 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (24.2)\n</pre> <pre>Requirement already satisfied: pi-heif in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (0.22.0)\n</pre> <pre>Requirement already satisfied: rich&gt;=13 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (14.0.0)\n</pre> <pre>Requirement already satisfied: Deprecated in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pikepdf-&gt;natural-pdf[all]) (1.2.18)\n</pre> <pre>Requirement already satisfied: lmdb in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddleocr-&gt;natural-pdf[all]) (1.6.2)\n</pre> <pre>Requirement already satisfied: rapidfuzz in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddleocr-&gt;natural-pdf[all]) (3.13.0)\n</pre> <pre>Requirement already satisfied: opencv-contrib-python in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddleocr-&gt;natural-pdf[all]) (4.11.0.86)\n</pre> <pre>Requirement already satisfied: cython in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddleocr-&gt;natural-pdf[all]) (3.0.12)\n</pre> <pre>Requirement already satisfied: fonttools&gt;=4.24.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddleocr-&gt;natural-pdf[all]) (4.57.0)\n</pre> <pre>Requirement already satisfied: fire&gt;=0.3.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddleocr-&gt;natural-pdf[all]) (0.7.0)\n</pre> <pre>Requirement already satisfied: albucore in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddleocr-&gt;natural-pdf[all]) (0.0.23)\n</pre> <pre>Requirement already satisfied: protobuf&gt;=3.20.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (3.20.3)\n</pre> <pre>Requirement already satisfied: decorator in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (5.2.1)\n</pre> <pre>Requirement already satisfied: astor in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (0.8.1)\n</pre> <pre>Requirement already satisfied: opt-einsum==3.3.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (3.3.0)\n</pre> <pre>Requirement already satisfied: iniconfig in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pytest-&gt;natural-pdf[all]) (2.1.0)\n</pre> <pre>Requirement already satisfied: scikit-learn in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from sentence-transformers-&gt;natural-pdf[all]) (1.6.1)\n</pre> <pre>Requirement already satisfied: click&lt;9.0.0,&gt;=8.1.8 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from surya-ocr-&gt;natural-pdf[all]) (8.1.8)\n</pre> <pre>Requirement already satisfied: platformdirs&lt;5.0.0,&gt;=4.3.6 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from surya-ocr-&gt;natural-pdf[all]) (4.3.7)\n</pre> <pre>Requirement already satisfied: python-dotenv&lt;2.0.0,&gt;=1.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from surya-ocr-&gt;natural-pdf[all]) (1.1.0)\n</pre> <pre>Requirement already satisfied: safetensors in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from timm-&gt;natural-pdf[all]) (0.5.3)\n</pre> <pre>Requirement already satisfied: stringzilla&gt;=3.10.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from albucore-&gt;paddleocr-&gt;natural-pdf[all]) (3.12.4)\n</pre> <pre>Requirement already satisfied: simsimd&gt;=5.9.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from albucore-&gt;paddleocr-&gt;natural-pdf[all]) (6.2.1)\n</pre> <pre>Requirement already satisfied: idna&gt;=2.8 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from anyio&lt;5,&gt;=3.5.0-&gt;openai&gt;=1.0-&gt;natural-pdf[all]) (3.10)\n</pre> <pre>Requirement already satisfied: soupsieve&gt;1.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from beautifulsoup4&lt;5.0.0,&gt;=4.12.3-&gt;docling-&gt;natural-pdf[all]) (2.6)\n</pre> <pre>Requirement already satisfied: build&gt;=1.0.3 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.2.2.post1)\n</pre> <pre>Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.7.6)\n</pre> <pre>Requirement already satisfied: fastapi==0.115.9 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.115.9)\n</pre> <pre>Requirement already satisfied: uvicorn&gt;=0.18.3 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.34.1)\n</pre> <pre>Requirement already satisfied: onnxruntime&gt;=1.14.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.21.0)\n</pre> <pre>Requirement already satisfied: opentelemetry-api&gt;=1.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.32.1)\n</pre> <pre>Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.11.1)\n</pre> <pre>Requirement already satisfied: opentelemetry-instrumentation-fastapi&gt;=0.41b0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.53b1)\n</pre> <pre>Requirement already satisfied: opentelemetry-sdk&gt;=1.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.32.1)\n</pre> <pre>Requirement already satisfied: tokenizers&gt;=0.13.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.21.1)\n</pre> <pre>Requirement already satisfied: pypika&gt;=0.48.9 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.48.9)\n</pre> <pre>Requirement already satisfied: overrides&gt;=7.3.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (7.7.0)\n</pre> <pre>Requirement already satisfied: importlib-resources in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (6.5.2)\n</pre> <pre>Requirement already satisfied: grpcio&gt;=1.58.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.71.0)\n</pre> <pre>Requirement already satisfied: bcrypt&gt;=4.0.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (4.3.0)\n</pre> <pre>Requirement already satisfied: kubernetes&gt;=28.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (32.0.1)\n</pre> <pre>Requirement already satisfied: mmh3&gt;=4.0.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (5.1.0)\n</pre> <pre>Requirement already satisfied: orjson&gt;=3.9.12 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.10.16)\n</pre> <pre>Requirement already satisfied: starlette&lt;0.46.0,&gt;=0.40.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from fastapi==0.115.9-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.45.3)\n</pre> <pre>Requirement already satisfied: cffi&gt;=1.12 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from cryptography&gt;=36.0.0-&gt;pdfminer.six==20250327-&gt;pdfplumber-&gt;natural-pdf[all]) (1.17.1)\n</pre> <pre>Requirement already satisfied: jsonref&lt;2.0.0,&gt;=1.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-core&lt;3.0.0,&gt;=2.26.0-&gt;docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (1.1.0)\n</pre> <pre>Requirement already satisfied: latex2mathml&lt;4.0.0,&gt;=3.77.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-core&lt;3.0.0,&gt;=2.26.0-&gt;docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (3.77.0)\n</pre> <pre>Requirement already satisfied: tabulate&lt;0.10.0,&gt;=0.9.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-core&lt;3.0.0,&gt;=2.26.0-&gt;docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (0.9.0)\n</pre> <pre>Requirement already satisfied: semchunk&lt;3.0.0,&gt;=2.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (2.2.2)\n</pre> <pre>Requirement already satisfied: jsonlines&lt;4.0.0,&gt;=3.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from docling-ibm-models&lt;4.0.0,&gt;=3.4.0-&gt;docling-&gt;natural-pdf[all]) (3.1.0)\n</pre> <pre>Requirement already satisfied: termcolor in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from fire&gt;=0.3.0-&gt;paddleocr-&gt;natural-pdf[all]) (3.0.1)\n</pre> <pre>Requirement already satisfied: httpcore==1.* in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from httpx&lt;1,&gt;=0.23.0-&gt;openai&gt;=1.0-&gt;natural-pdf[all]) (1.0.8)\n</pre> <pre>Requirement already satisfied: h11&lt;0.15,&gt;=0.13 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from httpcore==1.*-&gt;httpx&lt;1,&gt;=0.23.0-&gt;openai&gt;=1.0-&gt;natural-pdf[all]) (0.14.0)\n</pre> <pre>Requirement already satisfied: filelock in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from huggingface_hub&lt;1,&gt;=0.23-&gt;docling-&gt;natural-pdf[all]) (3.18.0)\n</pre> <pre>Requirement already satisfied: fsspec&gt;=2023.5.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from huggingface_hub&lt;1,&gt;=0.23-&gt;docling-&gt;natural-pdf[all]) (2025.3.2)\n</pre> <pre>Requirement already satisfied: ipython-pygments-lexers in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (1.1.1)\n</pre> <pre>Requirement already satisfied: jedi&gt;=0.16 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.19.2)\n</pre> <pre>Requirement already satisfied: matplotlib-inline in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.1.7)\n</pre> <pre>Requirement already satisfied: pexpect&gt;4.3 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (4.9.0)\n</pre> <pre>Requirement already satisfied: prompt_toolkit&lt;3.1.0,&gt;=3.0.41 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (3.0.51)\n</pre> <pre>Requirement already satisfied: pygments&gt;=2.4.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (2.19.1)\n</pre> <pre>Requirement already satisfied: stack_data in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.6.3)\n</pre> <pre>Requirement already satisfied: attrs&gt;=22.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (25.3.0)\n</pre> <pre>Requirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (2024.10.1)\n</pre> <pre>Requirement already satisfied: referencing&gt;=0.28.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (0.36.2)\n</pre> <pre>Requirement already satisfied: rpds-py&gt;=0.7.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (0.24.0)\n</pre> <pre>Requirement already satisfied: contourpy&gt;=1.0.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (1.3.2)\n</pre> <pre>Requirement already satisfied: cycler&gt;=0.10 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (0.12.1)\n</pre> <pre>Requirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (1.4.8)\n</pre> <pre>Requirement already satisfied: pyparsing&gt;=2.3.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (3.2.3)\n</pre> <pre>Requirement already satisfied: et-xmlfile in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from openpyxl&lt;4.0.0,&gt;=3.1.5-&gt;docling-&gt;natural-pdf[all]) (2.0.0)\n</pre> <pre>Requirement already satisfied: pytz&gt;=2020.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pandas&gt;=1.1.4-&gt;doclayout_yolo-&gt;natural-pdf[all]) (2025.2)\n</pre> <pre>Requirement already satisfied: tzdata&gt;=2022.7 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pandas&gt;=1.1.4-&gt;doclayout_yolo-&gt;natural-pdf[all]) (2025.2)\n</pre> <pre>Requirement already satisfied: six&gt;=1.5 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from posthog!=3.12.0-&gt;haystack-ai-&gt;natural-pdf[all]) (1.17.0)\n</pre> <pre>Requirement already satisfied: monotonic&gt;=1.5 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from posthog!=3.12.0-&gt;haystack-ai-&gt;natural-pdf[all]) (1.6)\n</pre> <pre>Requirement already satisfied: backoff&gt;=1.10.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from posthog!=3.12.0-&gt;haystack-ai-&gt;natural-pdf[all]) (1.11.1)\n</pre> <pre>Requirement already satisfied: XlsxWriter&gt;=0.5.7 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from python-pptx&lt;2.0.0,&gt;=1.0.2-&gt;docling-&gt;natural-pdf[all]) (3.2.2)\n</pre> <pre>Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from rich&gt;=13-&gt;ocrmypdf-&gt;natural-pdf[all]) (3.0.0)\n</pre> <pre>Requirement already satisfied: sympy==1.13.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from torch&gt;=2.0.1-&gt;doclayout_yolo-&gt;natural-pdf[all]) (1.13.1)\n</pre> <pre>Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from sympy==1.13.1-&gt;torch&gt;=2.0.1-&gt;doclayout_yolo-&gt;natural-pdf[all]) (1.3.0)\n</pre> <pre>Requirement already satisfied: regex!=2019.12.17 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from transformers[sentencepiece]-&gt;natural-pdf[all]) (2024.11.6)\n</pre> <pre>Requirement already satisfied: shellingham&gt;=1.3.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from typer&lt;0.16.0,&gt;=0.12.5-&gt;docling-&gt;natural-pdf[all]) (1.5.4)\n</pre> <pre>Requirement already satisfied: wrapt&lt;2,&gt;=1.10 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from Deprecated-&gt;pikepdf-&gt;natural-pdf[all]) (1.17.2)\n</pre> <pre>Requirement already satisfied: MarkupSafe&gt;=2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from jinja2-&gt;haystack-ai-&gt;natural-pdf[all]) (3.0.2)\n</pre> <pre>Requirement already satisfied: imageio!=2.35.0,&gt;=2.33 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from scikit-image-&gt;easyocr-&gt;natural-pdf[all]) (2.37.0)\n</pre> <pre>Requirement already satisfied: tifffile&gt;=2022.8.12 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from scikit-image-&gt;easyocr-&gt;natural-pdf[all]) (2025.3.30)\n</pre> <pre>Requirement already satisfied: lazy-loader&gt;=0.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from scikit-image-&gt;easyocr-&gt;natural-pdf[all]) (0.4)\n</pre> <pre>Requirement already satisfied: joblib&gt;=1.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from scikit-learn-&gt;sentence-transformers-&gt;natural-pdf[all]) (1.4.2)\n</pre> <pre>Requirement already satisfied: threadpoolctl&gt;=3.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from scikit-learn-&gt;sentence-transformers-&gt;natural-pdf[all]) (3.6.0)\n</pre> <pre>Requirement already satisfied: sentencepiece!=0.1.92,&gt;=0.1.91 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from transformers[sentencepiece]; extra == \"core-ml\"-&gt;natural-pdf[all]) (0.2.0)\n</pre> <pre>Requirement already satisfied: pyproject_hooks in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from build&gt;=1.0.3-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.2.0)\n</pre> <pre>Requirement already satisfied: pycparser in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=36.0.0-&gt;pdfminer.six==20250327-&gt;pdfplumber-&gt;natural-pdf[all]) (2.22)\n</pre> <pre>Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.8.4)\n</pre> <pre>Requirement already satisfied: google-auth&gt;=1.0.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (2.39.0)\n</pre> <pre>Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,&gt;=0.32.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.8.0)\n</pre> <pre>Requirement already satisfied: requests-oauthlib in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (2.0.0)\n</pre> <pre>Requirement already satisfied: oauthlib&gt;=3.2.2 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.2.2)\n</pre> <pre>Requirement already satisfied: durationpy&gt;=0.7 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.9)\n</pre> <pre>Requirement already satisfied: mdurl~=0.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=13-&gt;ocrmypdf-&gt;natural-pdf[all]) (0.1.2)\n</pre> <pre>Requirement already satisfied: coloredlogs in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from onnxruntime&gt;=1.14.1-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (15.0.1)\n</pre> <pre>Requirement already satisfied: flatbuffers in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from onnxruntime&gt;=1.14.1-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (25.2.10)\n</pre> <pre>Requirement already satisfied: importlib-metadata&lt;8.7.0,&gt;=6.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-api&gt;=1.2.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (8.6.1)\n</pre> <pre>Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.70.0)\n</pre> <pre>Requirement already satisfied: opentelemetry-proto==1.11.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.11.1)\n</pre> <pre>Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.53b1)\n</pre> <pre>Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.53b1)\n</pre> <pre>Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.53b1)\n</pre> <pre>Requirement already satisfied: opentelemetry-util-http==0.53b1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.53b1)\n</pre> <pre>Requirement already satisfied: asgiref~=3.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation-asgi==0.53b1-&gt;opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.8.1)\n</pre> <pre>Requirement already satisfied: ptyprocess&gt;=0.5 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.7.0)\n</pre> <pre>Requirement already satisfied: wcwidth in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from prompt_toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.2.13)\n</pre> <pre>Requirement already satisfied: mpire[dill] in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from semchunk&lt;3.0.0,&gt;=2.2.0-&gt;docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (2.10.2)\n</pre> <pre>Requirement already satisfied: httptools&gt;=0.6.3 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.6.4)\n</pre> <pre>Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,&gt;=0.14.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.21.0)\n</pre> <pre>Requirement already satisfied: watchfiles&gt;=0.13 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.0.5)\n</pre> <pre>Requirement already satisfied: websockets&gt;=10.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (15.0.1)\n</pre> <pre>Requirement already satisfied: executing&gt;=1.2.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (2.2.0)\n</pre> <pre>Requirement already satisfied: asttokens&gt;=2.1.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (3.0.0)\n</pre> <pre>Requirement already satisfied: pure-eval in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.2.3)\n</pre> <pre>Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (5.5.2)\n</pre> <pre>Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.4.2)\n</pre> <pre>Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (4.9.1)\n</pre> <pre>Requirement already satisfied: zipp&gt;=3.20 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from importlib-metadata&lt;8.7.0,&gt;=6.0-&gt;opentelemetry-api&gt;=1.2.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.21.0)\n</pre> <pre>Requirement already satisfied: humanfriendly&gt;=9.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from coloredlogs-&gt;onnxruntime&gt;=1.14.1-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (10.0)\n</pre> <pre>Requirement already satisfied: multiprocess&gt;=0.70.15 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from mpire[dill]-&gt;semchunk&lt;3.0.0,&gt;=2.2.0-&gt;docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (0.70.18)\n</pre> <pre>Requirement already satisfied: dill&gt;=0.4.0 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from multiprocess&gt;=0.70.15-&gt;mpire[dill]-&gt;semchunk&lt;3.0.0,&gt;=2.2.0-&gt;docling-core[chunking]&lt;3.0.0,&gt;=2.26.0-&gt;docling-&gt;natural-pdf[all]) (0.4.0)\n</pre> <pre>Requirement already satisfied: pyasn1&lt;0.7.0,&gt;=0.6.1 in /Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=1.0.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.6.1)\n</pre>"},{"location":"tutorials/01-loading-and-extraction/#loading-and-basic-text-extraction","title":"Loading and Basic Text Extraction\u00b6","text":""},{"location":"tutorials/01-loading-and-extraction/#loading-a-pdf","title":"Loading a PDF\u00b6","text":"<p>Let's start by loading a PDF file:</p>"},{"location":"tutorials/01-loading-and-extraction/#extracting-text","title":"Extracting Text\u00b6","text":"<p>Now that we have loaded the PDF, let's extract the text from the first page:</p>"},{"location":"tutorials/01-loading-and-extraction/#finding-and-extracting-specific-elements","title":"Finding and Extracting Specific Elements\u00b6","text":"<p>We can find specific elements using spatial queries and text content:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>We can analyze the layout of the page to identify different regions:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>You can also work with multiple pages:</p>"},{"location":"tutorials/02-finding-elements/","title":"Finding Specific Elements","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page (index 0)\npage = pdf.pages[0]\n\n# Find the text element containing \"Site:\"\n# The ':contains()' pseudo-class looks for text content.\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Find the text element containing \"Date:\"\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Visualize the found elements\nsite_label.highlight(color=\"red\", label=\"Site Label\")\ndate_label.highlight(color=\"blue\", label=\"Date Label\")\n\n# Access the text content directly\n{\n    \"Site Label\": site_label.text,\n    \"Date Label\": date_label.text\n}\n\n# Display the page image to see the visualized elements\npage.to_image()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page (index 0) page = pdf.pages[0]  # Find the text element containing \"Site:\" # The ':contains()' pseudo-class looks for text content. site_label = page.find('text:contains(\"Site:\")')  # Find the text element containing \"Date:\" date_label = page.find('text:contains(\"Date:\")')  # Visualize the found elements site_label.highlight(color=\"red\", label=\"Site Label\") date_label.highlight(color=\"blue\", label=\"Date Label\")  # Access the text content directly {     \"Site Label\": site_label.text,     \"Date Label\": date_label.text }  # Display the page image to see the visualized elements page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Find text elements that are red\nred_text = page.find('text[color~=red]')\nred_text.highlight(color=\"red\", label=\"Red Text\")\nprint(f\"Found red text: {red_text.text}\")\n\n# Find elements with specific RGB colors\nblue_text = page.find('text[color=rgb(0,0,255)]')\n</pre> # Find text elements that are red red_text = page.find('text[color~=red]') red_text.highlight(color=\"red\", label=\"Red Text\") print(f\"Found red text: {red_text.text}\")  # Find elements with specific RGB colors blue_text = page.find('text[color=rgb(0,0,255)]') <pre>2025-04-27T16:30:58.439578Z [warning  ] Unsupported operator '~=' encountered during filter building for attribute 'color' lineno=386 module=natural_pdf.selectors.parser\n</pre> <pre>[2025-04-27 12:30:58,439] [ WARNING] parser.py:386 - Unsupported operator '~=' encountered during filter building for attribute 'color'\n</pre> <pre>Found red text: Jungle Health and Safety Inspection Service\n</pre> In\u00a0[4]: Copied! <pre># Find horizontal lines\nhorizontal_lines = page.find_all('line[horizontal]')\n\n# Find thick lines (width &gt;= 2)\nthick_lines = page.find_all('line[width&gt;=2]')\n\n# Find rectangles\nrectangles = page.find_all('rect')\n\n# Visualize what we found\npage.clear_highlights()\nhorizontal_lines.highlight(color=\"blue\", label=\"Horizontal Lines\")\nthick_lines.highlight(color=\"red\", label=\"Thick Lines\")\nrectangles.highlight(color=\"green\", label=\"Rectangles\")\npage.to_image()\n</pre> # Find horizontal lines horizontal_lines = page.find_all('line[horizontal]')  # Find thick lines (width &gt;= 2) thick_lines = page.find_all('line[width&gt;=2]')  # Find rectangles rectangles = page.find_all('rect')  # Visualize what we found page.clear_highlights() horizontal_lines.highlight(color=\"blue\", label=\"Horizontal Lines\") thick_lines.highlight(color=\"red\", label=\"Thick Lines\") rectangles.highlight(color=\"green\", label=\"Rectangles\") page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Find text with specific font properties\nbold_text = page.find_all('text[style~=bold]')\nlarge_text = page.find_all('text[size&gt;=12]')\n\n# Find text with specific font names\nhelvetica_text = page.find_all('text[fontname~=Helvetica]')\n</pre> # Find text with specific font properties bold_text = page.find_all('text[style~=bold]') large_text = page.find_all('text[size&gt;=12]')  # Find text with specific font names helvetica_text = page.find_all('text[fontname~=Helvetica]') <pre>2025-04-27T16:30:58.553833Z [warning  ] Unsupported operator '~=' encountered during filter building for attribute 'style' lineno=386 module=natural_pdf.selectors.parser\n</pre> <pre>[2025-04-27 12:30:58,553] [ WARNING] parser.py:386 - Unsupported operator '~=' encountered during filter building for attribute 'style'\n</pre> <pre>2025-04-27T16:30:58.554703Z [warning  ] Unsupported operator '~=' encountered during filter building for attribute 'fontname' lineno=386 module=natural_pdf.selectors.parser\n</pre> <pre>[2025-04-27 12:30:58,554] [ WARNING] parser.py:386 - Unsupported operator '~=' encountered during filter building for attribute 'fontname'\n</pre> In\u00a0[6]: Copied! <pre># Find text above a specific element\nabove_text = page.find('line[width=2]').above().extract_text()\n\n# Find text below a specific element\nbelow_text = page.find('text:contains(\"Summary\")').below().extract_text()\n\n# Find text to the right of a specific element\nnearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text()\n</pre> # Find text above a specific element above_text = page.find('line[width=2]').above().extract_text()  # Find text below a specific element below_text = page.find('text:contains(\"Summary\")').below().extract_text()  # Find text to the right of a specific element nearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text() In\u00a0[7]: Copied! <pre># Find large, bold text that contains specific words\nimportant_text = page.find_all('text[size&gt;=12][style~=bold]:contains(\"Critical\")')\n\n# Find red text inside a rectangle\nhighlighted_text = page.find('rect').find_all('text[color~=red]')\n</pre> # Find large, bold text that contains specific words important_text = page.find_all('text[size&gt;=12][style~=bold]:contains(\"Critical\")')  # Find red text inside a rectangle highlighted_text = page.find('rect').find_all('text[color~=red]') <pre>2025-04-27T16:30:58.568990Z [warning  ] Unsupported operator '~=' encountered during filter building for attribute 'style' lineno=386 module=natural_pdf.selectors.parser\n</pre> <pre>[2025-04-27 12:30:58,568] [ WARNING] parser.py:386 - Unsupported operator '~=' encountered during filter building for attribute 'style'\n</pre> <pre>2025-04-27T16:30:58.569753Z [warning  ] Unsupported operator '~=' encountered during filter building for attribute 'color' lineno=386 module=natural_pdf.selectors.parser\n</pre> <pre>[2025-04-27 12:30:58,569] [ WARNING] parser.py:386 - Unsupported operator '~=' encountered during filter building for attribute 'color'\n</pre> <p>Handling Missing Elements</p> <pre><code>In these examples, we know certain elements exist in the PDF. In real-world scenarios, `page.find()` might not find a match and would return `None`. Production code should check for this:\n\n```py\nsite_label = page.find('text:contains(\"Site:\")')\nif site_label:\n    # Found it! Proceed...\n    site_label.highlight(color=\"red\", label=\"Site Label\")\n    site_label.text  # Display or use the text\nelse:\n    # Didn't find it, handle appropriately...\n    \"Warning: 'Site:' label not found.\"\n```</code></pre> <p>Visual Debugging</p> <pre><code>When working with complex selectors, it's helpful to visualize what you're finding:\n\n```py\n# Clear any existing highlights\npage.clear_highlights()\n\n# Find and highlight elements\nelements = page.find_all('text[color~=red]')\nelements.highlight(color=\"red\", label=\"Red Text\")\n\n# Display the page to see what was found\npage.to_image(width=800)\n```</code></pre>"},{"location":"tutorials/02-finding-elements/#finding-specific-elements","title":"Finding Specific Elements\u00b6","text":"<p>Extracting all the text is useful, but often you need specific pieces of information. <code>natural-pdf</code> lets you find elements using selectors, similar to CSS.</p> <p>Let's find the \"Site\" and \"Date\" information from our <code>01-practice.pdf</code>:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-color","title":"Finding Elements by Color\u00b6","text":"<p>You can find elements based on their color:</p>"},{"location":"tutorials/02-finding-elements/#finding-lines-and-shapes","title":"Finding Lines and Shapes\u00b6","text":"<p>Find lines and rectangles based on their properties:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-font-properties","title":"Finding Elements by Font Properties\u00b6","text":""},{"location":"tutorials/02-finding-elements/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>You can find elements based on their position relative to other elements:</p>"},{"location":"tutorials/02-finding-elements/#combining-selectors","title":"Combining Selectors\u00b6","text":"<p>You can combine multiple conditions to find exactly what you need:</p>"},{"location":"tutorials/03-extracting-blocks/","title":"Extracting Text Blocks","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the starting element (\"Summary:\")\nstart_marker = page.find('text:contains(\"Summary:\")')\n\n# Select elements below the start_marker, stopping *before*\n# the thick horizontal line (a line with height &gt; 1).\nsummary_elements = start_marker.below(\n    include_element=True, # Include the \"Summary:\" text itself\n    until=\"line[height &gt; 1]\"\n)\n\n# Visualize the elements found in this block\nsummary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")\n\n# Extract and display the text from the collection of summary elements\nsummary_elements.extract_text()\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the starting element (\"Summary:\") start_marker = page.find('text:contains(\"Summary:\")')  # Select elements below the start_marker, stopping *before* # the thick horizontal line (a line with height &gt; 1). summary_elements = start_marker.below(     include_element=True, # Include the \"Summary:\" text itself     until=\"line[height &gt; 1]\" )  # Visualize the elements found in this block summary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")  # Extract and display the text from the collection of summary elements summary_elements.extract_text()  Out[2]: <pre>'Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\ninto the vats; and when they were fished out, there was never enough of them left to be worth\\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\\nto the world as Durham\u2019s Pure Leaf Lard!\\nViolations\\nStatute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[3]: Copied! <pre># Display the page image to see the visualization\npage.to_image()\n</pre> # Display the page image to see the visualization page.to_image() Out[3]: <p>This selects the elements using <code>.below(until=...)</code> and extracts their text. The second code block displays the page image with the visualized section.</p> <p>Selector Specificity</p> <pre><code>We used `line[height &gt; 1]` to find the thick horizontal line. You might need to adjust selectors based on the specific PDF structure. Inspecting element properties can help you find reliable start and end markers.</code></pre>"},{"location":"tutorials/03-extracting-blocks/#extracting-text-blocks","title":"Extracting Text Blocks\u00b6","text":"<p>Often, you need a specific section, like a paragraph between two headings. You can find a starting element and select everything below it until an ending element.</p> <p>Let's extract the \"Summary\" section from <code>01-practice.pdf</code>. It starts after \"Summary:\" and ends before the thick horizontal line.</p>"},{"location":"tutorials/04-table-extraction/","title":"Basic Table Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Use extract_tables() to find all tables on the page.\n# It returns a list of tables, where each table is a list of lists.\ntables_data = page.extract_tables()\n\n# Display the first table found\ntables_data[0] if tables_data else \"No tables found\"\n\n# You can also visualize the general area of the first table \n# by finding elements in that region\nif tables_data:\n    # Find a header element in the table\n    statute_header = page.find('text:contains(\"Statute\")')\n    if statute_header:\n        # Show the area\n        statute_header.below(height=100).highlight(color=\"green\", label=\"Table Area\")\n        page.to_image()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Use extract_tables() to find all tables on the page. # It returns a list of tables, where each table is a list of lists. tables_data = page.extract_tables()  # Display the first table found tables_data[0] if tables_data else \"No tables found\"  # You can also visualize the general area of the first table  # by finding elements in that region if tables_data:     # Find a header element in the table     statute_header = page.find('text:contains(\"Statute\")')     if statute_header:         # Show the area         statute_header.below(height=100).highlight(color=\"green\", label=\"Table Area\")         page.to_image() <p>This code uses <code>page.extract_tables()</code> which attempts to automatically detect tables based on visual cues like lines and whitespace. The result is a list of lists, representing the rows and cells of the table.</p> <p>Table Settings and Limitations</p> <pre><code>The default `extract_tables()` works well for simple, clearly defined tables. However, it might struggle with:\n*   Tables without clear borders or lines.\n*   Complex merged cells.\n*   Tables spanning multiple pages.\n\n`pdfplumber` (and thus `natural-pdf`) allows passing `table_settings` dictionaries to `extract_tables()` for more control over the detection strategy (e.g., `\"vertical_strategy\": \"text\"`, `\"horizontal_strategy\": \"text\"`).\n\nFor even more robust table detection, especially for tables without explicit lines, using Layout Analysis (like `page.analyze_layout(engine='tatr')`) first, finding the table `region`, and then calling `region.extract_table()` can yield better results. We'll explore layout analysis in a later tutorial.</code></pre>"},{"location":"tutorials/04-table-extraction/#basic-table-extraction","title":"Basic Table Extraction\u00b6","text":"<p>PDFs often contain tables, and <code>natural-pdf</code> provides methods to extract their data, building on <code>pdfplumber</code>'s capabilities.</p> <p>Let's extract the \"Violations\" table from our practice PDF.</p>"},{"location":"tutorials/05-excluding-content/","title":"Excluding Content (Headers/Footers)","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\n\n# Load the PDF\npdf = PDF(pdf_url)\npage = pdf.pages[0]\n\n# Let's see the bottom part of the text WITHOUT exclusions\n# It likely contains page numbers or other footer info.\nfull_text_unfiltered = page.extract_text()\n\n# Show the last 200 characters (likely containing footer text)\nfull_text_unfiltered[-200:]\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"  # Load the PDF pdf = PDF(pdf_url) page = pdf.pages[0]  # Let's see the bottom part of the text WITHOUT exclusions # It likely contains page numbers or other footer info. full_text_unfiltered = page.extract_text()  # Show the last 200 characters (likely containing footer text) full_text_unfiltered[-200:] Out[2]: <pre>' C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0\\nWrite-In Totals 0 0 0 0\\nPrecinct Summary - 11/06/2024 12:22 AM Page 1 of 387\\nReport generated with Electionware Copyright \u00a9 2007-2020'</pre> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\n\n# Define the exclusion region on every page using a lambda function\nfooter_height = 200\npdf.add_exclusion(\n    lambda page: page.region(top=page.height - footer_height),\n    label=\"Bottom 200pt Footer\"\n)\n\n# Now extract text from the first page again, exclusions are active by default\npage = pdf.pages[0]\n\n# Visualize the excluded area\nfooter_region_viz = page.region(top=page.height - footer_height)\nfooter_region_viz.highlight(label=\"Excluded Footer Area\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url)  # Define the exclusion region on every page using a lambda function footer_height = 200 pdf.add_exclusion(     lambda page: page.region(top=page.height - footer_height),     label=\"Bottom 200pt Footer\" )  # Now extract text from the first page again, exclusions are active by default page = pdf.pages[0]  # Visualize the excluded area footer_region_viz = page.region(top=page.height - footer_height) footer_region_viz.highlight(label=\"Excluded Footer Area\") page.to_image() Out[3]: In\u00a0[4]: Copied! <pre>filtered_text = page.extract_text() # use_exclusions=True is default\n\n# Show the last 200 chars with footer area excluded\nfiltered_text[-200:]\n</pre> filtered_text = page.extract_text() # use_exclusions=True is default  # Show the last 200 chars with footer area excluded filtered_text[-200:] Out[4]: <pre>'TOR\\nVote For 1\\nElection Provisional\\nTOTAL Mail Votes\\nDay Votes\\nDEM ROBERT P CASEY JR 99 70 29 0\\nREP DAVE MCCORMICK 79 69 10 0\\nLIB JOHN C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0'</pre> <p>This method is simple but might cut off content if the footer height varies or content extends lower on some pages.</p> In\u00a0[5]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\npage = pdf.pages[0] # Get page for finding elements\n\n# Find the last horizontal line on the first page\n# We'll use this logic to define our exclusion for all pages\nlast_line = page.find_all('line')[-1]\n\n# Define the exclusion function using a lambda\n# This finds the last line on *each* page and excludes below it\npdf.add_exclusion(\n    lambda p: p.find_all('line')[-1].below(),\n    label=\"Element-Based Footer\"\n)\n\n# Extract text again, with the element-based exclusion active\nfiltered_text_element = page.extract_text()\n\n# Show the last 200 chars with element-based footer exclusion\n\"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]\n\n# Visualize the element-based exclusion area\npage.clear_highlights()\n# Need to find the region again for visualization\nfooter_boundary = page.find_all('line')[-1]\nfooter_region_element = footer_boundary.below()\nfooter_region_element.show(label=\"Excluded Footer Area (Element)\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url) page = pdf.pages[0] # Get page for finding elements  # Find the last horizontal line on the first page # We'll use this logic to define our exclusion for all pages last_line = page.find_all('line')[-1]  # Define the exclusion function using a lambda # This finds the last line on *each* page and excludes below it pdf.add_exclusion(     lambda p: p.find_all('line')[-1].below(),     label=\"Element-Based Footer\" )  # Extract text again, with the element-based exclusion active filtered_text_element = page.extract_text()  # Show the last 200 chars with element-based footer exclusion \"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]  # Visualize the element-based exclusion area page.clear_highlights() # Need to find the region again for visualization footer_boundary = page.find_all('line')[-1] footer_region_element = footer_boundary.below() footer_region_element.show(label=\"Excluded Footer Area (Element)\") page.to_image() Out[5]: <p>This element-based approach is usually more reliable as it adapts to the content's position, but it depends on finding consistent boundary elements (like lines or specific text markers).</p> <p>Applying Exclusions</p> <pre><code>*   `pdf.add_exclusion(func)` applies the exclusion function (which takes a page and returns a region) to *all* pages in the PDF.\n*   `page.add_exclusion(region)` adds an exclusion region only to that specific page.\n*   `extract_text(use_exclusions=False)` can be used to temporarily disable exclusions.</code></pre>"},{"location":"tutorials/05-excluding-content/#excluding-content-headersfooters","title":"Excluding Content (Headers/Footers)\u00b6","text":"<p>Often, PDFs have repeating headers or footers on every page that you want to ignore when extracting the main content. <code>natural-pdf</code> allows you to define exclusion regions.</p> <p>We'll use a different PDF for this example, which has a distinct header and footer section: <code>0500000US42007.pdf</code>.</p>"},{"location":"tutorials/05-excluding-content/#approach-1-excluding-a-fixed-area","title":"Approach 1: Excluding a Fixed Area\u00b6","text":"<p>A simple way to exclude headers or footers is to define a fixed region based on page coordinates. Let's exclude the bottom 200 pixels of the page.</p>"},{"location":"tutorials/05-excluding-content/#approach-2-excluding-based-on-elements","title":"Approach 2: Excluding Based on Elements\u00b6","text":"<p>A more robust way is to find specific elements that reliably mark the start of the footer (or end of the header) and exclude everything below (or above) them. In <code>Examples.md</code>, the footer was defined as everything below the last horizontal line.</p>"},{"location":"tutorials/06-document-qa/","title":"Document Question Answering (QA)","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Ask about the date\nquestion_1 = \"What is the inspection date?\"\nanswer_1 = page.ask(question_1)\n\n# The result is a dictionary with the answer, confidence, etc.\nanswer_1\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Ask about the date question_1 = \"What is the inspection date?\" answer_1 = page.ask(question_1)  # The result is a dictionary with the answer, confidence, etc. answer_1 <pre>Device set to use mps:0\n</pre> Out[2]: <pre>{'answer': 'February 3, 1905',\n 'confidence': 0.9979940056800842,\n 'start': 6,\n 'end': 6,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre># Ask about the company name\nquestion_2 = \"What company was inspected?\"\nanswer_2 = page.ask(question_2)\n\n# Display the answer dictionary\nanswer_2\n</pre> # Ask about the company name question_2 = \"What company was inspected?\" answer_2 = page.ask(question_2)  # Display the answer dictionary answer_2 Out[3]: <pre>{'answer': 'Jungle Health and Safety Inspection Service',\n 'confidence': 0.9988948106765747,\n 'start': 0,\n 'end': 0,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre># Ask about specific content from the table\nquestion_3 = \"What is statute 5.8.3 about?\"\nanswer_3 = page.ask(question_3)\n\n# Display the answer\nanswer_3\n</pre> # Ask about specific content from the table question_3 = \"What is statute 5.8.3 about?\" answer_3 = page.ask(question_3)  # Display the answer answer_3 Out[4]: <pre>{'answer': 'Inadequate Protective Equipment.',\n 'confidence': 0.9997999668121338,\n 'start': 26,\n 'end': 26,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> <p>The results include the extracted <code>answer</code>, a <code>confidence</code> score (useful for filtering uncertain answers), the <code>page_num</code>, and the <code>source_elements</code>.</p> In\u00a0[5]: Copied! <pre>from natural_pdf import PDF\nimport pandas as pd\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# List of questions to ask\nquestions = [\n    \"What is the inspection date?\",\n    \"What company was inspected?\",\n    \"What is statute 5.8.3 about?\",\n    \"How many violations were there in total?\" # This might be less reliable\n]\n\n# Collect answers for each question\nresults = []\nfor q in questions:\n    answer_dict = page.ask(q)\n    # Add the original question to the dictionary\n    answer_dict['question'] = q\n    results.append(answer_dict)\n\n# Convert the list of dictionaries to a DataFrame\n# We select only the most relevant columns here\ndf_results = pd.DataFrame(results)[['question', 'answer', 'confidence']]\n\n# Display the DataFrame\ndf_results\n</pre> from natural_pdf import PDF import pandas as pd  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # List of questions to ask questions = [     \"What is the inspection date?\",     \"What company was inspected?\",     \"What is statute 5.8.3 about?\",     \"How many violations were there in total?\" # This might be less reliable ]  # Collect answers for each question results = [] for q in questions:     answer_dict = page.ask(q)     # Add the original question to the dictionary     answer_dict['question'] = q     results.append(answer_dict)  # Convert the list of dictionaries to a DataFrame # We select only the most relevant columns here df_results = pd.DataFrame(results)[['question', 'answer', 'confidence']]  # Display the DataFrame df_results Out[5]: question answer confidence 0 What is the inspection date? February 3, 1905 0.997994 1 What company was inspected? Jungle Health and Safety Inspection Service 0.998895 2 What is statute 5.8.3 about? Inadequate Protective Equipment. 0.999800 3 How many violations were there in total? 4.12.7 0.662560 <p>This shows how you can iterate through questions, collect the answer dictionaries, and then create a structured DataFrame, making it easy to review questions, answers, and their confidence levels together.</p> <p>QA Model and Limitations</p> <pre><code>*   The QA system relies on underlying transformer models. Performance and confidence scores vary.\n*   It works best for questions where the answer is explicitly stated. It cannot synthesize information or perform calculations (e.g., counting items might fail or return text containing a number rather than the count itself).\n*   You can potentially specify different QA models via the `model=` argument in `page.ask()` if others are configured.</code></pre>"},{"location":"tutorials/06-document-qa/#document-question-answering-qa","title":"Document Question Answering (QA)\u00b6","text":"<p>Sometimes, instead of searching for specific text patterns, you just want to ask the document a question directly. <code>natural-pdf</code> includes an extractive Question Answering feature.</p> <p>\"Extractive\" means it finds the literal answer text within the document, rather than generating a new answer or summarizing.</p> <p>Let's ask our <code>01-practice.pdf</code> a few questions.</p>"},{"location":"tutorials/06-document-qa/#collecting-results-into-a-dataframe","title":"Collecting Results into a DataFrame\u00b6","text":"<p>If you're asking multiple questions, it's often useful to collect the results into a pandas DataFrame for easier analysis.</p>"},{"location":"tutorials/07-layout-analysis/","title":"Layout Analysis","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Analyze the layout using the default model\n# This adds 'detected' Region objects to the page\n# It returns an ElementCollection of the detected regions\npage.analyze_layout()\ndetected_regions = page.find_all('region[source=\"detected\"]')\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Analyze the layout using the default model # This adds 'detected' Region objects to the page # It returns an ElementCollection of the detected regions page.analyze_layout() detected_regions = page.find_all('region[source=\"detected\"]') <pre>2025-04-27T16:32:53.855675Z [warning  ] GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available. lineno=76 module=natural_pdf.analyzers.layout.gemini\n</pre> <pre>[2025-04-27 12:32:53,855] [ WARNING] gemini.py:76 - GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available.\n</pre> <pre>2025-04-27T16:32:53.856604Z [warning  ] GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available. lineno=76 module=natural_pdf.analyzers.layout.gemini\n</pre> <pre>[2025-04-27 12:32:53,856] [ WARNING] gemini.py:76 - GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available.\n</pre> <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp_1yw4_22/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1467.7ms\n</pre> <pre>Speed: 4.1ms preprocess, 1467.7ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[3]: Copied! <pre># Visualize all detected regions, using default colors based on type\npage.clear_highlights() # Clear previous highlights\ndetected_regions.highlight(group_by='type', include_attrs=['confidence'])\n\n# Show the image with region overlays\npage.to_image(width=900)\n</pre> # Visualize all detected regions, using default colors based on type page.clear_highlights() # Clear previous highlights detected_regions.highlight(group_by='type', include_attrs=['confidence'])  # Show the image with region overlays page.to_image(width=900) Out[3]: In\u00a0[4]: Copied! <pre># Find and visualize only the detected table region(s)\ntables = page.find_all('region[type=table]')\ntables.show(color='lightgreen', label='Detected Table')\n</pre> # Find and visualize only the detected table region(s) tables = page.find_all('region[type=table]') tables.show(color='lightgreen', label='Detected Table') Out[4]: In\u00a0[5]: Copied! <pre># Extract text specifically from the detected table region\ntable_region = tables.first # Assuming only one table was detected\n# Extract text preserving layout\ntable_text_layout = table_region.extract_text(layout=True)\ntable_text_layout\n</pre> # Extract text specifically from the detected table region table_region = tables.first # Assuming only one table was detected # Extract text preserving layout table_text_layout = table_region.extract_text(layout=True) table_text_layout Out[5]: <pre>'Statute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[6]: Copied! <pre># Layout-detected regions can also be used for table extraction\n# This can be more robust than the basic page.extract_tables()\n# especially for tables without clear lines.\ntable_data = table_region.extract_table()\ntable_data\n</pre> # Layout-detected regions can also be used for table extraction # This can be more robust than the basic page.extract_tables() # especially for tables without clear lines. table_data = table_region.extract_table() table_data Out[6]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>Layout analysis provides structured <code>Region</code> objects. You can filter these regions by their predicted <code>type</code> and then perform actions like visualization or extracting text/tables specifically from those regions.</p> <p>Layout Models and Configuration</p> <pre><code>*   Layout analysis requires external models. Ensure these are installed.\n*   You can specify different models (`engine='yolo'`, `engine='detr'`, `engine='paddle'`) or configurations (confidence thresholds, specific classes) via arguments to `page.analyze_layout()`. Different models may perform better on different document types.\n*   The detected regions are added to the page and can be found using selectors like `page.find_all('region[type=paragraph]')`.</code></pre> <pre><code></code></pre>"},{"location":"tutorials/07-layout-analysis/#layout-analysis","title":"Layout Analysis\u00b6","text":"<p>Beyond simple text and lines, <code>natural-pdf</code> can use layout analysis models (like YOLO or DETR) to identify semantic regions within a page, such as paragraphs, tables, figures, headers, etc. This provides a higher-level understanding of the document structure.</p> <p>Let's analyze the layout of our <code>01-practice.pdf</code>.</p>"},{"location":"tutorials/07-working-with-regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Create a region in the top portion of the page\ntop_region = page.create_region(\n    50,          # x0 (left)\n    50,          # y0 (top)\n    page.width - 50,  # x1 (right)\n    200          # y1 (bottom)\n)\n\n# Visualize the region\ntop_region.show(color=\"blue\", label=\"Top Region\")\n\n# Extract text from this region\ntop_region.extract_text()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Create a region in the top portion of the page top_region = page.create_region(     50,          # x0 (left)     50,          # y0 (top)     page.width - 50,  # x1 (right)     200          # y1 (bottom) )  # Visualize the region top_region.show(color=\"blue\", label=\"Top Region\")  # Extract text from this region top_region.extract_text() Out[2]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'</pre> In\u00a0[3]: Copied! <pre># Find an element to create regions around\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Create regions relative to this element\nbelow_title = title.below(height=100)\nright_of_title = title.right(width=200) \nabove_title = title.above(height=50)\n\n# Visualize these regions\nbelow_title.show(color=\"green\", label=\"Below\")\nright_of_title.show(color=\"red\", label=\"Right\")\nabove_title.show(color=\"orange\", label=\"Above\")\n\n# Extract text from the region below the title\nbelow_title.extract_text()\n</pre> # Find an element to create regions around title = page.find('text:contains(\"Jungle Health\")')  # Create regions relative to this element below_title = title.below(height=100) right_of_title = title.right(width=200)  above_title = title.above(height=50)  # Visualize these regions below_title.show(color=\"green\", label=\"Below\") right_of_title.show(color=\"red\", label=\"Right\") above_title.show(color=\"orange\", label=\"Above\")  # Extract text from the region below the title below_title.extract_text() Out[3]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[4]: Copied! <pre># Create a region for a specific document section\nform_region = page.create_region(50, 100, page.width - 50, 300)\n\n# Find elements only within this region\nlabels = form_region.find_all('text:contains(\":\")') \n\n# Visualize the region and the elements found\nform_region.show(color=(0, 0, 1, 0.2), label=\"Form Region\")\nlabels.show(color=\"purple\", label=\"Labels\")\n\n# Count the elements found\nlen(labels)\n</pre> # Create a region for a specific document section form_region = page.create_region(50, 100, page.width - 50, 300)  # Find elements only within this region labels = form_region.find_all('text:contains(\":\")')   # Visualize the region and the elements found form_region.show(color=(0, 0, 1, 0.2), label=\"Form Region\") labels.show(color=\"purple\", label=\"Labels\")  # Count the elements found len(labels) Out[4]: <pre>3</pre> In\u00a0[5]: Copied! <pre># Find an element to work with\nelement = page.find('text:contains(\"Summary:\")')\n\n# Create a tight region around the element\ntight_region = page.create_region(\n    element.x0, element.top, \n    element.x1, element.bottom\n)\n\n# Expand it to include surrounding content\nexpanded_region = tight_region.expand(\n    left=10,       # Expand 10 points to the left\n    right=200,     # Expand 200 points to the right\n    top=5,  # Expand 5 points above\n    bottom=100  # Expand 100 points below\n)\n\n# Visualize both regions\ntight_region.show(color=\"red\", label=\"Original\")\nexpanded_region.show(color=\"blue\", label=\"Expanded\")\n\n# Extract the content from the expanded region\nexpanded_region.extract_text()\n</pre> # Find an element to work with element = page.find('text:contains(\"Summary:\")')  # Create a tight region around the element tight_region = page.create_region(     element.x0, element.top,      element.x1, element.bottom )  # Expand it to include surrounding content expanded_region = tight_region.expand(     left=10,       # Expand 10 points to the left     right=200,     # Expand 200 points to the right     top=5,  # Expand 5 points above     bottom=100  # Expand 100 points below )  # Visualize both regions tight_region.show(color=\"red\", label=\"Original\") expanded_region.show(color=\"blue\", label=\"Expanded\")  # Extract the content from the expanded region expanded_region.extract_text() Out[5]: <pre>'Summary: Worst of any, however, were the fertilizer men\\nThese people could not be shown to the visitor - for the o\\nvisitor at a hundred yards, and as for the other men, who\\nsome of which there were open vats near the level of the\\ninto the vats; and when they were fished out, there was n\\nexhibiting - sometimes they would be overlooked for days\\nto the world as Durham\u2019s Pure Leaf Lard!'</pre> In\u00a0[6]: Copied! <pre># Find two elements to serve as boundaries\nstart_elem = page.find('text:contains(\"Summary:\")')\nend_elem = page.find('text:contains(\"Statute\")')\n\n# Create a region from start to end element\nbounded_region = start_elem.until(end_elem)\n\n# Visualize the bounded region\nbounded_region.show(color=\"green\", label=\"Bounded Region\")\n\n# Extract text from this bounded region\nbounded_region.extract_text()[:200] + \"...\" if len(bounded_region.extract_text()) &gt; 200 else bounded_region.extract_text()\n</pre> # Find two elements to serve as boundaries start_elem = page.find('text:contains(\"Summary:\")') end_elem = page.find('text:contains(\"Statute\")')  # Create a region from start to end element bounded_region = start_elem.until(end_elem)  # Visualize the bounded region bounded_region.show(color=\"green\", label=\"Bounded Region\")  # Extract text from this bounded region bounded_region.extract_text()[:200] + \"...\" if len(bounded_region.extract_text()) &gt; 200 else bounded_region.extract_text() Out[6]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men...'</pre> In\u00a0[7]: Copied! <pre># Define multiple regions to extract different parts of the document\nheader_region = page.create_region(0, 0, page.width, 100)\nmain_region = page.create_region(100, 100, page.width - 100, page.height - 150)\nfooter_region = page.create_region(0, page.height - 50, page.width, page.height)\n\n# Visualize all regions\nheader_region.show(color=\"blue\", label=\"Header\")\nmain_region.show(color=\"green\", label=\"Main Content\")\nfooter_region.show(color=\"red\", label=\"Footer\")\n\n# Extract content from each region\ndocument_parts = {\n    \"header\": header_region.extract_text(),\n    \"main\": main_region.extract_text()[:100] + \"...\",\n    \"footer\": footer_region.extract_text()\n}\n\n# Show what we extracted\ndocument_parts\n</pre> # Define multiple regions to extract different parts of the document header_region = page.create_region(0, 0, page.width, 100) main_region = page.create_region(100, 100, page.width - 100, page.height - 150) footer_region = page.create_region(0, page.height - 50, page.width, page.height)  # Visualize all regions header_region.show(color=\"blue\", label=\"Header\") main_region.show(color=\"green\", label=\"Main Content\") footer_region.show(color=\"red\", label=\"Footer\")  # Extract content from each region document_parts = {     \"header\": header_region.extract_text(),     \"main\": main_region.extract_text()[:100] + \"...\",     \"footer\": footer_region.extract_text() }  # Show what we extracted document_parts Out[7]: <pre>{'header': 'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.',\n 'main': 'ruary 3, 1905\\nCount: 7\\nWorst of any, however, were the fertilizer men, and those who served in the c...',\n 'footer': 'Jungle Health and Safety Inspection Service'}</pre> In\u00a0[8]: Copied! <pre># Find a region of interest\ntable_header = page.find('text:contains(\"Statute\")')\ntable_region = table_header.below(height=100)\n\n# Visualize the region\ntable_region.show(color=\"purple\", label=\"Table Region\")\n\n# Create an image of just this region\ntable_region.to_image(resolution=150)\n</pre> # Find a region of interest table_header = page.find('text:contains(\"Statute\")') table_region = table_header.below(height=100)  # Visualize the region table_region.show(color=\"purple\", label=\"Table Region\")  # Create an image of just this region table_region.to_image(resolution=150) Out[8]: <p>Regions allow you to precisely target specific parts of a document for extraction and analysis. They're essential for handling complex document layouts and isolating the exact content you need.</p>"},{"location":"tutorials/07-working-with-regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that let you focus on specific parts of a document. They're perfect for extracting text from defined areas, finding elements within certain boundaries, and working with document sections.</p>"},{"location":"tutorials/07-working-with-regions/#creating-regions-from-elements","title":"Creating Regions from Elements\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#finding-elements-within-regions","title":"Finding Elements Within Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#expanding-and-adjusting-regions","title":"Expanding and Adjusting Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-bounded-regions","title":"Creating Bounded Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#working-with-multiple-regions","title":"Working with Multiple Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-an-image-of-a-region","title":"Creating an Image of a Region\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/","title":"Spatial Navigation","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the title of the document\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Visualize our starting point\ntitle.show(color=\"red\", label=\"Document Title\")\n\n# Display the title text\ntitle.text\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the title of the document title = page.find('text:contains(\"Jungle Health\")')  # Visualize our starting point title.show(color=\"red\", label=\"Document Title\")  # Display the title text title.text Out[2]: <pre>'Jungle Health and Safety Inspection Service'</pre> In\u00a0[3]: Copied! <pre># Create a region below the title\nregion_below = title.below(height=100)\n\n# Visualize the region\nregion_below.show(color=\"blue\", label=\"Below Title\")\n\n# Find and extract text from this region\ntext_below = region_below.extract_text()\ntext_below\n</pre> # Create a region below the title region_below = title.below(height=100)  # Visualize the region region_below.show(color=\"blue\", label=\"Below Title\")  # Find and extract text from this region text_below = region_below.extract_text() text_below Out[3]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[4]: Copied! <pre># Find two labels to serve as boundaries\nsite_label = page.find('text:contains(\"Site:\")')\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Get the region between these labels\nbetween_region = site_label.below(\n    include_element=True,     # Include starting element\n    until='text:contains(\"Date:\")',  # Stop at this element\n    include_endpoint=False    # Don't include ending element\n)\n\n# Visualize the region between labels\nbetween_region.show(color=\"green\", label=\"Between\")\n\n# Extract text from this bounded area\nbetween_region.extract_text()\n</pre> # Find two labels to serve as boundaries site_label = page.find('text:contains(\"Site:\")') date_label = page.find('text:contains(\"Date:\")')  # Get the region between these labels between_region = site_label.below(     include_element=True,     # Include starting element     until='text:contains(\"Date:\")',  # Stop at this element     include_endpoint=False    # Don't include ending element )  # Visualize the region between labels between_region.show(color=\"green\", label=\"Between\")  # Extract text from this bounded area between_region.extract_text() Out[4]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.'</pre> In\u00a0[5]: Copied! <pre># Find a field label\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Get the content to the right (the field value)\nvalue_region = site_label.right(width=200)\n\n# Visualize the label and value regions\nsite_label.show(color=\"red\", label=\"Label\")\nvalue_region.show(color=\"blue\", label=\"Value\")\n\n# Extract just the value text\nvalue_region.extract_text()\n</pre> # Find a field label site_label = page.find('text:contains(\"Site:\")')  # Get the content to the right (the field value) value_region = site_label.right(width=200)  # Visualize the label and value regions site_label.show(color=\"red\", label=\"Label\") value_region.show(color=\"blue\", label=\"Value\")  # Extract just the value text value_region.extract_text() Out[5]: <pre>'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt'</pre> In\u00a0[6]: Copied! <pre># Start with a label element\nlabel = page.find('text:contains(\"Site:\")')\n\n# Find the next and previous elements in reading order\nnext_elem = label.next()\nprev_elem = label.prev()\n\n# Visualize all three elements\nlabel.show(color=\"red\", label=\"Current\")\nnext_elem.show(color=\"green\", label=\"Next\") if next_elem else None\nprev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None\n\n# Show the text of adjacent elements\n{\n    \"current\": label.text,\n    \"next\": next_elem.text if next_elem else \"None\",\n    \"previous\": prev_elem.text if prev_elem else \"None\"\n}\n</pre> # Start with a label element label = page.find('text:contains(\"Site:\")')  # Find the next and previous elements in reading order next_elem = label.next() prev_elem = label.prev()  # Visualize all three elements label.show(color=\"red\", label=\"Current\") next_elem.show(color=\"green\", label=\"Next\") if next_elem else None prev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None  # Show the text of adjacent elements {     \"current\": label.text,     \"next\": next_elem.text if next_elem else \"None\",     \"previous\": prev_elem.text if prev_elem else \"None\" } Out[6]: <pre>{'current': 'Site: ', 'next': 'i', 'previous': 'S'}</pre> In\u00a0[7]: Copied! <pre># Find a section label\nsummary = page.find('text:contains(\"Summary:\")')\n\n# Find the next bold text element\nnext_bold = summary.next('text:bold', limit=20)\n\n# Find the nearest line element\nnearest_line = summary.nearest('line')\n\n# Visualize what we found\nsummary.show(color=\"red\", label=\"Summary\")\nnext_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None\nnearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None\n\n# Show the content we found\n{\n    \"summary\": summary.text,\n    \"next_bold\": next_bold.text if next_bold else \"None found\",\n    \"nearest_line\": nearest_line if nearest_line else \"None found\"\n}\n</pre> # Find a section label summary = page.find('text:contains(\"Summary:\")')  # Find the next bold text element next_bold = summary.next('text:bold', limit=20)  # Find the nearest line element nearest_line = summary.nearest('line')  # Visualize what we found summary.show(color=\"red\", label=\"Summary\") next_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None nearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None  # Show the content we found {     \"summary\": summary.text,     \"next_bold\": next_bold.text if next_bold else \"None found\",     \"nearest_line\": nearest_line if nearest_line else \"None found\" } Out[7]: <pre>{'summary': 'Summary: ',\n 'next_bold': 'u',\n 'nearest_line': &lt;LineElement type=horizontal width=2.0 bbox=(50, 352, 550, 352)&gt;}</pre> In\u00a0[8]: Copied! <pre># Find a table heading\ntable_heading = page.find('text:contains(\"Statute\")')\ntable_heading.show(color=\"purple\", label=\"Table Header\")\n\n# Extract table rows using spatial navigation\nrows = []\ncurrent = table_heading\n\n# Get the next 4 rows\nfor i in range(4):\n    # Find the next row below the current one\n    next_row = current.below(height=15)\n    \n    if next_row:\n        rows.append(next_row)\n        current = next_row  # Move to the next row\n    else:\n        break\n\n# Visualize all found rows\npage.clear_highlights()\nfor i, row in enumerate(rows):\n    row.highlight(label=f\"Row {i+1}\")\npage.to_image(width=700)\n</pre> # Find a table heading table_heading = page.find('text:contains(\"Statute\")') table_heading.show(color=\"purple\", label=\"Table Header\")  # Extract table rows using spatial navigation rows = [] current = table_heading  # Get the next 4 rows for i in range(4):     # Find the next row below the current one     next_row = current.below(height=15)          if next_row:         rows.append(next_row)         current = next_row  # Move to the next row     else:         break  # Visualize all found rows page.clear_highlights() for i, row in enumerate(rows):     row.highlight(label=f\"Row {i+1}\") page.to_image(width=700) Out[8]: In\u00a0[9]: Copied! <pre># Extract text from each row\n[row.extract_text() for row in rows]\n</pre> # Extract text from each row [row.extract_text() for row in rows] Out[9]: <pre>['4.12.7 Unsanitary Working Conditions. Critical',\n '4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious',\n '5.8.3 Inadequate Protective Equipment. Serious',\n '6.3.9 Ineffective Injury Prevention. Serious']</pre> In\u00a0[10]: Copied! <pre># Find all potential field labels (text with a colon)\nlabels = page.find_all('text:contains(\":\")') \n\n# Visualize the labels\nlabels.show(color=\"blue\", label=\"Labels\")\n\n# Extract key-value pairs\nfield_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    key = label.text.strip().rstrip(':')\n    \n    # Skip if not a proper label\n    if not key:\n        continue\n    \n    # Get the value to the right\n    value = label.right(width=200).extract_text().strip()\n    \n    # Add to our collection\n    field_data[key] = value\n\n# Show the extracted data\nfield_data\n</pre> # Find all potential field labels (text with a colon) labels = page.find_all('text:contains(\":\")')   # Visualize the labels labels.show(color=\"blue\", label=\"Labels\")  # Extract key-value pairs field_data = {}  for label in labels:     # Clean up the label text     key = label.text.strip().rstrip(':')          # Skip if not a proper label     if not key:         continue          # Get the value to the right     value = label.right(width=200).extract_text().strip()          # Add to our collection     field_data[key] = value  # Show the extracted data field_data Out[10]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> <p>Spatial navigation mimics how humans read documents, letting you navigate content based on physical relationships between elements. It's especially useful for extracting structured data from forms, tables, and formatted documents.</p>"},{"location":"tutorials/08-spatial-navigation/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>Spatial navigation lets you work with PDF content based on the physical layout of elements on the page. It's perfect for finding elements relative to each other and extracting information in context.</p>"},{"location":"tutorials/08-spatial-navigation/#finding-elements-above-and-below","title":"Finding Elements Above and Below\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-content-between-elements","title":"Finding Content Between Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#navigating-left-and-right","title":"Navigating Left and Right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-adjacent-elements","title":"Finding Adjacent Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#combining-with-element-selectors","title":"Combining with Element Selectors\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-table-rows-with-spatial-navigation","title":"Extracting Table Rows with Spatial Navigation\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-key-value-pairs","title":"Extracting Key-Value Pairs\u00b6","text":""},{"location":"tutorials/09-section-extraction/","title":"Section Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF using the relative path\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\n# Find horizontal lines that separate book entries\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Visualize the potential section boundaries\nhorizontal_lines.highlight(color=\"red\", label=\"Section Boundaries\")\npage.to_image()\n</pre> from natural_pdf import PDF  # Load the PDF using the relative path pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  # Find horizontal lines that separate book entries horizontal_lines = page.find_all('line:horizontal')  # Visualize the potential section boundaries horizontal_lines.highlight(color=\"red\", label=\"Section Boundaries\") page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Count what we found\nlen(horizontal_lines)\n</pre> # Count what we found len(horizontal_lines) Out[3]: <pre>9</pre> In\u00a0[4]: Copied! <pre># Extract sections based on horizontal lines\n# Each section starts at a horizontal line and ends at the next one\nbook_sections = page.get_sections(\n    start_elements=horizontal_lines,\n    boundary_inclusion='start'  # Include the boundary in the section\n)\n\n# Visualize each section\npage.clear_highlights()\nfor section in book_sections:\n    section.highlight()\npage.to_image()\n</pre> # Extract sections based on horizontal lines # Each section starts at a horizontal line and ends at the next one book_sections = page.get_sections(     start_elements=horizontal_lines,     boundary_inclusion='start'  # Include the boundary in the section )  # Visualize each section page.clear_highlights() for section in book_sections:     section.highlight() page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Display section count and preview the first section\n{\n    \"total_sections\": len(book_sections),\n    \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\"\n}\n</pre> # Display section count and preview the first section {     \"total_sections\": len(book_sections),     \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\" } Out[5]: <pre>{'total_sections': 9,\n 'first_section_text': '6/12/2023 - Copies Removed: 2\\nTristan Strong punches a hole in the sky (Removed: 1)\\nAuthor: Mbalia, ...'}</pre> In\u00a0[6]: Copied! <pre># Extract and display content from the first few book entries\nbook_entries = []\n\nfor i, section in enumerate(book_sections[:5]):\n    # Extract the section text\n    text = section.extract_text().strip()\n    \n    # Try to parse book information\n    title = \"\"\n    author = \"\"\n    isbn = \"\"\n    \n    # Extract title (typically the first line)\n    title_match = section.find('text:contains(\"Title:\")')\n    if title_match:\n        title_value = title_match.right(width=400).extract_text()\n        title = title_value.strip()\n    \n    # Extract author\n    author_match = section.find('text:contains(\"Author:\")')\n    if author_match:\n        author_value = author_match.right(width=400).extract_text()\n        author = author_value.strip()\n    \n    # Extract ISBN\n    isbn_match = section.find('text:contains(\"ISBN:\")')\n    if isbn_match:\n        isbn_value = isbn_match.right(width=400).extract_text()\n        isbn = isbn_value.strip()\n    \n    # Add to our collection\n    book_entries.append({\n        \"number\": i + 1,\n        \"title\": title,\n        \"author\": author,\n        \"isbn\": isbn,\n        \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text\n    })\n\n# Display the structured book entries\nimport pandas as pd\npd.DataFrame(book_entries)\n</pre> # Extract and display content from the first few book entries book_entries = []  for i, section in enumerate(book_sections[:5]):     # Extract the section text     text = section.extract_text().strip()          # Try to parse book information     title = \"\"     author = \"\"     isbn = \"\"          # Extract title (typically the first line)     title_match = section.find('text:contains(\"Title:\")')     if title_match:         title_value = title_match.right(width=400).extract_text()         title = title_value.strip()          # Extract author     author_match = section.find('text:contains(\"Author:\")')     if author_match:         author_value = author_match.right(width=400).extract_text()         author = author_value.strip()          # Extract ISBN     isbn_match = section.find('text:contains(\"ISBN:\")')     if isbn_match:         isbn_value = isbn_match.right(width=400).extract_text()         isbn = isbn_value.strip()          # Add to our collection     book_entries.append({         \"number\": i + 1,         \"title\": title,         \"author\": author,         \"isbn\": isbn,         \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text     })  # Display the structured book entries import pandas as pd pd.DataFrame(book_entries) Out[6]: number title author isbn preview 0 1 Log Atlanta Public S\\n023\\nemoved: 2\\na hole i... Atlanta Public Schools\\nPublished: 2019\\nAcqui... 6/12/2023 - Copies Removed: 2\\nTristan Strong ... 1 2 6/7/2023 - Copies Removed: 2 2 3 Atlanta Public School\\nved: 2\\nin the sky (Rem... Atlanta Public Schools\\n93-2 Published: 2019\\n... Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Ba... 3 4 Atlanta Public Schools\\nd: 2\\nn the sky (Remov... Atlanta Public Schools\\nPublished: 2019\\nAcqui... Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book ... 4 5 6/6/2023 - Copies Removed: 130 In\u00a0[7]: Copied! <pre>page.viewer()\n</pre> page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[7]: In\u00a0[8]: Copied! <pre># Find title elements with specific selectors\ntitle_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\ntitle_elements.show()\n</pre> # Find title elements with specific selectors title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]') title_elements.show() Out[8]: In\u00a0[9]: Copied! <pre># Extract sections starting from each title\n# This now directly returns an ElementCollection\ntitle_sections = page.get_sections(\n    start_elements=title_elements,\n    boundary_inclusion='start'\n)\n\n# Show the title-based sections\npage.clear_highlights()\ntitle_sections.highlight()\npage.to_image()\n</pre> # Extract sections starting from each title # This now directly returns an ElementCollection title_sections = page.get_sections(     start_elements=title_elements,     boundary_inclusion='start' )  # Show the title-based sections page.clear_highlights() title_sections.highlight() page.to_image() Out[9]: In\u00a0[10]: Copied! <pre># Count the sections found\nlen(title_sections)\n</pre> # Count the sections found len(title_sections) Out[10]: <pre>7</pre> In\u00a0[11]: Copied! <pre># Use horizontal line elements as section dividers\ndividers = page.find_all('line[horizontal]')\n\n# Compare the different boundary inclusion options\ninclusion_options = {\n    'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),\n    'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),\n    'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),\n    'both': page.get_sections(start_elements=dividers, boundary_inclusion='both')\n}\n\n# Count sections with each option\nsection_counts = {option: len(sections) for option, sections in inclusion_options.items()}\nsection_counts\n</pre> # Use horizontal line elements as section dividers dividers = page.find_all('line[horizontal]')  # Compare the different boundary inclusion options inclusion_options = {     'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),     'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),     'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),     'both': page.get_sections(start_elements=dividers, boundary_inclusion='both') }  # Count sections with each option section_counts = {option: len(sections) for option, sections in inclusion_options.items()} section_counts Out[11]: <pre>{'none': 0, 'start': 0, 'end': 0, 'both': 0}</pre> In\u00a0[12]: Copied! <pre># Define specific start and end points - let's extract just one book entry\n# We'll look for the first and second horizontal lines\npage.clear_highlights()\n\nstart_point = title_elements[0]\nend_point = title_elements[1]\n\n# Extract the section between these points\nsingle_book_entry = page.get_sections(\n    start_elements=[start_point],\n    end_elements=[end_point],\n    boundary_inclusion='start'  # Include the start but not the end\n)\n    \n# Visualize the custom section\nsingle_book_entry.highlight(color=\"green\", label=\"Single Book Entry\")\n    \nprint(single_book_entry[0].extract_text())\n\npage.to_image()\n</pre> # Define specific start and end points - let's extract just one book entry # We'll look for the first and second horizontal lines page.clear_highlights()  start_point = title_elements[0] end_point = title_elements[1]  # Extract the section between these points single_book_entry = page.get_sections(     start_elements=[start_point],     end_elements=[end_point],     boundary_inclusion='start'  # Include the start but not the end )      # Visualize the custom section single_book_entry.highlight(color=\"green\", label=\"Single Book Entry\")      print(single_book_entry[0].extract_text())  page.to_image() <pre>Tristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-2 Published: 2019\nSite Barcode Price Acquired Removed By\nJoseph Humphries 32441014018707 6/11/2021 113396-42441\nElementary School\nWas Available -- Weeded\nUpside down in the middle of nowhere (Removed: 1)\n</pre> Out[12]: In\u00a0[13]: Copied! <pre># Get sections across the first two pages\nmulti_page_sections = [] # Initialize as a list\n\nfor page_num in range(min(2, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page (returns ElementCollection)\n    page_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Add elements from the collection to our list\n    multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection\n\n# Display info about each section (showing first 3)\n[{\n    \"page\": section.page.number + 1,  # 1-indexed page number for display\n    \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text()\n} for section in multi_page_sections]\n</pre> # Get sections across the first two pages multi_page_sections = [] # Initialize as a list  for page_num in range(min(2, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page (returns ElementCollection)     page_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Add elements from the collection to our list     multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection  # Display info about each section (showing first 3) [{     \"page\": section.page.number + 1,  # 1-indexed page number for display     \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text() } for section in multi_page_sections] Out[13]: <pre>[{'page': 2, 'text': 'Tristan Strong punches a hole in the sky (Removed:...'},\n {'page': 2, 'text': 'Upside down in the middle of nowhere (Removed: 1)\\n...'},\n {'page': 2, 'text': 'Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Bazaz. ...'},\n {'page': 2, 'text': 'Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book edito...'},\n {'page': 2, 'text': 'The Abenaki (Removed: 1)\\nAuthor: Landau, Elaine. I...'},\n {'page': 2, 'text': 'Afghanistan (Removed: 1)\\nAuthor: Milivojevic, Jova...'},\n {'page': 2, 'text': 'Alexander the Great rocks the world (Removed: 1)\\nA...'},\n {'page': 3, 'text': 'The Anasazi (Removed: 1)\\nAuthor: Petersen, David. ...'},\n {'page': 3, 'text': 'And then what happened, Paul Revere? (Removed: 1)\\n...'},\n {'page': 3, 'text': 'The assassination of Martin Luther King Jr (Remove...'},\n {'page': 3, 'text': 'Barbara Jordan. (Removed: 1)\\nAuthor: Wexler, Diane...'},\n {'page': 3, 'text': 'Bedtime for Batman (Removed: 1)\\nAuthor: Dahl, Mich...'},\n {'page': 3, 'text': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskeg...'},\n {'page': 3, 'text': 'Bigfoot Wallace (Removed: 1)\\nAuthor: Harper,Jo. IS...'},\n {'page': 3, 'text': 'The blaze engulfs : January 1939 to December 1941 ...'}]</pre> In\u00a0[14]: Copied! <pre># Extract all book entries across multiple pages\nbook_database = []\n\n# Process first 3 pages (or fewer if the document is shorter)\nfor page_num in range(min(3, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page\n    book_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Process each book section\n    for section in book_sections:\n        # Skip sections that are too short (might be headers/footers)\n        if len(section.extract_text()) &lt; 50:\n            continue\n            \n        # Extract book information\n        book_info = {\"page\": page_num + 1}\n        \n        for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.strip(':').lower()\n                field_value = field_element.extract_text().replace(field, '').strip()\n                book_info[field_name] = field_value\n\n        # Below the field name\n        for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.lower()\n                field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()\n                book_info[field_name] = field_value\n\n        book_database.append(book_info)\n\n# Display sample entries (first 3)\nimport pandas as pd\n\ndf = pd.json_normalize(book_database)\ndf.head()\n</pre> # Extract all book entries across multiple pages book_database = []  # Process first 3 pages (or fewer if the document is shorter) for page_num in range(min(3, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page     book_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Process each book section     for section in book_sections:         # Skip sections that are too short (might be headers/footers)         if len(section.extract_text()) &lt; 50:             continue                      # Extract book information         book_info = {\"page\": page_num + 1}                  for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.strip(':').lower()                 field_value = field_element.extract_text().replace(field, '').strip()                 book_info[field_name] = field_value          # Below the field name         for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.lower()                 field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()                 book_info[field_name] = field_value          book_database.append(book_info)  # Display sample entries (first 3) import pandas as pd  df = pd.json_normalize(book_database) df.head() Out[14]: page author isbn price acquired barcode removed by 0 1 Mbalia, Kwame. 978-1-36803993-2 6/11/2021 11 32441014018707 113396-42441 1 1 Lamana, Julie T. 978-1-45212456-8 (alk. $15.00 6/12/2023 11 32441012580849 113396-42441 2 1 Wangu, Madhu Bazaz. 0-8160-2442-1 $10.00 4/19/2018 ch 33343000017835 christen.mcclain 3 1 Kelly Wand, book editor. 0-7377-1314-3 (lib.) $19.95 3/21/2006 ch *3431000028742 christen.mcclain 4 1 Landau, Elaine. 0-531-20227-5 $16.50 2/21/2000 33 33170000506628 33554-43170 <p>Section extraction lets you break down documents into logical parts, making it easier to generate summaries, extract specific content, and create structured data from semi-structured documents. In this example, we've shown how to convert a PDF library catalog into a structured book database.</p>"},{"location":"tutorials/09-section-extraction/#section-extraction","title":"Section Extraction\u00b6","text":"<p>Documents are often organized into logical sections like chapters, articles, or content blocks. This tutorial shows how to extract these sections using natural-pdf, using a library weeding log as an example.</p>"},{"location":"tutorials/09-section-extraction/#basic-section-extraction","title":"Basic Section Extraction\u00b6","text":""},{"location":"tutorials/09-section-extraction/#working-with-section-content","title":"Working with Section Content\u00b6","text":""},{"location":"tutorials/09-section-extraction/#using-different-section-boundaries","title":"Using Different Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#section-boundary-inclusion-options","title":"Section Boundary Inclusion Options\u00b6","text":""},{"location":"tutorials/09-section-extraction/#custom-section-boundaries","title":"Custom Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#multi-page-sections","title":"Multi-page Sections\u00b6","text":""},{"location":"tutorials/09-section-extraction/#building-a-book-database","title":"Building a Book Database\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/","title":"Form Field Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find fields with labels ending in colon\nlabels = page.find_all('text:contains(\":\")')\n\n# Visualize the found labels\nlabels.show(color=\"blue\", label=\"Field Labels\")\n\n# Count how many potential fields we found\nlen(labels)\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find fields with labels ending in colon labels = page.find_all('text:contains(\":\")')  # Visualize the found labels labels.show(color=\"blue\", label=\"Field Labels\")  # Count how many potential fields we found len(labels) Out[2]: <pre>4</pre> In\u00a0[3]: Copied! <pre># Extract the value for each field label\nform_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    field_name = label.text.strip().rstrip(':')\n    \n    # Find the value to the right of the label\n    value_region = label.right(width=200)\n    value = value_region.extract_text().strip()\n    \n    # Store in our dictionary\n    form_data[field_name] = value\n\n# Display the extracted data\nform_data\n</pre> # Extract the value for each field label form_data = {}  for label in labels:     # Clean up the label text     field_name = label.text.strip().rstrip(':')          # Find the value to the right of the label     value_region = label.right(width=200)     value = value_region.extract_text().strip()          # Store in our dictionary     form_data[field_name] = value  # Display the extracted data form_data Out[3]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> In\u00a0[4]: Copied! <pre># Clear previous highlights\npage.clear_highlights()\n\n# Highlight both labels and their values\nfor label in labels:\n    # Highlight the label in red\n    label.show(color=\"red\", label=\"Label\")\n    \n    # Highlight the value area in blue\n    label.right(width=200).show(color=\"blue\", label=\"Value\")\n\n# Show the page image with highlighted elements\npage.to_image()\n</pre> # Clear previous highlights page.clear_highlights()  # Highlight both labels and their values for label in labels:     # Highlight the label in red     label.show(color=\"red\", label=\"Label\")          # Highlight the value area in blue     label.right(width=200).show(color=\"blue\", label=\"Value\")  # Show the page image with highlighted elements page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Extract values that might span multiple lines\nmulti_line_data = {}\n\nfor label in labels:\n    # Get the field name\n    field_name = label.text.strip().rstrip(':')\n    \n    # Look both to the right and below\n    right_value = label.right(width=200).extract_text().strip()\n    below_value = label.below(height=50).extract_text().strip()\n    \n    # Combine the values if they're different\n    if right_value in below_value:\n        value = below_value\n    else:\n        value = f\"{right_value} {below_value}\".strip()\n    \n    # Add to results\n    multi_line_data[field_name] = value\n\n# Show fields with potential multi-line values\nmulti_line_data\n</pre> # Extract values that might span multiple lines multi_line_data = {}  for label in labels:     # Get the field name     field_name = label.text.strip().rstrip(':')          # Look both to the right and below     right_value = label.right(width=200).extract_text().strip()     below_value = label.below(height=50).extract_text().strip()          # Combine the values if they're different     if right_value in below_value:         value = below_value     else:         value = f\"{right_value} {below_value}\".strip()          # Add to results     multi_line_data[field_name] = value  # Show fields with potential multi-line values multi_line_data Out[5]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt Date: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health Violation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'}</pre> In\u00a0[6]: Copied! <pre>import re\n\n# Find dates in the format July 31, YYY\ndate_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'\n\n# Search all text elements for dates\ntext_elements = page.find_all('text')\nprint([elem.text for elem in text_elements])\ndates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))\n\n# Visualize the date fields\ndates.show(color=\"green\", label=\"Date\")\n\n# Extract just the date values\ndate_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates]\ndate_texts\n</pre> import re  # Find dates in the format July 31, YYY date_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'  # Search all text elements for dates text_elements = page.find_all('text') print([elem.text for elem in text_elements]) dates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))  # Visualize the date fields dates.show(color=\"green\", label=\"Date\")  # Extract just the date values date_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates] date_texts <pre>['Jungle Health and Safety Inspection Service', 'INS-UP70N51NCL41R', 'Site: ', 'Durham\u2019s Meatpacking  ', 'Chicago, Ill.', 'Date:  ', 'February 3, 1905', 'Violation Count: ', '7', 'Summary: ', 'Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.', 'These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary ', 'visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in ', 'some of which there were open vats near the level of the floor, their peculiar trouble was that they fell', 'into the vats; and when they were fished out, there was never enough of them left to be worth ', 'exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out', 'to the world as Durham\u2019s Pure Leaf Lard!', 'Violations', 'Statute', 'Description', 'Level', 'Repeat?', '4.12.7', 'Unsanitary Working Conditions.', 'Critical', '5.8.3', 'Inadequate Protective Equipment.', 'Serious', '6.3.9', 'Ineffective Injury Prevention.', 'Serious', '7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', '8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', '9.6.4', 'Inadequate Ventilation Systems.', 'Serious', '10.2.7', 'Insufficient Employee Training for Safe Work Practices.', 'Serious', 'Jungle Health and Safety Inspection Service']\n</pre> Out[6]: <pre>['February 3, 1905']</pre> In\u00a0[7]: Copied! <pre># Run layout analysis to find table structures\npage.analyze_layout()\n\n# Find possible form tables\ntables = page.find_all('region[type=table]')\n\nif tables:\n    # Visualize the tables\n    tables.show(color=\"purple\", label=\"Form Table\")\n    \n    # Extract data from the first table\n    first_table = tables[0]\n    table_data = first_table.extract_table()\n    table_data\nelse:\n    # Try to find form-like structure using text alignment\n    # Create a region where a form might be\n    form_region = page.create_region(50, 200, page.width - 50, 500)\n    \n    # Group text by vertical position\n    rows = {}\n    text_elements = form_region.find_all('text')\n    \n    for elem in text_elements:\n        # Round y-position to group elements in the same row\n        row_pos = round(elem.top / 5) * 5\n        if row_pos not in rows:\n            rows[row_pos] = []\n        rows[row_pos].append(elem)\n    \n    # Extract data from rows (first 5 rows)\n    row_data = []\n    for y in sorted(rows.keys())[:5]:\n        # Sort elements by x-position (left to right)\n        elements = sorted(rows[y], key=lambda e: e.x0)\n        \n        # Show the row\n        row_box = form_region.create_region(\n            min(e.x0 for e in elements), \n            min(e.top for e in elements),\n            max(e.x1 for e in elements),\n            max(e.bottom for e in elements)\n        )\n        row_box.show(color=None, use_color_cycling=True)\n        \n        # Extract text from row\n        row_text = [e.text for e in elements]\n        row_data.append(row_text)\n    \n    # Show the extracted rows\n    row_data\n</pre> # Run layout analysis to find table structures page.analyze_layout()  # Find possible form tables tables = page.find_all('region[type=table]')  if tables:     # Visualize the tables     tables.show(color=\"purple\", label=\"Form Table\")          # Extract data from the first table     first_table = tables[0]     table_data = first_table.extract_table()     table_data else:     # Try to find form-like structure using text alignment     # Create a region where a form might be     form_region = page.create_region(50, 200, page.width - 50, 500)          # Group text by vertical position     rows = {}     text_elements = form_region.find_all('text')          for elem in text_elements:         # Round y-position to group elements in the same row         row_pos = round(elem.top / 5) * 5         if row_pos not in rows:             rows[row_pos] = []         rows[row_pos].append(elem)          # Extract data from rows (first 5 rows)     row_data = []     for y in sorted(rows.keys())[:5]:         # Sort elements by x-position (left to right)         elements = sorted(rows[y], key=lambda e: e.x0)                  # Show the row         row_box = form_region.create_region(             min(e.x0 for e in elements),              min(e.top for e in elements),             max(e.x1 for e in elements),             max(e.bottom for e in elements)         )         row_box.show(color=None, use_color_cycling=True)                  # Extract text from row         row_text = [e.text for e in elements]         row_data.append(row_text)          # Show the extracted rows     row_data <pre>2025-04-27T16:33:32.219597Z [warning  ] GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available. lineno=76 module=natural_pdf.analyzers.layout.gemini\n</pre> <pre>[2025-04-27 12:33:32,219] [ WARNING] gemini.py:76 - GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available.\n</pre> <pre>2025-04-27T16:33:32.220225Z [warning  ] GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available. lineno=76 module=natural_pdf.analyzers.layout.gemini\n</pre> <pre>[2025-04-27 12:33:32,220] [ WARNING] gemini.py:76 - GOOGLE_API_KEY environment variable not set. Gemini detector (via OpenAI lib) will not be available.\n</pre> <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp8cxd7muz/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1520.2ms\n</pre> <pre>Speed: 4.2ms preprocess, 1520.2ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[8]: Copied! <pre># Combine label-based and pattern-based extraction\nall_fields = {}\n\n# 1. First get fields with explicit labels\nfor label in labels:\n    field_name = label.text.strip().rstrip(':')\n    value = label.right(width=200).extract_text().strip()\n    all_fields[field_name] = value\n\n# 2. Add date fields that we found with pattern matching\nfor date_elem in dates:\n    # Find the nearest label\n    nearby_label = date_elem.nearest('text:contains(\":\")')\n    \n    if nearby_label:\n        # Extract the label text\n        label_text = nearby_label.text.strip().rstrip(':')\n        \n        # Get the date value\n        date_value = re.search(date_pattern, date_elem.text).group(0)\n        \n        # Add to our results if not already present\n        if label_text not in all_fields:\n            all_fields[label_text] = date_value\n\n# Show all extracted fields\nall_fields\n</pre> # Combine label-based and pattern-based extraction all_fields = {}  # 1. First get fields with explicit labels for label in labels:     field_name = label.text.strip().rstrip(':')     value = label.right(width=200).extract_text().strip()     all_fields[field_name] = value  # 2. Add date fields that we found with pattern matching for date_elem in dates:     # Find the nearest label     nearby_label = date_elem.nearest('text:contains(\":\")')          if nearby_label:         # Extract the label text         label_text = nearby_label.text.strip().rstrip(':')                  # Get the date value         date_value = re.search(date_pattern, date_elem.text).group(0)                  # Add to our results if not already present         if label_text not in all_fields:             all_fields[label_text] = date_value  # Show all extracted fields all_fields Out[8]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> <p>Form field extraction enables you to automate data entry and document processing. By combining different techniques like label detection, spatial navigation, and pattern matching, you can handle a wide variety of form layouts.</p>"},{"location":"tutorials/10-form-field-extraction/#form-field-extraction","title":"Form Field Extraction\u00b6","text":"<p>Business documents like invoices, forms, and applications contain field-value pairs that need to be extracted. This tutorial shows how to identify and extract these form fields.</p>"},{"location":"tutorials/10-form-field-extraction/#extracting-field-values","title":"Extracting Field Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#visualizing-labels-and-values","title":"Visualizing Labels and Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#handling-multi-line-values","title":"Handling Multi-line Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#finding-pattern-based-fields","title":"Finding Pattern-Based Fields\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#working-with-form-tables","title":"Working with Form Tables\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#combining-different-extraction-techniques","title":"Combining Different Extraction Techniques\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/","title":"Enhanced Table Processing","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\""},{"location":"tutorials/11-enhanced-table-processing/#enhanced-table-processing","title":"Enhanced Table Processing\u00b6","text":"<p>Tables are a common way to present structured data in documents, but they can be challenging to extract correctly. This tutorial demonstrates advanced techniques for working with tables in natural-pdf.</p> <p>TK</p>"},{"location":"tutorials/12-ocr-integration/","title":"OCR Integration for Scanned Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Try extracting text without OCR\ntext_without_ocr = page.extract_text()\nf\"Without OCR: {len(text_without_ocr)} characters extracted\"\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # Try extracting text without OCR text_without_ocr = page.extract_text() f\"Without OCR: {len(text_without_ocr)} characters extracted\" Out[2]: <pre>'Without OCR: 0 characters extracted'</pre> In\u00a0[3]: Copied! <pre># Apply OCR using the default engine (EasyOCR) for English\npage.apply_ocr(languages=['en'])\n\n# Select all text pieces found by OCR\ntext_elements = page.find_all('text[source=ocr]')\nprint(f\"Found {len(text_elements)} text elements using default OCR\")\n\n# Visualize the elements\ntext_elements.highlight()\n\n# Apply OCR using PaddleOCR for English and Chinese\npage.apply_ocr(engine='paddle', languages=['en', 'ch_sim'])\n\n# Apply OCR using SuryaOCR for English and German\npage.apply_ocr(engine='surya', languages=['en', 'de'])\n\ntext_with_ocr = page.extract_text()\nprint(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\")\n</pre> # Apply OCR using the default engine (EasyOCR) for English page.apply_ocr(languages=['en'])  # Select all text pieces found by OCR text_elements = page.find_all('text[source=ocr]') print(f\"Found {len(text_elements)} text elements using default OCR\")  # Visualize the elements text_elements.highlight()  # Apply OCR using PaddleOCR for English and Chinese page.apply_ocr(engine='paddle', languages=['en', 'ch_sim'])  # Apply OCR using SuryaOCR for English and German page.apply_ocr(engine='surya', languages=['en', 'de'])  text_with_ocr = page.extract_text() print(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\") <pre>2025-04-27T16:33:44.559124Z [warning  ] Using CPU. Note: This module is much faster with a GPU. lineno=71 module=easyocr.easyocr\n</pre> <pre>[2025-04-27 12:33:44,559] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>Found 47 text elements using default OCR\n</pre> <pre>[2025/04/27 12:33:56] ppocr WARNING: Since the angle classifier is not initialized, it will not be used during the forward process\n</pre> <pre>Loaded detection model s3://text_detection/2025_02_18 on device mps with dtype torch.float16\n</pre> <pre>Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n</pre> <pre>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 /Users/soma/Development/natural-pdf/natural_pdf/ocr/ocr_manager.py:195 in apply_ocr              \u2502\n\u2502                                                                                                  \u2502\n\u2502   192 \u2502   \u2502   \u2502   \u2502   logger.debug(f\"[{thread_id}] Acquired inference lock for {selected_engin   \u2502\n\u2502   193 \u2502   \u2502   \u2502   \u2502   inference_start_time = time.monotonic()                                    \u2502\n\u2502   194 \u2502   \u2502   \u2502   \u2502                                                                              \u2502\n\u2502 \u2771 195 \u2502   \u2502   \u2502   \u2502   results = engine_instance.process_image(                                   \u2502\n\u2502   196 \u2502   \u2502   \u2502   \u2502   \u2502   images=images,                                                         \u2502\n\u2502   197 \u2502   \u2502   \u2502   \u2502   \u2502   languages=languages,                                                   \u2502\n\u2502   198 \u2502   \u2502   \u2502   \u2502   \u2502   min_confidence=min_confidence,                                         \u2502\n\u2502                                                                                                  \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502\n\u2502 \u2502             detect_only = False                                                              \u2502 \u2502\n\u2502 \u2502                  device = None                                                               \u2502 \u2502\n\u2502 \u2502                       e = AssertionError('You need to pass in one list of languages for each \u2502 \u2502\n\u2502 \u2502                           image')                                                            \u2502 \u2502\n\u2502 \u2502                  engine = 'surya'                                                            \u2502 \u2502\n\u2502 \u2502         engine_instance = &lt;natural_pdf.ocr.engine_surya.SuryaOCREngine object at             \u2502 \u2502\n\u2502 \u2502                           0x35512cfd0&gt;                                                       \u2502 \u2502\n\u2502 \u2502           final_options = None                                                               \u2502 \u2502\n\u2502 \u2502              image_dims = ['1275x1651']                                                      \u2502 \u2502\n\u2502 \u2502                  images = [&lt;PIL.Image.Image image mode=RGB size=1275x1651 at 0x35512CCD0&gt;]   \u2502 \u2502\n\u2502 \u2502 inference_acquired_time = 272326.616208958                                                   \u2502 \u2502\n\u2502 \u2502          inference_lock = &lt;unlocked _thread.lock object at 0x35511eec0&gt;                      \u2502 \u2502\n\u2502 \u2502    inference_start_time = 272326.616209666                                                   \u2502 \u2502\n\u2502 \u2502    inference_wait_start = 272326.616208708                                                   \u2502 \u2502\n\u2502 \u2502                is_batch = True                                                               \u2502 \u2502\n\u2502 \u2502               languages = ['en', 'de']                                                       \u2502 \u2502\n\u2502 \u2502          min_confidence = None                                                               \u2502 \u2502\n\u2502 \u2502                 options = None                                                               \u2502 \u2502\n\u2502 \u2502         processing_mode = 'batch'                                                            \u2502 \u2502\n\u2502 \u2502    selected_engine_name = 'surya'                                                            \u2502 \u2502\n\u2502 \u2502                    self = &lt;natural_pdf.ocr.ocr_manager.OCRManager object at 0x104beb580&gt;     \u2502 \u2502\n\u2502 \u2502               thread_id = 'MainThread'                                                       \u2502 \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f \u2502\n\u2502                                                                                                  \u2502\n\u2502 /Users/soma/Development/natural-pdf/natural_pdf/ocr/engine.py:117 in process_image               \u2502\n\u2502                                                                                                  \u2502\n\u2502   114 \u2502   \u2502   \u2502   processed_img = self._preprocess_image(img)                                    \u2502\n\u2502   115 \u2502   \u2502   \u2502                                                                                  \u2502\n\u2502   116 \u2502   \u2502   \u2502   # Process the image with the engine-specific implementation                    \u2502\n\u2502 \u2771 117 \u2502   \u2502   \u2502   raw_results = self._process_single_image(processed_img, detect_only, options   \u2502\n\u2502   118 \u2502   \u2502   \u2502                                                                                  \u2502\n\u2502   119 \u2502   \u2502   \u2502   # Convert results to standardized format                                       \u2502\n\u2502   120 \u2502   \u2502   \u2502   text_regions = self._standardize_results(raw_results, effective_confidence,    \u2502\n\u2502                                                                                                  \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e   \u2502\n\u2502 \u2502          detect_only = False                                                               \u2502   \u2502\n\u2502 \u2502               device = None                                                                \u2502   \u2502\n\u2502 \u2502 effective_confidence = 0.2                                                                 \u2502   \u2502\n\u2502 \u2502     effective_device = 'cpu'                                                               \u2502   \u2502\n\u2502 \u2502  effective_languages = ['en', 'de']                                                        \u2502   \u2502\n\u2502 \u2502          image_batch = [&lt;PIL.Image.Image image mode=RGB size=1275x1651 at 0x35512CCD0&gt;]    \u2502   \u2502\n\u2502 \u2502               images = [&lt;PIL.Image.Image image mode=RGB size=1275x1651 at 0x35512CCD0&gt;]    \u2502   \u2502\n\u2502 \u2502                  img = &lt;PIL.Image.Image image mode=RGB size=1275x1651 at 0x35512CCD0&gt;      \u2502   \u2502\n\u2502 \u2502            languages = ['en', 'de']                                                        \u2502   \u2502\n\u2502 \u2502       min_confidence = None                                                                \u2502   \u2502\n\u2502 \u2502              options = None                                                                \u2502   \u2502\n\u2502 \u2502        processed_img = &lt;PIL.Image.Image image mode=RGB size=1275x1651 at 0x35512CCD0&gt;      \u2502   \u2502\n\u2502 \u2502              results = []                                                                  \u2502   \u2502\n\u2502 \u2502                 self = &lt;natural_pdf.ocr.engine_surya.SuryaOCREngine object at 0x35512cfd0&gt; \u2502   \u2502\n\u2502 \u2502         single_image = False                                                               \u2502   \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f   \u2502\n\u2502                                                                                                  \u2502\n\u2502 /Users/soma/Development/natural-pdf/natural_pdf/ocr/engine_surya.py:71 in _process_single_image  \u2502\n\u2502                                                                                                  \u2502\n\u2502    68 \u2502   \u2502   if detect_only:                                                                    \u2502\n\u2502    69 \u2502   \u2502   \u2502   results = self._detection_predictor(images=[image])                            \u2502\n\u2502    70 \u2502   \u2502   else:                                                                              \u2502\n\u2502 \u2771  71 \u2502   \u2502   \u2502   results = self._recognition_predictor(                                         \u2502\n\u2502    72 \u2502   \u2502   \u2502   \u2502   images=[image],                                                            \u2502\n\u2502    73 \u2502   \u2502   \u2502   \u2502   langs=langs,  # Use the languages set during initialization                \u2502\n\u2502    74 \u2502   \u2502   \u2502   \u2502   det_predictor=self._detection_predictor,                                   \u2502\n\u2502                                                                                                  \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e            \u2502\n\u2502 \u2502 detect_only = False                                                               \u2502            \u2502\n\u2502 \u2502       image = &lt;PIL.Image.Image image mode=RGB size=1275x1651 at 0x35512CCD0&gt;      \u2502            \u2502\n\u2502 \u2502       langs = [['en'], ['de']]                                                    \u2502            \u2502\n\u2502 \u2502     options = None                                                                \u2502            \u2502\n\u2502 \u2502        self = &lt;natural_pdf.ocr.engine_surya.SuryaOCREngine object at 0x35512cfd0&gt; \u2502            \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f            \u2502\n\u2502                                                                                                  \u2502\n\u2502 /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages/surya/recognition/__init__.py:4 \u2502\n\u2502 4 in __call__                                                                                    \u2502\n\u2502                                                                                                  \u2502\n\u2502    41 \u2502   \u2502   \u2502   polygons: List[List[List[List[int]]]] | None = None,                           \u2502\n\u2502    42 \u2502   \u2502   \u2502   sort_lines: bool = True                                                        \u2502\n\u2502    43 \u2502   ) -&gt; List[OCRResult]:                                                                  \u2502\n\u2502 \u2771  44 \u2502   \u2502   \u2502   assert len(images) == len(langs), \"You need to pass in one list of languages   \u2502\n\u2502    45 \u2502   \u2502   \u2502   images = convert_if_not_rgb(images)                                            \u2502\n\u2502    46 \u2502   \u2502   \u2502   if highres_images is not None:                                                 \u2502\n\u2502    47 \u2502   \u2502   \u2502   \u2502   assert len(images) == len(highres_images), \"You need to pass in one high   \u2502\n\u2502                                                                                                  \u2502\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e    \u2502\n\u2502 \u2502                 bboxes = None                                                             \u2502    \u2502\n\u2502 \u2502          det_predictor = &lt;surya.detection.DetectionPredictor object at 0x3550b5390&gt;       \u2502    \u2502\n\u2502 \u2502   detection_batch_size = None                                                             \u2502    \u2502\n\u2502 \u2502         highres_images = None                                                             \u2502    \u2502\n\u2502 \u2502                 images = [&lt;PIL.Image.Image image mode=RGB size=1275x1651 at 0x35512CCD0&gt;] \u2502    \u2502\n\u2502 \u2502                  langs = [['en'], ['de']]                                                 \u2502    \u2502\n\u2502 \u2502               polygons = None                                                             \u2502    \u2502\n\u2502 \u2502 recognition_batch_size = None                                                             \u2502    \u2502\n\u2502 \u2502                   self = &lt;surya.recognition.RecognitionPredictor object at 0x35512c5b0&gt;   \u2502    \u2502\n\u2502 \u2502             sort_lines = True                                                             \u2502    \u2502\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nAssertionError: You need to pass in one list of languages for each image\n</pre> <pre>2025-04-27T16:34:08.337905Z [error    ] An unexpected error occurred during OCR processing: You need to pass in one list of languages for each image lineno=236 module=natural_pdf.ocr.ocr_manager\n\n</pre> <pre>[2025-04-27 12:34:08,337] [   ERROR] ocr_manager.py:236 - An unexpected error occurred during OCR processing: You need to pass in one list of languages for each image\nTraceback (most recent call last):\n  File \"/Users/soma/Development/natural-pdf/natural_pdf/ocr/ocr_manager.py\", line 195, in apply_ocr\n    results = engine_instance.process_image(\n  File \"/Users/soma/Development/natural-pdf/natural_pdf/ocr/engine.py\", line 117, in process_image\n    raw_results = self._process_single_image(processed_img, detect_only, options)\n  File \"/Users/soma/Development/natural-pdf/natural_pdf/ocr/engine_surya.py\", line 71, in _process_single_image\n    results = self._recognition_predictor(\n  File \"/Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages/surya/recognition/__init__.py\", line 44, in __call__\n    assert len(images) == len(langs), \"You need to pass in one list of languages for each image\"\nAssertionError: You need to pass in one list of languages for each image\n</pre> <pre>2025-04-27T16:34:08.409507Z [error    ] Batch OCR processing failed: You need to pass in one list of languages for each image lineno=366 module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-27 12:34:08,409] [   ERROR] pdf.py:366 - Batch OCR processing failed: You need to pass in one list of languages for each image\n</pre> <pre>\nExtracted text after OCR:\n...\n</pre> In\u00a0[4]: Copied! <pre>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\n\n# Re-apply OCR using EasyOCR with specific options\neasy_opts = EasyOCROptions(\n    paragraph=False,\n)\npage.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)\n\npaddle_opts = PaddleOCROptions(\n    use_angle_cls=False,\n    det_db_thresh=0.3,\n)\npage.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)\n\nsurya_opts = SuryaOCROptions()\npage.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts)\n</pre> from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions  # Re-apply OCR using EasyOCR with specific options easy_opts = EasyOCROptions(     paragraph=False, ) page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)  paddle_opts = PaddleOCROptions(     use_angle_cls=False,     det_db_thresh=0.3, ) page.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)  surya_opts = SuryaOCROptions() page.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts) <pre>\rDetecting bboxes:   0%|                                                           | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.74it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.73it/s]</pre> <pre>\n</pre> Out[4]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[5]: Copied! <pre># Process all pages in the document\n\n# Apply OCR to all pages (example using EasyOCR)\npdf.apply_ocr(engine='easyocr', languages=['en'])\nprint(f\"Applied OCR to {len(pdf.pages)} pages.\")\n\n# Or apply layout analysis to all pages (example using Paddle)\n# pdf.apply_layout(engine='paddle')\n# print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")\n\n# Extract text from all pages (uses OCR results if available)\nall_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")\n\nprint(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\")\n</pre> # Process all pages in the document  # Apply OCR to all pages (example using EasyOCR) pdf.apply_ocr(engine='easyocr', languages=['en']) print(f\"Applied OCR to {len(pdf.pages)} pages.\")  # Or apply layout analysis to all pages (example using Paddle) # pdf.apply_layout(engine='paddle') # print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")  # Extract text from all pages (uses OCR results if available) all_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")  print(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\") <pre>2025-04-27T16:34:36.840807Z [warning  ] Ignoring unsupported layout keyword argument: 'page_separator' lineno=57 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-04-27 12:34:36,840] [ WARNING] text_extraction.py:57 - Ignoring unsupported layout keyword argument: 'page_separator'\n</pre> <pre>Applied OCR to 1 pages.\n\nCombined text from all pages:\nRed (ZGB tuple]\nJungle Health and Satety Inspection Service\nINS-UPONSINCLAIR\nSite: Durham's Meatpacking Chicago, IIl.\nDate: February 3, 1905\nViolation Count:\nSummary: Worst of any, however; were the fertilizer men, and those who served in the cooking rooms\nThese people could not be shown to the visitorfor the odor of a fertilizer man would scare anyordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near...\n</pre> In\u00a0[6]: Copied! <pre>from natural_pdf import PDF\n\ninput_pdf_path = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\"\n\npdf = PDF(input_pdf_path)\n# Apply OCR to all pages before saving\n# Use desired engine and options\npdf.apply_ocr(engine='easyocr', languages=['en'])\n\npdf.save_searchable(\"needs-ocr-searchable.pdf\")\n\nprint(\"Saved searchable PDF to needs-ocr-searchable.pdf\")\n</pre> from natural_pdf import PDF  input_pdf_path = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\"  pdf = PDF(input_pdf_path) # Apply OCR to all pages before saving # Use desired engine and options pdf.apply_ocr(engine='easyocr', languages=['en'])  pdf.save_searchable(\"needs-ocr-searchable.pdf\")  print(\"Saved searchable PDF to needs-ocr-searchable.pdf\") <pre>2025-04-27T16:34:37.509530Z [warning  ] Using CPU. Note: This module is much faster with a GPU. lineno=71 module=easyocr.easyocr\n</pre> <pre>[2025-04-27 12:34:37,509] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>Saved searchable PDF to needs-ocr-searchable.pdf\n</pre> <p>This creates <code>needs-ocr-searchable.pdf</code>, which looks identical to the original but now has a text layer corresponding to the OCR results. You can adjust the rendering resolution used during saving with the <code>dpi</code> parameter (default is 300).</p> <p>OCR integration enables you to work with scanned documents, historical archives, and image-based PDFs that don't have embedded text. By combining OCR with natural-pdf's layout analysis capabilities, you can turn any document into structured, searchable data.</p>"},{"location":"tutorials/12-ocr-integration/#ocr-integration-for-scanned-documents","title":"OCR Integration for Scanned Documents\u00b6","text":"<p>Optical Character Recognition (OCR) allows you to extract text from scanned documents where the text isn't embedded in the PDF. This tutorial demonstrates how to work with scanned documents.</p>"},{"location":"tutorials/12-ocr-integration/#applying-ocr-and-finding-elements","title":"Applying OCR and Finding Elements\u00b6","text":"<p>The core method is <code>page.apply_ocr()</code>. This runs the OCR process and adds <code>TextElement</code> objects to the page. You can specify the engine and languages.</p> <p>Note: Re-applying OCR to the same page or region will automatically remove any previously generated OCR elements for that area before adding the new ones.</p>"},{"location":"tutorials/12-ocr-integration/#advanced-ocr-configuration","title":"Advanced OCR Configuration\u00b6","text":"<p>For more control, import and use the specific <code>Options</code> class for your chosen engine within the <code>apply_ocr</code> call.</p>"},{"location":"tutorials/12-ocr-integration/#interactive-ocr-correction-debugging","title":"Interactive OCR Correction / Debugging\u00b6","text":"<p>If OCR results aren't perfect, you can use the bundled interactive web application (SPA) to review and correct them.</p> <ol> <li><p>Package the data: After running <code>apply_ocr</code> (or <code>apply_layout</code>), use <code>create_correction_task_package</code> to create a zip file containing the PDF images and detected elements.</p> <pre>from natural_pdf.utils.packaging import create_correction_task_package\n\npage.apply_ocr()\n\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</pre> </li> <li><p>Run the SPA: Navigate to the SPA directory within the installed <code>natural_pdf</code> library in your terminal and start a simple web server.</p> </li> <li><p>Use the SPA: Open <code>http://localhost:8000</code> in your browser. Drag the <code>correction_package.zip</code> file onto the page to load the document. You can then click on text elements to correct the OCR results.</p> </li> </ol>"},{"location":"tutorials/12-ocr-integration/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>Apply OCR or layout analysis to all pages using the <code>PDF</code> object.</p>"},{"location":"tutorials/12-ocr-integration/#saving-pdfs-with-searchable-text","title":"Saving PDFs with Searchable Text\u00b6","text":"<p>After applying OCR to a PDF, you can save a new version of the PDF where the recognized text is embedded as an invisible layer. This makes the text searchable and copyable in standard PDF viewers.</p> <p>Use the <code>save_searchable()</code> method on the <code>PDF</code> object:</p>"},{"location":"tutorials/13-semantic-search/","title":"Semantic Search Across Multiple Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n#%pip install \"natural-pdf[search]\"  # Ensure search dependencies are installed\n</pre> #%pip install \"natural-pdf[all]\" #%pip install \"natural-pdf[search]\"  # Ensure search dependencies are installed In\u00a0[2]: Copied! <pre>import logging\nimport natural_pdf\n\n# Optional: Configure logging to see progress\nnatural_pdf.configure_logging(level=logging.INFO)\n\n# Define the paths to your PDF files\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n    # Add more PDF paths as needed\n]\n\n# Create a PDFCollection\ncollection = natural_pdf.PDFCollection(pdf_paths)\nprint(f\"Created collection with {len(collection.pdfs)} PDFs.\")\n</pre> import logging import natural_pdf  # Optional: Configure logging to see progress natural_pdf.configure_logging(level=logging.INFO)  # Define the paths to your PDF files pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"     # Add more PDF paths as needed ]  # Create a PDFCollection collection = natural_pdf.PDFCollection(pdf_paths) print(f\"Created collection with {len(collection.pdfs)} PDFs.\") <pre>natural_pdf.collections.pdf_collection - INFO - Initializing 2 PDF objects...\n</pre> <pre>natural_pdf.core.pdf - INFO - Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpc7n5dufd.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpc7n5dufd.pdf\n</pre> <pre>natural_pdf.ocr.ocr_manager - INFO - OCRManager initialized.\n</pre> <pre>natural_pdf.analyzers.layout.layout_manager - INFO - LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling', 'gemini']\n</pre> <pre>natural_pdf.core.highlighting_service - INFO - HighlightingService initialized with ColorManager.\n</pre> <pre>natural_pdf.classification.manager - INFO - ClassificationManager initialized on device: None\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf' initialized with 1 pages.\n</pre> <pre>natural_pdf.classification.manager - INFO - ClassificationManager initialized on device: None\n</pre> <pre>natural_pdf.extraction.manager - INFO - Initialized StructuredDataManager.\n</pre> <pre>natural_pdf.core.pdf - INFO - Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5mocjexv.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5mocjexv.pdf\n</pre> <pre>natural_pdf.ocr.ocr_manager - INFO - OCRManager initialized.\n</pre> <pre>natural_pdf.analyzers.layout.layout_manager - INFO - LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling', 'gemini']\n</pre> <pre>natural_pdf.core.highlighting_service - INFO - HighlightingService initialized with ColorManager.\n</pre> <pre>natural_pdf.classification.manager - INFO - ClassificationManager initialized on device: None\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf' initialized with 5 pages.\n</pre> <pre>natural_pdf.classification.manager - INFO - ClassificationManager initialized on device: None\n</pre> <pre>natural_pdf.extraction.manager - INFO - Initialized StructuredDataManager.\n</pre> <pre>natural_pdf.collections.pdf_collection - INFO - Successfully initialized 2 PDFs. Failed: 0\n</pre> <pre>Created collection with 2 PDFs.\n</pre> In\u00a0[3]: Copied! <pre># Initialize search. 'index=True' builds the index immediately.\n# This might take some time depending on the number and size of PDFs.\ncollection.init_search(index=True) \nprint(\"Search index initialized.\")\n</pre> # Initialize search. 'index=True' builds the index immediately. # This might take some time depending on the number and size of PDFs. collection.init_search(index=True)  print(\"Search index initialized.\") <pre>natural_pdf.search.searchable_mixin - INFO - Using default collection name 'default_collection' for in-memory service.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Creating new SearchService: name='default_collection', persist=False, model=default\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - HaystackSearchService initialized for collection='default_collection' (persist=False, model='sentence-transformers/all-MiniLM-L6-v2'). Default path: './natural_pdf_index'\n</pre> <pre>natural_pdf.search - INFO - Created new HaystackSearchService instance for collection 'default_collection'.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - index=True: Proceeding to index collection immediately after search initialization.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Starting internal indexing process into SearchService collection 'default_collection'...\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Prepared 6 indexable items for indexing.\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Index request for collection='default_collection', docs=6, model='sentence-transformers/all-MiniLM-L6-v2', force=False, persist=False\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Created SentenceTransformersDocumentEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None)\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Preparing Haystack Documents from 6 indexable items...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Embedding 6 documents using 'sentence-transformers/all-MiniLM-L6-v2'...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Successfully embedded 6 documents.\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Writing 6 embedded documents to store 'default_collection'...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Successfully wrote 6 documents to store 'default_collection'.\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Store 'default_collection' document count after write: 6\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Successfully completed indexing into SearchService collection 'default_collection'.\n</pre> <pre>Search index initialized.\n</pre> In\u00a0[4]: Copied! <pre># Perform a search query\nquery = \"american president\"\nresults = collection.find_relevant(query)\n\nprint(f\"Found {len(results)} results for '{query}':\")\n</pre> # Perform a search query query = \"american president\" results = collection.find_relevant(query)  print(f\"Found {len(results)} results for '{query}':\") <pre>natural_pdf.search.searchable_mixin - INFO - Searching collection 'default_collection' via HaystackSearchService...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Search request for collection='default_collection', query_type=str, options=TextSearchOptions(top_k=10, retriever_top_k=20, filters=None, use_reranker=True, reranker_instance=None, reranker_model=None, reranker_api_key=None)\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Created SentenceTransformersTextEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None)\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Running retrieval pipeline for collection 'default_collection'...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Retrieved 6 documents.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - SearchService returned 6 results from collection 'default_collection'.\n</pre> <pre>Found 6 results for 'american president':\n</pre> In\u00a0[5]: Copied! <pre># Process and display the results\nif results:\n    for i, result in enumerate(results):\n        print(f\"  {i+1}. PDF: {result['pdf_path']}\")\n        print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")\n        # Display a snippet of the content\n        snippet = result.get('content_snippet', '')\n        print(f\"     Snippet: {snippet}...\") \nelse:\n    print(\"  No relevant results found.\")\n\n# You can access the full content if needed via the result object, \n# though 'content_snippet' is usually sufficient for display.\n</pre> # Process and display the results if results:     for i, result in enumerate(results):         print(f\"  {i+1}. PDF: {result['pdf_path']}\")         print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")         # Display a snippet of the content         snippet = result.get('content_snippet', '')         print(f\"     Snippet: {snippet}...\")  else:     print(\"  No relevant results found.\")  # You can access the full content if needed via the result object,  # though 'content_snippet' is usually sufficient for display. <pre>  1. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5mocjexv.pdf\n     Page: 2 (Score: 0.0708)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nThe Anasazi (Removed: 1)\nAuthor: Petersen, David. ISBN: 0-516-01121-9 (trade) Published: 1991\nSit...\n  2. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5mocjexv.pdf\n     Page: 5 (Score: 0.0669)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000562167 $13.10 11/5/1999 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  3. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpc7n5dufd.pdf\n     Page: 1 (Score: -0.0040)\n     Snippet: Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men...\n  4. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5mocjexv.pdf\n     Page: 4 (Score: -0.0245)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nChildren of the Philippines (Removed: 1)\nAuthor: Kinkade, Sheila, 1962- ISBN: 0-87614-993-X Publi...\n  5. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5mocjexv.pdf\n     Page: 3 (Score: -0.0445)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000507600 $19.45 2/21/2000 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  6. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp5mocjexv.pdf\n     Page: 1 (Score: -0.0473)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/12/2023 - Copies Removed: 2\nTristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-...\n</pre> <p>Semantic search allows you to efficiently query large sets of documents to find the most relevant information without needing exact keyword matches, leveraging the meaning and context of your query.</p>"},{"location":"tutorials/13-semantic-search/#semantic-search-across-multiple-documents","title":"Semantic Search Across Multiple Documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to find information relevant to a specific query across all documents, not just within a single one. This tutorial demonstrates how to perform semantic search over a <code>PDFCollection</code>.</p>"},{"location":"tutorials/13-semantic-search/#initializing-the-search-index","title":"Initializing the Search Index\u00b6","text":"<p>Before performing a search, you need to initialize the search capabilities for the collection. This involves processing the documents and building an index.</p>"},{"location":"tutorials/13-semantic-search/#performing-a-semantic-search","title":"Performing a Semantic Search\u00b6","text":"<p>Once the index is ready, you can use the <code>find_relevant()</code> method to search for content semantically related to your query.</p>"},{"location":"tutorials/13-semantic-search/#understanding-search-results","title":"Understanding Search Results\u00b6","text":"<p>The <code>find_relevant()</code> method returns a list of dictionaries, each representing a relevant text chunk found in one of the PDFs. Each result includes:</p> <ul> <li><code>pdf_path</code>: The path to the PDF document where the result was found.</li> <li><code>page_number</code>: The page number within the PDF.</li> <li><code>score</code>: A relevance score (higher means more relevant).</li> <li><code>content_snippet</code>: A snippet of the text chunk that matched the query.</li> </ul>"},{"location":"visual-debugging/","title":"Visual Debugging","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find a specific element and add a persistent highlight\npage.find_all('text:contains(\"Summary\")').highlight()\npage.find_all('text:contains(\"Date\")').highlight()\npage.find_all('line').highlight()\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find a specific element and add a persistent highlight page.find_all('text:contains(\"Summary\")').highlight() page.find_all('text:contains(\"Date\")').highlight() page.find_all('line').highlight() page.to_image(width=700) Out[1]: In\u00a0[2]: Copied! <pre>page.clear_highlights()\n\ntitle = page.find('text:bold[size&gt;=12]')\n\n# Highlight with a specific color (string name, hex, or RGB/RGBA tuple)\n# title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity\n# title.highlight(color=\"#FF0000\")        # Hex color\ntitle.highlight(color=\"red\")           # Color name\n\ntext = page.find('text:contains(\"Critical\")')\n\n# Add a label to the highlight (appears in legend)\ntext.highlight(label=\"Critical\")\n\n# Combine color and label\nrect = page.find('rect')\nrect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")\n\npage.to_image(width=700)\n</pre> page.clear_highlights()  title = page.find('text:bold[size&gt;=12]')  # Highlight with a specific color (string name, hex, or RGB/RGBA tuple) # title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity # title.highlight(color=\"#FF0000\")        # Hex color title.highlight(color=\"red\")           # Color name  text = page.find('text:contains(\"Critical\")')  # Add a label to the highlight (appears in legend) text.highlight(label=\"Critical\")  # Combine color and label rect = page.find('rect') rect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")  page.to_image(width=700) Out[2]: In\u00a0[3]: Copied! <pre># Find and highlight all headings with a single color/label\nheadings = page.find_all('text[size&gt;=14]:bold')\nheadings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")\n\n# Find and highlight all tables\ntables = page.find_all('region[type=table]')\ntables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")\n\n# View the result\npage.viewer()\n</pre> # Find and highlight all headings with a single color/label headings = page.find_all('text[size&gt;=14]:bold') headings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")  # Find and highlight all tables tables = page.find_all('region[type=table]') tables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")  # View the result page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[3]: In\u00a0[4]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Highlight the region\ncontent.show()\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Highlight the region content.show() Out[4]: <p>Or look at just the region by itself</p> In\u00a0[5]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Crop to the region\ncontent.to_image(crop_only=True, include_highlights=False)\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Crop to the region content.to_image(crop_only=True, include_highlights=False) Out[5]: In\u00a0[6]: Copied! <pre># Analyze and highlight text styles\npage.clear_highlights()\n\npage.analyze_text_styles()\npage.find_all('text').highlight(group_by='style_label')\n\npage.to_image(width=700)\n</pre> # Analyze and highlight text styles page.clear_highlights()  page.analyze_text_styles() page.find_all('text').highlight(group_by='style_label')  page.to_image(width=700) Out[6]: In\u00a0[7]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\ntext = page.find_all('line')\ntext.highlight(include_attrs=['width', 'color'])\n\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  text = page.find_all('line') text.highlight(include_attrs=['width', 'color'])  page.to_image(width=700) Out[7]: <p>Does it get busy? YES.</p> In\u00a0[8]: Copied! <pre># Clear all highlights on the page\npage.clear_highlights()\n\n# Apply new highlights\npage.find_all('text:bold').highlight(label=\"Bold Text\")\npage.viewer()\n</pre> # Clear all highlights on the page page.clear_highlights()  # Apply new highlights page.find_all('text:bold').highlight(label=\"Bold Text\") page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[8]: In\u00a0[9]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\")\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\") page = pdf.pages[0] page.to_image(width=700) Out[9]: In\u00a0[10]: Copied! <pre>response = page.ask(\"How many votes did Kamala Harris get on Election Day?\")\nresponse\n</pre> response = page.ask(\"How many votes did Kamala Harris get on Election Day?\") response <pre>Device set to use cpu\n</pre> Out[10]: <pre>{'answer': '60',\n 'confidence': 0.3211139440536499,\n 'start': 31,\n 'end': 31,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[11]: Copied! <pre>response['source_elements'].show()\n</pre> response['source_elements'].show() Out[11]:"},{"location":"visual-debugging/#visual-debugging","title":"Visual Debugging\u00b6","text":"<p>Sometimes it's hard to understand what's happening when working with PDFs. Natural PDF provides powerful visual debugging tools to help you see what you're extracting.</p>"},{"location":"visual-debugging/#adding-persistent-highlights","title":"Adding Persistent Highlights\u00b6","text":"<p>Use the <code>.highlight()</code> method on <code>Element</code> or <code>ElementCollection</code> objects to add persistent highlights to a page. These highlights are stored and will appear when viewing the page later.</p>"},{"location":"visual-debugging/#customizing-persistent-highlights","title":"Customizing Persistent Highlights\u00b6","text":"<p>Customize the appearance of persistent highlights added with <code>.highlight()</code>:</p>"},{"location":"visual-debugging/#highlighting-multiple-elements","title":"Highlighting Multiple Elements\u00b6","text":"<p>Highlighting an <code>ElementCollection</code> applies the highlight to all elements within it. By default, all elements in the collection get the same color and a label based on their type.</p>"},{"location":"visual-debugging/#highlighting-regions","title":"Highlighting Regions\u00b6","text":"<p>You can highlight regions to see what area you're working with:</p>"},{"location":"visual-debugging/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>Visualize text styles to understand the document structure:</p>"},{"location":"visual-debugging/#displaying-attributes","title":"Displaying Attributes\u00b6","text":"<p>You can display element attributes directly on the highlights:</p>"},{"location":"visual-debugging/#clearing-highlights","title":"Clearing Highlights\u00b6","text":"<p>You can clear persistent highlights from a page:</p>"},{"location":"visual-debugging/#document-qa-visualization","title":"Document QA Visualization\u00b6","text":"<p>Visualize document QA results:</p>"},{"location":"visual-debugging/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to visualize PDF content, you might want to explore:</p> <ul> <li>OCR capabilities for working with scanned documents</li> <li>Layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"}]}