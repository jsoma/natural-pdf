{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Natural PDF","text":"<p>A friendly library for working with PDFs, built on top of pdfplumber.</p> <p>Natural PDF lets you find and extract content from PDFs using simple code that makes sense.</p> <ul> <li>Live demo here</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install natural_pdf\n# All the extras\npip install \"natural_pdf[all]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from natural_pdf import PDF\n\npdf = PDF('document.pdf')\npage = pdf.pages[0]\n\n# Find the title and get content below it\ntitle = page.find('text:contains(\"Summary\"):bold')\ncontent = title.below().extract_text()\n\n# Exclude everything above 'CONFIDENTIAL' and below last line on page\npage.add_exclusion(page.find('text:contains(\"CONFIDENTIAL\")').above())\npage.add_exclusion(page.find_all('line')[-1].below())\n\n# Get the clean text without header/footer\nclean_text = page.extract_text()\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<p>Here are a few highlights of what you can do:</p>"},{"location":"#find-elements-with-selectors","title":"Find Elements with Selectors","text":"<p>Use CSS-like selectors to find text, shapes, and more.</p> <pre><code># Find bold text containing \"Revenue\"\npage.find('text:contains(\"Revenue\"):bold').extract_text()\n\n# Find all large text\npage.find_all('text[size&gt;=12]').extract_text()\n</code></pre> <p>Learn more about selectors \u2192</p>"},{"location":"#navigate-spatially","title":"Navigate Spatially","text":"<p>Move around the page relative to elements, not just coordinates.</p> <pre><code># Extract text below a specific heading\nintro_text = page.find('text:contains(\"Introduction\")').below().extract_text()\n\n# Extract text from one heading to the next\nmethods_text = page.find('text:contains(\"Methods\")').below(\n    until='text:contains(\"Results\")'\n).extract_text()\n</code></pre> <p>Explore more navigation methods \u2192</p>"},{"location":"#extract-clean-text","title":"Extract Clean Text","text":"<p>Easily extract text content, automatically handling common page elements like headers and footers (if exclusions are set).</p> <pre><code># Extract all text from the page (respecting exclusions)\npage_text = page.extract_text()\n\n# Extract text from a specific region\nsome_region = page.find(...)\nregion_text = some_region.extract_text()\n</code></pre> <p>Learn about text extraction \u2192 Learn about exclusion zones \u2192</p>"},{"location":"#apply-ocr","title":"Apply OCR","text":"<p>Extract text from scanned documents using various OCR engines.</p> <pre><code># Apply OCR using the default engine\nocr_elements = page.apply_ocr()\n\n# Extract text (will use OCR results if available)\ntext = page.extract_text()\n</code></pre> <p>Explore OCR options \u2192</p>"},{"location":"#analyze-document-layout","title":"Analyze Document Layout","text":"<p>Use AI models to detect document structures like titles, paragraphs, and tables.</p> <pre><code># Detect document structure\npage.analyze_layout()\n\n# Highlight titles and tables\npage.find_all('region[type=title]').highlight(color=\"purple\")\npage.find_all('region[type=table]').highlight(color=\"blue\")\n\n# Extract data from the first table\ntable_data = page.find('region[type=table]').extract_table()\n</code></pre> <p>Learn about layout models \u2192 Working with tables? \u2192</p>"},{"location":"#document-question-answering","title":"Document Question Answering","text":"<p>Ask natural language questions directly to your documents.</p> <pre><code># Ask a question\nresult = pdf.ask(\"What was the company's revenue in 2022?\")\nif result.get(\"found\", False):\n    print(f\"Answer: {result['answer']}\")\n</code></pre> <p>Learn about Document QA \u2192</p>"},{"location":"#visualize-your-work","title":"Visualize Your Work","text":"<p>Debug and understand your extractions visually.</p> <pre><code># Highlight headings\npage.find_all('text[size&gt;=14]').highlight(color=\"red\", label=\"Headings\")\n\n# Launch the interactive viewer (Jupyter)\n# Requires: pip install natural-pdf[interactive]\npage.viewer()\n\n# Or save an image\n# page.save_image(\"highlighted.png\")\n</code></pre> <p>See more visualization options \u2192</p>"},{"location":"#documentation-topics","title":"Documentation Topics","text":"<p>Choose what you want to learn about:</p>"},{"location":"#task-based-guides","title":"Task-based Guides","text":"<ul> <li>Getting Started: Install the library and run your first extraction</li> <li>PDF Navigation: Open PDFs and work with pages</li> <li>Element Selection: Find text and other elements using selectors</li> <li>Text Extraction: Extract clean text from documents</li> <li>Regions: Work with specific areas of a page</li> <li>Visual Debugging: See what you're extracting</li> <li>OCR: Extract text from scanned documents</li> <li>Layout Analysis: Detect document structure</li> <li>Tables: Extract tabular data</li> <li>Document QA: Ask questions to your documents</li> </ul>"},{"location":"#reference","title":"Reference","text":"<ul> <li>API Reference: Complete library reference</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all the classes and methods in Natural PDF.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#pdf-class","title":"PDF Class","text":"<p>The main entry point for working with PDFs.</p> <pre><code>class PDF:\n    \"\"\"\n    The main entry point for working with PDFs.\n\n    Parameters:\n        path (str): Path to the PDF file.\n        password (str, optional): Password for encrypted PDFs. Default: None\n        reading_order (bool, optional): Sort elements in reading order. Default: True\n        keep_spaces (bool, optional): Keep spaces in word elements. Default: True\n        font_attrs (list, optional): Font attributes to use for text grouping. \n                                    Default: ['fontname', 'size']\n        ocr (bool/dict/str, optional): OCR configuration. Default: False\n        ocr_engine (str/Engine, optional): OCR engine to use. Default: \"easyocr\"\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>pages</code> Access pages in the document N/A (property) <code>PageCollection</code> <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>add_exclusion(func, label=None)</code> Add a document-wide exclusion zone <code>func</code>: Function taking a page and returning region<code>label</code>: Optional label for the exclusion <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections across all pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries ('start', 'end', 'both', 'none') <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None)</code> Ask a question about the document content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path <code>dict</code>: Result with answer and metadata"},{"location":"api/#page-class","title":"Page Class","text":"<p>Represents a single page in a PDF document.</p> <pre><code>class Page:\n    \"\"\"\n    Represents a single page in a PDF document.\n\n    Properties:\n        page_number (int): 1-indexed page number\n        page_index (int): 0-indexed page position\n        width (float): Page width in points\n        height (float): Page height in points\n        pdf (PDF): Parent PDF object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the page <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>create_region(x0, top, x1, bottom)</code> Create a region at specific coordinates <code>x0</code>: Left coordinate<code>top</code>: Top coordinate<code>x1</code>: Right coordinate<code>bottom</code>: Bottom coordinate <code>Region</code> <code>highlight(elements, color=None, label=None)</code> Highlight elements on the page <code>elements</code>: Elements to highlight<code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight <code>Page</code> (self) <code>highlight_all(include_types=None, include_text_styles=False, include_layout_regions=False)</code> Highlight all elements on the page <code>include_types</code>: Element types to include<code>include_text_styles</code>: Whether to include text styles<code>include_layout_regions</code>: Whether to include layout regions <code>Page</code> (self) <code>save_image(path, resolution=72, labels=True)</code> Save an image of the page with highlights <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>None</code> <code>to_image(resolution=72, labels=True)</code> Get a PIL Image of the page with highlights <code>resolution</code>: Image resolution in DPI<code>labels</code>: Whether to include labels <code>PIL.Image</code> <code>analyze_text_styles()</code> Group text by visual style properties None <code>dict</code>: Mapping of style name to elements <code>analyze_layout(engine=\"yolo\", confidence=0.2, existing=\"replace\")</code> Detect layout regions using ML models <code>model</code>: Model to use (\"yolo\", \"tatr\")<code>confidence</code>: Confidence threshold<code>existing</code>: How to handle existing regions <code>ElementCollection</code>: Detected regions <code>add_exclusion(region, label=None)</code> Add an exclusion zone to the page <code>region</code>: Region to exclude<code>label</code>: Optional label for the exclusion <code>Region</code>: The exclusion region <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections from the page <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the page content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>apply_ocr(languages=None, min_confidence=0.0, **kwargs)</code> Apply OCR to the page <code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold<code>**kwargs</code>: Additional OCR engine parameters <code>ElementCollection</code>: OCR text elements"},{"location":"api/#region-class","title":"Region Class","text":"<p>Represents a rectangular area on a page.</p> <pre><code>class Region:\n    \"\"\"\n    Represents a rectangular area on a page.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the region\n        height (float): Height of the region\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True, ocr=None)</code> Extract text from the region <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones<code>ocr</code>: Whether to force OCR <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector within the region <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>expand(left=0, top=0, right=0, bottom=0, width_factor=1.0, height_factor=1.0)</code> Expand the region in specified directions <code>left/top/right/bottom</code>: Points to expand in each direction<code>width_factor/height_factor</code>: Scale width/height by this factor <code>Region</code>: Expanded region <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight the region <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Region attributes to display <code>Region</code> (self) <code>to_image(resolution=72, crop_only=False)</code> Get a PIL Image of just the region <code>resolution</code>: Image resolution in DPI<code>crop_only</code>: Whether to exclude border <code>PIL.Image</code> <code>save_image(path, resolution=72, crop_only=False)</code> Save an image of just the region <code>path</code>: Path to save image<code>resolution</code>: Image resolution in DPI<code>crop_only</code>: Whether to exclude border <code>None</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start')</code> Get sections within the region <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries <code>list[Region]</code> <code>ask(question, min_confidence=0.0, model=None, debug=False)</code> Ask a question about the region content <code>question</code>: Question to ask<code>min_confidence</code>: Minimum confidence threshold<code>model</code>: Optional model name or path<code>debug</code>: Whether to save debug files <code>dict</code>: Result with answer and metadata <code>extract_table(method=None, table_settings=None, use_ocr=False)</code> Extract table data from the region <code>method</code>: Extraction method (\"plumber\", \"tatr\")<code>table_settings</code>: Custom settings for extraction<code>use_ocr</code>: Whether to use OCR text <code>list</code>: Table data as rows and columns <code>intersects(other)</code> Check if this region intersects with another <code>other</code>: Another region <code>bool</code>: True if regions intersect <code>contains(x, y)</code> Check if a point is within the region <code>x</code>: X coordinate<code>y</code>: Y coordinate <code>bool</code>: True if point is in region"},{"location":"api/#element-types","title":"Element Types","text":""},{"location":"api/#element-base-class","title":"Element Base Class","text":"<p>The base class for all PDF elements.</p> <pre><code>class Element:\n    \"\"\"\n    Base class for all PDF elements.\n\n    Properties:\n        x0 (float): Left coordinate\n        top (float): Top coordinate\n        x1 (float): Right coordinate\n        bottom (float): Bottom coordinate\n        width (float): Width of the element\n        height (float): Height of the element\n        page (Page): Parent page object\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>above(height=None, full_width=True, until=None, include_until=True)</code> Create a region above the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>below(height=None, full_width=True, until=None, include_until=True)</code> Create a region below the element <code>height</code>: Height of region<code>full_width</code>: Whether to span page width<code>until</code>: Selector for boundary<code>include_until</code>: Whether to include boundary <code>Region</code> <code>select_until(selector, include_endpoint=True, full_width=True)</code> Create a region from this element to another <code>selector</code>: Selector for endpoint<code>include_endpoint</code>: Whether to include endpoint<code>full_width</code>: Whether to span page width <code>Region</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight this element <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Element attributes to display <code>Element</code> (self) <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from this element <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>next(selector=None, limit=None, apply_exclusions=True)</code> Get the next element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>prev(selector=None, limit=None, apply_exclusions=True)</code> Get the previous element in reading order <code>selector</code>: Optional selector to filter<code>limit</code>: How many elements to search<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>nearest(selector, max_distance=None, apply_exclusions=True)</code> Get the nearest element matching selector <code>selector</code>: Selector for elements<code>max_distance</code>: Maximum distance in points<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code>"},{"location":"api/#textelement","title":"TextElement","text":"<p>Represents text elements in the PDF.</p> <pre><code>class TextElement(Element):\n    \"\"\"\n    Represents text elements in the PDF.\n\n    Additional Properties:\n        text (str): The text content\n        fontname (str): The font name\n        size (float): The font size\n        bold (bool): Whether the text is bold\n        italic (bool): Whether the text is italic\n        color (tuple): The text color as RGB tuple\n        confidence (float): OCR confidence (for OCR text)\n        source (str): 'pdf' or 'ocr'\n    \"\"\"\n</code></pre> <p>Main Properties</p> Property Type Description <code>text</code> <code>str</code> The text content <code>fontname</code> <code>str</code> The font name <code>size</code> <code>float</code> The font size <code>bold</code> <code>bool</code> Whether the text is bold <code>italic</code> <code>bool</code> Whether the text is italic <code>color</code> <code>tuple</code> The text color as RGB tuple <code>confidence</code> <code>float</code> OCR confidence (for OCR text) <code>source</code> <code>str</code> 'pdf' or 'ocr' <code>font_variant</code> <code>str</code> Font variant identifier (e.g., 'AAAAAB+') <p>Additional Methods</p> Method Description Parameters Returns <code>font_info()</code> Get detailed font information None <code>dict</code>: Font properties"},{"location":"api/#collections","title":"Collections","text":""},{"location":"api/#elementcollection","title":"ElementCollection","text":"<p>A collection of elements with batch operations.</p> <pre><code>class ElementCollection:\n    \"\"\"\n    A collection of elements with batch operations.\n\n    This class provides operations that can be applied to multiple elements at once.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all elements <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>filter(selector)</code> Filter elements by selector <code>selector</code>: CSS-like selector string <code>ElementCollection</code> <code>highlight(color=None, label=None, include_attrs=None)</code> Highlight all elements <code>color</code>: RGBA color tuple<code>label</code>: Label for the highlight<code>include_attrs</code>: Attributes to display <code>ElementCollection</code> (self) <code>first</code> Get the first element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>last</code> Get the last element in the collection N/A (property) <code>Element</code> or <code>None</code> <code>highest()</code> Get the highest element on the page None <code>Element</code> or <code>None</code> <code>lowest()</code> Get the lowest element on the page None <code>Element</code> or <code>None</code> <code>leftmost()</code> Get the leftmost element on the page None <code>Element</code> or <code>None</code> <code>rightmost()</code> Get the rightmost element on the page None <code>Element</code> or <code>None</code> <code>__len__()</code> Get the number of elements None <code>int</code> <code>__getitem__(index)</code> Get an element by index <code>index</code>: Index or slice <code>Element</code> or <code>ElementCollection</code>"},{"location":"api/#pagecollection","title":"PageCollection","text":"<p>A collection of pages with cross-page operations.</p> <pre><code>class PageCollection:\n    \"\"\"\n    A collection of pages with cross-page operations.\n\n    This class provides operations that can be applied across multiple pages.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>extract_text(keep_blank_chars=True, apply_exclusions=True)</code> Extract text from all pages <code>keep_blank_chars</code>: Whether to keep blank characters<code>apply_exclusions</code>: Whether to apply exclusion zones <code>str</code>: Extracted text <code>find(selector, case=True, regex=False, apply_exclusions=True)</code> Find the first element matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>Element</code> or <code>None</code> <code>find_all(selector, case=True, regex=False, apply_exclusions=True)</code> Find all elements matching selector across all pages <code>selector</code>: CSS-like selector string<code>case</code>: Case-sensitive search<code>regex</code>: Use regex for :contains()<code>apply_exclusions</code>: Whether to apply exclusion zones <code>ElementCollection</code> <code>get_sections(start_elements, end_elements=None, boundary_inclusion='start', new_section_on_page_break=False)</code> Get sections spanning multiple pages <code>start_elements</code>: Elements marking section starts<code>end_elements</code>: Elements marking section ends<code>boundary_inclusion</code>: How to include boundaries<code>new_section_on_page_break</code>: Whether to start new sections at page breaks <code>list[Region]</code> <code>__len__()</code> Get the number of pages None <code>int</code> <code>__getitem__(index)</code> Get a page by index <code>index</code>: Index or slice <code>Page</code> or <code>PageCollection</code>"},{"location":"api/#ocr-classes","title":"OCR Classes","text":""},{"location":"api/#ocrengine","title":"OCREngine","text":"<p>Base class for OCR engines.</p> <pre><code>class OCREngine:\n    \"\"\"\n    Base class for OCR engines.\n\n    This class provides the interface for OCR engines.\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>process_image(image, languages=None, min_confidence=0.0, **kwargs)</code> Process an image with OCR <code>image</code>: PIL Image<code>languages</code>: Languages to use<code>min_confidence</code>: Minimum confidence threshold <code>list</code>: OCR results"},{"location":"api/#easyocrengine","title":"EasyOCREngine","text":"<p>OCR engine using EasyOCR.</p> <pre><code>class EasyOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using EasyOCR.\n\n    Parameters:\n        model_dir (str, optional): Directory for models. Default: None\n    \"\"\"\n</code></pre>"},{"location":"api/#paddleocrengine","title":"PaddleOCREngine","text":"<p>OCR engine using PaddleOCR.</p> <pre><code>class PaddleOCREngine(OCREngine):\n    \"\"\"\n    OCR engine using PaddleOCR.\n\n    Parameters:\n        use_angle_cls (bool, optional): Use text direction classification. Default: False\n        lang (str, optional): Language code. Default: \"en\"\n        det (bool, optional): Use text detection. Default: True\n        rec (bool, optional): Use text recognition. Default: True\n        cls (bool, optional): Use text direction classification. Default: False\n        det_model_dir (str, optional): Detection model directory. Default: None\n        rec_model_dir (str, optional): Recognition model directory. Default: None\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre>"},{"location":"api/#document-qa-classes","title":"Document QA Classes","text":""},{"location":"api/#documentqa","title":"DocumentQA","text":"<p>Class for document question answering.</p> <pre><code>class DocumentQA:\n    \"\"\"\n    Class for document question answering.\n\n    Parameters:\n        model (str, optional): Model name or path. Default: \"microsoft/layoutlmv3-base\"\n        device (str, optional): Device to use. Default: \"cpu\"\n        verbose (bool, optional): Enable verbose output. Default: False\n    \"\"\"\n</code></pre> <p>Main Methods</p> Method Description Parameters Returns <code>ask(question, image, word_boxes, min_confidence=0.0, max_answer_length=None, language=None)</code> Ask a question about a document <code>question</code>: Question to ask<code>image</code>: Document image<code>word_boxes</code>: Text positions<code>min_confidence</code>: Minimum confidence threshold<code>max_answer_length</code>: Maximum answer length<code>language</code>: Language code <code>dict</code>: Result with answer and metadata"},{"location":"api/#selector-syntax","title":"Selector Syntax","text":"<p>Natural PDF uses a CSS-like selector syntax to find elements in PDFs.</p>"},{"location":"api/#basic-selectors","title":"Basic Selectors","text":"Selector Description Example <code>element_type</code> Match elements of this type <code>text</code>, <code>rect</code>, <code>line</code> <code>[attribute=value]</code> Match elements with this attribute value <code>[fontname=Arial]</code>, <code>[size=12]</code> <code>[attribute&gt;=value]</code> Match elements with attribute &gt;= value <code>[size&gt;=12]</code> <code>[attribute&lt;=value]</code> Match elements with attribute &lt;= value <code>[size&lt;=10]</code> <code>[attribute~=value]</code> Match elements with attribute approximately equal <code>[color~=red]</code>, <code>[color~=(1,0,0)]</code> <code>[attribute*=value]</code> Match elements with attribute containing value <code>[fontname*=Arial]</code>"},{"location":"api/#pseudo-classes","title":"Pseudo-Classes","text":"Pseudo-Class Description Example <code>:contains(\"text\")</code> Match elements containing text <code>text:contains(\"Summary\")</code> <code>:starts-with(\"text\")</code> Match elements starting with text <code>text:starts-with(\"Summary\")</code> <code>:ends-with(\"text\")</code> Match elements ending with text <code>text:ends-with(\"2023\")</code> <code>:bold</code> Match bold text <code>text:bold</code> <code>:italic</code> Match italic text <code>text:italic</code>"},{"location":"api/#attribute-names","title":"Attribute Names","text":"Attribute Element Types Description <code>fontname</code> text Font name <code>size</code> text Font size <code>color</code> text, rect, line Color <code>width</code> rect, line Width <code>height</code> rect Height <code>confidence</code> text (OCR) OCR confidence score <code>source</code> text Source ('pdf' or 'ocr') <code>type</code> region Region type (e.g., 'table', 'title') <code>model</code> region Layout model that detected the region <code>font-variant</code> text Font variant identifier"},{"location":"api/#constants-and-configuration","title":"Constants and Configuration","text":""},{"location":"api/#color-names","title":"Color Names","text":"<p>Natural PDF supports color names in selectors.</p> Color Name RGB Value Example <code>red</code> (1, 0, 0) <code>[color~=red]</code> <code>green</code> (0, 1, 0) <code>[color~=green]</code> <code>blue</code> (0, 0, 1) <code>[color~=blue]</code> <code>black</code> (0, 0, 0) <code>[color~=black]</code> <code>white</code> (1, 1, 1) <code>[color~=white]</code>"},{"location":"api/#region-types","title":"Region Types","text":"<p>Layout analysis models detect the following region types:</p> Model Region Types YOLO <code>title</code>, <code>plain-text</code>, <code>table</code>, <code>figure</code>, <code>figure_caption</code>, <code>table_caption</code>, <code>table_footnote</code>, <code>isolate_formula</code>, <code>formula_caption</code>, <code>abandon</code> TATR <code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>"},{"location":"document-qa/","title":"Document Question Answering","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n\n# Display the first page \npage = pdf.pages[0]\npage.show()\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")  # Display the first page  page = pdf.pages[0] page.show() Out[1]: In\u00a0[2]: Copied! <pre># Ask a question about the entire document\npage.ask(\"How many votes did Harris and Waltz get?\")\n</pre> # Ask a question about the entire document page.ask(\"How many votes did Harris and Waltz get?\") <pre>Device set to use cpu\n</pre> Out[2]: <pre>{'answer': '148',\n 'confidence': 0.9995507001876831,\n 'start': 20,\n 'end': 20,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre>page.ask(\"Who got the most votes for Attorney General?\")\n</pre> page.ask(\"Who got the most votes for Attorney General?\") Out[3]: <pre>{'answer': 'DEM EUGENE DEPASQUALE',\n 'confidence': 0.9180715084075928,\n 'start': 63,\n 'end': 63,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre>page.ask(\"Who was the Republican candidate for Attorney General?\")\n</pre> page.ask(\"Who was the Republican candidate for Attorney General?\") Out[4]: <pre>{'answer': 'LIB ROBERT COWBURN',\n 'confidence': 0.2159084975719452,\n 'start': 67,\n 'end': 67,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[5]: Copied! <pre># Get a specific page\nregion = page.find('text:contains(\"Attorney General\")').below()\nregion.show()\n</pre> # Get a specific page region = page.find('text:contains(\"Attorney General\")').below() region.show() Out[5]: In\u00a0[6]: Copied! <pre>region.ask(\"How many write-in votes were cast?\")\n</pre> region.ask(\"How many write-in votes were cast?\") Out[6]: <pre>{'answer': '498',\n 'confidence': 0.9988918304443359,\n 'start': 17,\n 'end': 17,\n 'found': True,\n 'region': &lt;natural_pdf.elements.region.Region at 0x37f5846d0&gt;,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\n\nquestions = [\n    \"How many votes did Harris and Walz get?\",\n    \"How many votes did Trump get?\",\n    \"How many votes did Natural PDF get?\",\n    \"What was the date of this form?\"\n]\n\n# You can actually do this but with multiple questions\n# in the model itself buuuut Natural PDF can'd do it yet\nresults = [page.ask(q) for q in questions]\n\ndf = pd.json_normalize(results)\ndf.insert(0, 'question', questions)\ndf\n</pre> import pandas as pd  questions = [     \"How many votes did Harris and Walz get?\",     \"How many votes did Trump get?\",     \"How many votes did Natural PDF get?\",     \"What was the date of this form?\" ]  # You can actually do this but with multiple questions # in the model itself buuuut Natural PDF can'd do it yet results = [page.ask(q) for q in questions]  df = pd.json_normalize(results) df.insert(0, 'question', questions) df Out[7]: question answer confidence start end found page_num source_elements 0 How many votes did Harris and Walz get? 148 0.999671 20 20 True 0 (&lt;TextElement text='148' font='Helvetica' size... 1 How many votes did Trump get? 348 0.310207 22 22 True 0 (&lt;TextElement text='348' font='Helvetica' size... 2 How many votes did Natural PDF get? November 5, 2024 0.237137 3 3 True 0 (&lt;TextElement text='November 5...' font='Helve... 3 What was the date of this form? November 5, 2024 0.792704 3 3 True 0 (&lt;TextElement text='November 5...' font='Helve..."},{"location":"document-qa/#document-question-answering","title":"Document Question Answering\u00b6","text":"<p>Natural PDF includes document QA functionality that allows you to ask natural language questions about your PDFs and get relevant answers. This feature uses LayoutLM models to understand both the text content and the visual layout of your documents.</p>"},{"location":"document-qa/#setup","title":"Setup\u00b6","text":"<p>Let's start by loading a sample PDF to experiment with question answering.</p>"},{"location":"document-qa/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here's how to ask questions to a PDF page:</p>"},{"location":"document-qa/#asking-questions-to-part-of-a-page-questions","title":"Asking questions to part of a page questions\u00b6","text":"<p>You can also ask questions to a specific region of a page*:</p>"},{"location":"document-qa/#asking-multiple-questions","title":"Asking multiple questions\u00b6","text":""},{"location":"document-qa/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you've learned about document QA, explore:</p> <ul> <li>Element Selection: Find specific elements to focus your questions.</li> <li>Layout Analysis: Automatically detect document structure.</li> <li>Working with Regions: Define custom areas for targeted questioning.</li> <li>Text Extraction: Extract and preprocess text before QA.</li> </ul>"},{"location":"element-selection/","title":"Finding Elements with Selectors","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() Out[1]: In\u00a0[2]: Copied! <pre># Find the first text element containing \"Summary\"\nsummary_text = page.find('text:contains(\"Summary\")')\nsummary_text\n</pre> # Find the first text element containing \"Summary\" summary_text = page.find('text:contains(\"Summary\")') summary_text Out[2]: <pre>&lt;TextElement text='Summary: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 144.07000000000005, 101.68, 154.07000000000005)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all text elements containing \"Inadequate\"\ncontains_inadequate = page.find_all('text:contains(\"Inadequate\")')\nlen(contains_inadequate)\n</pre> # Find all text elements containing \"Inadequate\" contains_inadequate = page.find_all('text:contains(\"Inadequate\")') len(contains_inadequate) Out[3]: <pre>2</pre> In\u00a0[4]: Copied! <pre>summary_text.highlight(label='summary')\ncontains_inadequate.highlight(label=\"inadequate\")\npage.to_image(width=700)\n</pre> summary_text.highlight(label='summary') contains_inadequate.highlight(label=\"inadequate\") page.to_image(width=700) Out[4]: In\u00a0[5]: Copied! <pre># Find all text elements\nall_text = page.find_all('text')\nlen(all_text)\n</pre> # Find all text elements all_text = page.find_all('text') len(all_text) Out[5]: <pre>43</pre> In\u00a0[6]: Copied! <pre># Find all rectangle elements\nall_rects = page.find_all('rect')\nlen(all_rects)\n</pre> # Find all rectangle elements all_rects = page.find_all('rect') len(all_rects) Out[6]: <pre>8</pre> In\u00a0[7]: Copied! <pre># Find all line elements\nall_lines = page.find_all('line')\nlen(all_lines)\n</pre> # Find all line elements all_lines = page.find_all('line') len(all_lines) Out[7]: <pre>21</pre> In\u00a0[8]: Copied! <pre>page.find_all('line').show()\n</pre> page.find_all('line').show() Out[8]: In\u00a0[9]: Copied! <pre># Find large text (size &gt;= 11 points)\npage.find_all('text[size&gt;=11]')\n</pre> # Find large text (size &gt;= 11 points) page.find_all('text[size&gt;=11]') Out[9]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[10]: Copied! <pre># Find text with 'Helvetica' in the font name\npage.find_all('text[fontname*=Helvetica]')\n</pre> # Find text with 'Helvetica' in the font name page.find_all('text[fontname*=Helvetica]') Out[10]: <pre>&lt;ElementCollection[TextElement](count=43)&gt;</pre> In\u00a0[11]: Copied! <pre># Find red text (using approximate color match)\n# This PDF has text with color (0.8, 0.0, 0.0)\nred_text = page.find_all('text[color~=red]')\n</pre> # Find red text (using approximate color match) # This PDF has text with color (0.8, 0.0, 0.0) red_text = page.find_all('text[color~=red]') In\u00a0[12]: Copied! <pre># Highlight the red text (ignoring existing highlights)\nred_text.show()\n</pre> # Highlight the red text (ignoring existing highlights) red_text.show() Out[12]: In\u00a0[13]: Copied! <pre># Find thick lines (width &gt;= 2)\npage.find_all('line[width&gt;=2]')\n</pre> # Find thick lines (width &gt;= 2) page.find_all('line[width&gt;=2]') Out[13]: <pre>&lt;ElementCollection[LineElement](count=1)&gt;</pre> In\u00a0[14]: Copied! <pre># Find bold text\npage.find_all('text:bold').show()\n</pre> # Find bold text page.find_all('text:bold').show() Out[14]: In\u00a0[15]: Copied! <pre># Combine attribute and pseudo-class: bold text size &gt;= 11\npage.find_all('text[size&gt;=11]:bold')\n</pre> # Combine attribute and pseudo-class: bold text size &gt;= 11 page.find_all('text[size&gt;=11]:bold') Out[15]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[16]: Copied! <pre># Find the thick horizontal line first\nref_line = page.find('line[width&gt;=2]')\n\n# Find text elements strictly above that line\ntext_above_line = page.find_all('text:above(\"line[width&gt;=2]\")')\ntext_above_line\n</pre> # Find the thick horizontal line first ref_line = page.find('line[width&gt;=2]')  # Find text elements strictly above that line text_above_line = page.find_all('text:above(\"line[width&gt;=2]\")') text_above_line Out[16]: <pre>&lt;ElementCollection[TextElement](count=16)&gt;</pre> In\u00a0[17]: Copied! <pre># Case-insensitive search for \"summary\"\npage.find_all('text:contains(\"summary\")', case=False)\n</pre> # Case-insensitive search for \"summary\" page.find_all('text:contains(\"summary\")', case=False) Out[17]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[18]: Copied! <pre># Regular expression search for the inspection ID (e.g., INS-XXX...)\n# The ID is in the red text we found earlier\npage.find_all('text:contains(\"INS-\\\\w+\")', regex=True)\n</pre> # Regular expression search for the inspection ID (e.g., INS-XXX...) # The ID is in the red text we found earlier page.find_all('text:contains(\"INS-\\\\w+\")', regex=True) Out[18]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[19]: Copied! <pre># Combine regex and case-insensitivity\npage.find_all('text:contains(\"jungle health\")', regex=True, case=False)\n</pre> # Combine regex and case-insensitivity page.find_all('text:contains(\"jungle health\")', regex=True, case=False) Out[19]: <pre>&lt;ElementCollection[TextElement](count=2)&gt;</pre> In\u00a0[20]: Copied! <pre># Get all headings (using a selector for large, bold text)\nheadings = page.find_all('text[size&gt;=11]:bold')\nheadings\n</pre> # Get all headings (using a selector for large, bold text) headings = page.find_all('text[size&gt;=11]:bold') headings Out[20]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[21]: Copied! <pre># Get the first and last heading in reading order\nfirst = headings.first\nlast = headings.last\n(first, last)\n</pre> # Get the first and last heading in reading order first = headings.first last = headings.last (first, last) Out[21]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[22]: Copied! <pre># Get the physically highest/lowest element in the collection\nhighest = headings.highest()\nlowest = headings.lowest()\n(highest, lowest)\n</pre> # Get the physically highest/lowest element in the collection highest = headings.highest() lowest = headings.lowest() (highest, lowest) Out[22]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[23]: Copied! <pre># Filter the collection further: headings containing \"Service\"\nservice_headings = headings.find_all('text:contains(\"Service\")')\nservice_headings\n</pre> # Filter the collection further: headings containing \"Service\" service_headings = headings.find_all('text:contains(\"Service\")') service_headings Out[23]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre> In\u00a0[24]: Copied! <pre># Extract text from all elements in the collection\nheadings.extract_text()\n</pre> # Extract text from all elements in the collection headings.extract_text() Out[24]: <pre>'Violations'</pre> <p>Remember: <code>.highest()</code>, <code>.lowest()</code>, <code>.leftmost()</code>, <code>.rightmost()</code> raise errors if the collection spans multiple pages.</p> In\u00a0[25]: Copied! <pre># Find text elements with a specific font variant prefix (if any exist)\n# This example PDF doesn't use variants, but the selector works like this:\npage.find_all('text[font-variant=AAAAAB]')\n</pre> # Find text elements with a specific font variant prefix (if any exist) # This example PDF doesn't use variants, but the selector works like this: page.find_all('text[font-variant=AAAAAB]') Out[25]: <pre>&lt;ElementCollection[TextElement](count=43)&gt;</pre>"},{"location":"element-selection/#finding-elements-with-selectors","title":"Finding Elements with Selectors\u00b6","text":"<p>Natural PDF uses CSS-like selectors to find elements (text, lines, images, etc.) within a PDF page or document. This guide demonstrates how to use these selectors effectively.</p>"},{"location":"element-selection/#setup","title":"Setup\u00b6","text":"<p>Let's load a sample PDF to work with. We'll use <code>01-practice.pdf</code> which has various elements.</p>"},{"location":"element-selection/#basic-element-finding","title":"Basic Element Finding\u00b6","text":"<p>The core methods are <code>find()</code> (returns the first match) and <code>find_all()</code> (returns all matches as an <code>ElementCollection</code>).</p> <p>The basic selector structure is <code>element_type[attribute_filter]:pseudo_class</code>.</p>"},{"location":"element-selection/#finding-text-by-content","title":"Finding Text by Content\u00b6","text":""},{"location":"element-selection/#selecting-by-element-type","title":"Selecting by Element Type\u00b6","text":"<p>You can select specific types of elements found in PDFs.</p>"},{"location":"element-selection/#filtering-by-attributes","title":"Filtering by Attributes\u00b6","text":"<p>Use square brackets <code>[]</code> to filter elements by their properties (attributes).</p>"},{"location":"element-selection/#common-attributes-operators","title":"Common Attributes &amp; Operators\u00b6","text":"Attribute Example Usage Operators Notes <code>size</code> (text) <code>text[size&gt;=12]</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Font size in points <code>fontname</code> <code>text[fontname*=Bold]</code> <code>=</code>, <code>*=</code> <code>*=</code> for contains substring <code>color</code> (text) <code>text[color~=red]</code> <code>~=</code> Approx. match (name, rgb, hex) <code>width</code> (line) <code>line[width&gt;1]</code> <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> Line thickness <code>source</code> <code>text[source=ocr]</code> <code>=</code> <code>pdf</code>, <code>ocr</code>, <code>detected</code> <code>type</code> (region) <code>region[type=table]</code> <code>=</code> Layout analysis region type"},{"location":"element-selection/#using-pseudo-classes","title":"Using Pseudo-Classes\u00b6","text":"<p>Use colons <code>:</code> for special conditions (pseudo-classes).</p>"},{"location":"element-selection/#common-pseudo-classes","title":"Common Pseudo-Classes\u00b6","text":"Pseudo-Class Example Usage Notes <code>:contains('text')</code> <code>text:contains('Report')</code> Finds elements containing specific text <code>:bold</code> <code>text:bold</code> Finds text heuristically identified as bold <code>:italic</code> <code>text:italic</code> Finds text heuristically identified as italic <code>:below(selector)</code> <code>text:below('line[width&gt;=2]')</code> Finds elements physically below the reference element <code>:above(selector)</code> <code>text:above('text:contains(\"Summary\")')</code> Finds elements physically above the reference element <code>:left-of(selector)</code> <code>line:left-of('rect')</code> Finds elements physically left of the reference element <code>:right-of(selector)</code> <code>text:right-of('rect')</code> Finds elements physically right of the reference element <code>:near(selector)</code> <code>text:near('image')</code> Finds elements physically near the reference element <p>Note: Spatial pseudo-classes like <code>:below</code>, <code>:above</code> identify elements based on bounding box positions relative to the first element matched by the inner selector.</p>"},{"location":"element-selection/#spatial-pseudo-classes-examples","title":"Spatial Pseudo-Classes Examples\u00b6","text":""},{"location":"element-selection/#advanced-text-searching-options","title":"Advanced Text Searching Options\u00b6","text":"<p>Pass options to <code>find()</code> or <code>find_all()</code> for more control over text matching.</p>"},{"location":"element-selection/#working-with-elementcollections","title":"Working with ElementCollections\u00b6","text":"<p><code>find_all()</code> returns an <code>ElementCollection</code>, which is like a list but with extra PDF-specific methods.</p>"},{"location":"element-selection/#font-variants","title":"Font Variants\u00b6","text":"<p>Sometimes PDFs use font variants (prefixes like <code>AAAAAB+</code>) which can be useful for selection.</p>"},{"location":"element-selection/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you can find elements, explore:</p> <ul> <li>Text Extraction: Get text content from found elements.</li> <li>Spatial Navigation: Use found elements as anchors to navigate (<code>.above()</code>, <code>.below()</code>, etc.).</li> <li>Working with Regions: Define areas based on found elements.</li> <li>Visual Debugging: Techniques for highlighting and visualizing elements.</li> </ul>"},{"location":"installation/","title":"Getting Started with Natural PDF","text":"<p>Let's get Natural PDF installed and run your first extraction.</p>"},{"location":"installation/#installation","title":"Installation","text":"<p>The base installation includes the core library and necessary AI dependencies (like PyTorch and Transformers):</p> <pre><code>pip install natural-pdf\n</code></pre>"},{"location":"installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>Natural PDF has modular dependencies for different features. Install them based on your needs:</p> <pre><code># --- OCR Engines ---\n# Install support for EasyOCR\npip install natural-pdf[easyocr]\n\n# Install support for PaddleOCR (requires paddlepaddle)\npip install natural-pdf[paddle]\n\n# Install support for Surya OCR\npip install natural-pdf[surya]\n\n# --- Layout Detection ---\n# Install support for YOLO layout model\npip install natural-pdf[layout_yolo]\n\n# --- Interactive Widget ---\n# Install support for the interactive .viewer() widget in Jupyter\npip install natural-pdf[interactive]\n\n# --- All Features ---\n# Install all optional dependencies\npip install natural-pdf[all]\n</code></pre>"},{"location":"installation/#your-first-pdf-extraction","title":"Your First PDF Extraction","text":"<p>Here's a quick example to make sure everything is working:</p> <pre><code>from natural_pdf import PDF\n\n# Open a PDF\npdf = PDF('your_document.pdf')\n\n# Get the first page\npage = pdf.pages[0]\n\n# Extract all text\ntext = page.extract_text()\nprint(text)\n\n# Find something specific\ntitle = page.find('text:bold')\nprint(f\"Found title: {title.text}\")\n</code></pre>"},{"location":"installation/#whats-next","title":"What's Next?","text":"<p>Now that you have Natural PDF installed, you can:</p> <ul> <li>Learn to navigate PDFs</li> <li>Explore how to select elements</li> <li>See how to extract text</li> </ul>"},{"location":"interactive-widget/","title":"Interactive widget","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.viewer()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[1]:"},{"location":"interactive-widget/#interactive-widget","title":"Interactive widget\u00b6","text":"<p>This is the best possible way, in all of history, to explore a PDF.</p>"},{"location":"layout-analysis/","title":"Document Layout Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.to_image(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Analyze the layout using the default engine (YOLO)\n# This adds 'region' elements to the page\npage.analyze_layout()\n</pre> # Analyze the layout using the default engine (YOLO) # This adds 'region' elements to the page page.analyze_layout() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmprdeq3z9_/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 3113.1ms\n</pre> <pre>Speed: 5.5ms preprocess, 3113.1ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[2]: <pre>&lt;ElementCollection[Region](count=8)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all detected regions\nregions = page.find_all('region')\nlen(regions) # Show how many regions were detected\n</pre> # Find all detected regions regions = page.find_all('region') len(regions) # Show how many regions were detected Out[3]: <pre>8</pre> In\u00a0[4]: Copied! <pre>first_region = regions[0]\nf\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\"\n</pre> first_region = regions[0] f\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\" Out[4]: <pre>\"First region: type='abandon', confidence=0.74\"</pre> In\u00a0[5]: Copied! <pre># Highlight all detected regions, colored by type\nregions.highlight(group_by='type')\npage.to_image(width=700)\n</pre> # Highlight all detected regions, colored by type regions.highlight(group_by='type') page.to_image(width=700) Out[5]: In\u00a0[6]: Copied! <pre># Find all detected titles\ntitles = page.find_all('region[type=title]')\ntitles\n</pre> # Find all detected titles titles = page.find_all('region[type=title]') titles Out[6]: <pre>&lt;ElementCollection[Region](count=2)&gt;</pre> In\u00a0[7]: Copied! <pre>titles.show()\n</pre> titles.show() Out[7]: In\u00a0[8]: Copied! <pre>page.find_all('region[type=table]').show()\n</pre> page.find_all('region[type=table]').show() Out[8]: In\u00a0[9]: Copied! <pre>page.find('region[type=table]').extract_text(layout=True)\n</pre> page.find('region[type=table]').extract_text(layout=True) Out[9]: <pre>'Statute Description Level Repeat? 4.12.7 Unsanitary Working Conditions. Critical 5.8.3 Inadequate Protective Equipment. Serious 6.3.9 Ineffective Injury Prevention. Serious 7.1.5 Failure to Properly Store Hazardous Materials. Critical 8.9.2 Lack of Adequate Fire Safety Measures. Serious 9.6.4 Inadequate Ventilation Systems. Serious 10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[10]: Copied! <pre>page.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"paddle\")\npage.find_all('region[model=paddle]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"paddle\") page.find_all('region[model=paddle]').highlight(group_by='region_type') page.to_image(width=700) Out[10]: In\u00a0[11]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[11]: In\u00a0[12]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"docling\")\npage.find_all('region[model=docling]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"docling\") page.find_all('region[model=docling]').highlight(group_by='region_type') page.to_image(width=700) Out[12]: In\u00a0[13]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"surya\")\npage.find_all('region[model=surya]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"surya\") page.find_all('region[model=surya]').highlight(group_by='region_type') page.to_image(width=700) <pre>Loaded layout model s3://layout/2025_02_18 on device cpu with dtype torch.float16\n</pre> <pre>\rRecognizing layout:   0%|                                                   | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:08&lt;00:00,  8.59s/it]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:08&lt;00:00,  8.59s/it]</pre> <pre>\n</pre> Out[13]: <p>Note: Calling <code>analyze_layout</code> multiple times (even with the same engine) can add duplicate regions. You might want to use <code>page.clear_detected_layout_regions()</code> first, or filter by model using <code>region[model=yolo]</code>.</p> In\u00a0[14]: Copied! <pre># Re-run YOLO analysis (clearing previous results might be good practice)\npage.clear_detected_layout_regions()\npage.analyze_layout(engine=\"yolo\")\n\n# Find only high-confidence regions (e.g., &gt;= 0.8)\nhigh_conf_regions = page.find_all('region[confidence&gt;=0.8]')\nlen(high_conf_regions)\n</pre> # Re-run YOLO analysis (clearing previous results might be good practice) page.clear_detected_layout_regions() page.analyze_layout(engine=\"yolo\")  # Find only high-confidence regions (e.g., &gt;= 0.8) high_conf_regions = page.find_all('region[confidence&gt;=0.8]') len(high_conf_regions) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpgr5id04j/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1478.1ms\n</pre> <pre>Speed: 4.7ms preprocess, 1478.1ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre># Ensure TATR analysis has been run\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').highlight(group_by='region_type')\npage.to_image(width=700)\n</pre> # Ensure TATR analysis has been run page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').highlight(group_by='region_type') page.to_image(width=700) Out[15]: In\u00a0[16]: Copied! <pre># Find different structural elements from TATR\ntables = page.find_all('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\n\nf\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\"\n</pre> # Find different structural elements from TATR tables = page.find_all('region[type=table][model=tatr]') rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]')  f\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\" Out[16]: <pre>'Found: 2 tables, 8 rows, 4 columns, 1 headers (from TATR)'</pre> In\u00a0[17]: Copied! <pre># Find the TATR table region again\ntatr_table = page.find('region[type=table][model=tatr]')\n\n# This extraction uses the detected rows/columns\ntatr_table.extract_table()\n</pre> # Find the TATR table region again tatr_table = page.find('region[type=table][model=tatr]')  # This extraction uses the detected rows/columns tatr_table.extract_table() Out[17]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>if you'd like the normal approach instead of the \"intelligent\" one, you can ask for pdfplumber.</p> In\u00a0[18]: Copied! <pre># This extraction uses the detected rows/columns\ntatr_table.extract_table(method='pdfplumber')\n</pre> # This extraction uses the detected rows/columns tatr_table.extract_table(method='pdfplumber') Out[18]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre>"},{"location":"layout-analysis/#document-layout-analysis","title":"Document Layout Analysis\u00b6","text":"<p>Natural PDF can automatically detect the structure of a document (titles, paragraphs, tables, figures) using layout analysis models. This guide shows how to use this feature.</p>"},{"location":"layout-analysis/#setup","title":"Setup\u00b6","text":"<p>We'll use a sample PDF that includes various layout elements.</p>"},{"location":"layout-analysis/#running-basic-layout-analysis","title":"Running Basic Layout Analysis\u00b6","text":"<p>Use the <code>analyze_layout()</code> method. By default, it uses the YOLO model.</p>"},{"location":"layout-analysis/#visualizing-detected-layout","title":"Visualizing Detected Layout\u00b6","text":"<p>Use <code>highlight()</code> or <code>show()</code> on the detected regions.</p>"},{"location":"layout-analysis/#finding-specific-region-types","title":"Finding Specific Region Types\u00b6","text":"<p>Use attribute selectors to find regions of a specific type.</p>"},{"location":"layout-analysis/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>Detected regions are like any other <code>Region</code> object. You can extract text, find elements within them, etc.</p>"},{"location":"layout-analysis/#using-different-layout-models","title":"Using Different Layout Models\u00b6","text":"<p>Natural PDF supports multiple engines (<code>yolo</code>, <code>paddle</code>, <code>tatr</code>). Specify the engine when calling <code>analyze_layout</code>.</p> <p>Note: Using different engines requires installing the corresponding extras (e.g., <code>natural-pdf[layout_paddle]</code>). <code>yolo</code> is the default.</p>"},{"location":"layout-analysis/#controlling-confidence-threshold","title":"Controlling Confidence Threshold\u00b6","text":"<p>Filter detections by their confidence score.</p>"},{"location":"layout-analysis/#table-structure-with-tatr","title":"Table Structure with TATR\u00b6","text":"<p>The TATR engine provides detailed table structure elements (<code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>). This is very useful for precise table extraction.</p>"},{"location":"layout-analysis/#enhanced-table-extraction-with-tatr","title":"Enhanced Table Extraction with TATR\u00b6","text":"<p>When a <code>region[type=table]</code> comes from the TATR model, <code>extract_table()</code> can use the underlying row/column structure for more robust extraction.</p>"},{"location":"layout-analysis/#next-steps","title":"Next Steps\u00b6","text":"<p>Layout analysis provides regions that you can use for:</p> <ul> <li>Table Extraction: Especially powerful with TATR regions.</li> <li>Text Extraction: Extract text only from specific region types (e.g., paragraphs).</li> <li>Document QA: Focus question answering on specific detected regions.</li> </ul>"},{"location":"ocr/","title":"OCR Integration","text":"<p>Natural PDF includes OCR (Optical Character Recognition) to extract text from scanned documents or images embedded in PDFs.</p>"},{"location":"ocr/#ocr-engine-comparison","title":"OCR Engine Comparison","text":"<p>Natural PDF supports multiple OCR engines:</p> Feature EasyOCR PaddleOCR Surya OCR Installation <code>natural-pdf[easyocr]</code> <code>natural-pdf[paddle]</code> <code>natural-pdf[surya]</code> Primary Strength Good general performance, simpler Excellent Asian language, speed High accuracy, multilingual lines Speed Moderate Fast Moderate (GPU recommended) Memory Usage Higher Efficient Higher (GPU recommended) Paragraph Detect Yes (via option) No No (focuses on lines) Handwritten Better support Limited Limited Small Text Moderate Good Good When to Use General documents, handwritten text Asian languages, speed-critical tasks Highest accuracy needed, line-level"},{"location":"ocr/#basic-ocr-usage","title":"Basic OCR Usage","text":"<p>Apply OCR directly to a page or region:</p> <pre><code>from natural_pdf import PDF\n\n# Assume 'page' is a Page object from a PDF\npage = pdf.pages[0]\n\n# Apply OCR using the default engine (or specify one)\nocr_elements = page.apply_ocr(languages=['en'])\n\n# Extract text (will use the results from apply_ocr if run previously)\ntext = page.extract_text()\nprint(text)\n</code></pre>"},{"location":"ocr/#configuring-ocr","title":"Configuring OCR","text":"<p>Specify the engine and basic options directly:</p>"},{"location":"ocr/#ocr-configuration","title":"OCR Configuration","text":"<pre><code># Use PaddleOCR for Chinese and English\nocr_elements = page.apply_ocr(engine='paddle', languages=['zh-cn', 'en'])\n\n# Use EasyOCR with a lower confidence threshold\nocr_elements = page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.3)\n</code></pre> <p>For advanced, engine-specific settings, use the Options classes:</p> <pre><code>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\n\n# --- Configure PaddleOCR ---\npaddle_opts = PaddleOCROptions(\n    languages=['en', 'zh-cn'],\n    use_gpu=True,         # Explicitly enable GPU if available\n    use_angle_cls=False,  # Disable text direction classification (if text is upright)\n    det_db_thresh=0.25,   # Lower detection threshold (more boxes, potentially noisy)\n    rec_batch_num=16      # Increase recognition batch size for potential speedup on GPU\n    # rec_char_dict_path='/path/to/custom_dict.txt' # Optional: Path to a custom character dictionary\n    # See PaddleOCROptions documentation or source code for all parameters\n )\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# --- Configure EasyOCR ---\neasy_opts = EasyOCROptions(\n    languages=['en', 'fr'],\n    gpu=True,            # Explicitly enable GPU if available\n    paragraph=True,      # Group results into paragraphs (if structure is clear)\n    detail=1,            # Ensure bounding boxes are returned (required)\n    text_threshold=0.6,  # Confidence threshold for text detection (adjust based on tuning table)\n    link_threshold=0.4,  # Standard EasyOCR param, uncomment if confirmed in wrapper\n    low_text=0.4,        # Standard EasyOCR param, uncomment if confirmed in wrapper\n    batch_size=8         # Processing batch size (adjust based on memory)\n    # See EasyOCROptions documentation or source code for all parameters\n )\nocr_elements = page.apply_ocr(engine='easyocr', options=easy_opts)\n\n# --- Configure Surya OCR ---\n# Surya focuses on line detection and recognition\nsurya_opts = SuryaOCROptions(\n    languages=['en', 'de'], # Specify languages for recognition\n    # device='cuda',       # Use GPU ('cuda') or CPU ('cpu') &lt;-- Set via env var TORCH_DEVICE\n    min_confidence=0.4   # Example: Adjust minimum confidence for results\n    # Core Surya options like device, batch size, and thresholds are typically\n    # set via environment variables (see note below).\n)\nocr_elements = page.apply_ocr(engine='surya', options=surya_opts)\n</code></pre>"},{"location":"ocr/#multiple-languages","title":"Multiple Languages","text":"<p>OCR supports multiple languages:</p> <pre><code># Recognize English and Spanish text\npdf = PDF('multilingual.pdf', ocr={\n    'enabled': True,\n    'languages': ['en', 'es']\n})\n\n# Multiple languages with PaddleOCR\npdf = PDF('multilingual_document.pdf', \n          ocr_engine='paddleocr',\n          ocr={\n              'enabled': True,\n              'languages': ['zh', 'ja', 'ko', 'en']  # Chinese, Japanese, Korean, English\n          })\n</code></pre>"},{"location":"ocr/#applying-ocr-directly","title":"Applying OCR Directly","text":"<p>The <code>page.apply_ocr(...)</code> and <code>region.apply_ocr(...)</code> methods are the primary way to run OCR:</p> <pre><code># Apply OCR to a page and get the OCR elements\nocr_elements = page.apply_ocr(engine='easyocr')\nprint(f\"Found {len(ocr_elements)} text elements via OCR\")\n\n# Apply OCR to a specific region\ntitle = page.find('text:contains(\"Title\")')\ncontent_region = title.below(height=300)\nregion_ocr_elements = content_region.apply_ocr(engine='paddle', languages=['en'])\n</code></pre>"},{"location":"ocr/#ocr-engines","title":"OCR Engines","text":"<p>Choose the engine best suited for your document and language requirements using the <code>engine</code> parameter in <code>apply_ocr</code>.</p>"},{"location":"ocr/#finding-and-working-with-ocr-text","title":"Finding and Working with OCR Text","text":"<p>After applying OCR, work with the text just like regular text:</p> <pre><code># Find all OCR text elements\nocr_text = page.find_all('text[source=ocr]')\n\n# Find high-confidence OCR text\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\n\n# Extract text only from OCR elements\nocr_text_content = page.find_all('text[source=ocr]').extract_text()\n\n# Filter OCR text by content\nnames = page.find_all('text[source=ocr]:contains(\"Smith\")', case=False)\n</code></pre>"},{"location":"ocr/#visualizing-ocr-results","title":"Visualizing OCR Results","text":"<p>See OCR results to help debug issues:</p> <pre><code># Apply OCR \nocr_elements = page.apply_ocr()\n\n# Highlight all OCR elements\nfor element in ocr_elements:\n    # Color based on confidence\n    if element.confidence &gt;= 0.8:\n        color = \"green\"  # High confidence\n    elif element.confidence &gt;= 0.5:\n        color = \"yellow\"  # Medium confidence\n    else:\n        color = \"red\"  # Low confidence\n\n    element.highlight(color=color, label=f\"OCR ({element.confidence:.2f})\")\n\n# Get the visualization as an image\nimage = page.to_image(labels=True)\n# Just return the image in a Jupyter cell\nimage\n\n# Highlight only high-confidence elements\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nhigh_conf.highlight(color=\"green\", label=\"High Confidence OCR\")\n</code></pre>"},{"location":"ocr/#ocr-debugging","title":"OCR Debugging","text":"<p>For troubleshooting OCR problems:</p> <pre><code># Create an interactive HTML debug report\npdf.debug_ocr(\"ocr_debug.html\")\n\n# Specify which pages to include\npdf.debug_ocr(\"ocr_debug.html\", pages=[0, 1, 2])\n</code></pre> <p>The debug report shows: - The original image - Text found with confidence scores - Boxes around each detected word - Options to sort and filter results</p>"},{"location":"ocr/#ocr-parameter-tuning","title":"OCR Parameter Tuning","text":""},{"location":"ocr/#parameter-recommendation-table","title":"Parameter Recommendation Table","text":"Issue Engine Parameter Recommended Value Effect Missing text EasyOCR <code>text_threshold</code> 0.1 - 0.3 (default: 0.7) Lower values detect more text but may increase false positives Missing text PaddleOCR <code>det_db_thresh</code> 0.1 - 0.3 (default: 0.3) Lower values detect more text areas Low quality scan EasyOCR <code>contrast_ths</code> 0.05 - 0.1 (default: 0.1) Lower values help with low contrast documents Low quality scan PaddleOCR <code>det_limit_side_len</code> 1280 - 2560 (default: 960) Higher values improve detail detection Accuracy vs. speed EasyOCR <code>decoder</code> \"wordbeamsearch\" (accuracy)\"greedy\" (speed) Word beam search is more accurate but slower Accuracy vs. speed PaddleOCR <code>rec_batch_num</code> 1 (accuracy)8+ (speed) Larger batches process faster but use more memory Small text Both <code>min_confidence</code> 0.3 - 0.4 (default: 0.5) Lower confidence threshold to capture small/blurry text Text orientation PaddleOCR <code>use_angle_cls</code> <code>True</code> Enable angle classification for rotated text Asian languages PaddleOCR <code>lang</code> \"ch\", \"japan\", \"korea\" Use PaddleOCR for Asian languages"},{"location":"ocr/#next-steps","title":"Next Steps","text":"<p>With OCR capabilities, you can explore:</p> <ul> <li>Layout Analysis for automatically detecting document structure</li> <li>Document QA for asking questions about your documents</li> <li>Visual Debugging for visualizing OCR results</li> </ul>"},{"location":"pdf-navigation/","title":"PDF Navigation","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Open a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n</pre> from natural_pdf import PDF  # Open a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\") In\u00a0[2]: Copied! <pre># Get the total number of pages\nnum_pages = len(pdf)\nprint(f\"This PDF has {num_pages} pages\")\n\n# Get a specific page (0-indexed)\nfirst_page = pdf.pages[0]\nlast_page = pdf.pages[-1]\n\n# Iterate through the first 20 pages\nfor page in pdf.pages[:20]:\n    print(f\"Page {page.number} has {len(page.extract_text())} characters\")\n</pre> # Get the total number of pages num_pages = len(pdf) print(f\"This PDF has {num_pages} pages\")  # Get a specific page (0-indexed) first_page = pdf.pages[0] last_page = pdf.pages[-1]  # Iterate through the first 20 pages for page in pdf.pages[:20]:     print(f\"Page {page.number} has {len(page.extract_text())} characters\") <pre>This PDF has 153 pages\nPage 1 has 985 characters\nPage 2 has 778 characters\nPage 3 has 522 characters\nPage 4 has 984 characters\nPage 5 has 778 characters\nPage 6 has 523 characters\n</pre> <pre>Page 7 has 982 characters\nPage 8 has 772 characters\nPage 9 has 522 characters\nPage 10 has 1008 characters\n</pre> <pre>Page 11 has 796 characters\nPage 12 has 532 characters\nPage 13 has 986 characters\nPage 14 has 780 characters\nPage 15 has 523 characters\nPage 16 has 990 characters\nPage 17 has 782 characters\n</pre> <pre>Page 18 has 520 characters\nPage 19 has 1006 characters\nPage 20 has 795 characters\n</pre> In\u00a0[3]: Copied! <pre># Page dimensions in points (1/72 inch)\nprint(page.width, page.height)\n\n# Page number (1-indexed as shown in PDF viewers)\nprint(page.number)\n\n# Page index (0-indexed position in the PDF)\nprint(page.index)\n</pre> # Page dimensions in points (1/72 inch) print(page.width, page.height)  # Page number (1-indexed as shown in PDF viewers) print(page.number)  # Page index (0-indexed position in the PDF) print(page.index) <pre>612 792\n20\n19\n</pre> In\u00a0[4]: Copied! <pre># Extract text from all pages\nall_text = pdf.extract_text()\n\n# Find elements across all pages\nall_headings = pdf.find_all('text[size&gt;=14]:bold')\n\n# Add exclusion zones to all pages (like headers/footers)\npdf.add_exclusion(\n    lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n    label=\"header\"\n)\n</pre> # Extract text from all pages all_text = pdf.extract_text()  # Find elements across all pages all_headings = pdf.find_all('text[size&gt;=14]:bold')  # Add exclusion zones to all pages (like headers/footers) pdf.add_exclusion(     lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,     label=\"header\" ) Out[4]: <pre>&lt;natural_pdf.core.pdf.PDF at 0x1045224d0&gt;</pre> In\u00a0[5]: Copied! <pre># Extract text from specific pages\ntext = pdf.pages[2:5].extract_text()\n\n# Find elements across specific pages\nelements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")')\n</pre> # Extract text from specific pages text = pdf.pages[2:5].extract_text()  # Find elements across specific pages elements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")') In\u00a0[6]: Copied! <pre># Get sections with headings as section starts\nsections = pdf.pages.get_sections(\n    start_elements='text[size&gt;=14]:bold',\n    new_section_on_page_break=False\n)\n</pre> # Get sections with headings as section starts sections = pdf.pages.get_sections(     start_elements='text[size&gt;=14]:bold',     new_section_on_page_break=False )"},{"location":"pdf-navigation/#pdf-navigation","title":"PDF Navigation\u00b6","text":"<p>This guide covers the basics of working with PDFs in Natural PDF - opening documents, accessing pages, and navigating through content.</p>"},{"location":"pdf-navigation/#opening-a-pdf","title":"Opening a PDF\u00b6","text":"<p>The main entry point to Natural PDF is the <code>PDF</code> class:</p>"},{"location":"pdf-navigation/#accessing-pages","title":"Accessing Pages\u00b6","text":"<p>Once you have a PDF object, you can access its pages:</p>"},{"location":"pdf-navigation/#page-properties","title":"Page Properties\u00b6","text":"<p>Each <code>Page</code> object has useful properties:</p>"},{"location":"pdf-navigation/#working-across-pages","title":"Working Across Pages\u00b6","text":"<p>Natural PDF makes it easy to work with content across multiple pages:</p>"},{"location":"pdf-navigation/#the-page-collection","title":"The Page Collection\u00b6","text":"<p>The <code>pdf.pages</code> object is a <code>PageCollection</code> that allows batch operations on pages:</p>"},{"location":"pdf-navigation/#document-sections-across-pages","title":"Document Sections Across Pages\u00b6","text":"<p>You can extract sections that span across multiple pages:</p>"},{"location":"pdf-navigation/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to navigate PDFs, you can:</p> <ul> <li>Find elements using selectors</li> <li>Extract text from your documents</li> <li>Work with specific regions</li> </ul>"},{"location":"regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page page = pdf.pages[0]  # Display the page page.show(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Create a region by specifying (x0, top, x1, bottom) coordinates\n# Let's create a region in the middle of the page\nmid_region = page.create_region(\n    x0=100,         # Left edge\n    top=200,        # Top edge\n    x1=500,         # Right edge\n    bottom=400      # Bottom edge\n)\n\n# Highlight the region to see it\nmid_region.highlight(color=\"blue\").show()\n</pre> # Create a region by specifying (x0, top, x1, bottom) coordinates # Let's create a region in the middle of the page mid_region = page.create_region(     x0=100,         # Left edge     top=200,        # Top edge     x1=500,         # Right edge     bottom=400      # Bottom edge )  # Highlight the region to see it mid_region.highlight(color=\"blue\").show() Out[2]: In\u00a0[3]: Copied! <pre># Find a heading-like element\nheading = page.find('text[size&gt;=12]:bold')\n\n# Create a region below this heading element\nif heading:\n    region_below = heading.below()\n    \n    # Highlight the heading and the region below it\n    heading.highlight(color=\"red\")\n    region_below.highlight(color=\"blue\")\n    page.show()\n</pre> # Find a heading-like element heading = page.find('text[size&gt;=12]:bold')  # Create a region below this heading element if heading:     region_below = heading.below()          # Highlight the heading and the region below it     heading.highlight(color=\"red\")     region_below.highlight(color=\"blue\")     page.show() In\u00a0[4]: Copied! <pre># Create a region with height limit\nif heading:\n    # Only include 100px below the heading\n    small_region_below = heading.below(height=100)\n    \n    page.clear_highlights()\n    heading.highlight(color=\"red\")\n    small_region_below.highlight(color=\"green\")\n    page.show()\n</pre> # Create a region with height limit if heading:     # Only include 100px below the heading     small_region_below = heading.below(height=100)          page.clear_highlights()     heading.highlight(color=\"red\")     small_region_below.highlight(color=\"green\")     page.show() In\u00a0[5]: Copied! <pre># Find a line or other element to create a region above\nline = page.find('line')\nif line:\n    # Create a region above the line\n    region_above = line.above()\n    \n    page.clear_highlights()\n    line.highlight(color=\"black\")\n    region_above.highlight(color=\"purple\")\n    page.show()\n</pre> # Find a line or other element to create a region above line = page.find('line') if line:     # Create a region above the line     region_above = line.above()          page.clear_highlights()     line.highlight(color=\"black\")     region_above.highlight(color=\"purple\")     page.show() In\u00a0[6]: Copied! <pre># Find two elements to use as boundaries\nfirst_heading = page.find('text[size&gt;=11]:bold')\nnext_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None\n\nif first_heading and next_heading:\n    # Create a region from the first heading until the next heading\n    section = first_heading.below(until=next_heading, include_endpoint=False)\n    \n    # Highlight both elements and the region between them\n    page.clear_highlights()\n    first_heading.highlight(color=\"red\")\n    next_heading.highlight(color=\"red\")\n    section.highlight(color=\"yellow\")\n    page.show()\n</pre> # Find two elements to use as boundaries first_heading = page.find('text[size&gt;=11]:bold') next_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None  if first_heading and next_heading:     # Create a region from the first heading until the next heading     section = first_heading.below(until=next_heading, include_endpoint=False)          # Highlight both elements and the region between them     page.clear_highlights()     first_heading.highlight(color=\"red\")     next_heading.highlight(color=\"red\")     section.highlight(color=\"yellow\")     page.show() In\u00a0[7]: Copied! <pre># Find a region to work with (e.g., from a title to the next bold text)\ntitle = page.find('text:contains(\"Site\")')  # Adjust if needed\nif title:\n    # Create a region from title down to the next bold text\n    content_region = title.below(until='line:horizontal', include_endpoint=False)\n    \n    # Extract text from just this region\n    region_text = content_region.extract_text()\n    \n    # Show the region and the extracted text\n    page.clear_highlights()\n    content_region.highlight(color=\"green\")\n    page.show()\n    \n    # Displaying the text (first 300 chars if long)\n    print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text)\n</pre> # Find a region to work with (e.g., from a title to the next bold text) title = page.find('text:contains(\"Site\")')  # Adjust if needed if title:     # Create a region from title down to the next bold text     content_region = title.below(until='line:horizontal', include_endpoint=False)          # Extract text from just this region     region_text = content_region.extract_text()          # Show the region and the extracted text     page.clear_highlights()     content_region.highlight(color=\"green\")     page.show()          # Displaying the text (first 300 chars if long)     print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text) <pre>Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the othe...\n</pre> In\u00a0[8]: Copied! <pre># Create a region in an interesting part of the page\ntest_region = page.create_region(\n    x0=page.width * 0.1, \n    top=page.height * 0.25, \n    x1=page.width * 0.9, \n    bottom=page.height * 0.75\n)\n\n# Find all text elements ONLY within this region\ntext_in_region = test_region.find_all('text')\n\n# Display result\npage.clear_highlights()\ntest_region.highlight(color=\"blue\")\ntext_in_region.highlight(color=\"red\")\npage.show()\n\nlen(text_in_region)  # Number of text elements found in region\n</pre> # Create a region in an interesting part of the page test_region = page.create_region(     x0=page.width * 0.1,      top=page.height * 0.25,      x1=page.width * 0.9,      bottom=page.height * 0.75 )  # Find all text elements ONLY within this region text_in_region = test_region.find_all('text')  # Display result page.clear_highlights() test_region.highlight(color=\"blue\") text_in_region.highlight(color=\"red\") page.show()  len(text_in_region)  # Number of text elements found in region Out[8]: <pre>29</pre> In\u00a0[9]: Copied! <pre># Find a specific region to capture\n# (Could be a table, figure, or any significant area)\nregion_for_image = page.create_region(\n    x0=100, \n    top=150,\n    x1=page.width - 100,\n    bottom=300\n)\n\n# Generate an image of just this region\nregion_for_image.to_image(crop_only=True)  # Shows just the region\n</pre> # Find a specific region to capture # (Could be a table, figure, or any significant area) region_for_image = page.create_region(     x0=100,      top=150,     x1=page.width - 100,     bottom=300 )  # Generate an image of just this region region_for_image.to_image(crop_only=True)  # Shows just the region Out[9]: In\u00a0[10]: Copied! <pre># Take an existing region and expand it\nregion_a = page.create_region(200, 200, 400, 400)\n\n# Expand by a certain number of points in each direction\nexpanded = region_a.expand(left=20, right=20, top=20, bottom=20)\n\n# Visualize original and expanded regions\npage.clear_highlights()\nregion_a.highlight(color=\"blue\", label=\"Original\")\nexpanded.highlight(color=\"red\", label=\"Expanded\")\npage.to_image()\n</pre> # Take an existing region and expand it region_a = page.create_region(200, 200, 400, 400)  # Expand by a certain number of points in each direction expanded = region_a.expand(left=20, right=20, top=20, bottom=20)  # Visualize original and expanded regions page.clear_highlights() region_a.highlight(color=\"blue\", label=\"Original\") expanded.highlight(color=\"red\", label=\"Expanded\") page.to_image() Out[10]: In\u00a0[11]: Copied! <pre># Create a region for the whole page\nfull_page_region = page.create_region(0, 0, page.width, page.height)\n\n# Extract text without exclusions as baseline\nfull_text = full_page_region.extract_text()\nprint(f\"Full page text length: {len(full_text)} characters\")\n</pre> # Create a region for the whole page full_page_region = page.create_region(0, 0, page.width, page.height)  # Extract text without exclusions as baseline full_text = full_page_region.extract_text() print(f\"Full page text length: {len(full_text)} characters\") <pre>Full page text length: 1262 characters\n</pre> In\u00a0[12]: Copied! <pre># Define an area we want to exclude (like a header)\n# Let's exclude the top 10% of the page\nheader_zone = page.create_region(0, 0, page.width, page.height * 0.1)\n\n# Add this as an exclusion for the page\npage.add_exclusion(header_zone)\n\n# Visualize the exclusion\npage.clear_highlights()\nheader_zone.highlight(color=\"red\", label=\"Excluded\")\npage.show()\n</pre> # Define an area we want to exclude (like a header) # Let's exclude the top 10% of the page header_zone = page.create_region(0, 0, page.width, page.height * 0.1)  # Add this as an exclusion for the page page.add_exclusion(header_zone)  # Visualize the exclusion page.clear_highlights() header_zone.highlight(color=\"red\", label=\"Excluded\") page.show() Out[12]: In\u00a0[13]: Copied! <pre># Now extract text again - the header should be excluded\ntext_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default\n\n# Compare text lengths\nprint(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\")\nprint(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\")\n</pre> # Now extract text again - the header should be excluded text_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default  # Compare text lengths print(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\") print(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\") <pre>Original text: 1262 chars\nText with exclusion: 1198 chars\nDifference: 64 chars excluded\n</pre> In\u00a0[14]: Copied! <pre># When done with this page, clear exclusions\npage.clear_exclusions()\n</pre> # When done with this page, clear exclusions page.clear_exclusions() Out[14]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[15]: Copied! <pre># Define a PDF-level exclusion for headers\n# This will exclude the top 30% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.3),\n    label=\"Header zone\"\n)\n\n# Define a PDF-level exclusion for footers\n# This will exclude the bottom 20% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),\n    label=\"Footer zone\"\n)\n\n# PDF-level exclusions are used whenever you extract text\n# Let's try on the first three pages\nfor page in pdf.pages[:3]:\n    text = page.extract_text()\n    text_original = page.extract_text(use_exclusions=False)\n    print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\")\n</pre> # Define a PDF-level exclusion for headers # This will exclude the top 30% of every page pdf.add_exclusion(     lambda p: p.create_region(0, 0, p.width, p.height * 0.3),     label=\"Header zone\" )  # Define a PDF-level exclusion for footers # This will exclude the bottom 20% of every page pdf.add_exclusion(     lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),     label=\"Footer zone\" )  # PDF-level exclusions are used whenever you extract text # Let's try on the first three pages for page in pdf.pages[:3]:     text = page.extract_text()     text_original = page.extract_text(use_exclusions=False)     print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\") <pre>Page 1 \u2013 Before: 1260 After: 456\n</pre> In\u00a0[16]: Copied! <pre># Clear PDF-level exclusions when done\npdf.clear_exclusions()\nprint(\"Cleared all PDF-level exclusions\")\n</pre> # Clear PDF-level exclusions when done pdf.clear_exclusions() print(\"Cleared all PDF-level exclusions\") <pre>Cleared all PDF-level exclusions\n</pre> In\u00a0[17]: Copied! <pre># First, run layout analysis to detect regions\npage.analyze_layout()  # Uses 'yolo' engine by default\n\n# Find all detected regions\ndetected_regions = page.find_all('region')\nprint(f\"Found {len(detected_regions)} layout regions\")\n</pre> # First, run layout analysis to detect regions page.analyze_layout()  # Uses 'yolo' engine by default  # Find all detected regions detected_regions = page.find_all('region') print(f\"Found {len(detected_regions)} layout regions\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpw8v45ahq/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1574.9ms\n</pre> <pre>Speed: 4.1ms preprocess, 1574.9ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Found 8 layout regions\n</pre> In\u00a0[18]: Copied! <pre># Highlight all detected regions by type\ndetected_regions.highlight(group_by='region_type').show()\n</pre> # Highlight all detected regions by type detected_regions.highlight(group_by='region_type').show() Out[18]: In\u00a0[19]: Copied! <pre># Extract text from a specific region type (e.g., title)\ntitle_regions = page.find_all('region[type=title]')\nif title_regions:\n    titles_text = title_regions.extract_text()\n    print(f\"Title text: {titles_text}\")\n</pre> # Extract text from a specific region type (e.g., title) title_regions = page.find_all('region[type=title]') if title_regions:     titles_text = title_regions.extract_text()     print(f\"Title text: {titles_text}\") <pre>Title text: Violation Count:  7 Violations\n</pre>"},{"location":"regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that define boundaries for operations like text extraction, element finding, or visualization. They're one of Natural PDF's most powerful features for working with specific parts of a document.</p>"},{"location":"regions/#setup","title":"Setup\u00b6","text":"<p>Let's set up a PDF to experiment with regions.</p>"},{"location":"regions/#creating-regions","title":"Creating Regions\u00b6","text":"<p>There are several ways to create regions in Natural PDF.</p>"},{"location":"regions/#using-create_region-with-coordinates","title":"Using <code>create_region()</code> with Coordinates\u00b6","text":"<p>This is the most direct method - provide the coordinates directly.</p>"},{"location":"regions/#using-element-methods-above-below-left-right","title":"Using Element Methods: <code>above()</code>, <code>below()</code>, <code>left()</code>, <code>right()</code>\u00b6","text":"<p>You can create regions relative to existing elements.</p>"},{"location":"regions/#creating-a-region-between-elements-with-until","title":"Creating a Region Between Elements with <code>until()</code>\u00b6","text":""},{"location":"regions/#using-regions","title":"Using Regions\u00b6","text":"<p>Once you have a region, here's what you can do with it.</p>"},{"location":"regions/#extract-text-from-a-region","title":"Extract Text from a Region\u00b6","text":""},{"location":"regions/#find-elements-within-a-region","title":"Find Elements Within a Region\u00b6","text":"<p>You can use a region as a \"filter\" to only find elements within its boundaries.</p>"},{"location":"regions/#generate-an-image-of-a-region","title":"Generate an Image of a Region\u00b6","text":""},{"location":"regions/#adjust-and-expand-regions","title":"Adjust and Expand Regions\u00b6","text":""},{"location":"regions/#using-exclusion-zones-with-regions","title":"Using Exclusion Zones with Regions\u00b6","text":"<p>Exclusion zones are regions that you want to ignore during operations like text extraction.</p>"},{"location":"regions/#document-level-exclusions","title":"Document-Level Exclusions\u00b6","text":"<p>PDF-level exclusions apply to all pages and use functions to adapt to each page.</p>"},{"location":"regions/#working-with-layout-analysis-regions","title":"Working with Layout Analysis Regions\u00b6","text":"<p>When you run layout analysis, the detected regions (tables, titles, etc.) are also Region objects.</p>"},{"location":"regions/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you understand regions, you can:</p> <ul> <li>Extract tables from table regions</li> <li>Ask questions about specific regions</li> <li>Exclude content from extraction</li> </ul>"},{"location":"tables/","title":"Table Extraction","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() Out[1]: In\u00a0[2]: Copied! <pre># Extract the first table found on the page using pdfplumber\n# This works best for simple tables with clear lines\ntable_data = page.extract_table() # Returns a list of lists\ntable_data\n</pre> # Extract the first table found on the page using pdfplumber # This works best for simple tables with clear lines table_data = page.extract_table() # Returns a list of lists table_data Out[2]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>This might fail or give poor results if there are multiple tables or the table structure is complex.</p> In\u00a0[3]: Copied! <pre># Detect layout elements using YOLO (default)\npage.analyze_layout(engine='yolo')\n\n# Find regions detected as tables\ntable_regions_yolo = page.find_all('region[type=table][model=yolo]')\ntable_regions_yolo.show()\n</pre> # Detect layout elements using YOLO (default) page.analyze_layout(engine='yolo')  # Find regions detected as tables table_regions_yolo = page.find_all('region[type=table][model=yolo]') table_regions_yolo.show() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmptrpq_4cf/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 2941.3ms\n</pre> <pre>Speed: 7.3ms preprocess, 2941.3ms inference, 0.9ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[3]: In\u00a0[4]: Copied! <pre>table_regions_yolo[0].extract_table()\n</pre> table_regions_yolo[0].extract_table() Out[4]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[5]: Copied! <pre>page.clear_detected_layout_regions() # Clear previous YOLO regions for clarity\npage.analyze_layout(engine='tatr')\n</pre> page.clear_detected_layout_regions() # Clear previous YOLO regions for clarity page.analyze_layout(engine='tatr') Out[5]: <pre>&lt;ElementCollection[Region](count=15)&gt;</pre> In\u00a0[6]: Copied! <pre># Find the main table region(s) detected by TATR\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.show()\n</pre> # Find the main table region(s) detected by TATR tatr_table = page.find('region[type=table][model=tatr]') tatr_table.show() Out[6]: In\u00a0[7]: Copied! <pre># Find rows, columns, headers detected by TATR\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\nf\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\"\n</pre> # Find rows, columns, headers detected by TATR rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]') f\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\" Out[7]: <pre>'TATR found: 8 rows, 4 columns, 1 headers'</pre> In\u00a0[8]: Copied! <pre>tatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.extract_table(method='tatr')\n</pre> tatr_table = page.find('region[type=table][model=tatr]') tatr_table.extract_table(method='tatr') Out[8]: <pre>[['Statute Description Level Repeat?'],\n ['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> In\u00a0[9]: Copied! <pre># Force using pdfplumber even on a TATR-detected region\n# (Might be useful for comparison or if TATR structure is flawed)\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.extract_table(method='pdfplumber')\n</pre> # Force using pdfplumber even on a TATR-detected region # (Might be useful for comparison or if TATR structure is flawed) tatr_table = page.find('region[type=table][model=tatr]') tatr_table.extract_table(method='pdfplumber') Out[9]: <pre>[['Unsanitary Working Conditions.', 'Critical'],\n ['Inadequate Protective Equipment.', 'Serious'],\n ['Ineffective Injury Prevention.', 'Serious'],\n ['Failure to Properly Store Hazardous Materials.', 'Critical'],\n ['Lack of Adequate Fire Safety Measures.', 'Serious'],\n ['Inadequate Ventilation Systems.', 'Serious']]</pre> In\u00a0[10]: Copied! <pre># Example: Use text alignment for vertical lines, explicit lines for horizontal\n# See pdfplumber documentation for all settings\ntable_settings = {\n    \"vertical_strategy\": \"text\",\n    \"horizontal_strategy\": \"lines\",\n    \"intersection_x_tolerance\": 5, # Increase tolerance for intersections\n}\n\nresults = page.extract_table(\n    table_settings=table_settings\n)\n</pre> # Example: Use text alignment for vertical lines, explicit lines for horizontal # See pdfplumber documentation for all settings table_settings = {     \"vertical_strategy\": \"text\",     \"horizontal_strategy\": \"lines\",     \"intersection_x_tolerance\": 5, # Increase tolerance for intersections }  results = page.extract_table(     table_settings=table_settings ) In\u00a0[11]: Copied! <pre>import pandas as pd\n\npd.DataFrame(page.extract_table())\n</pre> import pandas as pd  pd.DataFrame(page.extract_table()) Out[11]: 0 1 2 3 0 Statute Description Level Repeat? 1 4.12.7 Unsanitary Working Conditions. Critical 2 5.8.3 Inadequate Protective Equipment. Serious 3 6.3.9 Ineffective Injury Prevention. Serious 4 7.1.5 Failure to Properly Store Hazardous Materials. Critical 5 8.9.2 Lack of Adequate Fire Safety Measures. Serious 6 9.6.4 Inadequate Ventilation Systems. Serious 7 10.2.7 Insufficient Employee Training for Safe Work P... Serious In\u00a0[12]: Copied! <pre># This doesn't work! I forget why, I should troubleshoot later.\n# tatr_table.cells\n</pre> # This doesn't work! I forget why, I should troubleshoot later. # tatr_table.cells"},{"location":"tables/#table-extraction","title":"Table Extraction\u00b6","text":"<p>Extracting tables from PDFs can range from straightforward to complex. Natural PDF provides several tools and methods to handle different scenarios, leveraging both rule-based (<code>pdfplumber</code>) and model-based (<code>TATR</code>) approaches.</p>"},{"location":"tables/#setup","title":"Setup\u00b6","text":"<p>Let's load a PDF containing tables.</p>"},{"location":"tables/#basic-table-extraction-no-detection","title":"Basic Table Extraction (No Detection)\u00b6","text":"<p>If you know a table exists, you can try <code>extract_table()</code> directly on the page or a region. This uses <code>pdfplumber</code> behind the scenes.</p>"},{"location":"tables/#layout-analysis-for-table-detection","title":"Layout Analysis for Table Detection\u00b6","text":"<p>A more robust approach can be to first detect the table boundaries using layout analysis.</p>"},{"location":"tables/#using-yolo-default","title":"Using YOLO (Default)\u00b6","text":"<p>The default YOLO model finds the overall bounding box of tables.</p>"},{"location":"tables/#using-tatr-table-transformer","title":"Using TATR (Table Transformer)\u00b6","text":"<p>The TATR model provides detailed table structure (rows, columns, headers).</p>"},{"location":"tables/#controlling-extraction-method-plumber-vs-tatr","title":"Controlling Extraction Method (<code>plumber</code> vs <code>tatr</code>)\u00b6","text":"<p>When you call <code>extract_table()</code> on a region:</p> <ul> <li>If the region was detected by YOLO (or not detected at all), it uses the <code>plumber</code> method.</li> <li>If the region was detected by TATR, it defaults to the <code>tatr</code> method, which uses the detected row/column structure.</li> </ul> <p>You can override this using the <code>method</code> argument.</p>"},{"location":"tables/#when-to-use-which-method","title":"When to Use Which Method?\u00b6","text":"<ul> <li><code>pdfplumber</code>: Good for simple tables with clear grid lines. Faster.</li> <li><code>tatr</code>: Better for tables without clear lines, complex cell merging, or irregular layouts. Leverages the model's understanding of rows and columns.</li> </ul>"},{"location":"tables/#customizing-pdfplumber-settings","title":"Customizing <code>pdfplumber</code> Settings\u00b6","text":"<p>If using the <code>pdfplumber</code> method (explicitly or implicitly), you can pass <code>pdfplumber</code> settings via <code>table_settings</code>.</p>"},{"location":"tables/#saving-extracted-tables","title":"Saving Extracted Tables\u00b6","text":"<p>You can easily save the extracted data (list of lists) to common formats.</p>"},{"location":"tables/#working-directly-with-tatr-cells","title":"Working Directly with TATR Cells\u00b6","text":"<p>The TATR engine implicitly creates cell regions at the intersection of detected rows and columns. You can access these for fine-grained control.</p>"},{"location":"tables/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Layout Analysis: Understand how table detection fits into overall document structure analysis.</li> <li>Working with Regions: Manually define table areas if detection fails.</li> </ul>"},{"location":"text-analysis/","title":"Text Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0] In\u00a0[2]: Copied! <pre># Find the first word element\nword = page.find('word') \n\nprint(f\"Text:\", word.text)\nprint(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name\nprint(f\"Size:\", word.size)\nprint(f\"Color:\", word.color) # Non-stroking color\nprint(f\"Is Bold:\", word.bold)\nprint(f\"Is Italic:\", word.italic)\n</pre> # Find the first word element word = page.find('word')   print(f\"Text:\", word.text) print(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name print(f\"Size:\", word.size) print(f\"Color:\", word.color) # Non-stroking color print(f\"Is Bold:\", word.bold) print(f\"Is Italic:\", word.italic) <pre>Text: Jungle Health and Safety Inspection Service\nFont Name: Helvetica\nSize: 8.0\nColor: (0, 0, 0)\nIs Bold: False\nIs Italic: False\n</pre> <ul> <li><code>fontname</code>: Often an internal reference (like 'F1', 'F2') or a basic name.</li> <li><code>size</code>: Font size in points.</li> <li><code>color</code>: The non-stroking color, typically a tuple representing RGB or Grayscale values (e.g., <code>(0.0, 0.0, 0.0)</code> for black).</li> <li><code>bold</code>, <code>italic</code>: Boolean flags indicating if the font style is bold or italic (heuristically determined based on font name conventions).</li> </ul> In\u00a0[3]: Copied! <pre># Find all bold text elements\nbold_text = page.find_all('text:bold')\n\n# Find all italic text elements\nitalic_text = page.find_all('text:italic')\n\n# Find text that is both bold and larger than 12pt\nbold_headings = page.find_all('text:bold[size&gt;=12]')\n\nprint(f\"Found {len(bold_text)} bold elements.\")\nprint(f\"Found {len(italic_text)} italic elements.\")\nprint(f\"Found {len(bold_headings)} bold headings.\")\n</pre> # Find all bold text elements bold_text = page.find_all('text:bold')  # Find all italic text elements italic_text = page.find_all('text:italic')  # Find text that is both bold and larger than 12pt bold_headings = page.find_all('text:bold[size&gt;=12]')  print(f\"Found {len(bold_text)} bold elements.\") print(f\"Found {len(italic_text)} italic elements.\") print(f\"Found {len(bold_headings)} bold headings.\") <pre>Found 9 bold elements.\nFound 0 italic elements.\nFound 1 bold headings.\n</pre> In\u00a0[4]: Copied! <pre>page.analyze_text_styles()\npage.text_style_labels\n</pre> page.analyze_text_styles() page.text_style_labels Out[4]: <pre>['10.0pt Bold Helvetica',\n '10.0pt Helvetica',\n '12.0pt Bold Helvetica',\n '8.0pt Helvetica']</pre> <p>One they're assigned, you can filter based on <code>style_label</code> instead of going bit-by-bit.</p> In\u00a0[5]: Copied! <pre>page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]')\n</pre> page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]') Out[5]: <pre>&lt;ElementCollection[TextElement](count=8)&gt;</pre> In\u00a0[6]: Copied! <pre>page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700)\n</pre> page.find_all('text').highlight(group_by='style_label', replace=True).to_image(width=700) Out[6]: <p>This allows you to quickly see patterns in font usage across the page layout.</p> In\u00a0[7]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Select the first page page = pdf.pages[0] page.to_image(width=700) Out[7]: <p>Look!</p> In\u00a0[8]: Copied! <pre>page.find_all('text')[0].fontname\n</pre> page.find_all('text')[0].fontname Out[8]: <pre>'AAAAAB+font000000002a8d158a'</pre> <p>The part before the <code>+</code> is the variant \u2013 bold, italic, etc \u2013 while the part after it is the \"real\" font name.</p>"},{"location":"text-analysis/#text-analysis","title":"Text Analysis\u00b6","text":"<p>Analyzing the properties of text elements, such as their font, size, style, and color, can be crucial for understanding document structure and extracting specific information. Natural PDF provides tools to access and analyze these properties.</p>"},{"location":"text-analysis/#introduction","title":"Introduction\u00b6","text":"<p>Beyond just the sequence of characters, the style of text carries significant meaning. Headings are often larger and bolder, important terms might be italicized, and different sections might use distinct fonts. This page covers how to access and utilize this stylistic information.</p>"},{"location":"text-analysis/#accessing-font-information","title":"Accessing Font Information\u00b6","text":"<p>Every <code>TextElement</code> (representing characters or words) holds information about its font properties.</p>"},{"location":"text-analysis/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>You can directly select text based on its style using pseudo-classes in selectors:</p>"},{"location":"text-analysis/#analyzing-fonts-on-a-page","title":"Analyzing Fonts on a Page\u00b6","text":"<p>You can use <code>analyze_text_styles</code> to assign labels to text based on font sizes, bold/italic and font names.</p>"},{"location":"text-analysis/#visualizing-text-properties","title":"Visualizing Text Properties\u00b6","text":"<p>Use highlighting to visually inspect text properties. Grouping by attributes like <code>fontname</code> or <code>size</code> can be very insightful. In the example below we go right to grouping by the <code>style_label</code>, which combines font name, size and variant.</p>"},{"location":"text-analysis/#weird-font-names","title":"Weird font names\u00b6","text":"<p>Oftentimes font names aren't what you're used to \u2013 Arial, Helvetica, etc \u2013 the PDF has given them weird, weird names. Relax, it's okay, they're normal fonts.</p>"},{"location":"text-extraction/","title":"Text Extraction Guide","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page for initial examples\npage = pdf.pages[0]\n\n# Display the first page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page for initial examples page = pdf.pages[0]  # Display the first page page.show(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Extract all text from the first page\n# Displaying first 500 characters\nprint(page.extract_text()[:500])\n</pre> # Extract all text from the first page # Displaying first 500 characters print(page.extract_text()[:500]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\nDate:  February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \nsome of which there were open vats near the\n</pre> <p>You can also preserve layout with <code>layout=True</code>.</p> In\u00a0[3]: Copied! <pre># Extract text from the entire document (may take time)\n# Uncomment to run:\nprint(page.extract_text(layout=True)[:2000])\n</pre> # Extract text from the entire document (may take time) # Uncomment to run: print(page.extract_text(layout=True)[:2000]) <pre>                                                                                    \n                                                                                    \n                                                                                    \n                                                     Jungle Health and Safety Inspection Service\n                                                     INS-UP70N51NCL41R              \n                                                                                    \n       Site: Durham\u2019s Meatpacking  Chicago, Ill.                                    \n                                                                                    \n       Date:  February 3, 1905                                                      \n                                                                                    \n       Violation Count: 7                                                           \n       Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\n       These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \n                                                                                    \n       visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \n       some of which there were open vats near the level of the floor, their peculiar trouble was that they fell\n       into the vats; and when they were fished out, there was never enough of them left to be worth \n       exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\n       to the world as Durham\u2019s Pure Leaf Lard!                                     \n                                                                                    \n                                                                                    \n                                                          \n</pre> In\u00a0[4]: Copied! <pre># Find a single element, e.g., a title containing \"Summary\"\n# Adjust selector as needed\ndate_element = page.find('text:contains(\"Site\")')\ndate_element # Display the found element object\n</pre> # Find a single element, e.g., a title containing \"Summary\" # Adjust selector as needed date_element = page.find('text:contains(\"Site\")') date_element # Display the found element object Out[4]: <pre>&lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;</pre> In\u00a0[5]: Copied! <pre>date_element.show()\n</pre> date_element.show() Out[5]: In\u00a0[6]: Copied! <pre>date_element.text\n</pre> date_element.text Out[6]: <pre>'Site: '</pre> In\u00a0[7]: Copied! <pre># Find multiple elements, e.g., bold headings (size &gt;= 8)\nheading_elements = page.find_all('text[size&gt;=8]:bold')\nheading_elements \n</pre> # Find multiple elements, e.g., bold headings (size &gt;= 8) heading_elements = page.find_all('text[size&gt;=8]:bold') heading_elements  Out[7]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[8]: Copied! <pre>page.find_all('text[size&gt;=8]:bold').show()\n</pre> page.find_all('text[size&gt;=8]:bold').show() Out[8]: In\u00a0[9]: Copied! <pre># Pull out all of their text (why? I don't know!)\nprint(heading_elements.extract_text())\n</pre> # Pull out all of their text (why? I don't know!) print(heading_elements.extract_text()) <pre>Site:  Date:  Violation Count:  Summary:  Violations Statute Description Level Repeat?\n</pre> In\u00a0[10]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"Hazardous Materials\")').text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"Hazardous Materials\")').text Out[10]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[11]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text Out[11]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[12]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\nregex = \"\\d+, \\d{4}\"\npage.find(f'text:contains(\"{regex}\")', regex=True)\n</pre> # Regular expression (e.g., \"YYYY Report\") regex = \"\\d+, \\d{4}\" page.find(f'text:contains(\"{regex}\")', regex=True) Out[12]: <pre>&lt;TextElement text='February 3...' font='Helvetica' size=10.0 bbox=(80.56, 104.07000000000005, 156.71000000000004, 114.07000000000005)&gt;</pre> In\u00a0[13]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\npage.find_all('text[fontname=\"Helvetica\"][size=10]')\n</pre> # Regular expression (e.g., \"YYYY Report\") page.find_all('text[fontname=\"Helvetica\"][size=10]') Out[13]: <pre>&lt;ElementCollection[TextElement](count=31)&gt;</pre> In\u00a0[14]: Copied! <pre># Region below an element (e.g., below \"Introduction\")\n# Adjust selector as needed\npage.find('text:contains(\"Summary\")').below(include_element=True).show()\n</pre> # Region below an element (e.g., below \"Introduction\") # Adjust selector as needed page.find('text:contains(\"Summary\")').below(include_element=True).show() Out[14]: In\u00a0[15]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_element=True)\n    .extract_text()\n    [:500]\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_element=True)     .extract_text()     [:500] ) Out[15]: <pre>'Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left t'</pre> In\u00a0[16]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_element=True, until='line:horizontal')\n    .show()\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_element=True, until='line:horizontal')     .show() ) Out[16]: In\u00a0[17]: Copied! <pre># Manually defined region via coordinates (x0, top, x1, bottom)\nmanual_region = page.create_region(30, 60, 600, 300)\nmanual_region.show()\n</pre> # Manually defined region via coordinates (x0, top, x1, bottom) manual_region = page.create_region(30, 60, 600, 300) manual_region.show() Out[17]: In\u00a0[18]: Copied! <pre># Extract text from the manual region\nmanual_region.extract_text()[:500]\n</pre> # Extract text from the manual region manual_region.extract_text()[:500] Out[18]: <pre>'Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fe'</pre> In\u00a0[19]: Copied! <pre>header_content = page.find('rect')\nfooter_content = page.find_all('line')[-1].below()\n\nheader_content.highlight()\nfooter_content.highlight()\npage.to_image()\n</pre> header_content = page.find('rect') footer_content = page.find_all('line')[-1].below()  header_content.highlight() footer_content.highlight() page.to_image() Out[19]: In\u00a0[20]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[20]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\\nDate:  February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \\nsome of which there were open vats near the'</pre> In\u00a0[21]: Copied! <pre>page.add_exclusion(header_content)\npage.add_exclusion(footer_content)\n</pre> page.add_exclusion(header_content) page.add_exclusion(footer_content) Out[21]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[22]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[22]: <pre>'Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fe'</pre> In\u00a0[23]: Copied! <pre>full_text_no_exclusions = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text()\nf\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\"\n</pre> full_text_no_exclusions = page.extract_text(use_exclusions=False) clean_text = page.extract_text() f\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\" Out[23]: <pre>'Original length: 1260, Excluded length: 1156'</pre> In\u00a0[24]: Copied! <pre>page.clear_exclusions()\n</pre> page.clear_exclusions() Out[24]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>Exclusions can also be defined globally at the PDF level using <code>pdf.add_exclusion()</code> with a function.</p> In\u00a0[25]: Copied! <pre>print(page.extract_text())\n</pre> print(page.extract_text()) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\nDate:  February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth \nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[26]: Copied! <pre>print(page.extract_text(use_exclusions=False, layout=True))\n</pre> print(page.extract_text(use_exclusions=False, layout=True)) <pre>                                                                                    \n                                                                                    \n                                                                                    \n                                                     Jungle Health and Safety Inspection Service\n                                                     INS-UP70N51NCL41R              \n                                                                                    \n       Site: Durham\u2019s Meatpacking  Chicago, Ill.                                    \n                                                                                    \n       Date:  February 3, 1905                                                      \n                                                                                    \n       Violation Count: 7                                                           \n       Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\n       These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary \n                                                                                    \n       visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in \n       some of which there were open vats near the level of the floor, their peculiar trouble was that they fell\n       into the vats; and when they were fished out, there was never enough of them left to be worth \n       exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\n       to the world as Durham\u2019s Pure Leaf Lard!                                     \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n       Violations                                                                   \n                                                                                    \n        Statute Description                                    Level  Repeat?       \n        4.12.7 Unsanitary Working Conditions.                  Critical             \n                                                                                    \n        5.8.3 Inadequate Protective Equipment.                 Serious              \n        6.3.9 Ineffective Injury Prevention.                   Serious              \n                                                                                    \n        7.1.5 Failure to Properly Store Hazardous Materials.   Critical             \n        8.9.2 Lack of Adequate Fire Safety Measures.           Serious              \n                                                                                    \n        9.6.4 Inadequate Ventilation Systems.                  Serious              \n        10.2.7 Insufficient Employee Training for Safe Work Practices. Serious      \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                                                                    \n                                Jungle Health and Safety Inspection Service         \n</pre> In\u00a0[27]: Copied! <pre># Find the first text element on the page\nfirst_text = page.find_all('text')[1]\nfirst_text # Display basic info\n</pre> # Find the first text element on the page first_text = page.find_all('text')[1] first_text # Display basic info Out[27]: <pre>&lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;</pre> In\u00a0[28]: Copied! <pre># Highlight the first text element\nfirst_text.show()\n</pre> # Highlight the first text element first_text.show() Out[28]: In\u00a0[29]: Copied! <pre># Get detailed font properties dictionary\nfirst_text.font_info()\n</pre> # Get detailed font properties dictionary first_text.font_info() Out[29]: <pre>{'text': 'INS-UP70N51NCL41R',\n 'fontname': 'Helvetica',\n 'font_family': 'Helvetica',\n 'font_variant': '',\n 'size': 8.0,\n 'bold': False,\n 'italic': False,\n 'color': (1, 0, 0)}</pre> In\u00a0[30]: Copied! <pre># Check specific style properties directly\nf\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\"\n</pre> # Check specific style properties directly f\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\" Out[30]: <pre>'Is Bold: False, Is Italic: False, Font: Helvetica, Size: 8.0'</pre> In\u00a0[31]: Copied! <pre># Find elements by font attributes (adjust selectors)\n# Example: Find Arial fonts\narial_text = page.find_all('text[fontname*=Helvetica]')\narial_text # Display list of found elements\n</pre> # Find elements by font attributes (adjust selectors) # Example: Find Arial fonts arial_text = page.find_all('text[fontname*=Helvetica]') arial_text # Display list of found elements Out[31]: <pre>&lt;ElementCollection[TextElement](count=43)&gt;</pre> In\u00a0[32]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nlarge_text = page.find_all('text[size&gt;=12]')\nlarge_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) large_text = page.find_all('text[size&gt;=12]') large_text Out[32]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[33]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nbold_text = page.find_all('text:bold')\nbold_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) bold_text = page.find_all('text:bold') bold_text Out[33]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[34]: Copied! <pre># Analyze styles on the page\n# This returns a dictionary mapping style names to ElementList objects\npage.analyze_text_styles()\npage.text_style_labels\n</pre> # Analyze styles on the page # This returns a dictionary mapping style names to ElementList objects page.analyze_text_styles() page.text_style_labels Out[34]: <pre>['10.0pt Bold Helvetica',\n '10.0pt Helvetica',\n '12.0pt Bold Helvetica',\n '8.0pt Helvetica']</pre> In\u00a0[35]: Copied! <pre>page.find_all('text').highlight(group_by='style_label').to_image()\n</pre> page.find_all('text').highlight(group_by='style_label').to_image() Out[35]: In\u00a0[36]: Copied! <pre>page.find_all('text[style_label=\"8.0pt Helvetica\"]')\n</pre> page.find_all('text[style_label=\"8.0pt Helvetica\"]') Out[36]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> In\u00a0[37]: Copied! <pre>page.find_all('text[fontname=\"Helvetica\"][size=8]')\n</pre> page.find_all('text[fontname=\"Helvetica\"][size=8]') Out[37]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> <p>Font variants (e.g., <code>AAAAAB+FontName</code>) are also accessible via the <code>font-variant</code> attribute selector: <code>page.find_all('text[font-variant=\"AAAAAB\"]')</code>.</p> In\u00a0[38]: Copied! <pre># Get first 5 text elements in reading order\nelements_in_order = page.find_all('text')\nelements_in_order[:5]\n</pre> # Get first 5 text elements in reading order elements_in_order = page.find_all('text') elements_in_order[:5] Out[38]: <pre>[&lt;TextElement text='Jungle Hea...' font='Helvetica' size=8.0 bbox=(385.0, 35.65599999999995, 541.9680000000001, 43.65599999999995)&gt;,\n &lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;,\n &lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;,\n &lt;TextElement text='Durham\u2019s M...' font='Helvetica' size=10.0 bbox=(74.45, 84.07000000000005, 234.50000000000003, 94.07000000000005)&gt;,\n &lt;TextElement text='Date: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 104.07000000000005, 80.56, 114.07000000000005)&gt;]</pre> In\u00a0[39]: Copied! <pre># Text extracted via page.extract_text() respects this order automatically\n# (Result already shown in Basic Text Extraction section)\npage.extract_text()[:100]\n</pre> # Text extracted via page.extract_text() respects this order automatically # (Result already shown in Basic Text Extraction section) page.extract_text()[:100] Out[39]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking  Chicago, I'</pre> In\u00a0[40]: Copied! <pre>page.clear_highlights()\n\nstart = page.find('text:contains(\"Date\")')\nstart.highlight(label='Date label')\nstart.next().highlight(label='Maybe the date', color='green')\nstart.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')\n\npage.to_image()\n</pre> page.clear_highlights()  start = page.find('text:contains(\"Date\")') start.highlight(label='Date label') start.next().highlight(label='Maybe the date', color='green') start.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')  page.to_image() Out[40]:"},{"location":"text-extraction/#text-extraction-guide","title":"Text Extraction Guide\u00b6","text":"<p>This guide demonstrates various ways to extract text from PDFs using Natural PDF, from simple page dumps to targeted extraction based on elements, regions, and styles.</p>"},{"location":"text-extraction/#setup","title":"Setup\u00b6","text":"<p>First, let's import necessary libraries and load a sample PDF. We'll use <code>example.pdf</code> from the tutorials' <code>pdfs</code> directory. Adjust the path if your setup differs.</p>"},{"location":"text-extraction/#basic-text-extraction","title":"Basic Text Extraction\u00b6","text":"<p>Get all text from a page or the entire document.</p>"},{"location":"text-extraction/#extracting-text-from-specific-elements","title":"Extracting Text from Specific Elements\u00b6","text":"<p>Use selectors with <code>find()</code> or <code>find_all()</code> to target specific elements. Selectors like <code>:contains(\"Summary\")</code> are examples; adapt them to your PDF.</p>"},{"location":"text-extraction/#advanced-text-searches","title":"Advanced text searches\u00b6","text":""},{"location":"text-extraction/#regions","title":"Regions\u00b6","text":""},{"location":"text-extraction/#filtering-out-headers-and-footers","title":"Filtering Out Headers and Footers\u00b6","text":"<p>Use Exclusion Zones to remove unwanted content before extraction. Adjust selectors for typical header/footer content.</p>"},{"location":"text-extraction/#controlling-whitespace","title":"Controlling Whitespace\u00b6","text":"<p>Manage how spaces and blank lines are handled during extraction using <code>layout</code>.</p>"},{"location":"text-extraction/#font-information-access","title":"Font Information Access\u00b6","text":"<p>Inspect font details of text elements.</p>"},{"location":"text-extraction/#working-with-font-styles","title":"Working with Font Styles\u00b6","text":"<p>Analyze and group text elements by their computed font style, which combines attributes like font name, size, boldness, etc., into logical groups.</p>"},{"location":"text-extraction/#reading-order","title":"Reading Order\u00b6","text":"<p>Text extraction respects a pathetic attempt at natural reading order (top-to-bottom, left-to-right by default). <code>page.find_all('text')</code> returns elements already sorted this way.</p>"},{"location":"text-extraction/#element-navigation","title":"Element Navigation\u00b6","text":"<p>Move between elements sequentially based on reading order using <code>.next()</code> and <code>.previous()</code>.</p>"},{"location":"text-extraction/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to extract text, you might want to explore:</p> <ul> <li>Working with regions for more precise extraction</li> <li>OCR capabilities for scanned documents</li> <li>Document layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"},{"location":"tutorials/01-loading-and-extraction/","title":"Loading and Basic Text Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" <p>In this tutorial, we'll learn how to:</p> <ol> <li>Load a PDF document</li> <li>Extract text from pages</li> <li>Extract specific elements</li> </ol> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\nimport os\n\n# Load a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Basic info about the document\n{\n    \"Filename\": os.path.basename(pdf.path),\n    \"Pages\": len(pdf.pages),\n    \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),\n    \"Author\": pdf.metadata.get(\"Author\", \"N/A\")\n}\n</pre> from natural_pdf import PDF import os  # Load a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Basic info about the document {     \"Filename\": os.path.basename(pdf.path),     \"Pages\": len(pdf.pages),     \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),     \"Author\": pdf.metadata.get(\"Author\", \"N/A\") } Out[2]: <pre>{'Filename': 'tmp41lso0gq.pdf', 'Pages': 1, 'Title': 'N/A', 'Author': 'N/A'}</pre> In\u00a0[3]: Copied! <pre># Get the first page\npage = pdf.pages[0]\n\n# Extract text from the page\ntext = page.extract_text()\n\n# Show the first 200 characters of the text\nprint(text[:200])\n</pre> # Get the first page page = pdf.pages[0]  # Extract text from the page text = page.extract_text()  # Show the first 200 characters of the text print(text[:200]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\nDate:  February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer m\n</pre> In\u00a0[4]: Copied! <pre># Find text elements containing specific words\nelements = page.find_all('text:contains(\"Inadequate\")')\n\n# Show these elements on the page\npage.clear_highlights()\nelements.highlight(color=\"red\", label=\"Contains 'Inadequate'\")\n\n# Display the page to see them\npage.to_image(width=700)\n</pre> # Find text elements containing specific words elements = page.find_all('text:contains(\"Inadequate\")')  # Show these elements on the page page.clear_highlights() elements.highlight(color=\"red\", label=\"Contains 'Inadequate'\")  # Display the page to see them page.to_image(width=700) Out[4]: In\u00a0[5]: Copied! <pre># Analyze the page layout\npage.analyze_layout(engine='yolo')\n\n# Find and highlight all detected regions\npage.clear_highlights()\npage.find_all('region').highlight(group_by='type')\n\n# Display the page to see the regions\npage.to_image(width=900)\n</pre> # Analyze the page layout page.analyze_layout(engine='yolo')  # Find and highlight all detected regions page.clear_highlights() page.find_all('region').highlight(group_by='type')  # Display the page to see the regions page.to_image(width=900) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmppupd1m13/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1523.9ms\n</pre> <pre>Speed: 4.2ms preprocess, 1523.9ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[5]: In\u00a0[6]: Copied! <pre># Process all pages\nfor page in pdf.pages:\n    page_text = page.extract_text()\n    print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page\n</pre> # Process all pages for page in pdf.pages:     page_text = page.extract_text()     print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page <pre>Page 1 Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking  Chicago, I\n</pre> <p>This tutorial covered the basics of loading PDFs and extracting text. In the next tutorials, we'll explore more advanced features like searching for specific elements, extracting structured content, and working with tables.</p> In\u00a0[7]: Copied! <pre>%%bash\npip install \"natural-pdf[all]\"\n</pre> %%bash pip install \"natural-pdf[all]\" <pre>Requirement already satisfied: natural-pdf[all] in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (0.1.4.dev0+gef2362b5.d20250412)\n</pre> <pre>Requirement already satisfied: pdfplumber in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (0.11.4)\n</pre> <pre>Requirement already satisfied: Pillow in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (10.4.0)\n</pre> <pre>Requirement already satisfied: colour in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (0.1.5)\n</pre> <pre>Requirement already satisfied: numpy in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (1.26.4)\n</pre> <pre>Requirement already satisfied: urllib3 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (2.3.0)\n</pre> <pre>Requirement already satisfied: torch in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (2.6.0)\n</pre> <pre>Requirement already satisfied: torchvision in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (0.21.0)\n</pre> <pre>Requirement already satisfied: transformers in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (4.50.0.dev0)\n</pre> <pre>Requirement already satisfied: huggingface_hub in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (0.29.3)\n</pre> <pre>Requirement already satisfied: ocrmypdf in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (16.4.0)\n</pre> <pre>Requirement already satisfied: pikepdf in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (9.0.0)\n</pre> <pre>Requirement already satisfied: ipywidgets&lt;9.0.0,&gt;=7.0.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (8.1.5)\n</pre> <pre>Requirement already satisfied: easyocr in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (1.7.1)\n</pre> <pre>Requirement already satisfied: paddlepaddle in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (2.5.2)\n</pre> <pre>Requirement already satisfied: paddleocr in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (2.10.0)\n</pre> <pre>Requirement already satisfied: doclayout_yolo in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (0.0.3)\n</pre> <pre>Requirement already satisfied: surya-ocr in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (0.12.1)\n</pre> <pre>Requirement already satisfied: haystack-ai in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (2.12.0)\n</pre> <pre>Requirement already satisfied: chroma-haystack in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (3.0.0)\n</pre> <pre>Requirement already satisfied: sentence-transformers in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from natural-pdf[all]) (4.0.2)\n</pre> <pre>Requirement already satisfied: comm&gt;=0.1.3 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.2.2)\n</pre> <pre>Requirement already satisfied: ipython&gt;=6.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (8.31.0)\n</pre> <pre>Requirement already satisfied: traitlets&gt;=4.3.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (5.14.3)\n</pre> <pre>Requirement already satisfied: widgetsnbextension~=4.0.12 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (4.0.13)\n</pre> <pre>Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (3.0.13)\n</pre> <pre>Requirement already satisfied: chromadb&gt;=0.6.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chroma-haystack-&gt;natural-pdf[all]) (0.6.3)\n</pre> <pre>Requirement already satisfied: haystack-experimental in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (0.8.0)\n</pre> <pre>Requirement already satisfied: jinja2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (3.1.5)\n</pre> <pre>Requirement already satisfied: jsonschema in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (4.23.0)\n</pre> <pre>Requirement already satisfied: lazy-imports in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (0.4.0)\n</pre> <pre>Requirement already satisfied: more-itertools in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (10.2.0)\n</pre> <pre>Requirement already satisfied: networkx in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (3.4.2)\n</pre> <pre>Requirement already satisfied: openai&gt;=1.56.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (1.65.1)\n</pre> <pre>Requirement already satisfied: posthog!=3.12.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (3.5.0)\n</pre> <pre>Requirement already satisfied: pydantic in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (2.10.6)\n</pre> <pre>Requirement already satisfied: python-dateutil in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (2.9.0.post0)\n</pre> <pre>Requirement already satisfied: pyyaml in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (6.0.2)\n</pre> <pre>Requirement already satisfied: requests in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (2.32.3)\n</pre> <pre>Requirement already satisfied: tenacity!=8.4.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (8.5.0)\n</pre> <pre>Requirement already satisfied: tqdm in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (4.66.5)\n</pre> <pre>Requirement already satisfied: typing-extensions&gt;=4.7 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from haystack-ai-&gt;natural-pdf[all]) (4.12.2)\n</pre> <pre>Requirement already satisfied: matplotlib&gt;=3.3.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (3.10.0)\n</pre> <pre>Requirement already satisfied: opencv-python&gt;=4.6.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (4.10.0.84)\n</pre> <pre>Requirement already satisfied: scipy&gt;=1.4.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (1.11.4)\n</pre> <pre>Requirement already satisfied: psutil in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (6.1.1)\n</pre> <pre>Requirement already satisfied: py-cpuinfo in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (9.0.0)\n</pre> <pre>Requirement already satisfied: thop&gt;=0.1.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (0.1.1.post2209072238)\n</pre> <pre>Requirement already satisfied: pandas&gt;=1.1.4 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (2.2.3)\n</pre> <pre>Requirement already satisfied: seaborn&gt;=0.11.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (0.13.2)\n</pre> <pre>Requirement already satisfied: albumentations&gt;=1.4.11 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from doclayout_yolo-&gt;natural-pdf[all]) (2.0.5)\n</pre> <pre>Requirement already satisfied: filelock in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch-&gt;natural-pdf[all]) (3.13.1)\n</pre> <pre>Requirement already satisfied: fsspec in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch-&gt;natural-pdf[all]) (2024.2.0)\n</pre> <pre>Requirement already satisfied: sympy==1.13.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from torch-&gt;natural-pdf[all]) (1.13.1)\n</pre> <pre>Requirement already satisfied: mpmath&lt;1.4,&gt;=1.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from sympy==1.13.1-&gt;torch-&gt;natural-pdf[all]) (1.3.0)\n</pre> <pre>Requirement already satisfied: opencv-python-headless in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from easyocr-&gt;natural-pdf[all]) (4.11.0.86)\n</pre> <pre>Requirement already satisfied: scikit-image in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from easyocr-&gt;natural-pdf[all]) (0.22.0)\n</pre> <pre>Requirement already satisfied: python-bidi in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from easyocr-&gt;natural-pdf[all]) (0.4.2)\n</pre> <pre>Requirement already satisfied: Shapely in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from easyocr-&gt;natural-pdf[all]) (2.0.3)\n</pre> <pre>Requirement already satisfied: pyclipper in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from easyocr-&gt;natural-pdf[all]) (1.3.0.post5)\n</pre> <pre>Requirement already satisfied: ninja in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from easyocr-&gt;natural-pdf[all]) (1.11.1.1)\n</pre> <pre>Requirement already satisfied: packaging&gt;=20.9 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from huggingface_hub-&gt;natural-pdf[all]) (24.2)\n</pre> <pre>Requirement already satisfied: deprecation&gt;=2.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (2.1.0)\n</pre> <pre>Requirement already satisfied: img2pdf&gt;=0.5 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (0.5.1)\n</pre> <pre>Requirement already satisfied: pdfminer.six&gt;=20220319 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (20231228)\n</pre> <pre>Requirement already satisfied: pi-heif in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (0.16.0)\n</pre> <pre>Requirement already satisfied: pluggy&gt;=1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (1.5.0)\n</pre> <pre>Requirement already satisfied: rich&gt;=13 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ocrmypdf-&gt;natural-pdf[all]) (13.9.4)\n</pre> <pre>Requirement already satisfied: Deprecated in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pikepdf-&gt;natural-pdf[all]) (1.2.14)\n</pre> <pre>Requirement already satisfied: lxml&gt;=4.8 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pikepdf-&gt;natural-pdf[all]) (4.9.4)\n</pre> <pre>Requirement already satisfied: lmdb in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (1.4.1)\n</pre> <pre>Requirement already satisfied: rapidfuzz in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (3.10.1)\n</pre> <pre>Requirement already satisfied: opencv-contrib-python in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (4.10.0.84)\n</pre> <pre>Requirement already satisfied: cython in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (3.0.9)\n</pre> <pre>Requirement already satisfied: python-docx in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (1.1.2)\n</pre> <pre>Requirement already satisfied: beautifulsoup4 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (4.12.3)\n</pre> <pre>Requirement already satisfied: fonttools&gt;=4.24.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (4.55.3)\n</pre> <pre>Requirement already satisfied: fire&gt;=0.3.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (0.6.0)\n</pre> <pre>Requirement already satisfied: albucore in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddleocr-&gt;natural-pdf[all]) (0.0.23)\n</pre> <pre>Requirement already satisfied: httpx in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (0.28.1)\n</pre> <pre>Requirement already satisfied: decorator in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (5.1.1)\n</pre> <pre>Requirement already satisfied: astor in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (0.8.1)\n</pre> <pre>Requirement already satisfied: opt-einsum==3.3.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (3.3.0)\n</pre> <pre>Requirement already satisfied: protobuf&gt;=3.20.2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from paddlepaddle-&gt;natural-pdf[all]) (3.20.3)\n</pre> <pre>Requirement already satisfied: pypdfium2&gt;=4.18.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pdfplumber-&gt;natural-pdf[all]) (4.30.0)\n</pre> <pre>Requirement already satisfied: charset-normalizer&gt;=2.0.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pdfminer.six&gt;=20220319-&gt;ocrmypdf-&gt;natural-pdf[all]) (3.4.1)\n</pre> <pre>Requirement already satisfied: cryptography&gt;=36.0.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pdfminer.six&gt;=20220319-&gt;ocrmypdf-&gt;natural-pdf[all]) (42.0.5)\n</pre> <pre>Requirement already satisfied: scikit-learn in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from sentence-transformers-&gt;natural-pdf[all]) (1.6.1)\n</pre> <pre>Requirement already satisfied: regex!=2019.12.17 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers-&gt;natural-pdf[all]) (2024.11.6)\n</pre> <pre>Requirement already satisfied: tokenizers&lt;0.22,&gt;=0.21 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers-&gt;natural-pdf[all]) (0.21.0)\n</pre> <pre>Requirement already satisfied: safetensors&gt;=0.4.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from transformers-&gt;natural-pdf[all]) (0.5.2)\n</pre> <pre>Requirement already satisfied: click&lt;9.0.0,&gt;=8.1.8 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from surya-ocr-&gt;natural-pdf[all]) (8.1.8)\n</pre> <pre>Requirement already satisfied: filetype&lt;2.0.0,&gt;=1.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from surya-ocr-&gt;natural-pdf[all]) (1.2.0)\n</pre> <pre>Requirement already satisfied: platformdirs&lt;5.0.0,&gt;=4.3.6 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from surya-ocr-&gt;natural-pdf[all]) (4.3.6)\n</pre> <pre>Requirement already satisfied: pydantic-settings&lt;3.0.0,&gt;=2.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from surya-ocr-&gt;natural-pdf[all]) (2.6.1)\n</pre> <pre>Requirement already satisfied: python-dotenv&lt;2.0.0,&gt;=1.0.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from surya-ocr-&gt;natural-pdf[all]) (1.0.1)\n</pre> <pre>Requirement already satisfied: stringzilla&gt;=3.10.4 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from albucore-&gt;paddleocr-&gt;natural-pdf[all]) (3.12.3)\n</pre> <pre>Requirement already satisfied: simsimd&gt;=5.9.2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from albucore-&gt;paddleocr-&gt;natural-pdf[all]) (6.2.1)\n</pre> <pre>Requirement already satisfied: build&gt;=1.0.3 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.2.2.post1)\n</pre> <pre>Requirement already satisfied: chroma-hnswlib==0.7.6 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.7.6)\n</pre> <pre>Requirement already satisfied: fastapi&gt;=0.95.2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.110.3)\n</pre> <pre>Requirement already satisfied: uvicorn&gt;=0.18.3 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.29.0)\n</pre> <pre>Requirement already satisfied: onnxruntime&gt;=1.14.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.19.2)\n</pre> <pre>Requirement already satisfied: opentelemetry-api&gt;=1.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.20.0)\n</pre> <pre>Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.20.0)\n</pre> <pre>Requirement already satisfied: opentelemetry-instrumentation-fastapi&gt;=0.41b0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.41b0)\n</pre> <pre>Requirement already satisfied: opentelemetry-sdk&gt;=1.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.20.0)\n</pre> <pre>Requirement already satisfied: pypika&gt;=0.48.9 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.48.9)\n</pre> <pre>Requirement already satisfied: overrides&gt;=7.3.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (7.7.0)\n</pre> <pre>Requirement already satisfied: importlib-resources in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (6.1.2)\n</pre> <pre>Requirement already satisfied: grpcio&gt;=1.58.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.62.0)\n</pre> <pre>Requirement already satisfied: bcrypt&gt;=4.0.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (4.1.2)\n</pre> <pre>Requirement already satisfied: typer&gt;=0.9.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.12.5)\n</pre> <pre>Requirement already satisfied: kubernetes&gt;=28.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (29.0.0)\n</pre> <pre>Requirement already satisfied: mmh3&gt;=4.0.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (4.1.0)\n</pre> <pre>Requirement already satisfied: orjson&gt;=3.9.12 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.9.15)\n</pre> <pre>Requirement already satisfied: six in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from fire&gt;=0.3.0-&gt;paddleocr-&gt;natural-pdf[all]) (1.17.0)\n</pre> <pre>Requirement already satisfied: termcolor in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from fire&gt;=0.3.0-&gt;paddleocr-&gt;natural-pdf[all]) (2.4.0)\n</pre> <pre>Requirement already satisfied: anyio in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx-&gt;paddlepaddle-&gt;natural-pdf[all]) (4.8.0)\n</pre> <pre>Requirement already satisfied: certifi in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx-&gt;paddlepaddle-&gt;natural-pdf[all]) (2024.12.14)\n</pre> <pre>Requirement already satisfied: httpcore==1.* in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx-&gt;paddlepaddle-&gt;natural-pdf[all]) (1.0.7)\n</pre> <pre>Requirement already satisfied: idna in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpx-&gt;paddlepaddle-&gt;natural-pdf[all]) (3.10)\n</pre> <pre>Requirement already satisfied: h11&lt;0.15,&gt;=0.13 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from httpcore==1.*-&gt;httpx-&gt;paddlepaddle-&gt;natural-pdf[all]) (0.14.0)\n</pre> <pre>Requirement already satisfied: exceptiongroup in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (1.2.2)\n</pre> <pre>Requirement already satisfied: jedi&gt;=0.16 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.19.2)\n</pre> <pre>Requirement already satisfied: matplotlib-inline in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.1.7)\n</pre> <pre>Requirement already satisfied: pexpect&gt;4.3 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (4.9.0)\n</pre> <pre>Requirement already satisfied: prompt_toolkit&lt;3.1.0,&gt;=3.0.41 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (3.0.48)\n</pre> <pre>Requirement already satisfied: pygments&gt;=2.4.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (2.19.1)\n</pre> <pre>Requirement already satisfied: stack_data in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.6.3)\n</pre> <pre>Requirement already satisfied: contourpy&gt;=1.0.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (1.3.1)\n</pre> <pre>Requirement already satisfied: cycler&gt;=0.10 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (0.12.1)\n</pre> <pre>Requirement already satisfied: kiwisolver&gt;=1.3.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (1.4.8)\n</pre> <pre>Requirement already satisfied: pyparsing&gt;=2.3.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from matplotlib&gt;=3.3.0-&gt;doclayout_yolo-&gt;natural-pdf[all]) (3.2.1)\n</pre> <pre>Requirement already satisfied: distro&lt;2,&gt;=1.7.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai&gt;=1.56.1-&gt;haystack-ai-&gt;natural-pdf[all]) (1.9.0)\n</pre> <pre>Requirement already satisfied: jiter&lt;1,&gt;=0.4.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai&gt;=1.56.1-&gt;haystack-ai-&gt;natural-pdf[all]) (0.5.0)\n</pre> <pre>Requirement already satisfied: sniffio in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from openai&gt;=1.56.1-&gt;haystack-ai-&gt;natural-pdf[all]) (1.3.1)\n</pre> <pre>Requirement already satisfied: pytz&gt;=2020.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pandas&gt;=1.1.4-&gt;doclayout_yolo-&gt;natural-pdf[all]) (2022.7.1)\n</pre> <pre>Requirement already satisfied: tzdata&gt;=2022.7 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pandas&gt;=1.1.4-&gt;doclayout_yolo-&gt;natural-pdf[all]) (2024.1)\n</pre> <pre>Requirement already satisfied: monotonic&gt;=1.5 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from posthog!=3.12.0-&gt;haystack-ai-&gt;natural-pdf[all]) (1.6)\n</pre> <pre>Requirement already satisfied: backoff&gt;=1.10.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from posthog!=3.12.0-&gt;haystack-ai-&gt;natural-pdf[all]) (2.2.1)\n</pre> <pre>Requirement already satisfied: annotated-types&gt;=0.6.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic-&gt;haystack-ai-&gt;natural-pdf[all]) (0.6.0)\n</pre> <pre>Requirement already satisfied: pydantic-core==2.27.2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pydantic-&gt;haystack-ai-&gt;natural-pdf[all]) (2.27.2)\n</pre> <pre>Requirement already satisfied: markdown-it-py&gt;=2.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from rich&gt;=13-&gt;ocrmypdf-&gt;natural-pdf[all]) (3.0.0)\n</pre> <pre>Requirement already satisfied: soupsieve&gt;1.2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from beautifulsoup4-&gt;paddleocr-&gt;natural-pdf[all]) (2.6)\n</pre> <pre>Requirement already satisfied: wrapt&lt;2,&gt;=1.10 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from Deprecated-&gt;pikepdf-&gt;natural-pdf[all]) (1.16.0)\n</pre> <pre>Requirement already satisfied: MarkupSafe&gt;=2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jinja2-&gt;haystack-ai-&gt;natural-pdf[all]) (3.0.2)\n</pre> <pre>Requirement already satisfied: attrs&gt;=22.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (24.3.0)\n</pre> <pre>Requirement already satisfied: jsonschema-specifications&gt;=2023.03.6 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (2024.10.1)\n</pre> <pre>Requirement already satisfied: referencing&gt;=0.28.4 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (0.35.1)\n</pre> <pre>Requirement already satisfied: rpds-py&gt;=0.7.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jsonschema-&gt;haystack-ai-&gt;natural-pdf[all]) (0.22.3)\n</pre> <pre>Requirement already satisfied: imageio&gt;=2.27 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-image-&gt;easyocr-&gt;natural-pdf[all]) (2.34.0)\n</pre> <pre>Requirement already satisfied: tifffile&gt;=2022.8.12 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-image-&gt;easyocr-&gt;natural-pdf[all]) (2024.2.12)\n</pre> <pre>Requirement already satisfied: lazy_loader&gt;=0.3 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-image-&gt;easyocr-&gt;natural-pdf[all]) (0.3)\n</pre> <pre>Requirement already satisfied: joblib&gt;=1.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn-&gt;sentence-transformers-&gt;natural-pdf[all]) (1.4.0)\n</pre> <pre>Requirement already satisfied: threadpoolctl&gt;=3.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn-&gt;sentence-transformers-&gt;natural-pdf[all]) (3.3.0)\n</pre> <pre>Requirement already satisfied: pyproject_hooks in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from build&gt;=1.0.3-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.0.0)\n</pre> <pre>Requirement already satisfied: tomli&gt;=1.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from build&gt;=1.0.3-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (2.2.1)\n</pre> <pre>Requirement already satisfied: cffi&gt;=1.12 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from cryptography&gt;=36.0.0-&gt;pdfminer.six&gt;=20220319-&gt;ocrmypdf-&gt;natural-pdf[all]) (1.17.1)\n</pre> <pre>Requirement already satisfied: starlette&lt;0.38.0,&gt;=0.37.2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from fastapi&gt;=0.95.2-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.37.2)\n</pre> <pre>Requirement already satisfied: parso&lt;0.9.0,&gt;=0.8.4 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from jedi&gt;=0.16-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.8.4)\n</pre> <pre>Requirement already satisfied: google-auth&gt;=1.0.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (2.28.2)\n</pre> <pre>Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,&gt;=0.32.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.8.0)\n</pre> <pre>Requirement already satisfied: requests-oauthlib in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.3.1)\n</pre> <pre>Requirement already satisfied: oauthlib&gt;=3.2.2 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.2.2)\n</pre> <pre>Requirement already satisfied: mdurl~=0.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich&gt;=13-&gt;ocrmypdf-&gt;natural-pdf[all]) (0.1.2)\n</pre> <pre>Requirement already satisfied: coloredlogs in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from onnxruntime&gt;=1.14.1-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (15.0.1)\n</pre> <pre>Requirement already satisfied: flatbuffers in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from onnxruntime&gt;=1.14.1-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (24.3.7)\n</pre> <pre>Requirement already satisfied: importlib-metadata&lt;7.0,&gt;=6.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-api&gt;=1.2.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (6.11.0)\n</pre> <pre>Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.62.0)\n</pre> <pre>Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.20.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.20.0)\n</pre> <pre>Requirement already satisfied: opentelemetry-proto==1.20.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc&gt;=1.2.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.20.0)\n</pre> <pre>Requirement already satisfied: opentelemetry-instrumentation-asgi==0.41b0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.41b0)\n</pre> <pre>Requirement already satisfied: opentelemetry-instrumentation==0.41b0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.41b0)\n</pre> <pre>Requirement already satisfied: opentelemetry-semantic-conventions==0.41b0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.41b0)\n</pre> <pre>Requirement already satisfied: opentelemetry-util-http==0.41b0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.41b0)\n</pre> <pre>Requirement already satisfied: setuptools&gt;=16.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.41b0-&gt;opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (75.8.0)\n</pre> <pre>Requirement already satisfied: asgiref~=3.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.41b0-&gt;opentelemetry-instrumentation-fastapi&gt;=0.41b0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.7.2)\n</pre> <pre>Requirement already satisfied: ptyprocess&gt;=0.5 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pexpect&gt;4.3-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.7.0)\n</pre> <pre>Requirement already satisfied: wcwidth in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from prompt_toolkit&lt;3.1.0,&gt;=3.0.41-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.2.13)\n</pre> <pre>Requirement already satisfied: shellingham&gt;=1.3.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from typer&gt;=0.9.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (1.5.4)\n</pre> <pre>Requirement already satisfied: httptools&gt;=0.5.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.6.1)\n</pre> <pre>Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,&gt;=0.14.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.19.0)\n</pre> <pre>Requirement already satisfied: watchfiles&gt;=0.13 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.21.0)\n</pre> <pre>Requirement already satisfied: websockets&gt;=10.4 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from uvicorn[standard]&gt;=0.18.3-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (14.2)\n</pre> <pre>Requirement already satisfied: executing&gt;=1.2.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (2.1.0)\n</pre> <pre>Requirement already satisfied: asttokens&gt;=2.1.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (3.0.0)\n</pre> <pre>Requirement already satisfied: pure-eval in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from stack_data-&gt;ipython&gt;=6.1.0-&gt;ipywidgets&lt;9.0.0,&gt;=7.0.0-&gt;natural-pdf[all]) (0.2.3)\n</pre> <pre>Requirement already satisfied: pycparser in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from cffi&gt;=1.12-&gt;cryptography&gt;=36.0.0-&gt;pdfminer.six&gt;=20220319-&gt;ocrmypdf-&gt;natural-pdf[all]) (2.22)\n</pre> <pre>Requirement already satisfied: cachetools&lt;6.0,&gt;=2.0.0 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (5.3.3)\n</pre> <pre>Requirement already satisfied: pyasn1-modules&gt;=0.2.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.3.0)\n</pre> <pre>Requirement already satisfied: rsa&lt;5,&gt;=3.1.4 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (4.9)\n</pre> <pre>Requirement already satisfied: zipp&gt;=0.5 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from importlib-metadata&lt;7.0,&gt;=6.0-&gt;opentelemetry-api&gt;=1.2.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (3.17.0)\n</pre> <pre>Requirement already satisfied: humanfriendly&gt;=9.1 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from coloredlogs-&gt;onnxruntime&gt;=1.14.1-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (10.0)\n</pre> <pre>Requirement already satisfied: pyasn1&lt;0.6.0,&gt;=0.4.6 in /Users/soma/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&gt;=1.0.1-&gt;kubernetes&gt;=28.1.0-&gt;chromadb&gt;=0.6.0-&gt;chroma-haystack-&gt;natural-pdf[all]) (0.5.1)\n</pre>"},{"location":"tutorials/01-loading-and-extraction/#loading-and-basic-text-extraction","title":"Loading and Basic Text Extraction\u00b6","text":""},{"location":"tutorials/01-loading-and-extraction/#loading-a-pdf","title":"Loading a PDF\u00b6","text":"<p>Let's start by loading a PDF file:</p>"},{"location":"tutorials/01-loading-and-extraction/#extracting-text","title":"Extracting Text\u00b6","text":"<p>Now that we have loaded the PDF, let's extract the text from the first page:</p>"},{"location":"tutorials/01-loading-and-extraction/#finding-and-extracting-specific-elements","title":"Finding and Extracting Specific Elements\u00b6","text":"<p>We can find specific elements using spatial queries and text content:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>We can analyze the layout of the page to identify different regions:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>You can also work with multiple pages:</p>"},{"location":"tutorials/02-finding-elements/","title":"Finding Specific Elements","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page (index 0)\npage = pdf.pages[0]\n\n# Find the text element containing \"Site:\"\n# The ':contains()' pseudo-class looks for text content.\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Find the text element containing \"Date:\"\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Visualize the found elements\nsite_label.highlight(color=\"red\", label=\"Site Label\")\ndate_label.highlight(color=\"blue\", label=\"Date Label\")\n\n# Access the text content directly\n{\n    \"Site Label\": site_label.text,\n    \"Date Label\": date_label.text\n}\n\n# Display the page image to see the visualized elements\npage.to_image()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page (index 0) page = pdf.pages[0]  # Find the text element containing \"Site:\" # The ':contains()' pseudo-class looks for text content. site_label = page.find('text:contains(\"Site:\")')  # Find the text element containing \"Date:\" date_label = page.find('text:contains(\"Date:\")')  # Visualize the found elements site_label.highlight(color=\"red\", label=\"Site Label\") date_label.highlight(color=\"blue\", label=\"Date Label\")  # Access the text content directly {     \"Site Label\": site_label.text,     \"Date Label\": date_label.text }  # Display the page image to see the visualized elements page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Find text elements that are red\nred_text = page.find('text[color~=red]')\nred_text.highlight(color=\"red\", label=\"Red Text\")\nprint(f\"Found red text: {red_text.text}\")\n\n# Find elements with specific RGB colors\nblue_text = page.find('text[color=rgb(0,0,255)]')\n</pre> # Find text elements that are red red_text = page.find('text[color~=red]') red_text.highlight(color=\"red\", label=\"Red Text\") print(f\"Found red text: {red_text.text}\")  # Find elements with specific RGB colors blue_text = page.find('text[color=rgb(0,0,255)]') <pre>Found red text: INS-UP70N51NCL41R\n</pre> In\u00a0[4]: Copied! <pre># Find horizontal lines\nhorizontal_lines = page.find_all('line[horizontal]')\n\n# Find thick lines (width &gt;= 2)\nthick_lines = page.find_all('line[width&gt;=2]')\n\n# Find rectangles\nrectangles = page.find_all('rect')\n\n# Visualize what we found\npage.clear_highlights()\nhorizontal_lines.highlight(color=\"blue\", label=\"Horizontal Lines\")\nthick_lines.highlight(color=\"red\", label=\"Thick Lines\")\nrectangles.highlight(color=\"green\", label=\"Rectangles\")\npage.to_image()\n</pre> # Find horizontal lines horizontal_lines = page.find_all('line[horizontal]')  # Find thick lines (width &gt;= 2) thick_lines = page.find_all('line[width&gt;=2]')  # Find rectangles rectangles = page.find_all('rect')  # Visualize what we found page.clear_highlights() horizontal_lines.highlight(color=\"blue\", label=\"Horizontal Lines\") thick_lines.highlight(color=\"red\", label=\"Thick Lines\") rectangles.highlight(color=\"green\", label=\"Rectangles\") page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Find text with specific font properties\nbold_text = page.find_all('text[style~=bold]')\nlarge_text = page.find_all('text[size&gt;=12]')\n\n# Find text with specific font names\nhelvetica_text = page.find_all('text[fontname~=Helvetica]')\n</pre> # Find text with specific font properties bold_text = page.find_all('text[style~=bold]') large_text = page.find_all('text[size&gt;=12]')  # Find text with specific font names helvetica_text = page.find_all('text[fontname~=Helvetica]') In\u00a0[6]: Copied! <pre># Find text above a specific element\nabove_text = page.find('line[width=2]').above().extract_text()\n\n# Find text below a specific element\nbelow_text = page.find('text:contains(\"Summary\")').below().extract_text()\n\n# Find text to the right of a specific element\nnearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text()\n</pre> # Find text above a specific element above_text = page.find('line[width=2]').above().extract_text()  # Find text below a specific element below_text = page.find('text:contains(\"Summary\")').below().extract_text()  # Find text to the right of a specific element nearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text() In\u00a0[7]: Copied! <pre># Find large, bold text that contains specific words\nimportant_text = page.find_all('text[size&gt;=12][style~=bold]:contains(\"Critical\")')\n\n# Find red text inside a rectangle\nhighlighted_text = page.find('rect').find_all('text[color~=red]')\n</pre> # Find large, bold text that contains specific words important_text = page.find_all('text[size&gt;=12][style~=bold]:contains(\"Critical\")')  # Find red text inside a rectangle highlighted_text = page.find('rect').find_all('text[color~=red]') <p>Handling Missing Elements</p> <pre><code>In these examples, we know certain elements exist in the PDF. In real-world scenarios, `page.find()` might not find a match and would return `None`. Production code should check for this:\n\n```py\nsite_label = page.find('text:contains(\"Site:\")')\nif site_label:\n    # Found it! Proceed...\n    site_label.highlight(color=\"red\", label=\"Site Label\")\n    site_label.text  # Display or use the text\nelse:\n    # Didn't find it, handle appropriately...\n    \"Warning: 'Site:' label not found.\"\n```</code></pre> <p>Visual Debugging</p> <pre><code>When working with complex selectors, it's helpful to visualize what you're finding:\n\n```py\n# Clear any existing highlights\npage.clear_highlights()\n\n# Find and highlight elements\nelements = page.find_all('text[color~=red]')\nelements.highlight(color=\"red\", label=\"Red Text\")\n\n# Display the page to see what was found\npage.to_image(width=800)\n```</code></pre>"},{"location":"tutorials/02-finding-elements/#finding-specific-elements","title":"Finding Specific Elements\u00b6","text":"<p>Extracting all the text is useful, but often you need specific pieces of information. <code>natural-pdf</code> lets you find elements using selectors, similar to CSS.</p> <p>Let's find the \"Site\" and \"Date\" information from our <code>01-practice.pdf</code>:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-color","title":"Finding Elements by Color\u00b6","text":"<p>You can find elements based on their color:</p>"},{"location":"tutorials/02-finding-elements/#finding-lines-and-shapes","title":"Finding Lines and Shapes\u00b6","text":"<p>Find lines and rectangles based on their properties:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-font-properties","title":"Finding Elements by Font Properties\u00b6","text":""},{"location":"tutorials/02-finding-elements/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>You can find elements based on their position relative to other elements:</p>"},{"location":"tutorials/02-finding-elements/#combining-selectors","title":"Combining Selectors\u00b6","text":"<p>You can combine multiple conditions to find exactly what you need:</p>"},{"location":"tutorials/03-extracting-blocks/","title":"Extracting Text Blocks","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the starting element (\"Summary:\")\nstart_marker = page.find('text:contains(\"Summary:\")')\n\n# Select elements below the start_marker, stopping *before*\n# the thick horizontal line (a line with height &gt; 1).\nsummary_elements = start_marker.below(\n    include_element=True, # Include the \"Summary:\" text itself\n    until=\"line[height &gt; 1]\"\n)\n\n# Visualize the elements found in this block\nsummary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")\n\n# Extract and display the text from the collection of summary elements\nsummary_elements.extract_text()\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the starting element (\"Summary:\") start_marker = page.find('text:contains(\"Summary:\")')  # Select elements below the start_marker, stopping *before* # the thick horizontal line (a line with height &gt; 1). summary_elements = start_marker.below(     include_element=True, # Include the \"Summary:\" text itself     until=\"line[height &gt; 1]\" )  # Visualize the elements found in this block summary_elements.highlight(color=\"lightgreen\", label=\"Summary Block\")  # Extract and display the text from the collection of summary elements summary_elements.extract_text()  Out[2]: <pre>'Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard!'</pre> In\u00a0[3]: Copied! <pre># Display the page image to see the visualization\npage.to_image()\n</pre> # Display the page image to see the visualization page.to_image() Out[3]: <p>This selects the elements using <code>.below(until=...)</code> and extracts their text. The second code block displays the page image with the visualized section.</p> <p>Selector Specificity</p> <pre><code>We used `line[height &gt; 1]` to find the thick horizontal line. You might need to adjust selectors based on the specific PDF structure. Inspecting element properties can help you find reliable start and end markers.</code></pre>"},{"location":"tutorials/03-extracting-blocks/#extracting-text-blocks","title":"Extracting Text Blocks\u00b6","text":"<p>Often, you need a specific section, like a paragraph between two headings. You can find a starting element and select everything below it until an ending element.</p> <p>Let's extract the \"Summary\" section from <code>01-practice.pdf</code>. It starts after \"Summary:\" and ends before the thick horizontal line.</p>"},{"location":"tutorials/04-table-extraction/","title":"Basic Table Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Use extract_tables() to find all tables on the page.\n# It returns a list of tables, where each table is a list of lists.\ntables_data = page.extract_tables()\n\n# Display the first table found\ntables_data[0] if tables_data else \"No tables found\"\n\n# You can also visualize the general area of the first table \n# by finding elements in that region\nif tables_data:\n    # Find a header element in the table\n    statute_header = page.find('text:contains(\"Statute\")')\n    if statute_header:\n        # Show the area\n        statute_header.below(height=100).highlight(color=\"green\", label=\"Table Area\")\n        page.to_image()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Use extract_tables() to find all tables on the page. # It returns a list of tables, where each table is a list of lists. tables_data = page.extract_tables()  # Display the first table found tables_data[0] if tables_data else \"No tables found\"  # You can also visualize the general area of the first table  # by finding elements in that region if tables_data:     # Find a header element in the table     statute_header = page.find('text:contains(\"Statute\")')     if statute_header:         # Show the area         statute_header.below(height=100).highlight(color=\"green\", label=\"Table Area\")         page.to_image() <p>This code uses <code>page.extract_tables()</code> which attempts to automatically detect tables based on visual cues like lines and whitespace. The result is a list of lists, representing the rows and cells of the table.</p> <p>Table Settings and Limitations</p> <pre><code>The default `extract_tables()` works well for simple, clearly defined tables. However, it might struggle with:\n*   Tables without clear borders or lines.\n*   Complex merged cells.\n*   Tables spanning multiple pages.\n\n`pdfplumber` (and thus `natural-pdf`) allows passing `table_settings` dictionaries to `extract_tables()` for more control over the detection strategy (e.g., `\"vertical_strategy\": \"text\"`, `\"horizontal_strategy\": \"text\"`).\n\nFor even more robust table detection, especially for tables without explicit lines, using Layout Analysis (like `page.analyze_layout(engine='tatr')`) first, finding the table `region`, and then calling `region.extract_table()` can yield better results. We'll explore layout analysis in a later tutorial.</code></pre>"},{"location":"tutorials/04-table-extraction/#basic-table-extraction","title":"Basic Table Extraction\u00b6","text":"<p>PDFs often contain tables, and <code>natural-pdf</code> provides methods to extract their data, building on <code>pdfplumber</code>'s capabilities.</p> <p>Let's extract the \"Violations\" table from our practice PDF.</p>"},{"location":"tutorials/05-excluding-content/","title":"Excluding Content (Headers/Footers)","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\n\n# Load the PDF\npdf = PDF(pdf_url)\npage = pdf.pages[0]\n\n# Let's see the bottom part of the text WITHOUT exclusions\n# It likely contains page numbers or other footer info.\nfull_text_unfiltered = page.extract_text()\n\n# Show the last 200 characters (likely containing footer text)\nfull_text_unfiltered[-200:]\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"  # Load the PDF pdf = PDF(pdf_url) page = pdf.pages[0]  # Let's see the bottom part of the text WITHOUT exclusions # It likely contains page numbers or other footer info. full_text_unfiltered = page.extract_text()  # Show the last 200 characters (likely containing footer text) full_text_unfiltered[-200:] Out[2]: <pre>'THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0\\nWrite-In Totals 0 0 0 0\\nPrecinct Summary - 11/06/2024    12:22 AM Page 1 of 387\\nReport generated with Electionware Copyright \u00a9 2007-2020'</pre> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\n\n# Define the exclusion region on every page using a lambda function\nfooter_height = 200\npdf.add_exclusion(\n    lambda page: page.region(top=page.height - footer_height),\n    label=\"Bottom 200pt Footer\"\n)\n\n# Now extract text from the first page again, exclusions are active by default\npage = pdf.pages[0]\n\n# Visualize the excluded area\nfooter_region_viz = page.region(top=page.height - footer_height)\nfooter_region_viz.highlight(label=\"Excluded Footer Area\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url)  # Define the exclusion region on every page using a lambda function footer_height = 200 pdf.add_exclusion(     lambda page: page.region(top=page.height - footer_height),     label=\"Bottom 200pt Footer\" )  # Now extract text from the first page again, exclusions are active by default page = pdf.pages[0]  # Visualize the excluded area footer_region_viz = page.region(top=page.height - footer_height) footer_region_viz.highlight(label=\"Excluded Footer Area\") page.to_image() Out[3]: In\u00a0[4]: Copied! <pre>filtered_text = page.extract_text() # use_exclusions=True is default\n\n# Show the last 200 chars with footer area excluded\nfiltered_text[-200:]\n</pre> filtered_text = page.extract_text() # use_exclusions=True is default  # Show the last 200 chars with footer area excluded filtered_text[-200:] Out[4]: <pre>'TOR Vote For 1 Election Provisional TOTAL Mail Votes Day Votes DEM ROBERT P CASEY JR 99 70 29 0 REP DAVE MCCORMICK 79 69 10 0 LIB JOHN C THOMAS 2 2 0 0 GRN LEILA HAZOU 2 1 1 0 CST MARTY SELKER 2 2 0 0'</pre> <p>This method is simple but might cut off content if the footer height varies or content extends lower on some pages.</p> In\u00a0[5]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\npage = pdf.pages[0] # Get page for finding elements\n\n# Find the last horizontal line on the first page\n# We'll use this logic to define our exclusion for all pages\nlast_line = page.find_all('line')[-1]\n\n# Define the exclusion function using a lambda\n# This finds the last line on *each* page and excludes below it\npdf.add_exclusion(\n    lambda p: p.find_all('line')[-1].below(),\n    label=\"Element-Based Footer\"\n)\n\n# Extract text again, with the element-based exclusion active\nfiltered_text_element = page.extract_text()\n\n# Show the last 200 chars with element-based footer exclusion\n\"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]\n\n# Visualize the element-based exclusion area\npage.clear_highlights()\n# Need to find the region again for visualization\nfooter_boundary = page.find_all('line')[-1]\nfooter_region_element = footer_boundary.below()\nfooter_region_element.show(label=\"Excluded Footer Area (Element)\")\npage.to_image()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url) page = pdf.pages[0] # Get page for finding elements  # Find the last horizontal line on the first page # We'll use this logic to define our exclusion for all pages last_line = page.find_all('line')[-1]  # Define the exclusion function using a lambda # This finds the last line on *each* page and excludes below it pdf.add_exclusion(     lambda p: p.find_all('line')[-1].below(),     label=\"Element-Based Footer\" )  # Extract text again, with the element-based exclusion active filtered_text_element = page.extract_text()  # Show the last 200 chars with element-based footer exclusion \"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]  # Visualize the element-based exclusion area page.clear_highlights() # Need to find the region again for visualization footer_boundary = page.find_all('line')[-1] footer_region_element = footer_boundary.below() footer_region_element.show(label=\"Excluded Footer Area (Element)\") page.to_image() Out[5]: <p>This element-based approach is usually more reliable as it adapts to the content's position, but it depends on finding consistent boundary elements (like lines or specific text markers).</p> <p>Applying Exclusions</p> <pre><code>*   `pdf.add_exclusion(func)` applies the exclusion function (which takes a page and returns a region) to *all* pages in the PDF.\n*   `page.add_exclusion(region)` adds an exclusion region only to that specific page.\n*   `extract_text(use_exclusions=False)` can be used to temporarily disable exclusions.</code></pre>"},{"location":"tutorials/05-excluding-content/#excluding-content-headersfooters","title":"Excluding Content (Headers/Footers)\u00b6","text":"<p>Often, PDFs have repeating headers or footers on every page that you want to ignore when extracting the main content. <code>natural-pdf</code> allows you to define exclusion regions.</p> <p>We'll use a different PDF for this example, which has a distinct header and footer section: <code>0500000US42007.pdf</code>.</p>"},{"location":"tutorials/05-excluding-content/#approach-1-excluding-a-fixed-area","title":"Approach 1: Excluding a Fixed Area\u00b6","text":"<p>A simple way to exclude headers or footers is to define a fixed region based on page coordinates. Let's exclude the bottom 200 pixels of the page.</p>"},{"location":"tutorials/05-excluding-content/#approach-2-excluding-based-on-elements","title":"Approach 2: Excluding Based on Elements\u00b6","text":"<p>A more robust way is to find specific elements that reliably mark the start of the footer (or end of the header) and exclude everything below (or above) them. In <code>Examples.md</code>, the footer was defined as everything below the last horizontal line.</p>"},{"location":"tutorials/06-document-qa/","title":"Document Question Answering (QA)","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Ask about the date\nquestion_1 = \"What is the inspection date?\"\nanswer_1 = page.ask(question_1)\n\n# The result is a dictionary with the answer, confidence, etc.\nanswer_1\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Ask about the date question_1 = \"What is the inspection date?\" answer_1 = page.ask(question_1)  # The result is a dictionary with the answer, confidence, etc. answer_1 <pre>Device set to use cpu\n</pre> Out[2]: <pre>{'answer': 'February 3, 1905',\n 'confidence': 0.998698353767395,\n 'start': 5,\n 'end': 5,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre># Ask about the company name\nquestion_2 = \"What company was inspected?\"\nanswer_2 = page.ask(question_2)\n\n# Display the answer dictionary\nanswer_2\n</pre> # Ask about the company name question_2 = \"What company was inspected?\" answer_2 = page.ask(question_2)  # Display the answer dictionary answer_2 Out[3]: <pre>{'answer': 'Jungle Health and Safety Inspection Service',\n 'confidence': 0.8496664762496948,\n 'start': 0,\n 'end': 0,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre># Ask about specific content from the table\nquestion_3 = \"What is statute 5.8.3 about?\"\nanswer_3 = page.ask(question_3)\n\n# Display the answer\nanswer_3\n</pre> # Ask about specific content from the table question_3 = \"What is statute 5.8.3 about?\" answer_3 = page.ask(question_3)  # Display the answer answer_3 Out[4]: <pre>{'answer': 'Inadequate Protective Equipment.',\n 'confidence': 0.9996328949928284,\n 'start': 25,\n 'end': 25,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> <p>The results include the extracted <code>answer</code>, a <code>confidence</code> score (useful for filtering uncertain answers), the <code>page_num</code>, and the <code>source_elements</code>.</p> In\u00a0[5]: Copied! <pre>from natural_pdf import PDF\nimport pandas as pd\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# List of questions to ask\nquestions = [\n    \"What is the inspection date?\",\n    \"What company was inspected?\",\n    \"What is statute 5.8.3 about?\",\n    \"How many violations were there in total?\" # This might be less reliable\n]\n\n# Collect answers for each question\nresults = []\nfor q in questions:\n    answer_dict = page.ask(q)\n    # Add the original question to the dictionary\n    answer_dict['question'] = q\n    results.append(answer_dict)\n\n# Convert the list of dictionaries to a DataFrame\n# We select only the most relevant columns here\ndf_results = pd.DataFrame(results)[['question', 'answer', 'confidence']]\n\n# Display the DataFrame\ndf_results\n</pre> from natural_pdf import PDF import pandas as pd  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # List of questions to ask questions = [     \"What is the inspection date?\",     \"What company was inspected?\",     \"What is statute 5.8.3 about?\",     \"How many violations were there in total?\" # This might be less reliable ]  # Collect answers for each question results = [] for q in questions:     answer_dict = page.ask(q)     # Add the original question to the dictionary     answer_dict['question'] = q     results.append(answer_dict)  # Convert the list of dictionaries to a DataFrame # We select only the most relevant columns here df_results = pd.DataFrame(results)[['question', 'answer', 'confidence']]  # Display the DataFrame df_results Out[5]: question answer confidence 0 What is the inspection date? February 3, 1905 0.998698 1 What company was inspected? Jungle Health and Safety Inspection Service 0.849666 2 What is statute 5.8.3 about? Inadequate Protective Equipment. 0.999633 3 How many violations were there in total? 7.1.5 0.749996 <p>This shows how you can iterate through questions, collect the answer dictionaries, and then create a structured DataFrame, making it easy to review questions, answers, and their confidence levels together.</p> <p>QA Model and Limitations</p> <pre><code>*   The QA system relies on underlying transformer models. Performance and confidence scores vary.\n*   It works best for questions where the answer is explicitly stated. It cannot synthesize information or perform calculations (e.g., counting items might fail or return text containing a number rather than the count itself).\n*   You can potentially specify different QA models via the `model=` argument in `page.ask()` if others are configured.</code></pre>"},{"location":"tutorials/06-document-qa/#document-question-answering-qa","title":"Document Question Answering (QA)\u00b6","text":"<p>Sometimes, instead of searching for specific text patterns, you just want to ask the document a question directly. <code>natural-pdf</code> includes an extractive Question Answering feature.</p> <p>\"Extractive\" means it finds the literal answer text within the document, rather than generating a new answer or summarizing.</p> <p>Let's ask our <code>01-practice.pdf</code> a few questions.</p>"},{"location":"tutorials/06-document-qa/#collecting-results-into-a-dataframe","title":"Collecting Results into a DataFrame\u00b6","text":"<p>If you're asking multiple questions, it's often useful to collect the results into a pandas DataFrame for easier analysis.</p>"},{"location":"tutorials/07-layout-analysis/","title":"Layout Analysis","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Analyze the layout using the default model\n# This adds 'detected' Region objects to the page\n# It returns an ElementCollection of the detected regions\npage.analyze_layout()\ndetected_regions = page.find_all('region[source=\"detected\"]')\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Analyze the layout using the default model # This adds 'detected' Region objects to the page # It returns an ElementCollection of the detected regions page.analyze_layout() detected_regions = page.find_all('region[source=\"detected\"]') <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpuddckg_0/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1568.5ms\n</pre> <pre>Speed: 4.6ms preprocess, 1568.5ms inference, 0.7ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[3]: Copied! <pre># Visualize all detected regions, using default colors based on type\npage.clear_highlights() # Clear previous highlights\ndetected_regions.highlight(group_by='type', include_attrs=['confidence'])\n\n# Show the image with region overlays\npage.to_image(width=900)\n</pre> # Visualize all detected regions, using default colors based on type page.clear_highlights() # Clear previous highlights detected_regions.highlight(group_by='type', include_attrs=['confidence'])  # Show the image with region overlays page.to_image(width=900) Out[3]: In\u00a0[4]: Copied! <pre># Find and visualize only the detected table region(s)\ntables = page.find_all('region[type=table]')\ntables.show(color='lightgreen', label='Detected Table')\n</pre> # Find and visualize only the detected table region(s) tables = page.find_all('region[type=table]') tables.show(color='lightgreen', label='Detected Table') Out[4]: In\u00a0[5]: Copied! <pre># Extract text specifically from the detected table region\ntable_region = tables.first # Assuming only one table was detected\n# Extract text preserving layout\ntable_text_layout = table_region.extract_text(layout=True)\ntable_text_layout\n</pre> # Extract text specifically from the detected table region table_region = tables.first # Assuming only one table was detected # Extract text preserving layout table_text_layout = table_region.extract_text(layout=True) table_text_layout Out[5]: <pre>'Statute Description Level Repeat? 4.12.7 Unsanitary Working Conditions. Critical 5.8.3 Inadequate Protective Equipment. Serious 6.3.9 Ineffective Injury Prevention. Serious 7.1.5 Failure to Properly Store Hazardous Materials. Critical 8.9.2 Lack of Adequate Fire Safety Measures. Serious 9.6.4 Inadequate Ventilation Systems. Serious 10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[6]: Copied! <pre># Layout-detected regions can also be used for table extraction\n# This can be more robust than the basic page.extract_tables()\n# especially for tables without clear lines.\ntable_data = table_region.extract_table()\ntable_data\n</pre> # Layout-detected regions can also be used for table extraction # This can be more robust than the basic page.extract_tables() # especially for tables without clear lines. table_data = table_region.extract_table() table_data Out[6]: <pre>[['Statute', 'Description', 'Level', 'Repeat?'],\n ['4.12.7', 'Unsanitary Working Conditions.', 'Critical', ''],\n ['5.8.3', 'Inadequate Protective Equipment.', 'Serious', ''],\n ['6.3.9', 'Ineffective Injury Prevention.', 'Serious', ''],\n ['7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', ''],\n ['8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', ''],\n ['9.6.4', 'Inadequate Ventilation Systems.', 'Serious', ''],\n ['10.2.7',\n  'Insufficient Employee Training for Safe Work Practices.',\n  'Serious',\n  '']]</pre> <p>Layout analysis provides structured <code>Region</code> objects. You can filter these regions by their predicted <code>type</code> and then perform actions like visualization or extracting text/tables specifically from those regions.</p> <p>Layout Models and Configuration</p> <pre><code>*   Layout analysis requires external models. Ensure these are installed.\n*   You can specify different models (`engine='yolo'`, `engine='detr'`, `engine='paddle'`) or configurations (confidence thresholds, specific classes) via arguments to `page.analyze_layout()`. Different models may perform better on different document types.\n*   The detected regions are added to the page and can be found using selectors like `page.find_all('region[type=paragraph]')`.</code></pre> <pre><code></code></pre>"},{"location":"tutorials/07-layout-analysis/#layout-analysis","title":"Layout Analysis\u00b6","text":"<p>Beyond simple text and lines, <code>natural-pdf</code> can use layout analysis models (like YOLO or DETR) to identify semantic regions within a page, such as paragraphs, tables, figures, headers, etc. This provides a higher-level understanding of the document structure.</p> <p>Let's analyze the layout of our <code>01-practice.pdf</code>.</p>"},{"location":"tutorials/07-working-with-regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Create a region in the top portion of the page\ntop_region = page.create_region(\n    50,          # x0 (left)\n    50,          # y0 (top)\n    page.width - 50,  # x1 (right)\n    200          # y1 (bottom)\n)\n\n# Visualize the region\ntop_region.show(color=\"blue\", label=\"Top Region\")\n\n# Extract text from this region\ntop_region.extract_text()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Create a region in the top portion of the page top_region = page.create_region(     50,          # x0 (left)     50,          # y0 (top)     page.width - 50,  # x1 (right)     200          # y1 (bottom) )  # Visualize the region top_region.show(color=\"blue\", label=\"Top Region\")  # Extract text from this region top_region.extract_text() Out[2]: <pre>'Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell'</pre> In\u00a0[3]: Copied! <pre># Find an element to create regions around\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Create regions relative to this element\nbelow_title = title.below(height=100)\nright_of_title = title.right(width=200) \nabove_title = title.above(height=50)\n\n# Visualize these regions\nbelow_title.show(color=\"green\", label=\"Below\")\nright_of_title.show(color=\"red\", label=\"Right\")\nabove_title.show(color=\"orange\", label=\"Above\")\n\n# Extract text from the region below the title\nbelow_title.extract_text()\n</pre> # Find an element to create regions around title = page.find('text:contains(\"Jungle Health\")')  # Create regions relative to this element below_title = title.below(height=100) right_of_title = title.right(width=200)  above_title = title.above(height=50)  # Visualize these regions below_title.show(color=\"green\", label=\"Below\") right_of_title.show(color=\"red\", label=\"Right\") above_title.show(color=\"orange\", label=\"Above\")  # Extract text from the region below the title below_title.extract_text() Out[3]: <pre>'INS-UP70N51NCL41R Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7'</pre> In\u00a0[4]: Copied! <pre># Create a region for a specific document section\nform_region = page.create_region(50, 100, page.width - 50, 300)\n\n# Find elements only within this region\nlabels = form_region.find_all('text:contains(\":\")') \n\n# Visualize the region and the elements found\nform_region.show(color=(0, 0, 1, 0.2), label=\"Form Region\")\nlabels.show(color=\"purple\", label=\"Labels\")\n\n# Count the elements found\nlen(labels)\n</pre> # Create a region for a specific document section form_region = page.create_region(50, 100, page.width - 50, 300)  # Find elements only within this region labels = form_region.find_all('text:contains(\":\")')   # Visualize the region and the elements found form_region.show(color=(0, 0, 1, 0.2), label=\"Form Region\") labels.show(color=\"purple\", label=\"Labels\")  # Count the elements found len(labels) Out[4]: <pre>3</pre> In\u00a0[5]: Copied! <pre># Find an element to work with\nelement = page.find('text:contains(\"Summary:\")')\n\n# Create a tight region around the element\ntight_region = page.create_region(\n    element.x0, element.top, \n    element.x1, element.bottom\n)\n\n# Expand it to include surrounding content\nexpanded_region = tight_region.expand(\n    left=10,       # Expand 10 points to the left\n    right=200,     # Expand 200 points to the right\n    top_expand=5,  # Expand 5 points above\n    bottom_expand=100  # Expand 100 points below\n)\n\n# Visualize both regions\ntight_region.show(color=\"red\", label=\"Original\")\nexpanded_region.show(color=\"blue\", label=\"Expanded\")\n\n# Extract the content from the expanded region\nexpanded_region.extract_text()\n</pre> # Find an element to work with element = page.find('text:contains(\"Summary:\")')  # Create a tight region around the element tight_region = page.create_region(     element.x0, element.top,      element.x1, element.bottom )  # Expand it to include surrounding content expanded_region = tight_region.expand(     left=10,       # Expand 10 points to the left     right=200,     # Expand 200 points to the right     top_expand=5,  # Expand 5 points above     bottom_expand=100  # Expand 100 points below )  # Visualize both regions tight_region.show(color=\"red\", label=\"Original\") expanded_region.show(color=\"blue\", label=\"Expanded\")  # Extract the content from the expanded region expanded_region.extract_text() Out[5]: <pre>'Summary:  These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard!'</pre> In\u00a0[6]: Copied! <pre># Find two elements to serve as boundaries\nstart_elem = page.find('text:contains(\"Summary:\")')\nend_elem = page.find('text:contains(\"Statute\")')\n\n# Create a region from start to end element\nbounded_region = start_elem.until(end_elem)\n\n# Visualize the bounded region\nbounded_region.show(color=\"green\", label=\"Bounded Region\")\n\n# Extract text from this bounded region\nbounded_region.extract_text()[:200] + \"...\" if len(bounded_region.extract_text()) &gt; 200 else bounded_region.extract_text()\n</pre> # Find two elements to serve as boundaries start_elem = page.find('text:contains(\"Summary:\")') end_elem = page.find('text:contains(\"Statute\")')  # Create a region from start to end element bounded_region = start_elem.until(end_elem)  # Visualize the bounded region bounded_region.show(color=\"green\", label=\"Bounded Region\")  # Extract text from this bounded region bounded_region.extract_text()[:200] + \"...\" if len(bounded_region.extract_text()) &gt; 200 else bounded_region.extract_text() Out[6]: <pre>'Jungle Health and Safety Inspection Service INS-UP70N51NCL41R Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7 Summary:  Worst of any, however, were the fertilizer...'</pre> In\u00a0[7]: Copied! <pre># Define multiple regions to extract different parts of the document\nheader_region = page.create_region(0, 0, page.width, 100)\nmain_region = page.create_region(100, 100, page.width - 100, page.height - 150)\nfooter_region = page.create_region(0, page.height - 50, page.width, page.height)\n\n# Visualize all regions\nheader_region.show(color=\"blue\", label=\"Header\")\nmain_region.show(color=\"green\", label=\"Main Content\")\nfooter_region.show(color=\"red\", label=\"Footer\")\n\n# Extract content from each region\ndocument_parts = {\n    \"header\": header_region.extract_text(),\n    \"main\": main_region.extract_text()[:100] + \"...\",\n    \"footer\": footer_region.extract_text()\n}\n\n# Show what we extracted\ndocument_parts\n</pre> # Define multiple regions to extract different parts of the document header_region = page.create_region(0, 0, page.width, 100) main_region = page.create_region(100, 100, page.width - 100, page.height - 150) footer_region = page.create_region(0, page.height - 50, page.width, page.height)  # Visualize all regions header_region.show(color=\"blue\", label=\"Header\") main_region.show(color=\"green\", label=\"Main Content\") footer_region.show(color=\"red\", label=\"Footer\")  # Extract content from each region document_parts = {     \"header\": header_region.extract_text(),     \"main\": main_region.extract_text()[:100] + \"...\",     \"footer\": footer_region.extract_text() }  # Show what we extracted document_parts Out[7]: <pre>{'header': 'Jungle Health and Safety Inspection Service INS-UP70N51NCL41R Site:  Durham\u2019s Meatpacking Chicago, Ill.',\n 'main': 'February 3, 1905 7 Worst of any, however, were the fertilizer men, and those who served in the cooki...',\n 'footer': 'Jungle Health and Safety Inspection Service'}</pre> In\u00a0[8]: Copied! <pre># Find a region of interest\ntable_header = page.find('text:contains(\"Statute\")')\ntable_region = table_header.below(height=100)\n\n# Visualize the region\ntable_region.show(color=\"purple\", label=\"Table Region\")\n\n# Create an image of just this region\ntable_region.to_image(resolution=150)\n</pre> # Find a region of interest table_header = page.find('text:contains(\"Statute\")') table_region = table_header.below(height=100)  # Visualize the region table_region.show(color=\"purple\", label=\"Table Region\")  # Create an image of just this region table_region.to_image(resolution=150) Out[8]: <p>Regions allow you to precisely target specific parts of a document for extraction and analysis. They're essential for handling complex document layouts and isolating the exact content you need.</p>"},{"location":"tutorials/07-working-with-regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that let you focus on specific parts of a document. They're perfect for extracting text from defined areas, finding elements within certain boundaries, and working with document sections.</p>"},{"location":"tutorials/07-working-with-regions/#creating-regions-from-elements","title":"Creating Regions from Elements\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#finding-elements-within-regions","title":"Finding Elements Within Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#expanding-and-adjusting-regions","title":"Expanding and Adjusting Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-bounded-regions","title":"Creating Bounded Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#working-with-multiple-regions","title":"Working with Multiple Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-an-image-of-a-region","title":"Creating an Image of a Region\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/","title":"Spatial Navigation","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the title of the document\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Visualize our starting point\ntitle.show(color=\"red\", label=\"Document Title\")\n\n# Display the title text\ntitle.text\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the title of the document title = page.find('text:contains(\"Jungle Health\")')  # Visualize our starting point title.show(color=\"red\", label=\"Document Title\")  # Display the title text title.text Out[2]: <pre>'Jungle Health and Safety Inspection Service'</pre> In\u00a0[3]: Copied! <pre># Create a region below the title\nregion_below = title.below(height=100)\n\n# Visualize the region\nregion_below.show(color=\"blue\", label=\"Below Title\")\n\n# Find and extract text from this region\ntext_below = region_below.extract_text()\ntext_below\n</pre> # Create a region below the title region_below = title.below(height=100)  # Visualize the region region_below.show(color=\"blue\", label=\"Below Title\")  # Find and extract text from this region text_below = region_below.extract_text() text_below Out[3]: <pre>'INS-UP70N51NCL41R Site:  Durham\u2019s Meatpacking Chicago, Ill. Date:  February 3, 1905 Violation Count:  7'</pre> In\u00a0[4]: Copied! <pre># Find two labels to serve as boundaries\nsite_label = page.find('text:contains(\"Site:\")')\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Get the region between these labels\nbetween_region = site_label.below(\n    include_element=True,     # Include starting element\n    until='text:contains(\"Date:\")',  # Stop at this element\n    include_endpoint=False    # Don't include ending element\n)\n\n# Visualize the region between labels\nbetween_region.show(color=\"green\", label=\"Between\")\n\n# Extract text from this bounded area\nbetween_region.extract_text()\n</pre> # Find two labels to serve as boundaries site_label = page.find('text:contains(\"Site:\")') date_label = page.find('text:contains(\"Date:\")')  # Get the region between these labels between_region = site_label.below(     include_element=True,     # Include starting element     until='text:contains(\"Date:\")',  # Stop at this element     include_endpoint=False    # Don't include ending element )  # Visualize the region between labels between_region.show(color=\"green\", label=\"Between\")  # Extract text from this bounded area between_region.extract_text() Out[4]: <pre>'Site:  Durham\u2019s Meatpacking Chicago, Ill.'</pre> In\u00a0[5]: Copied! <pre># Find a field label\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Get the content to the right (the field value)\nvalue_region = site_label.right(width=200)\n\n# Visualize the label and value regions\nsite_label.show(color=\"red\", label=\"Label\")\nvalue_region.show(color=\"blue\", label=\"Value\")\n\n# Extract just the value text\nvalue_region.extract_text()\n</pre> # Find a field label site_label = page.find('text:contains(\"Site:\")')  # Get the content to the right (the field value) value_region = site_label.right(width=200)  # Visualize the label and value regions site_label.show(color=\"red\", label=\"Label\") value_region.show(color=\"blue\", label=\"Value\")  # Extract just the value text value_region.extract_text() Out[5]: <pre>'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Violations Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.'</pre> In\u00a0[6]: Copied! <pre># Start with a label element\nlabel = page.find('text:contains(\"Site:\")')\n\n# Find the next and previous elements in reading order\nnext_elem = label.next()\nprev_elem = label.prev()\n\n# Visualize all three elements\nlabel.show(color=\"red\", label=\"Current\")\nnext_elem.show(color=\"green\", label=\"Next\") if next_elem else None\nprev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None\n\n# Show the text of adjacent elements\n{\n    \"current\": label.text,\n    \"next\": next_elem.text if next_elem else \"None\",\n    \"previous\": prev_elem.text if prev_elem else \"None\"\n}\n</pre> # Start with a label element label = page.find('text:contains(\"Site:\")')  # Find the next and previous elements in reading order next_elem = label.next() prev_elem = label.prev()  # Visualize all three elements label.show(color=\"red\", label=\"Current\") next_elem.show(color=\"green\", label=\"Next\") if next_elem else None prev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None  # Show the text of adjacent elements {     \"current\": label.text,     \"next\": next_elem.text if next_elem else \"None\",     \"previous\": prev_elem.text if prev_elem else \"None\" } Out[6]: <pre>{'current': 'Site: ', 'next': 'i', 'previous': 'S'}</pre> In\u00a0[7]: Copied! <pre># Find a section label\nsummary = page.find('text:contains(\"Summary:\")')\n\n# Find the next bold text element\nnext_bold = summary.next('text:bold', limit=20)\n\n# Find the nearest line element\nnearest_line = summary.nearest('line')\n\n# Visualize what we found\nsummary.show(color=\"red\", label=\"Summary\")\nnext_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None\nnearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None\n\n# Show the content we found\n{\n    \"summary\": summary.text,\n    \"next_bold\": next_bold.text if next_bold else \"None found\",\n    \"nearest_line\": nearest_line if nearest_line else \"None found\"\n}\n</pre> # Find a section label summary = page.find('text:contains(\"Summary:\")')  # Find the next bold text element next_bold = summary.next('text:bold', limit=20)  # Find the nearest line element nearest_line = summary.nearest('line')  # Visualize what we found summary.show(color=\"red\", label=\"Summary\") next_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None nearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None  # Show the content we found {     \"summary\": summary.text,     \"next_bold\": next_bold.text if next_bold else \"None found\",     \"nearest_line\": nearest_line if nearest_line else \"None found\" } Out[7]: <pre>{'summary': 'Summary: ',\n 'next_bold': 'u',\n 'nearest_line': &lt;LineElement type=horizontal width=2.0 bbox=(50, 352, 550, 352)&gt;}</pre> In\u00a0[8]: Copied! <pre># Find a table heading\ntable_heading = page.find('text:contains(\"Statute\")')\ntable_heading.show(color=\"purple\", label=\"Table Header\")\n\n# Extract table rows using spatial navigation\nrows = []\ncurrent = table_heading\n\n# Get the next 4 rows\nfor i in range(4):\n    # Find the next row below the current one\n    next_row = current.below(height=15)\n    \n    if next_row:\n        rows.append(next_row)\n        current = next_row  # Move to the next row\n    else:\n        break\n\n# Visualize all found rows\npage.clear_highlights()\nfor i, row in enumerate(rows):\n    row.highlight(label=f\"Row {i+1}\")\npage.to_image(width=700)\n</pre> # Find a table heading table_heading = page.find('text:contains(\"Statute\")') table_heading.show(color=\"purple\", label=\"Table Header\")  # Extract table rows using spatial navigation rows = [] current = table_heading  # Get the next 4 rows for i in range(4):     # Find the next row below the current one     next_row = current.below(height=15)          if next_row:         rows.append(next_row)         current = next_row  # Move to the next row     else:         break  # Visualize all found rows page.clear_highlights() for i, row in enumerate(rows):     row.highlight(label=f\"Row {i+1}\") page.to_image(width=700) Out[8]: In\u00a0[9]: Copied! <pre># Extract text from each row\n[row.extract_text() for row in rows]\n</pre> # Extract text from each row [row.extract_text() for row in rows] Out[9]: <pre>['',\n '',\n '5.8.3 Inadequate Protective Equipment. Serious',\n '6.3.9 Ineffective Injury Prevention. Serious']</pre> In\u00a0[10]: Copied! <pre># Find all potential field labels (text with a colon)\nlabels = page.find_all('text:contains(\":\")') \n\n# Visualize the labels\nlabels.show(color=\"blue\", label=\"Labels\")\n\n# Extract key-value pairs\nfield_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    key = label.text.strip().rstrip(':')\n    \n    # Skip if not a proper label\n    if not key:\n        continue\n    \n    # Get the value to the right\n    value = label.right(width=200).extract_text().strip()\n    \n    # Add to our collection\n    field_data[key] = value\n\n# Show the extracted data\nfield_data\n</pre> # Find all potential field labels (text with a colon) labels = page.find_all('text:contains(\":\")')   # Visualize the labels labels.show(color=\"blue\", label=\"Labels\")  # Extract key-value pairs field_data = {}  for label in labels:     # Clean up the label text     key = label.text.strip().rstrip(':')          # Skip if not a proper label     if not key:         continue          # Get the value to the right     value = label.right(width=200).extract_text().strip()          # Add to our collection     field_data[key] = value  # Show the extracted data field_data Out[10]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Violations Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.',\n 'Violation Count': 'Durham\u2019s Meatpacking Chicago, Ill. Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices. Jungle Health and Safety Inspection Service',\n 'Summary': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 7 These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.'}</pre> <p>Spatial navigation mimics how humans read documents, letting you navigate content based on physical relationships between elements. It's especially useful for extracting structured data from forms, tables, and formatted documents.</p>"},{"location":"tutorials/08-spatial-navigation/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>Spatial navigation lets you work with PDF content based on the physical layout of elements on the page. It's perfect for finding elements relative to each other and extracting information in context.</p>"},{"location":"tutorials/08-spatial-navigation/#finding-elements-above-and-below","title":"Finding Elements Above and Below\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-content-between-elements","title":"Finding Content Between Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#navigating-left-and-right","title":"Navigating Left and Right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-adjacent-elements","title":"Finding Adjacent Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#combining-with-element-selectors","title":"Combining with Element Selectors\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-table-rows-with-spatial-navigation","title":"Extracting Table Rows with Spatial Navigation\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-key-value-pairs","title":"Extracting Key-Value Pairs\u00b6","text":""},{"location":"tutorials/09-section-extraction/","title":"Section Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF using the relative path\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\n# Find horizontal lines that separate book entries\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Visualize the potential section boundaries\nhorizontal_lines.highlight(color=\"red\", label=\"Section Boundaries\")\npage.to_image()\n</pre> from natural_pdf import PDF  # Load the PDF using the relative path pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  # Find horizontal lines that separate book entries horizontal_lines = page.find_all('line:horizontal')  # Visualize the potential section boundaries horizontal_lines.highlight(color=\"red\", label=\"Section Boundaries\") page.to_image() Out[2]: In\u00a0[3]: Copied! <pre># Count what we found\nlen(horizontal_lines)\n</pre> # Count what we found len(horizontal_lines) Out[3]: <pre>9</pre> In\u00a0[4]: Copied! <pre># Extract sections based on horizontal lines\n# Each section starts at a horizontal line and ends at the next one\nbook_sections = page.get_sections(\n    start_elements=horizontal_lines,\n    boundary_inclusion='start'  # Include the boundary in the section\n)\n\n# Visualize each section\npage.clear_highlights()\nfor section in book_sections:\n    section.highlight()\npage.to_image()\n</pre> # Extract sections based on horizontal lines # Each section starts at a horizontal line and ends at the next one book_sections = page.get_sections(     start_elements=horizontal_lines,     boundary_inclusion='start'  # Include the boundary in the section )  # Visualize each section page.clear_highlights() for section in book_sections:     section.highlight() page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Display section count and preview the first section\n{\n    \"total_sections\": len(book_sections),\n    \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\"\n}\n</pre> # Display section count and preview the first section {     \"total_sections\": len(book_sections),     \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\" } Out[5]: <pre>{'total_sections': 9,\n 'first_section_text': '6/12/2023 - Copies Removed: 2 Tristan Strong punches a hole in the sky (Removed: 1) Author: Mbalia, ...'}</pre> In\u00a0[6]: Copied! <pre># Extract and display content from the first few book entries\nbook_entries = []\n\nfor i, section in enumerate(book_sections[:5]):\n    # Extract the section text\n    text = section.extract_text().strip()\n    \n    # Try to parse book information\n    title = \"\"\n    author = \"\"\n    isbn = \"\"\n    \n    # Extract title (typically the first line)\n    title_match = section.find('text:contains(\"Title:\")')\n    if title_match:\n        title_value = title_match.right(width=400).extract_text()\n        title = title_value.strip()\n    \n    # Extract author\n    author_match = section.find('text:contains(\"Author:\")')\n    if author_match:\n        author_value = author_match.right(width=400).extract_text()\n        author = author_value.strip()\n    \n    # Extract ISBN\n    isbn_match = section.find('text:contains(\"ISBN:\")')\n    if isbn_match:\n        isbn_value = isbn_match.right(width=400).extract_text()\n        isbn = isbn_value.strip()\n    \n    # Add to our collection\n    book_entries.append({\n        \"number\": i + 1,\n        \"title\": title,\n        \"author\": author,\n        \"isbn\": isbn,\n        \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text\n    })\n\n# Display the structured book entries\nimport pandas as pd\npd.DataFrame(book_entries)\n</pre> # Extract and display content from the first few book entries book_entries = []  for i, section in enumerate(book_sections[:5]):     # Extract the section text     text = section.extract_text().strip()          # Try to parse book information     title = \"\"     author = \"\"     isbn = \"\"          # Extract title (typically the first line)     title_match = section.find('text:contains(\"Title:\")')     if title_match:         title_value = title_match.right(width=400).extract_text()         title = title_value.strip()          # Extract author     author_match = section.find('text:contains(\"Author:\")')     if author_match:         author_value = author_match.right(width=400).extract_text()         author = author_value.strip()          # Extract ISBN     isbn_match = section.find('text:contains(\"ISBN:\")')     if isbn_match:         isbn_value = isbn_match.right(width=400).extract_text()         isbn = isbn_value.strip()          # Add to our collection     book_entries.append({         \"number\": i + 1,         \"title\": title,         \"author\": author,         \"isbn\": isbn,         \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text     })  # Display the structured book entries import pandas as pd pd.DataFrame(book_entries) Out[6]: number title author isbn preview 0 1 Atlanta Public Schools Tristan Strong punches ... Atlanta Public Schools Published: 2019 Acquire... 6/12/2023 - Copies Removed: 2 Tristan Strong p... 1 2 6/7/2023 - Copies Removed: 2 2 3 Atlanta Public Schools ISBN: 978-1-36803993-2 ... Atlanta Public Schools Published: 2019 Acquire... Buddhism (Removed: 1) Author: Wangu, Madhu Baz... 3 4 Atlanta Public Schools ISBN: 978-1-36803993-2 ... Atlanta Public Schools Published: 2019 Acquire... Voodoo (Removed: 1) Author: Kelly Wand, book e... 4 5 6/6/2023 - Copies Removed: 130 In\u00a0[7]: Copied! <pre>page.viewer()\n</pre> page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[7]: In\u00a0[8]: Copied! <pre># Find title elements with specific selectors\ntitle_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\ntitle_elements.show()\n</pre> # Find title elements with specific selectors title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]') title_elements.show() Out[8]: In\u00a0[9]: Copied! <pre># Extract sections starting from each title\n# This now directly returns an ElementCollection\ntitle_sections = page.get_sections(\n    start_elements=title_elements,\n    boundary_inclusion='start'\n)\n\n# Show the title-based sections\npage.clear_highlights()\ntitle_sections.highlight()\npage.to_image()\n</pre> # Extract sections starting from each title # This now directly returns an ElementCollection title_sections = page.get_sections(     start_elements=title_elements,     boundary_inclusion='start' )  # Show the title-based sections page.clear_highlights() title_sections.highlight() page.to_image() Out[9]: In\u00a0[10]: Copied! <pre># Count the sections found\nlen(title_sections)\n</pre> # Count the sections found len(title_sections) Out[10]: <pre>7</pre> In\u00a0[11]: Copied! <pre># Use horizontal line elements as section dividers\ndividers = page.find_all('line[horizontal]')\n\n# Compare the different boundary inclusion options\ninclusion_options = {\n    'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),\n    'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),\n    'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),\n    'both': page.get_sections(start_elements=dividers, boundary_inclusion='both')\n}\n\n# Count sections with each option\nsection_counts = {option: len(sections) for option, sections in inclusion_options.items()}\nsection_counts\n</pre> # Use horizontal line elements as section dividers dividers = page.find_all('line[horizontal]')  # Compare the different boundary inclusion options inclusion_options = {     'none': page.get_sections(start_elements=dividers, boundary_inclusion='none'),     'start': page.get_sections(start_elements=dividers, boundary_inclusion='start'),     'end': page.get_sections(start_elements=dividers, boundary_inclusion='end'),     'both': page.get_sections(start_elements=dividers, boundary_inclusion='both') }  # Count sections with each option section_counts = {option: len(sections) for option, sections in inclusion_options.items()} section_counts Out[11]: <pre>{'none': 9, 'start': 9, 'end': 9, 'both': 9}</pre> In\u00a0[12]: Copied! <pre># Define specific start and end points - let's extract just one book entry\n# We'll look for the first and second horizontal lines\npage.clear_highlights()\n\nstart_point = title_elements[0]\nend_point = title_elements[1]\n\n# Extract the section between these points\nsingle_book_entry = page.get_sections(\n    start_elements=[start_point],\n    end_elements=[end_point],\n    boundary_inclusion='start'  # Include the start but not the end\n)\n    \n# Visualize the custom section\nsingle_book_entry.highlight(color=\"green\", label=\"Single Book Entry\")\n    \nprint(single_book_entry[0].extract_text())\n\npage.to_image()\n</pre> # Define specific start and end points - let's extract just one book entry # We'll look for the first and second horizontal lines page.clear_highlights()  start_point = title_elements[0] end_point = title_elements[1]  # Extract the section between these points single_book_entry = page.get_sections(     start_elements=[start_point],     end_elements=[end_point],     boundary_inclusion='start'  # Include the start but not the end )      # Visualize the custom section single_book_entry.highlight(color=\"green\", label=\"Single Book Entry\")      print(single_book_entry[0].extract_text())  page.to_image() <pre>Tristan Strong punches a hole in the sky (Removed: 1) Author: Mbalia, Kwame. ISBN: 978-1-36803993-2 Published: 2019 Site Barcode Price Acquired Removed By Joseph Humphries 32441014018707 6/11/2021 113396-42441 Elementary School Was Available -- Weeded\n</pre> Out[12]: In\u00a0[13]: Copied! <pre># Get sections across the first two pages\nmulti_page_sections = [] # Initialize as a list\n\nfor page_num in range(min(2, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page (returns ElementCollection)\n    page_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Add elements from the collection to our list\n    multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection\n\n# Display info about each section (showing first 3)\n[{\n    \"page\": section.page.number + 1,  # 1-indexed page number for display\n    \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text()\n} for section in multi_page_sections]\n</pre> # Get sections across the first two pages multi_page_sections = [] # Initialize as a list  for page_num in range(min(2, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page (returns ElementCollection)     page_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Add elements from the collection to our list     multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection  # Display info about each section (showing first 3) [{     \"page\": section.page.number + 1,  # 1-indexed page number for display     \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text() } for section in multi_page_sections] Out[13]: <pre>[{'page': 2, 'text': 'Tristan Strong punches a hole in the sky (Removed:...'},\n {'page': 2, 'text': 'Upside down in the middle of nowhere (Removed: 1) ...'},\n {'page': 2, 'text': 'Buddhism (Removed: 1) Author: Wangu, Madhu Bazaz. ...'},\n {'page': 2, 'text': 'Voodoo (Removed: 1) Author: Kelly Wand, book edito...'},\n {'page': 2, 'text': 'The Abenaki (Removed: 1) Author: Landau, Elaine. I...'},\n {'page': 2, 'text': 'Afghanistan (Removed: 1) Author: Milivojevic, Jova...'},\n {'page': 2, 'text': 'Alexander the Great rocks the world (Removed: 1) A...'},\n {'page': 3, 'text': 'The Anasazi (Removed: 1) Author: Petersen, David. ...'},\n {'page': 3, 'text': 'And then what happened, Paul Revere? (Removed: 1) ...'},\n {'page': 3, 'text': 'The assassination of Martin Luther King Jr (Remove...'},\n {'page': 3, 'text': 'Barbara Jordan. (Removed: 1) Author: Wexler, Diane...'},\n {'page': 3, 'text': 'Bedtime for Batman (Removed: 1) Author: Dahl, Mich...'},\n {'page': 3, 'text': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskeg...'},\n {'page': 3, 'text': 'Bigfoot Wallace (Removed: 1) Author: Harper,Jo. IS...'},\n {'page': 3, 'text': 'The blaze engulfs : January 1939 to December 1941 ...'}]</pre> In\u00a0[14]: Copied! <pre># Extract all book entries across multiple pages\nbook_database = []\n\n# Process first 3 pages (or fewer if the document is shorter)\nfor page_num in range(min(3, len(pdf.pages))):\n    page = pdf.pages[page_num]\n    \n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n    \n    # Get sections for this page\n    book_sections = page.get_sections(\n        start_elements=title_elements,\n        boundary_inclusion='start'\n    )\n    \n    # Process each book section\n    for section in book_sections:\n        # Skip sections that are too short (might be headers/footers)\n        if len(section.extract_text()) &lt; 50:\n            continue\n            \n        # Extract book information\n        book_info = {\"page\": page_num + 1}\n        \n        for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.strip(':').lower()\n                field_value = field_element.extract_text().replace(field, '').strip()\n                book_info[field_name] = field_value\n\n        # Below the field name\n        for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.lower()\n                field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()\n                book_info[field_name] = field_value\n\n        book_database.append(book_info)\n\n# Display sample entries (first 3)\nimport pandas as pd\n\ndf = pd.json_normalize(book_database)\ndf.head()\n</pre> # Extract all book entries across multiple pages book_database = []  # Process first 3 pages (or fewer if the document is shorter) for page_num in range(min(3, len(pdf.pages))):     page = pdf.pages[page_num]          # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')          # Get sections for this page     book_sections = page.get_sections(         start_elements=title_elements,         boundary_inclusion='start'     )          # Process each book section     for section in book_sections:         # Skip sections that are too short (might be headers/footers)         if len(section.extract_text()) &lt; 50:             continue                      # Extract book information         book_info = {\"page\": page_num + 1}                  for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.strip(':').lower()                 field_value = field_element.extract_text().replace(field, '').strip()                 book_info[field_name] = field_value          # Below the field name         for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.lower()                 field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()                 book_info[field_name] = field_value          book_database.append(book_info)  # Display sample entries (first 3) import pandas as pd  df = pd.json_normalize(book_database) df.head() Out[14]: page author isbn price acquired barcode removed by 0 1 Mbalia, Kwame. 978-1-36803993-2 6/11/2021 32441014018707 113396-42441 1 1 Lamana, Julie T. 978-1-45212456-8 (alk. $15.00 6/12/2023 32441012580849 113396-42441 2 1 Wangu, Madhu Bazaz. 0-8160-2442-1 $10.00 4/19/2018 33343000017835 christen.mcclain 3 1 Kelly Wand, book editor. 0-7377-1314-3 (lib.) $19.95 3/21/2006 *3431000028742 christen.mcclain 4 1 Landau, Elaine. 0-531-20227-5 $16.50 2/21/2000 33170000506628 33554-43170 <p>Section extraction lets you break down documents into logical parts, making it easier to generate summaries, extract specific content, and create structured data from semi-structured documents. In this example, we've shown how to convert a PDF library catalog into a structured book database.</p>"},{"location":"tutorials/09-section-extraction/#section-extraction","title":"Section Extraction\u00b6","text":"<p>Documents are often organized into logical sections like chapters, articles, or content blocks. This tutorial shows how to extract these sections using natural-pdf, using a library weeding log as an example.</p>"},{"location":"tutorials/09-section-extraction/#basic-section-extraction","title":"Basic Section Extraction\u00b6","text":""},{"location":"tutorials/09-section-extraction/#working-with-section-content","title":"Working with Section Content\u00b6","text":""},{"location":"tutorials/09-section-extraction/#using-different-section-boundaries","title":"Using Different Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#section-boundary-inclusion-options","title":"Section Boundary Inclusion Options\u00b6","text":""},{"location":"tutorials/09-section-extraction/#custom-section-boundaries","title":"Custom Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#multi-page-sections","title":"Multi-page Sections\u00b6","text":""},{"location":"tutorials/09-section-extraction/#building-a-book-database","title":"Building a Book Database\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/","title":"Form Field Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find fields with labels ending in colon\nlabels = page.find_all('text:contains(\":\")')\n\n# Visualize the found labels\nlabels.show(color=\"blue\", label=\"Field Labels\")\n\n# Count how many potential fields we found\nlen(labels)\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find fields with labels ending in colon labels = page.find_all('text:contains(\":\")')  # Visualize the found labels labels.show(color=\"blue\", label=\"Field Labels\")  # Count how many potential fields we found len(labels) Out[2]: <pre>4</pre> In\u00a0[3]: Copied! <pre># Extract the value for each field label\nform_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    field_name = label.text.strip().rstrip(':')\n    \n    # Find the value to the right of the label\n    value_region = label.right(width=200)\n    value = value_region.extract_text().strip()\n    \n    # Store in our dictionary\n    form_data[field_name] = value\n\n# Display the extracted data\nform_data\n</pre> # Extract the value for each field label form_data = {}  for label in labels:     # Clean up the label text     field_name = label.text.strip().rstrip(':')          # Find the value to the right of the label     value_region = label.right(width=200)     value = value_region.extract_text().strip()          # Store in our dictionary     form_data[field_name] = value  # Display the extracted data form_data Out[3]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Violations Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.',\n 'Violation Count': 'Durham\u2019s Meatpacking Chicago, Ill. Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices. Jungle Health and Safety Inspection Service',\n 'Summary': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 7 These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.'}</pre> In\u00a0[4]: Copied! <pre># Clear previous highlights\npage.clear_highlights()\n\n# Highlight both labels and their values\nfor label in labels:\n    # Highlight the label in red\n    label.show(color=\"red\", label=\"Label\")\n    \n    # Highlight the value area in blue\n    label.right(width=200).show(color=\"blue\", label=\"Value\")\n\n# Show the page image with highlighted elements\npage.to_image()\n</pre> # Clear previous highlights page.clear_highlights()  # Highlight both labels and their values for label in labels:     # Highlight the label in red     label.show(color=\"red\", label=\"Label\")          # Highlight the value area in blue     label.right(width=200).show(color=\"blue\", label=\"Value\")  # Show the page image with highlighted elements page.to_image() Out[4]: In\u00a0[5]: Copied! <pre># Extract values that might span multiple lines\nmulti_line_data = {}\n\nfor label in labels:\n    # Get the field name\n    field_name = label.text.strip().rstrip(':')\n    \n    # Look both to the right and below\n    right_value = label.right(width=200).extract_text().strip()\n    below_value = label.below(height=50).extract_text().strip()\n    \n    # Combine the values if they're different\n    if right_value in below_value:\n        value = below_value\n    else:\n        value = f\"{right_value} {below_value}\".strip()\n    \n    # Add to results\n    multi_line_data[field_name] = value\n\n# Show fields with potential multi-line values\nmulti_line_data\n</pre> # Extract values that might span multiple lines multi_line_data = {}  for label in labels:     # Get the field name     field_name = label.text.strip().rstrip(':')          # Look both to the right and below     right_value = label.right(width=200).extract_text().strip()     below_value = label.below(height=50).extract_text().strip()          # Combine the values if they're different     if right_value in below_value:         value = below_value     else:         value = f\"{right_value} {below_value}\".strip()          # Add to results     multi_line_data[field_name] = value  # Show fields with potential multi-line values multi_line_data Out[5]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Violations Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices. Date:  February 3, 1905 Violation Count:  7',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices. Violation Count:  7 Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.',\n 'Violation Count': 'Durham\u2019s Meatpacking Chicago, Ill. Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices. Jungle Health and Safety Inspection Service Summary:  Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in',\n 'Summary': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 7 These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell'}</pre> In\u00a0[6]: Copied! <pre>import re\n\n# Find dates in the format July 31, YYY\ndate_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'\n\n# Search all text elements for dates\ntext_elements = page.find_all('text')\nprint([elem.text for elem in text_elements])\ndates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))\n\n# Visualize the date fields\ndates.show(color=\"green\", label=\"Date\")\n\n# Extract just the date values\ndate_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates]\ndate_texts\n</pre> import re  # Find dates in the format July 31, YYY date_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'  # Search all text elements for dates text_elements = page.find_all('text') print([elem.text for elem in text_elements]) dates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))  # Visualize the date fields dates.show(color=\"green\", label=\"Date\")  # Extract just the date values date_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates] date_texts <pre>['Jungle Health and Safety Inspection Service', 'INS-UP70N51NCL41R', 'Site: ', 'Durham\u2019s Meatpacking Chicago, Ill.', 'Date: ', 'February 3, 1905', 'Violation Count: ', '7', 'Summary: ', 'Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.', 'These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary ', 'visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in ', 'some of which there were open vats near the level of the floor, their peculiar trouble was that they fell', 'into the vats; and when they were fished out, there was never enough of them left to be worth ', 'exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out', 'to the world as Durham\u2019s Pure Leaf Lard!', 'Violations', 'Statute', 'Description', 'Level', 'Repeat?', '4.12.7', 'Unsanitary Working Conditions.', 'Critical', '5.8.3', 'Inadequate Protective Equipment.', 'Serious', '6.3.9', 'Ineffective Injury Prevention.', 'Serious', '7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', '8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', '9.6.4', 'Inadequate Ventilation Systems.', 'Serious', '10.2.7', 'Insufficient Employee Training for Safe Work Practices.', 'Serious', 'Jungle Health and Safety Inspection Service']\n</pre> Out[6]: <pre>['February 3, 1905']</pre> In\u00a0[7]: Copied! <pre># Run layout analysis to find table structures\npage.analyze_layout()\n\n# Find possible form tables\ntables = page.find_all('region[type=table]')\n\nif tables:\n    # Visualize the tables\n    tables.show(color=\"purple\", label=\"Form Table\")\n    \n    # Extract data from the first table\n    first_table = tables[0]\n    table_data = first_table.extract_table()\n    table_data\nelse:\n    # Try to find form-like structure using text alignment\n    # Create a region where a form might be\n    form_region = page.create_region(50, 200, page.width - 50, 500)\n    \n    # Group text by vertical position\n    rows = {}\n    text_elements = form_region.find_all('text')\n    \n    for elem in text_elements:\n        # Round y-position to group elements in the same row\n        row_pos = round(elem.top / 5) * 5\n        if row_pos not in rows:\n            rows[row_pos] = []\n        rows[row_pos].append(elem)\n    \n    # Extract data from rows (first 5 rows)\n    row_data = []\n    for y in sorted(rows.keys())[:5]:\n        # Sort elements by x-position (left to right)\n        elements = sorted(rows[y], key=lambda e: e.x0)\n        \n        # Show the row\n        row_box = form_region.create_region(\n            min(e.x0 for e in elements), \n            min(e.top for e in elements),\n            max(e.x1 for e in elements),\n            max(e.bottom for e in elements)\n        )\n        row_box.show(color=None, use_color_cycling=True)\n        \n        # Extract text from row\n        row_text = [e.text for e in elements]\n        row_data.append(row_text)\n    \n    # Show the extracted rows\n    row_data\n</pre> # Run layout analysis to find table structures page.analyze_layout()  # Find possible form tables tables = page.find_all('region[type=table]')  if tables:     # Visualize the tables     tables.show(color=\"purple\", label=\"Form Table\")          # Extract data from the first table     first_table = tables[0]     table_data = first_table.extract_table()     table_data else:     # Try to find form-like structure using text alignment     # Create a region where a form might be     form_region = page.create_region(50, 200, page.width - 50, 500)          # Group text by vertical position     rows = {}     text_elements = form_region.find_all('text')          for elem in text_elements:         # Round y-position to group elements in the same row         row_pos = round(elem.top / 5) * 5         if row_pos not in rows:             rows[row_pos] = []         rows[row_pos].append(elem)          # Extract data from rows (first 5 rows)     row_data = []     for y in sorted(rows.keys())[:5]:         # Sort elements by x-position (left to right)         elements = sorted(rows[y], key=lambda e: e.x0)                  # Show the row         row_box = form_region.create_region(             min(e.x0 for e in elements),              min(e.top for e in elements),             max(e.x1 for e in elements),             max(e.bottom for e in elements)         )         row_box.show(color=None, use_color_cycling=True)                  # Extract text from row         row_text = [e.text for e in elements]         row_data.append(row_text)          # Show the extracted rows     row_data <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp3hwvykrk/temp_layout_image.png: 1024x800 2 titles, 3 plain texts, 2 abandons, 1 table, 1520.4ms\n</pre> <pre>Speed: 4.2ms preprocess, 1520.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[8]: Copied! <pre># Combine label-based and pattern-based extraction\nall_fields = {}\n\n# 1. First get fields with explicit labels\nfor label in labels:\n    field_name = label.text.strip().rstrip(':')\n    value = label.right(width=200).extract_text().strip()\n    all_fields[field_name] = value\n\n# 2. Add date fields that we found with pattern matching\nfor date_elem in dates:\n    # Find the nearest label\n    nearby_label = date_elem.nearest('text:contains(\":\")')\n    \n    if nearby_label:\n        # Extract the label text\n        label_text = nearby_label.text.strip().rstrip(':')\n        \n        # Get the date value\n        date_value = re.search(date_pattern, date_elem.text).group(0)\n        \n        # Add to our results if not already present\n        if label_text not in all_fields:\n            all_fields[label_text] = date_value\n\n# Show all extracted fields\nall_fields\n</pre> # Combine label-based and pattern-based extraction all_fields = {}  # 1. First get fields with explicit labels for label in labels:     field_name = label.text.strip().rstrip(':')     value = label.right(width=200).extract_text().strip()     all_fields[field_name] = value  # 2. Add date fields that we found with pattern matching for date_elem in dates:     # Find the nearest label     nearby_label = date_elem.nearest('text:contains(\":\")')          if nearby_label:         # Extract the label text         label_text = nearby_label.text.strip().rstrip(':')                  # Get the date value         date_value = re.search(date_pattern, date_elem.text).group(0)                  # Add to our results if not already present         if label_text not in all_fields:             all_fields[label_text] = date_value  # Show all extracted fields all_fields Out[8]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Violations Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 Violation Count:  7 visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.',\n 'Violation Count': 'Durham\u2019s Meatpacking Chicago, Ill. Worst of any, however, were the fertilizer men, and those who served in the cooking rooms. These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices. Jungle Health and Safety Inspection Service',\n 'Summary': 'Durham\u2019s Meatpacking Chicago, Ill. February 3, 1905 7 These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary  visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in  some of which there were open vats near the level of the floor, their peculiar trouble was that they fell into the vats; and when they were fished out, there was never enough of them left to be worth  exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out to the world as Durham\u2019s Pure Leaf Lard! Description Unsanitary Working Conditions. Inadequate Protective Equipment. Ineffective Injury Prevention. Failure to Properly Store Hazardous Materials. Lack of Adequate Fire Safety Measures. Inadequate Ventilation Systems. Insufficient Employee Training for Safe Work Practices.'}</pre> <p>Form field extraction enables you to automate data entry and document processing. By combining different techniques like label detection, spatial navigation, and pattern matching, you can handle a wide variety of form layouts.</p>"},{"location":"tutorials/10-form-field-extraction/#form-field-extraction","title":"Form Field Extraction\u00b6","text":"<p>Business documents like invoices, forms, and applications contain field-value pairs that need to be extracted. This tutorial shows how to identify and extract these form fields.</p>"},{"location":"tutorials/10-form-field-extraction/#extracting-field-values","title":"Extracting Field Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#visualizing-labels-and-values","title":"Visualizing Labels and Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#handling-multi-line-values","title":"Handling Multi-line Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#finding-pattern-based-fields","title":"Finding Pattern-Based Fields\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#working-with-form-tables","title":"Working with Form Tables\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#combining-different-extraction-techniques","title":"Combining Different Extraction Techniques\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/","title":"Enhanced Table Processing","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\""},{"location":"tutorials/11-enhanced-table-processing/#enhanced-table-processing","title":"Enhanced Table Processing\u00b6","text":"<p>Tables are a common way to present structured data in documents, but they can be challenging to extract correctly. This tutorial demonstrates advanced techniques for working with tables in natural-pdf.</p> <p>TK</p>"},{"location":"tutorials/12-ocr-integration/","title":"OCR Integration for Scanned Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Try extracting text without OCR\ntext_without_ocr = page.extract_text()\nf\"Without OCR: {len(text_without_ocr)} characters extracted\"\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # Try extracting text without OCR text_without_ocr = page.extract_text() f\"Without OCR: {len(text_without_ocr)} characters extracted\" Out[2]: <pre>'Without OCR: 0 characters extracted'</pre> In\u00a0[3]: Copied! <pre># Enable OCR for text extraction\npage.use_ocr = True\n\n# Extract text with OCR enabled\ntext_with_ocr = page.extract_text()\n\n# Preview the extracted text\ntext_with_ocr[:200] + \"...\" if len(text_with_ocr) &gt; 200 else text_with_ocr\n</pre> # Enable OCR for text extraction page.use_ocr = True  # Extract text with OCR enabled text_with_ocr = page.extract_text()  # Preview the extracted text text_with_ocr[:200] + \"...\" if len(text_with_ocr) &gt; 200 else text_with_ocr Out[3]: <pre>''</pre> In\u00a0[4]: Copied! <pre># Convert text-as-image to text elements\npage.apply_ocr()\n\n# Select all text pieces on the page\ntext_elements = page.find_all('text')\nf\"Found {len(text_elements)} text elements\"\n\n# Visualize the elements\ntext_elements.highlight()\n</pre> # Convert text-as-image to text elements page.apply_ocr()  # Select all text pieces on the page text_elements = page.find_all('text') f\"Found {len(text_elements)} text elements\"  # Visualize the elements text_elements.highlight() <pre>2025-04-13T11:48:37.271539Z [warning  ] Using CPU. Note: This module is much faster with a GPU. lineno=71 module=easyocr.easyocr\n</pre> <pre>[2025-04-13 13:48:37,271] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n</pre> Out[4]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre> In\u00a0[5]: Copied! <pre># Set OCR configuration for better results\npage.ocr_config = {\n    'language': 'eng',  # English\n    'dpi': 300,         # Higher resolution\n}\n\n# Extract text with the improved configuration\nimproved_text = page.extract_text()\n\n# Preview the text\nimproved_text[:200] + \"...\" if len(improved_text) &gt; 200 else improved_text\n</pre> # Set OCR configuration for better results page.ocr_config = {     'language': 'eng',  # English     'dpi': 300,         # Higher resolution }  # Extract text with the improved configuration improved_text = page.extract_text()  # Preview the text improved_text[:200] + \"...\" if len(improved_text) &gt; 200 else improved_text Out[5]: <pre>''</pre> In\u00a0[6]: Copied! <pre># Configure for multiple languages\npage.ocr_config = {\n    'language': 'eng+fra+deu',  # English, French, German\n    'dpi': 300\n}\n\n# Extract text with multi-language support\nmultilang_text = page.extract_text()\nmultilang_text[:200]\n</pre> # Configure for multiple languages page.ocr_config = {     'language': 'eng+fra+deu',  # English, French, German     'dpi': 300 }  # Extract text with multi-language support multilang_text = page.extract_text() multilang_text[:200] Out[6]: <pre>''</pre> In\u00a0[7]: Copied! <pre># Enable OCR and analyze the document layout\npage.use_ocr = True\npage.analyze_layout()\n\n# Find table regions\ntable_regions = page.find_all('region[type=table]')\n\n# Visualize any detected tables\ntable_regions.highlight()\n\n# Extract the first table if found\nif table_regions:\n    table_data = table_regions[0].extract_table()\n    table_data\nelse:\n    \"No tables found in the document\"\n</pre> # Enable OCR and analyze the document layout page.use_ocr = True page.analyze_layout()  # Find table regions table_regions = page.find_all('region[type=table]')  # Visualize any detected tables table_regions.highlight()  # Extract the first table if found if table_regions:     table_data = table_regions[0].extract_table()     table_data else:     \"No tables found in the document\" <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpdtf16bnu/temp_layout_image.png: 1024x800 2 titles, 2 plain texts, 3 abandons, 1 table, 1675.0ms\n</pre> <pre>Speed: 5.2ms preprocess, 1675.0ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[8]: Copied! <pre># Look for potential form labels (containing a colon)\nlabels = page.find_all('text:contains(\":\")') \n\n# Visualize the labels\nlabels.highlight()\n\n# Extract form data by looking to the right of each label\nform_data = {}\nfor label in labels:\n    # Clean the label text\n    field_name = label.text.strip().rstrip(':')\n    \n    # Find the value to the right\n    value_element = label.right(width=200)\n    value = value_element.extract_text().strip()\n    \n    # Add to our dictionary\n    form_data[field_name] = value\n\n# Display the extracted data\nform_data\n</pre> # Look for potential form labels (containing a colon) labels = page.find_all('text:contains(\":\")')   # Visualize the labels labels.highlight()  # Extract form data by looking to the right of each label form_data = {} for label in labels:     # Clean the label text     field_name = label.text.strip().rstrip(':')          # Find the value to the right     value_element = label.right(width=200)     value = value_element.extract_text().strip()          # Add to our dictionary     form_data[field_name] = value  # Display the extracted data form_data Out[8]: <pre>{}</pre> In\u00a0[9]: Copied! <pre># Apply OCR and analyze layout\npage.use_ocr = True\npage.analyze_layout()\n\n# Find document structure elements\nheadings = page.find_all('region[type=heading]')\nparagraphs = page.find_all('region[type=paragraph]')\n\n# Visualize the structure\nheadings.highlight(color=\"red\", label=\"Headings\")\nparagraphs.highlight(color=\"blue\", label=\"Paragraphs\")\n\n# Create a simple document outline\ndocument_outline = []\nfor heading in headings:\n    heading_text = heading.extract_text()\n    document_outline.append(heading_text)\n\ndocument_outline\n</pre> # Apply OCR and analyze layout page.use_ocr = True page.analyze_layout()  # Find document structure elements headings = page.find_all('region[type=heading]') paragraphs = page.find_all('region[type=paragraph]')  # Visualize the structure headings.highlight(color=\"red\", label=\"Headings\") paragraphs.highlight(color=\"blue\", label=\"Paragraphs\")  # Create a simple document outline document_outline = [] for heading in headings:     heading_text = heading.extract_text()     document_outline.append(heading_text)  document_outline <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpvn6u02y_/temp_layout_image.png: 1024x800 2 titles, 2 plain texts, 3 abandons, 1 table, 1609.8ms\n</pre> <pre>Speed: 4.5ms preprocess, 1609.8ms inference, 0.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[9]: <pre>[]</pre> In\u00a0[10]: Copied! <pre># Process all pages in the document\nall_text = []\n\nfor i, page in enumerate(pdf.pages):\n    # Enable OCR for each page\n    page.use_ocr = True\n    \n    # Extract text\n    page_text = page.extract_text()\n    \n    # Add to our collection with page number\n    all_text.append(f\"Page {i+1}: {page_text[:100]}...\")\n\n# Show the first few pages\nall_text\n</pre> # Process all pages in the document all_text = []  for i, page in enumerate(pdf.pages):     # Enable OCR for each page     page.use_ocr = True          # Extract text     page_text = page.extract_text()          # Add to our collection with page number     all_text.append(f\"Page {i+1}: {page_text[:100]}...\")  # Show the first few pages all_text Out[10]: <pre>['Page 1: ...']</pre> In\u00a0[11]: Copied! <pre>from natural_pdf import PDF\n\ninput_pdf_path = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\"\n\npdf = PDF(input_pdf_path)\npdf.apply_ocr() \n\npdf.save_searchable(\"needs-ocr-searchable.pdf\")\n</pre> from natural_pdf import PDF  input_pdf_path = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\"  pdf = PDF(input_pdf_path) pdf.apply_ocr()   pdf.save_searchable(\"needs-ocr-searchable.pdf\") <pre>2025-04-13T11:48:51.998243Z [warning  ] Using CPU. Note: This module is much faster with a GPU. lineno=71 module=easyocr.easyocr\n</pre> <pre>[2025-04-13 13:48:51,998] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>2025-04-13T11:49:02.131991Z [warning  ] Page 1 has no OCR elements (text[source=ocr]) to generate hOCR from. lineno=86 module=natural_pdf.exporters.searchable_pdf pageno=None\n</pre> <pre>[2025-04-13 13:49:02,131] [ WARNING] searchable_pdf.py:86 - Page 1 has no OCR elements (text[source=ocr]) to generate hOCR from.\n</pre> <p>This creates <code>needs-ocr-searchable.pdf</code>, which looks identical to the original but now has a text layer corresponding to the OCR results. You can adjust the rendering resolution used during saving with the <code>dpi</code> parameter (default is 300).</p> <p>OCR integration enables you to work with scanned documents, historical archives, and image-based PDFs that don't have embedded text. By combining OCR with natural-pdf's layout analysis capabilities, you can turn any document into structured, searchable data.</p>"},{"location":"tutorials/12-ocr-integration/#ocr-integration-for-scanned-documents","title":"OCR Integration for Scanned Documents\u00b6","text":"<p>Optical Character Recognition (OCR) allows you to extract text from scanned documents where the text isn't embedded in the PDF. This tutorial demonstrates how to work with scanned documents.</p>"},{"location":"tutorials/12-ocr-integration/#enabling-ocr","title":"Enabling OCR\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#finding-text-elements-with-ocr","title":"Finding Text Elements with OCR\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#ocr-configuration-options","title":"OCR Configuration Options\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#working-with-multi-language-documents","title":"Working with Multi-language Documents\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#extracting-tables-from-scanned-documents","title":"Extracting Tables from Scanned Documents\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#finding-form-fields-in-scanned-documents","title":"Finding Form Fields in Scanned Documents\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#combining-ocr-with-layout-analysis","title":"Combining OCR with Layout Analysis\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":""},{"location":"tutorials/12-ocr-integration/#saving-pdfs-with-searchable-text","title":"Saving PDFs with Searchable Text\u00b6","text":"<p>After applying OCR to a PDF, you can save a new version of the PDF where the recognized text is embedded as an invisible layer. This makes the text searchable and copyable in standard PDF viewers.</p> <p>Use the <code>save_searchable()</code> method on the <code>PDF</code> object:</p>"},{"location":"tutorials/13-semantic-search/","title":"Semantic Search Across Multiple Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n#%pip install \"natural-pdf[search]\"  # Ensure search dependencies are installed\n</pre> #%pip install \"natural-pdf[all]\" #%pip install \"natural-pdf[search]\"  # Ensure search dependencies are installed In\u00a0[2]: Copied! <pre>import logging\nimport natural_pdf\n\n# Optional: Configure logging to see progress\nnatural_pdf.configure_logging(level=logging.INFO)\n\n# Define the paths to your PDF files\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n    # Add more PDF paths as needed\n]\n\n# Create a PDFCollection\ncollection = natural_pdf.PDFCollection(pdf_paths)\nprint(f\"Created collection with {len(collection.pdfs)} PDFs.\")\n</pre> import logging import natural_pdf  # Optional: Configure logging to see progress natural_pdf.configure_logging(level=logging.INFO)  # Define the paths to your PDF files pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"     # Add more PDF paths as needed ]  # Create a PDFCollection collection = natural_pdf.PDFCollection(pdf_paths) print(f\"Created collection with {len(collection.pdfs)} PDFs.\") <pre>natural_pdf.collections.pdf_collection - INFO - Initializing 2 PDF objects...\n</pre> <pre>2025-04-13T11:47:49.658337Z [info     ] Initializing 2 PDF objects...  lineno=145 message=Initializing 2 PDF objects... module=natural_pdf.collections.pdf_collection\n</pre> <pre>[2025-04-13 13:47:49,657] [    INFO] pdf_collection.py:145 - Initializing 2 PDF objects...\n</pre> <pre>\rLoading PDFs:   0%|                                              | 0/2 [00:00&lt;?, ?it/s]</pre> <pre>natural_pdf.core.pdf - INFO - Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\n</pre> <pre>2025-04-13T11:47:49.675516Z [info     ] Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf lineno=80 message=Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:49,675] [    INFO] pdf.py:80 - Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf\n</pre> <pre>2025-04-13T11:47:49.909076Z [info     ] PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf lineno=93 message=PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:49,908] [    INFO] pdf.py:93 - PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf\n</pre> <pre>2025-04-13T11:47:49.909730Z [info     ] Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf lineno=106 message=Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:49,909] [    INFO] pdf.py:106 - Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf\n</pre> <pre>natural_pdf.ocr.ocr_manager - INFO - OCRManager initialized.\n</pre> <pre>2025-04-13T11:47:49.911378Z [info     ] OCRManager initialized.        lineno=38 message=OCRManager initialized. module=natural_pdf.ocr.ocr_manager\n</pre> <pre>[2025-04-13 13:47:49,911] [    INFO] ocr_manager.py:38 - OCRManager initialized.\n</pre> <pre>natural_pdf.analyzers.layout.layout_manager - INFO - LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling']\n</pre> <pre>2025-04-13T11:47:49.912104Z [info     ] LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling'] lineno=68 message=LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling'] module=natural_pdf.analyzers.layout.layout_manager\n</pre> <pre>[2025-04-13 13:47:49,911] [    INFO] layout_manager.py:68 - LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling']\n</pre> <pre>natural_pdf.core.highlighting_service - INFO - HighlightingService initialized with ColorManager.\n</pre> <pre>2025-04-13T11:47:49.912762Z [info     ] HighlightingService initialized with ColorManager. lineno=286 message=HighlightingService initialized with ColorManager. module=natural_pdf.core.highlighting_service\n</pre> <pre>[2025-04-13 13:47:49,912] [    INFO] highlighting_service.py:286 - HighlightingService initialized with ColorManager.\n</pre> <pre>natural_pdf.core.pdf - INFO - Initialized HighlightingService.\n</pre> <pre>2025-04-13T11:47:49.914736Z [info     ] Initialized HighlightingService. lineno=141 message=Initialized HighlightingService. module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:49,914] [    INFO] pdf.py:141 - Initialized HighlightingService.\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf' initialized with 1 pages.\n</pre> <pre>2025-04-13T11:47:49.915592Z [info     ] PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf' initialized with 1 pages. lineno=142 message=PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf' initialized with 1 pages. module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:49,915] [    INFO] pdf.py:142 - PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf' initialized with 1 pages.\n</pre> <pre>\rLoading PDFs:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   | 1/2 [00:00&lt;00:00,  4.15it/s]</pre> <pre>natural_pdf.core.pdf - INFO - Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n</pre> <pre>2025-04-13T11:47:49.916368Z [info     ] Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf lineno=80 message=Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:49,916] [    INFO] pdf.py:80 - Downloading PDF from URL: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n</pre> <pre>2025-04-13T11:47:50.258874Z [info     ] PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf lineno=93 message=PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:50,258] [    INFO] pdf.py:93 - PDF downloaded to temporary file: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n</pre> <pre>natural_pdf.core.pdf - INFO - Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n</pre> <pre>2025-04-13T11:47:50.260105Z [info     ] Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf lineno=106 message=Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:50,259] [    INFO] pdf.py:106 - Initializing PDF from /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n</pre> <pre>natural_pdf.ocr.ocr_manager - INFO - OCRManager initialized.\n</pre> <pre>2025-04-13T11:47:50.261983Z [info     ] OCRManager initialized.        lineno=38 message=OCRManager initialized. module=natural_pdf.ocr.ocr_manager\n</pre> <pre>[2025-04-13 13:47:50,261] [    INFO] ocr_manager.py:38 - OCRManager initialized.\n</pre> <pre>natural_pdf.analyzers.layout.layout_manager - INFO - LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling']\n</pre> <pre>2025-04-13T11:47:50.262781Z [info     ] LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling'] lineno=68 message=LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling'] module=natural_pdf.analyzers.layout.layout_manager\n</pre> <pre>[2025-04-13 13:47:50,262] [    INFO] layout_manager.py:68 - LayoutManager initialized. Available engines: ['yolo', 'tatr', 'paddle', 'surya', 'docling']\n</pre> <pre>natural_pdf.core.highlighting_service - INFO - HighlightingService initialized with ColorManager.\n</pre> <pre>2025-04-13T11:47:50.263666Z [info     ] HighlightingService initialized with ColorManager. lineno=286 message=HighlightingService initialized with ColorManager. module=natural_pdf.core.highlighting_service\n</pre> <pre>[2025-04-13 13:47:50,263] [    INFO] highlighting_service.py:286 - HighlightingService initialized with ColorManager.\n</pre> <pre>natural_pdf.core.pdf - INFO - Initialized HighlightingService.\n</pre> <pre>2025-04-13T11:47:50.267510Z [info     ] Initialized HighlightingService. lineno=141 message=Initialized HighlightingService. module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:50,267] [    INFO] pdf.py:141 - Initialized HighlightingService.\n</pre> <pre>natural_pdf.core.pdf - INFO - PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf' initialized with 5 pages.\n</pre> <pre>2025-04-13T11:47:50.268089Z [info     ] PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf' initialized with 5 pages. lineno=142 message=PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf' initialized with 5 pages. module=natural_pdf.core.pdf\n</pre> <pre>[2025-04-13 13:47:50,267] [    INFO] pdf.py:142 - PDF 'https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf' initialized with 5 pages.\n</pre> <pre>\rLoading PDFs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  3.26it/s]</pre> <pre>\rLoading PDFs: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00&lt;00:00,  3.37it/s]</pre> <pre>\nnatural_pdf.collections.pdf_collection - INFO - Successfully initialized 2 PDFs. Failed: 0\n</pre> <pre>2025-04-13T11:47:50.269456Z [info     ] Successfully initialized 2 PDFs. Failed: 0 lineno=154 message=Successfully initialized 2 PDFs. Failed: 0 module=natural_pdf.collections.pdf_collection\n</pre> <pre>[2025-04-13 13:47:50,269] [    INFO] pdf_collection.py:154 - Successfully initialized 2 PDFs. Failed: 0\n</pre> <pre>Created collection with 2 PDFs.\n</pre> In\u00a0[3]: Copied! <pre># Initialize search. 'index=True' builds the index immediately.\n# This might take some time depending on the number and size of PDFs.\ncollection.init_search(index=True) \nprint(\"Search index initialized.\")\n</pre> # Initialize search. 'index=True' builds the index immediately. # This might take some time depending on the number and size of PDFs. collection.init_search(index=True)  print(\"Search index initialized.\") <pre>natural_pdf.search.searchable_mixin - INFO - Using default collection name 'default_collection' for in-memory service.\n</pre> <pre>2025-04-13T11:47:50.276848Z [info     ] Using default collection name 'default_collection' for in-memory service. lineno=104 message=Using default collection name 'default_collection' for in-memory service. module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:50,276] [    INFO] searchable_mixin.py:104 - Using default collection name 'default_collection' for in-memory service.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Creating new SearchService: name='default_collection', persist=False, model=default\n</pre> <pre>2025-04-13T11:47:50.277734Z [info     ] Creating new SearchService: name='default_collection', persist=False, model=default lineno=106 message=Creating new SearchService: name='default_collection', persist=False, model=default module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:50,277] [    INFO] searchable_mixin.py:106 - Creating new SearchService: name='default_collection', persist=False, model=default\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - HaystackSearchService initialized for collection='default_collection' (persist=False, model='sentence-transformers/all-MiniLM-L6-v2'). Default path: './natural_pdf_index'\n</pre> <pre>2025-04-13T11:47:50.278641Z [info     ] HaystackSearchService initialized for collection='default_collection' (persist=False, model='sentence-transformers/all-MiniLM-L6-v2'). Default path: './natural_pdf_index' lineno=106 message=HaystackSearchService initialized for collection='default_collection' (persist=False, model='sentence-transformers/all-MiniLM-L6-v2'). Default path: './natural_pdf_index' module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:50,278] [    INFO] haystack_search_service.py:106 - HaystackSearchService initialized for collection='default_collection' (persist=False, model='sentence-transformers/all-MiniLM-L6-v2'). Default path: './natural_pdf_index'\n</pre> <pre>natural_pdf.search - INFO - Created new HaystackSearchService instance for collection 'default_collection'.\n</pre> <pre>2025-04-13T11:47:50.279470Z [info     ] Created new HaystackSearchService instance for collection 'default_collection'. lineno=80 message=Created new HaystackSearchService instance for collection 'default_collection'. module=natural_pdf.search\n</pre> <pre>[2025-04-13 13:47:50,279] [    INFO] __init__.py:80 - Created new HaystackSearchService instance for collection 'default_collection'.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - index=True: Proceeding to index collection immediately after search initialization.\n</pre> <pre>2025-04-13T11:47:50.280210Z [info     ] index=True: Proceeding to index collection immediately after search initialization. lineno=141 message=index=True: Proceeding to index collection immediately after search initialization. module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:50,279] [    INFO] searchable_mixin.py:141 - index=True: Proceeding to index collection immediately after search initialization.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Starting internal indexing process into SearchService collection 'default_collection'...\n</pre> <pre>2025-04-13T11:47:50.280887Z [info     ] Starting internal indexing process into SearchService collection 'default_collection'... lineno=152 message=Starting internal indexing process into SearchService collection 'default_collection'... module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:50,280] [    INFO] searchable_mixin.py:152 - Starting internal indexing process into SearchService collection 'default_collection'...\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Prepared 6 indexable items for indexing.\n</pre> <pre>2025-04-13T11:47:50.281489Z [info     ] Prepared 6 indexable items for indexing. lineno=165 message=Prepared 6 indexable items for indexing. module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:50,281] [    INFO] searchable_mixin.py:165 - Prepared 6 indexable items for indexing.\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Index request for collection='default_collection', docs=6, model='sentence-transformers/all-MiniLM-L6-v2', force=False, persist=False\n</pre> <pre>2025-04-13T11:47:50.282015Z [info     ] Index request for collection='default_collection', docs=6, model='sentence-transformers/all-MiniLM-L6-v2', force=False, persist=False lineno=210 message=Index request for collection='default_collection', docs=6, model='sentence-transformers/all-MiniLM-L6-v2', force=False, persist=False module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:50,281] [    INFO] haystack_search_service.py:210 - Index request for collection='default_collection', docs=6, model='sentence-transformers/all-MiniLM-L6-v2', force=False, persist=False\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Created SentenceTransformersDocumentEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None)\n</pre> <pre>2025-04-13T11:47:53.718673Z [info     ] Created SentenceTransformersDocumentEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None) lineno=146 message=Created SentenceTransformersDocumentEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None) module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:53,718] [    INFO] haystack_search_service.py:146 - Created SentenceTransformersDocumentEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None)\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Preparing Haystack Documents from 6 indexable items...\n</pre> <pre>2025-04-13T11:47:53.719838Z [info     ] Preparing Haystack Documents from 6 indexable items... lineno=241 message=Preparing Haystack Documents from 6 indexable items... module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:53,719] [    INFO] haystack_search_service.py:241 - Preparing Haystack Documents from 6 indexable items...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Embedding 6 documents using 'sentence-transformers/all-MiniLM-L6-v2'...\n</pre> <pre>2025-04-13T11:47:54.243363Z [info     ] Embedding 6 documents using 'sentence-transformers/all-MiniLM-L6-v2'... lineno=281 message=Embedding 6 documents using 'sentence-transformers/all-MiniLM-L6-v2'... module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,242] [    INFO] haystack_search_service.py:281 - Embedding 6 documents using 'sentence-transformers/all-MiniLM-L6-v2'...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Successfully embedded 6 documents.\n</pre> <pre>2025-04-13T11:47:54.587734Z [info     ] Successfully embedded 6 documents. lineno=286 message=Successfully embedded 6 documents. module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,587] [    INFO] haystack_search_service.py:286 - Successfully embedded 6 documents.\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Writing 6 embedded documents to store 'default_collection'...\n</pre> <pre>2025-04-13T11:47:54.588580Z [info     ] Writing 6 embedded documents to store 'default_collection'... lineno=302 message=Writing 6 embedded documents to store 'default_collection'... module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,588] [    INFO] haystack_search_service.py:302 - Writing 6 embedded documents to store 'default_collection'...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Successfully wrote 6 documents to store 'default_collection'.\n</pre> <pre>2025-04-13T11:47:54.590136Z [info     ] Successfully wrote 6 documents to store 'default_collection'. lineno=308 message=Successfully wrote 6 documents to store 'default_collection'. module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,589] [    INFO] haystack_search_service.py:308 - Successfully wrote 6 documents to store 'default_collection'.\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Store 'default_collection' document count after write: 6\n</pre> <pre>2025-04-13T11:47:54.590961Z [info     ] Store 'default_collection' document count after write: 6 lineno=310 message=Store 'default_collection' document count after write: 6 module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,590] [    INFO] haystack_search_service.py:310 - Store 'default_collection' document count after write: 6\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - Successfully completed indexing into SearchService collection 'default_collection'.\n</pre> <pre>2025-04-13T11:47:54.591611Z [info     ] Successfully completed indexing into SearchService collection 'default_collection'. lineno=173 message=Successfully completed indexing into SearchService collection 'default_collection'. module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:54,591] [    INFO] searchable_mixin.py:173 - Successfully completed indexing into SearchService collection 'default_collection'.\n</pre> <pre>Search index initialized.\n</pre> In\u00a0[4]: Copied! <pre># Perform a search query\nquery = \"american president\"\nresults = collection.find_relevant(query)\n\nprint(f\"Found {len(results)} results for '{query}':\")\n</pre> # Perform a search query query = \"american president\" results = collection.find_relevant(query)  print(f\"Found {len(results)} results for '{query}':\") <pre>natural_pdf.search.searchable_mixin - INFO - Searching collection 'default_collection' via HaystackSearchService...\n</pre> <pre>2025-04-13T11:47:54.597061Z [info     ] Searching collection 'default_collection' via HaystackSearchService... lineno=244 message=Searching collection 'default_collection' via HaystackSearchService... module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:54,596] [    INFO] searchable_mixin.py:244 - Searching collection 'default_collection' via HaystackSearchService...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Search request for collection='default_collection', query_type=str, options=TextSearchOptions(top_k=10, retriever_top_k=20, filters=None, use_reranker=True, reranker_instance=None, reranker_model=None, reranker_api_key=None)\n</pre> <pre>2025-04-13T11:47:54.597645Z [info     ] Search request for collection='default_collection', query_type=str, options=TextSearchOptions(top_k=10, retriever_top_k=20, filters=None, use_reranker=True, reranker_instance=None, reranker_model=None, reranker_api_key=None) lineno=318 message=Search request for collection='default_collection', query_type=str, options=TextSearchOptions(top_k=10, retriever_top_k=20, filters=None, use_reranker=True, reranker_instance=None, reranker_model=None, reranker_api_key=None) module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,597] [    INFO] haystack_search_service.py:318 - Search request for collection='default_collection', query_type=str, options=TextSearchOptions(top_k=10, retriever_top_k=20, filters=None, use_reranker=True, reranker_instance=None, reranker_model=None, reranker_api_key=None)\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Created SentenceTransformersTextEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None)\n</pre> <pre>2025-04-13T11:47:54.598345Z [info     ] Created SentenceTransformersTextEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None) lineno=164 message=Created SentenceTransformersTextEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None) module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,598] [    INFO] haystack_search_service.py:164 - Created SentenceTransformersTextEmbedder. Model: sentence-transformers/all-MiniLM-L6-v2, Device: ComponentDevice(_single_device=Device(type=&lt;DeviceType.MPS: 'mps'&gt;, id=None), _multiple_devices=None)\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Running retrieval pipeline for collection 'default_collection'...\n</pre> <pre>2025-04-13T11:47:54.727173Z [info     ] Running retrieval pipeline for collection 'default_collection'... lineno=401 message=Running retrieval pipeline for collection 'default_collection'... module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,726] [    INFO] haystack_search_service.py:401 - Running retrieval pipeline for collection 'default_collection'...\n</pre> <pre>natural_pdf.search.haystack_search_service - INFO - Retrieved 6 documents.\n</pre> <pre>2025-04-13T11:47:54.731304Z [info     ] Retrieved 6 documents.         lineno=410 message=Retrieved 6 documents. module=natural_pdf.search.haystack_search_service\n</pre> <pre>[2025-04-13 13:47:54,730] [    INFO] haystack_search_service.py:410 - Retrieved 6 documents.\n</pre> <pre>natural_pdf.search.searchable_mixin - INFO - SearchService returned 6 results from collection 'default_collection'.\n</pre> <pre>2025-04-13T11:47:54.731883Z [info     ] SearchService returned 6 results from collection 'default_collection'. lineno=266 message=SearchService returned 6 results from collection 'default_collection'. module=natural_pdf.search.searchable_mixin\n</pre> <pre>[2025-04-13 13:47:54,731] [    INFO] searchable_mixin.py:266 - SearchService returned 6 results from collection 'default_collection'.\n</pre> <pre>Found 6 results for 'american president':\n</pre> In\u00a0[5]: Copied! <pre># Process and display the results\nif results:\n    for i, result in enumerate(results):\n        print(f\"  {i+1}. PDF: {result['pdf_path']}\")\n        print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")\n        # Display a snippet of the content\n        snippet = result.get('content_snippet', '')\n        print(f\"     Snippet: {snippet}...\") \nelse:\n    print(\"  No relevant results found.\")\n\n# You can access the full content if needed via the result object, \n# though 'content_snippet' is usually sufficient for display.\n</pre> # Process and display the results if results:     for i, result in enumerate(results):         print(f\"  {i+1}. PDF: {result['pdf_path']}\")         print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")         # Display a snippet of the content         snippet = result.get('content_snippet', '')         print(f\"     Snippet: {snippet}...\")  else:     print(\"  No relevant results found.\")  # You can access the full content if needed via the result object,  # though 'content_snippet' is usually sufficient for display. <pre>  1. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n     Page: 2 (Score: 0.0708)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nThe Anasazi (Removed: 1)\nAuthor: Petersen, David. ISBN: 0-516-01121-9 (trade) Published: 1991\nSit...\n  2. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n     Page: 5 (Score: 0.0669)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000562167 $13.10 11/5/1999 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  3. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmplduy73q3.pdf\n     Page: 1 (Score: -0.0040)\n     Snippet: Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking  Chicago, Ill.\nDate:  February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer m...\n  4. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n     Page: 4 (Score: -0.0245)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nChildren of the Philippines (Removed: 1)\nAuthor: Kinkade, Sheila, 1962- ISBN: 0-87614-993-X Publi...\n  5. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n     Page: 3 (Score: -0.0445)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000507600 $19.45 2/21/2000 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  6. PDF: /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp__h0cd9h.pdf\n     Page: 1 (Score: -0.0473)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/12/2023 - Copies Removed: 2\nTristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-...\n</pre> <p>Semantic search allows you to efficiently query large sets of documents to find the most relevant information without needing exact keyword matches, leveraging the meaning and context of your query.</p>"},{"location":"tutorials/13-semantic-search/#semantic-search-across-multiple-documents","title":"Semantic Search Across Multiple Documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to find information relevant to a specific query across all documents, not just within a single one. This tutorial demonstrates how to perform semantic search over a <code>PDFCollection</code>.</p>"},{"location":"tutorials/13-semantic-search/#initializing-the-search-index","title":"Initializing the Search Index\u00b6","text":"<p>Before performing a search, you need to initialize the search capabilities for the collection. This involves processing the documents and building an index.</p>"},{"location":"tutorials/13-semantic-search/#performing-a-semantic-search","title":"Performing a Semantic Search\u00b6","text":"<p>Once the index is ready, you can use the <code>find_relevant()</code> method to search for content semantically related to your query.</p>"},{"location":"tutorials/13-semantic-search/#understanding-search-results","title":"Understanding Search Results\u00b6","text":"<p>The <code>find_relevant()</code> method returns a list of dictionaries, each representing a relevant text chunk found in one of the PDFs. Each result includes:</p> <ul> <li><code>pdf_path</code>: The path to the PDF document where the result was found.</li> <li><code>page_number</code>: The page number within the PDF.</li> <li><code>score</code>: A relevance score (higher means more relevant).</li> <li><code>content_snippet</code>: A snippet of the text chunk that matched the query.</li> </ul>"},{"location":"visual-debugging/","title":"Visual Debugging","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find a specific element and add a persistent highlight\npage.find_all('text:contains(\"Summary\")').highlight()\npage.find_all('text:contains(\"Date\")').highlight()\npage.find_all('line').highlight()\npage.to_image(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find a specific element and add a persistent highlight page.find_all('text:contains(\"Summary\")').highlight() page.find_all('text:contains(\"Date\")').highlight() page.find_all('line').highlight() page.to_image(width=700) Out[1]: In\u00a0[2]: Copied! <pre>page.clear_highlights()\n\ntitle = page.find('text:bold[size&gt;=12]')\n\n# Highlight with a specific color (string name, hex, or RGB/RGBA tuple)\n# title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity\n# title.highlight(color=\"#FF0000\")        # Hex color\ntitle.highlight(color=\"red\")           # Color name\n\ntext = page.find('text:contains(\"Critical\")')\n\n# Add a label to the highlight (appears in legend)\ntext.highlight(label=\"Critical\")\n\n# Combine color and label\nrect = page.find('rect')\nrect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")\n\npage.to_image(width=700)\n</pre> page.clear_highlights()  title = page.find('text:bold[size&gt;=12]')  # Highlight with a specific color (string name, hex, or RGB/RGBA tuple) # title.highlight(color=(1, 0, 0, 0.3))  # Red with 30% opacity # title.highlight(color=\"#FF0000\")        # Hex color title.highlight(color=\"red\")           # Color name  text = page.find('text:contains(\"Critical\")')  # Add a label to the highlight (appears in legend) text.highlight(label=\"Critical\")  # Combine color and label rect = page.find('rect') rect.highlight(color=(0, 0, 1, 0.2), label=\"Box\")  page.to_image(width=700) Out[2]: In\u00a0[3]: Copied! <pre># Find and highlight all headings with a single color/label\nheadings = page.find_all('text[size&gt;=14]:bold')\nheadings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")\n\n# Find and highlight all tables\ntables = page.find_all('region[type=table]')\ntables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")\n\n# View the result\npage.viewer()\n</pre> # Find and highlight all headings with a single color/label headings = page.find_all('text[size&gt;=14]:bold') headings.highlight(color=(0, 0.5, 0, 0.3), label=\"Headings\")  # Find and highlight all tables tables = page.find_all('region[type=table]') tables.highlight(color=(0, 0, 1, 0.2), label=\"Tables\")  # View the result page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[3]: In\u00a0[4]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Highlight the region\ncontent.show()\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Highlight the region content.show() Out[4]: <p>Or look at just the region by itself</p> In\u00a0[5]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Crop to the region\ncontent.to_image(crop_only=True, include_highlights=False)\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Crop to the region content.to_image(crop_only=True, include_highlights=False) Out[5]: In\u00a0[6]: Copied! <pre># Analyze and highlight text styles\npage.clear_highlights()\n\npage.analyze_text_styles()\npage.find_all('text').highlight(group_by='style_label')\n\npage.to_image(width=700)\n</pre> # Analyze and highlight text styles page.clear_highlights()  page.analyze_text_styles() page.find_all('text').highlight(group_by='style_label')  page.to_image(width=700) Out[6]: In\u00a0[7]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\ntext = page.find_all('line')\ntext.highlight(include_attrs=['width', 'color'])\n\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  text = page.find_all('line') text.highlight(include_attrs=['width', 'color'])  page.to_image(width=700) Out[7]: <p>Does it get busy? YES.</p> In\u00a0[8]: Copied! <pre># Clear all highlights on the page\npage.clear_highlights()\n\n# Apply new highlights\npage.find_all('text:bold').highlight(label=\"Bold Text\")\npage.viewer()\n</pre> # Clear all highlights on the page page.clear_highlights()  # Apply new highlights page.find_all('text:bold').highlight(label=\"Bold Text\") page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[8]: In\u00a0[9]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\")\npage = pdf.pages[0]\npage.to_image(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\") page = pdf.pages[0] page.to_image(width=700) Out[9]: In\u00a0[10]: Copied! <pre>response = page.ask(\"How many votes did Kamala Harris get on Election Day?\")\nresponse\n</pre> response = page.ask(\"How many votes did Kamala Harris get on Election Day?\") response <pre>Device set to use cpu\n</pre> Out[10]: <pre>{'answer': '60',\n 'confidence': 0.3211139440536499,\n 'start': 31,\n 'end': 31,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[11]: Copied! <pre>response['source_elements'].show()\n</pre> response['source_elements'].show() Out[11]:"},{"location":"visual-debugging/#visual-debugging","title":"Visual Debugging\u00b6","text":"<p>Sometimes it's hard to understand what's happening when working with PDFs. Natural PDF provides powerful visual debugging tools to help you see what you're extracting.</p>"},{"location":"visual-debugging/#adding-persistent-highlights","title":"Adding Persistent Highlights\u00b6","text":"<p>Use the <code>.highlight()</code> method on <code>Element</code> or <code>ElementCollection</code> objects to add persistent highlights to a page. These highlights are stored and will appear when viewing the page later.</p>"},{"location":"visual-debugging/#customizing-persistent-highlights","title":"Customizing Persistent Highlights\u00b6","text":"<p>Customize the appearance of persistent highlights added with <code>.highlight()</code>:</p>"},{"location":"visual-debugging/#highlighting-multiple-elements","title":"Highlighting Multiple Elements\u00b6","text":"<p>Highlighting an <code>ElementCollection</code> applies the highlight to all elements within it. By default, all elements in the collection get the same color and a label based on their type.</p>"},{"location":"visual-debugging/#highlighting-regions","title":"Highlighting Regions\u00b6","text":"<p>You can highlight regions to see what area you're working with:</p>"},{"location":"visual-debugging/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>Visualize text styles to understand the document structure:</p>"},{"location":"visual-debugging/#displaying-attributes","title":"Displaying Attributes\u00b6","text":"<p>You can display element attributes directly on the highlights:</p>"},{"location":"visual-debugging/#clearing-highlights","title":"Clearing Highlights\u00b6","text":"<p>You can clear persistent highlights from a page:</p>"},{"location":"visual-debugging/#document-qa-visualization","title":"Document QA Visualization\u00b6","text":"<p>Visualize document QA results:</p>"},{"location":"visual-debugging/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to visualize PDF content, you might want to explore:</p> <ul> <li>OCR capabilities for working with scanned documents</li> <li>Layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"}]}