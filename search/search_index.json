{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Natural PDF","text":"<p>A friendly library for working with PDFs, built on top of pdfplumber.</p> <p>Natural PDF lets you find and extract content from PDFs using simple code that makes sense.</p> <ul> <li>Live demo here</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install natural_pdf\n# All the extras\npip install \"natural_pdf[all]\"\n</code></pre>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from natural_pdf import PDF\n\npdf = PDF('https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf')\npage = pdf.pages[0]\n\n# Find the title and get content below it\ntitle = page.find('text:contains(\"Summary\"):bold')\ncontent = title.below().extract_text()\n\n# Exclude everything above 'CONFIDENTIAL' and below last line on page\npage.add_exclusion(page.find('text:contains(\"CONFIDENTIAL\")').above())\npage.add_exclusion(page.find_all('line')[-1].below())\n\n# Get the clean text without header/footer\nclean_text = page.extract_text()\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#new-to-natural-pdf","title":"New to Natural PDF?","text":"<ul> <li>Installation - Get Natural PDF installed and run your first extraction</li> <li>Quick Reference - Essential commands and patterns in one place</li> <li>Tutorial Series - Step-by-step learning path through all features</li> </ul>"},{"location":"#learning-the-basics","title":"Learning the Basics","text":"<p>Follow the tutorial series to learn Natural PDF systematically:</p> <ol> <li>Loading and Basic Text Extraction</li> <li>Finding Specific Elements</li> <li>Extracting Content Blocks</li> <li>Table Extraction</li> <li>Excluding Unwanted Content</li> <li>Document Question Answering</li> <li>Layout Analysis</li> <li>Spatial Navigation</li> <li>Section Extraction</li> <li>Form Field Extraction</li> <li>Enhanced Table Processing</li> <li>OCR Integration</li> <li>Semantic Search</li> <li>Categorizing Documents</li> </ol>"},{"location":"#solving-specific-problems","title":"Solving Specific Problems","text":""},{"location":"#text-extraction-issues","title":"Text Extraction Issues","text":"<ul> <li>Extract Clean Text Without Headers and Footers - Remove repeated content that's cluttering your text extraction</li> <li>Getting Text from Scanned Documents - Use OCR to extract text from image-based PDFs</li> </ul>"},{"location":"#table-problems","title":"Table Problems","text":"<ul> <li>Fix Messy Table Extraction - Handle tables with no borders, merged cells, or poor alignment</li> <li>Getting Tables Out of PDFs - Basic to advanced table extraction techniques</li> </ul>"},{"location":"#data-extraction","title":"Data Extraction","text":"<ul> <li>Extract Data from Forms and Invoices - Pull structured information from standardized documents</li> <li>Pulling Structured Data from PDFs - Use AI to extract specific fields from any document</li> </ul>"},{"location":"#document-analysis","title":"Document Analysis","text":"<ul> <li>Ask Questions to Your Documents - Use natural language to find information</li> <li>Categorizing Pages and Regions - Automatically classify document types and content</li> </ul>"},{"location":"#finding-content","title":"Finding Content","text":"<ul> <li>Finding What You Need in PDFs - Master selectors to locate any element</li> <li>PDF Navigation - Move around documents and work with multiple pages</li> </ul>"},{"location":"#layout-and-structure","title":"Layout and Structure","text":"<ul> <li>Document Layout Analysis - Automatically detect titles, tables, and document structure</li> <li>Working with Regions - Define and work with specific areas of pages</li> <li>Visual Debugging - See what you're extracting and debug selector issues</li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#find-elements-with-selectors","title":"Find Elements with Selectors","text":"<p>Use CSS-like selectors to find text, shapes, and more.</p> <pre><code># Find bold text containing \"Revenue\"\npage.find('text:contains(\"Revenue\"):bold').extract_text()\n\n# Find all large text\npage.find_all('text[size&gt;=12]').extract_text()\n</code></pre>"},{"location":"#navigate-spatially","title":"Navigate Spatially","text":"<p>Move around the page relative to elements, not just coordinates.</p> <pre><code># Extract text below a specific heading\nintro_text = page.find('text:contains(\"Introduction\")').below().extract_text()\n\n# Extract text from one heading to the next\nmethods_text = page.find('text:contains(\"Methods\")').below(\n    until='text:contains(\"Results\")'\n).extract_text()\n</code></pre>"},{"location":"#extract-clean-text","title":"Extract Clean Text","text":"<p>Easily extract text content, automatically handling common page elements like headers and footers (if exclusions are set).</p> <pre><code># Extract all text from the page (respecting exclusions)\npage_text = page.extract_text()\n\n# Extract text from a specific region\nsome_region = page.find(...)\nregion_text = some_region.extract_text()\n</code></pre>"},{"location":"#apply-ocr","title":"Apply OCR","text":"<p>Extract text from scanned documents using various OCR engines.</p> <pre><code># Apply OCR using the default engine\nocr_elements = page.apply_ocr()\n\n# Extract text (will use OCR results if available)\ntext = page.extract_text()\n</code></pre>"},{"location":"#analyze-document-layout","title":"Analyze Document Layout","text":"<p>Use AI models to detect document structures like titles, paragraphs, and tables.</p> <pre><code># Detect document structure\npage.analyze_layout()\n\n# Highlight titles and tables\npage.find_all('region[type=title]').show()\npage.find_all('region[type=table]').show()\n\n# Extract data from the first table\ntable_data = page.find('region[type=table]').extract_table()\n</code></pre>"},{"location":"#document-question-answering","title":"Document Question Answering","text":"<p>Ask natural language questions directly to your documents.</p> <pre><code># Ask a question\nresult = page.ask(\"What was the company's revenue in 2022?\")\nif result.found:\n    print(f\"Answer: {result.answer}\")\n    result.show()  # Highlight where the answer was found\n</code></pre>"},{"location":"#classify-pages-and-regions","title":"Classify Pages and Regions","text":"<p>Categorize pages or specific regions based on their content using text or vision models.</p> <pre><code># Classify a page based on text\nlabels = [\"invoice\", \"scientific article\", \"presentation\"]\npage.classify(labels, using=\"text\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n\n# Classify a page based on what it looks like\npage.classify(labels, using=\"vision\")\nprint(f\"Page Category: {page.category} (Confidence: {page.category_confidence:.2f})\")\n</code></pre>"},{"location":"#visualize-your-work","title":"Visualize Your Work","text":"<p>Debug and understand your extractions visually.</p> <pre><code># Highlight headings\npage.find_all('text[size&gt;=14]').show(color=\"red\", label=\"Headings\")\n\n# Launch the interactive viewer (Jupyter)\npage.viewer()\n</code></pre>"},{"location":"#reference-documentation","title":"Reference Documentation","text":"<ul> <li>Quick Reference - Cheat sheet of essential commands and patterns</li> <li>API Reference - Complete library reference</li> </ul>"},{"location":"#understanding-natural-pdf","title":"Understanding Natural PDF","text":"<p>Coming soon: Conceptual guides explaining how Natural PDF thinks about PDFs and when to use different approaches.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed documentation for all the classes and methods in Natural PDF.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#natural_pdf","title":"<code>natural_pdf</code>","text":"<p>Natural PDF - A more intuitive interface for working with PDFs.</p>"},{"location":"api/#natural_pdf-classes","title":"Classes","text":""},{"location":"api/#natural_pdf.ConfigSection","title":"<code>natural_pdf.ConfigSection</code>","text":"<p>A configuration section that holds key-value option pairs.</p> Source code in <code>natural_pdf/__init__.py</code> <pre><code>class ConfigSection:\n    \"\"\"A configuration section that holds key-value option pairs.\"\"\"\n\n    def __init__(self, **defaults):\n        self.__dict__.update(defaults)\n\n    def __repr__(self):\n        items = [f\"{k}={v!r}\" for k, v in self.__dict__.items()]\n        return f\"{self.__class__.__name__}({', '.join(items)})\"\n</code></pre>"},{"location":"api/#natural_pdf.Flow","title":"<code>natural_pdf.Flow</code>","text":"<p>               Bases: <code>Visualizable</code></p> <p>Defines a logical flow or sequence of physical Page or Region objects.</p> <p>A Flow represents a continuous logical document structure that spans across multiple pages or regions, enabling operations on content that flows across boundaries. This is essential for handling multi-page tables, articles that span columns, or any content that requires reading order across segments.</p> <p>Flows specify arrangement (vertical/horizontal) and alignment rules to create a unified coordinate system for element extraction and text processing. They enable natural-pdf to treat fragmented content as a single continuous area for analysis and extraction operations.</p> <p>The Flow system is particularly useful for: - Multi-page tables that break across page boundaries - Multi-column articles with complex reading order - Forms that span multiple pages - Any content requiring logical continuation across segments</p> <p>Attributes:</p> Name Type Description <code>segments</code> <code>List[Region]</code> <p>List of Page or Region objects in flow order.</p> <code>arrangement</code> <code>Literal['vertical', 'horizontal']</code> <p>Primary flow direction ('vertical' or 'horizontal').</p> <code>alignment</code> <code>Literal['start', 'center', 'end', 'top', 'left', 'bottom', 'right']</code> <p>Cross-axis alignment for segments of different sizes.</p> <code>segment_gap</code> <code>float</code> <p>Virtual gap between segments in PDF points.</p> Example <p>Multi-page table flow: <pre><code>pdf = npdf.PDF(\"multi_page_table.pdf\")\n\n# Create flow for table spanning pages 2-4\ntable_flow = Flow(\n    segments=[pdf.pages[1], pdf.pages[2], pdf.pages[3]],\n    arrangement='vertical',\n    alignment='left',\n    segment_gap=10.0\n)\n\n# Extract table as if it were continuous\ntable_data = table_flow.extract_table()\ntext_content = table_flow.get_text()\n</code></pre></p> <p>Multi-column article flow: <pre><code>page = pdf.pages[0]\nleft_column = page.region(0, 0, 300, page.height)\nright_column = page.region(320, 0, page.width, page.height)\n\n# Create horizontal flow for columns\narticle_flow = Flow(\n    segments=[left_column, right_column],\n    arrangement='horizontal',\n    alignment='top'\n)\n\n# Read in proper order\narticle_text = article_flow.get_text()\n</code></pre></p> Note <p>Flows create virtual coordinate systems that map element positions across segments, enabling spatial navigation and element selection to work seamlessly across boundaries.</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>class Flow(Visualizable):\n    \"\"\"Defines a logical flow or sequence of physical Page or Region objects.\n\n    A Flow represents a continuous logical document structure that spans across\n    multiple pages or regions, enabling operations on content that flows across\n    boundaries. This is essential for handling multi-page tables, articles that\n    span columns, or any content that requires reading order across segments.\n\n    Flows specify arrangement (vertical/horizontal) and alignment rules to create\n    a unified coordinate system for element extraction and text processing. They\n    enable natural-pdf to treat fragmented content as a single continuous area\n    for analysis and extraction operations.\n\n    The Flow system is particularly useful for:\n    - Multi-page tables that break across page boundaries\n    - Multi-column articles with complex reading order\n    - Forms that span multiple pages\n    - Any content requiring logical continuation across segments\n\n    Attributes:\n        segments: List of Page or Region objects in flow order.\n        arrangement: Primary flow direction ('vertical' or 'horizontal').\n        alignment: Cross-axis alignment for segments of different sizes.\n        segment_gap: Virtual gap between segments in PDF points.\n\n    Example:\n        Multi-page table flow:\n        ```python\n        pdf = npdf.PDF(\"multi_page_table.pdf\")\n\n        # Create flow for table spanning pages 2-4\n        table_flow = Flow(\n            segments=[pdf.pages[1], pdf.pages[2], pdf.pages[3]],\n            arrangement='vertical',\n            alignment='left',\n            segment_gap=10.0\n        )\n\n        # Extract table as if it were continuous\n        table_data = table_flow.extract_table()\n        text_content = table_flow.get_text()\n        ```\n\n        Multi-column article flow:\n        ```python\n        page = pdf.pages[0]\n        left_column = page.region(0, 0, 300, page.height)\n        right_column = page.region(320, 0, page.width, page.height)\n\n        # Create horizontal flow for columns\n        article_flow = Flow(\n            segments=[left_column, right_column],\n            arrangement='horizontal',\n            alignment='top'\n        )\n\n        # Read in proper order\n        article_text = article_flow.get_text()\n        ```\n\n    Note:\n        Flows create virtual coordinate systems that map element positions across\n        segments, enabling spatial navigation and element selection to work\n        seamlessly across boundaries.\n    \"\"\"\n\n    def __init__(\n        self,\n        segments: Union[List[Union[\"Page\", \"PhysicalRegion\"]], \"PageCollection\"],\n        arrangement: Literal[\"vertical\", \"horizontal\"],\n        alignment: Literal[\"start\", \"center\", \"end\", \"top\", \"left\", \"bottom\", \"right\"] = \"start\",\n        segment_gap: float = 0.0,\n    ):\n        \"\"\"\n        Initializes a Flow object.\n\n        Args:\n            segments: An ordered list of natural_pdf.core.page.Page or\n                      natural_pdf.elements.region.Region objects that constitute the flow,\n                      or a PageCollection containing pages.\n            arrangement: The primary direction of the flow.\n                         - \"vertical\": Segments are stacked top-to-bottom.\n                         - \"horizontal\": Segments are arranged left-to-right.\n            alignment: How segments are aligned on their cross-axis if they have\n                       differing dimensions. For a \"vertical\" arrangement:\n                       - \"left\" (or \"start\"): Align left edges.\n                       - \"center\": Align centers.\n                       - \"right\" (or \"end\"): Align right edges.\n                       For a \"horizontal\" arrangement:\n                       - \"top\" (or \"start\"): Align top edges.\n                       - \"center\": Align centers.\n                       - \"bottom\" (or \"end\"): Align bottom edges.\n            segment_gap: The virtual gap (in PDF points) between segments.\n        \"\"\"\n        # Handle PageCollection input\n        if hasattr(segments, \"pages\"):  # It's a PageCollection\n            segments = list(segments.pages)\n\n        if not segments:\n            raise ValueError(\"Flow segments cannot be empty.\")\n        if arrangement not in [\"vertical\", \"horizontal\"]:\n            raise ValueError(\"Arrangement must be 'vertical' or 'horizontal'.\")\n\n        self.segments: List[\"PhysicalRegion\"] = self._normalize_segments(segments)\n        self.arrangement: Literal[\"vertical\", \"horizontal\"] = arrangement\n        self.alignment: Literal[\"start\", \"center\", \"end\", \"top\", \"left\", \"bottom\", \"right\"] = (\n            alignment\n        )\n        self.segment_gap: float = segment_gap\n\n        self._validate_alignment()\n\n        # TODO: Pre-calculate segment offsets for faster lookups if needed\n\n    def _normalize_segments(\n        self, segments: List[Union[\"Page\", \"PhysicalRegion\"]]\n    ) -&gt; List[\"PhysicalRegion\"]:\n        \"\"\"Converts all Page segments to full-page Region objects for uniform processing.\"\"\"\n        normalized = []\n        from natural_pdf.core.page import Page as CorePage\n        from natural_pdf.elements.region import Region as ElementsRegion\n\n        for i, segment in enumerate(segments):\n            if isinstance(segment, CorePage):\n                normalized.append(segment.region(0, 0, segment.width, segment.height))\n            elif isinstance(segment, ElementsRegion):\n                normalized.append(segment)\n            elif hasattr(segment, \"object_type\") and segment.object_type == \"page\":\n                if not isinstance(segment, CorePage):\n                    raise TypeError(\n                        f\"Segment {i} has object_type 'page' but is not an instance of natural_pdf.core.page.Page. Got {type(segment)}\"\n                    )\n                normalized.append(segment.region(0, 0, segment.width, segment.height))\n            elif hasattr(segment, \"object_type\") and segment.object_type == \"region\":\n                if not isinstance(segment, ElementsRegion):\n                    raise TypeError(\n                        f\"Segment {i} has object_type 'region' but is not an instance of natural_pdf.elements.region.Region. Got {type(segment)}\"\n                    )\n                normalized.append(segment)\n            else:\n                raise TypeError(\n                    f\"Segment {i} is not a valid Page or Region object. Got {type(segment)}.\"\n                )\n        return normalized\n\n    def _validate_alignment(self) -&gt; None:\n        \"\"\"Validates the alignment based on the arrangement.\"\"\"\n        valid_alignments = {\n            \"vertical\": [\"start\", \"center\", \"end\", \"left\", \"right\"],\n            \"horizontal\": [\"start\", \"center\", \"end\", \"top\", \"bottom\"],\n        }\n        if self.alignment not in valid_alignments[self.arrangement]:\n            raise ValueError(\n                f\"Invalid alignment '{self.alignment}' for '{self.arrangement}' arrangement. \"\n                f\"Valid options are: {valid_alignments[self.arrangement]}\"\n            )\n\n    def _get_highlighter(self):\n        \"\"\"Get the highlighting service from the first segment.\"\"\"\n        if not self.segments:\n            raise RuntimeError(\"Flow has no segments to get highlighter from\")\n\n        # Get highlighter from first segment\n        first_segment = self.segments[0]\n        if hasattr(first_segment, \"_highlighter\"):\n            return first_segment._highlighter\n        elif hasattr(first_segment, \"page\") and hasattr(first_segment.page, \"_highlighter\"):\n            return first_segment.page._highlighter\n        else:\n            raise RuntimeError(\n                f\"Cannot find HighlightingService from Flow segments. \"\n                f\"First segment type: {type(first_segment).__name__}\"\n            )\n\n    def show(\n        self,\n        *,\n        # Basic rendering options\n        resolution: Optional[float] = None,\n        width: Optional[int] = None,\n        # Highlight options\n        color: Optional[Union[str, Tuple[int, int, int]]] = None,\n        labels: bool = True,\n        label_format: Optional[str] = None,\n        highlights: Optional[List[Dict[str, Any]]] = None,\n        # Layout options for multi-page/region\n        layout: Literal[\"stack\", \"grid\", \"single\"] = \"stack\",\n        stack_direction: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n        gap: int = 5,\n        columns: Optional[int] = None,  # For grid layout\n        # Cropping options\n        crop: Union[bool, Literal[\"content\"]] = False,\n        crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n        # Flow-specific options\n        in_context: bool = False,\n        separator_color: Optional[Tuple[int, int, int]] = None,\n        separator_thickness: int = 2,\n        **kwargs,\n    ) -&gt; Optional[\"PIL_Image\"]:\n        \"\"\"Generate a preview image with highlights.\n\n        If in_context=True, shows segments as cropped images stacked together\n        with separators between segments.\n\n        Args:\n            resolution: DPI for rendering (default from global settings)\n            width: Target width in pixels (overrides resolution)\n            color: Default highlight color\n            labels: Whether to show labels for highlights\n            label_format: Format string for labels\n            highlights: Additional highlight groups to show\n            layout: How to arrange multiple pages/regions\n            stack_direction: Direction for stack layout\n            gap: Pixels between stacked images\n            columns: Number of columns for grid layout\n            crop: Whether to crop\n            crop_bbox: Explicit crop bounds\n            in_context: If True, use special Flow visualization with separators\n            separator_color: RGB color for separator lines (default: red)\n            separator_thickness: Thickness of separator lines\n            **kwargs: Additional parameters passed to rendering\n\n        Returns:\n            PIL Image object or None if nothing to render\n        \"\"\"\n        if in_context:\n            # Use the special in_context visualization\n            return self._show_in_context(\n                resolution=resolution or 150,\n                width=width,\n                stack_direction=stack_direction,\n                stack_gap=gap,\n                separator_color=separator_color or (255, 0, 0),\n                separator_thickness=separator_thickness,\n                **kwargs,\n            )\n\n        # Otherwise use the standard show method\n        return super().show(\n            resolution=resolution,\n            width=width,\n            color=color,\n            labels=labels,\n            label_format=label_format,\n            highlights=highlights,\n            layout=layout,\n            stack_direction=stack_direction,\n            gap=gap,\n            columns=columns,\n            crop=crop,\n            crop_bbox=crop_bbox,\n            **kwargs,\n        )\n\n    def find(\n        self,\n        selector: Optional[str] = None,\n        *,\n        text: Optional[str] = None,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[\"FlowElement\"]:\n        \"\"\"\n        Finds the first element within the flow that matches the given selector or text criteria.\n\n        Elements found are wrapped as FlowElement objects, anchored to this Flow.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for.\n            apply_exclusions: Whether to respect exclusion zones on the original pages/regions.\n            regex: Whether the text search uses regex.\n            case: Whether the text search is case-sensitive.\n            **kwargs: Additional filter parameters for the underlying find operation.\n\n        Returns:\n            A FlowElement if a match is found, otherwise None.\n        \"\"\"\n        results = self.find_all(\n            selector=selector,\n            text=text,\n            apply_exclusions=apply_exclusions,\n            regex=regex,\n            case=case,\n            **kwargs,\n        )\n        return results.first if results else None\n\n    def find_all(\n        self,\n        selector: Optional[str] = None,\n        *,\n        text: Optional[str] = None,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"FlowElementCollection\":\n        \"\"\"\n        Finds all elements within the flow that match the given selector or text criteria.\n\n        This method efficiently groups segments by their parent pages, searches at the page level,\n        then filters results appropriately for each segment. This ensures elements that intersect\n        with flow segments (but aren't fully contained) are still found.\n\n        Elements found are wrapped as FlowElement objects, anchored to this Flow,\n        and returned in a FlowElementCollection.\n        \"\"\"\n        from .collections import FlowElementCollection\n        from .element import FlowElement\n\n        # Step 1: Group segments by their parent pages (like in analyze_layout)\n        segments_by_page = {}  # Dict[Page, List[Segment]]\n\n        for i, segment in enumerate(self.segments):\n            # Determine the page for this segment - fix type detection\n            if hasattr(segment, \"page\") and hasattr(segment.page, \"find_all\"):\n                # It's a Region object (has a parent page)\n                page_obj = segment.page\n                segment_type = \"region\"\n            elif (\n                hasattr(segment, \"find_all\")\n                and hasattr(segment, \"width\")\n                and hasattr(segment, \"height\")\n                and not hasattr(segment, \"page\")\n            ):\n                # It's a Page object (has find_all but no parent page)\n                page_obj = segment\n                segment_type = \"page\"\n            else:\n                logger.warning(f\"Segment {i+1} does not support find_all, skipping\")\n                continue\n\n            if page_obj not in segments_by_page:\n                segments_by_page[page_obj] = []\n            segments_by_page[page_obj].append((segment, segment_type))\n\n        if not segments_by_page:\n            logger.warning(\"No segments with searchable pages found\")\n            return FlowElementCollection([])\n\n        # Step 2: Search each unique page only once\n        all_flow_elements: List[\"FlowElement\"] = []\n\n        for page_obj, page_segments in segments_by_page.items():\n            # Find all matching elements on this page\n            page_matches = page_obj.find_all(\n                selector=selector,\n                text=text,\n                apply_exclusions=apply_exclusions,\n                regex=regex,\n                case=case,\n                **kwargs,\n            )\n\n            if not page_matches:\n                continue\n\n            # Step 3: For each segment on this page, collect relevant elements\n            for segment, segment_type in page_segments:\n                if segment_type == \"page\":\n                    # Full page segment: include all elements\n                    for phys_elem in page_matches.elements:\n                        all_flow_elements.append(FlowElement(physical_object=phys_elem, flow=self))\n\n                elif segment_type == \"region\":\n                    # Region segment: filter to only intersecting elements\n                    for phys_elem in page_matches.elements:\n                        try:\n                            # Check if element intersects with this flow segment\n                            if segment.intersects(phys_elem):\n                                all_flow_elements.append(\n                                    FlowElement(physical_object=phys_elem, flow=self)\n                                )\n                        except Exception as intersect_error:\n                            logger.debug(\n                                f\"Error checking intersection for element: {intersect_error}\"\n                            )\n                            # Include the element anyway if intersection check fails\n                            all_flow_elements.append(\n                                FlowElement(physical_object=phys_elem, flow=self)\n                            )\n\n        # Step 4: Remove duplicates (can happen if multiple segments intersect the same element)\n        unique_flow_elements = []\n        seen_element_ids = set()\n\n        for flow_elem in all_flow_elements:\n            # Create a unique identifier for the underlying physical element\n            phys_elem = flow_elem.physical_object\n            elem_id = (\n                (\n                    getattr(phys_elem.page, \"index\", id(phys_elem.page))\n                    if hasattr(phys_elem, \"page\")\n                    else id(phys_elem)\n                ),\n                phys_elem.bbox if hasattr(phys_elem, \"bbox\") else id(phys_elem),\n            )\n\n            if elem_id not in seen_element_ids:\n                unique_flow_elements.append(flow_elem)\n                seen_element_ids.add(elem_id)\n\n        return FlowElementCollection(unique_flow_elements)\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"&lt;Flow segments={len(self.segments)}, \"\n            f\"arrangement='{self.arrangement}', alignment='{self.alignment}', gap={self.segment_gap}&gt;\"\n        )\n\n    @overload\n    def extract_table(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        use_ocr: bool = False,\n        ocr_config: Optional[dict] = None,\n        text_options: Optional[dict] = None,\n        cell_extraction_func: Optional[Any] = None,\n        show_progress: bool = False,\n        content_filter: Optional[Any] = None,\n        stitch_rows: Callable[[List[Optional[str]]], bool] = None,\n    ) -&gt; TableResult: ...\n\n    @overload\n    def extract_table(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        use_ocr: bool = False,\n        ocr_config: Optional[dict] = None,\n        text_options: Optional[dict] = None,\n        cell_extraction_func: Optional[Any] = None,\n        show_progress: bool = False,\n        content_filter: Optional[Any] = None,\n        stitch_rows: Callable[\n            [List[Optional[str]], List[Optional[str]], int, Union[\"Page\", \"PhysicalRegion\"]],\n            bool,\n        ] = None,\n    ) -&gt; TableResult: ...\n\n    def extract_table(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        use_ocr: bool = False,\n        ocr_config: Optional[dict] = None,\n        text_options: Optional[dict] = None,\n        cell_extraction_func: Optional[Any] = None,\n        show_progress: bool = False,\n        content_filter: Optional[Any] = None,\n        stitch_rows: Optional[Callable] = None,\n        merge_headers: Optional[bool] = None,\n    ) -&gt; TableResult:\n        \"\"\"\n        Extract table data from all segments in the flow, combining results sequentially.\n\n        This method extracts table data from each segment in flow order and combines\n        the results into a single logical table. This is particularly useful for\n        multi-page tables or tables that span across columns.\n\n        Args:\n            method: Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).\n            table_settings: Settings for pdfplumber table extraction.\n            use_ocr: Whether to use OCR for text extraction (currently only applicable with 'tatr' method).\n            ocr_config: OCR configuration parameters.\n            text_options: Dictionary of options for the 'text' method.\n            cell_extraction_func: Optional callable function that takes a cell Region object\n                                  and returns its string content. For 'text' method only.\n            show_progress: If True, display a progress bar during cell text extraction for the 'text' method.\n            content_filter: Optional content filter to apply during cell text extraction.\n            merge_headers: Whether to merge tables by removing repeated headers from subsequent\n                segments. If None (default), auto-detects by checking if the first row\n                of each segment matches the first row of the first segment. If segments have\n                inconsistent header patterns (some repeat, others don't), raises ValueError.\n                Useful for multi-page tables where headers repeat on each page.\n            stitch_rows: Optional callable to determine when rows should be merged across\n                         segment boundaries. Applied AFTER header removal if merge_headers\n                         is enabled. Two overloaded signatures are supported:\n\n                         \u2022 func(current_row) -&gt; bool\n                           Called only on the first row of each segment (after the first).\n                           Return True to merge this first row with the last row from\n                           the previous segment.\n\n                         \u2022 func(prev_row, current_row, row_index, segment) -&gt; bool\n                           Called for every row. Return True to merge current_row with\n                           the previous row in the aggregated results.\n\n                         When True is returned, rows are concatenated cell-by-cell.\n                         This is useful for handling table rows split across page\n                         boundaries or segments. If None, rows are never merged.\n\n        Returns:\n            TableResult object containing the aggregated table data from all segments.\n\n        Example:\n            Multi-page table extraction:\n            ```python\n            pdf = npdf.PDF(\"multi_page_table.pdf\")\n\n            # Create flow for table spanning pages 2-4\n            table_flow = Flow(\n                segments=[pdf.pages[1], pdf.pages[2], pdf.pages[3]],\n                arrangement='vertical'\n            )\n\n            # Extract table as if it were continuous\n            table_data = table_flow.extract_table()\n            df = table_data.df  # Convert to pandas DataFrame\n\n            # Custom row stitching - single parameter (simple case)\n            table_data = table_flow.extract_table(\n                stitch_rows=lambda row: row and not (row[0] or \"\").strip()\n            )\n\n            # Custom row stitching - full parameters (advanced case)\n            table_data = table_flow.extract_table(\n                stitch_rows=lambda prev, curr, idx, seg: idx == 0 and curr and not (curr[0] or \"\").strip()\n            )\n            ```\n        \"\"\"\n        logger.info(\n            f\"Extracting table from Flow with {len(self.segments)} segments (method: {method or 'auto'})\"\n        )\n\n        if not self.segments:\n            logger.warning(\"Flow has no segments, returning empty table\")\n            return TableResult([])\n\n        # Resolve predicate and determine its signature\n        predicate: Optional[Callable] = None\n        predicate_type: str = \"none\"\n\n        if callable(stitch_rows):\n            import inspect\n\n            sig = inspect.signature(stitch_rows)\n            param_count = len(sig.parameters)\n\n            if param_count == 1:\n                predicate = stitch_rows\n                predicate_type = \"single_param\"\n            elif param_count == 4:\n                predicate = stitch_rows\n                predicate_type = \"full_params\"\n            else:\n                logger.warning(\n                    f\"stitch_rows function has {param_count} parameters, expected 1 or 4. Ignoring.\"\n                )\n                predicate = None\n                predicate_type = \"none\"\n\n        def _default_merge(\n            prev_row: List[Optional[str]], cur_row: List[Optional[str]]\n        ) -&gt; List[Optional[str]]:\n            from itertools import zip_longest\n\n            merged: List[Optional[str]] = []\n            for p, c in zip_longest(prev_row, cur_row, fillvalue=\"\"):\n                if (p or \"\").strip() and (c or \"\").strip():\n                    merged.append(f\"{p} {c}\".strip())\n                else:\n                    merged.append((p or \"\") + (c or \"\"))\n            return merged\n\n        aggregated_rows: List[List[Optional[str]]] = []\n        processed_segments = 0\n        header_row: Optional[List[Optional[str]]] = None\n        merge_headers_enabled = False\n        headers_warned = False  # Track if we've already warned about dropping headers\n        segment_has_repeated_header = []  # Track which segments have repeated headers\n\n        for seg_idx, segment in enumerate(self.segments):\n            try:\n                logger.debug(f\"  Extracting table from segment {seg_idx+1}/{len(self.segments)}\")\n\n                segment_result = segment.extract_table(\n                    method=method,\n                    table_settings=table_settings.copy() if table_settings else None,\n                    use_ocr=use_ocr,\n                    ocr_config=ocr_config,\n                    text_options=text_options.copy() if text_options else None,\n                    cell_extraction_func=cell_extraction_func,\n                    show_progress=show_progress,\n                    content_filter=content_filter,\n                )\n\n                if not segment_result:\n                    continue\n\n                if hasattr(segment_result, \"_rows\"):\n                    segment_rows = list(segment_result._rows)\n                else:\n                    segment_rows = list(segment_result)\n\n                if not segment_rows:\n                    logger.debug(f\"    No table data found in segment {seg_idx+1}\")\n                    continue\n\n                # Handle header detection and merging for multi-page tables\n                if seg_idx == 0:\n                    # First segment: capture potential header row\n                    if segment_rows:\n                        header_row = segment_rows[0]\n                        # Determine if we should merge headers\n                        if merge_headers is None:\n                            # Auto-detect: we'll check all subsequent segments\n                            merge_headers_enabled = False  # Will be determined later\n                        else:\n                            merge_headers_enabled = merge_headers\n                        # Track that first segment exists (for consistency checking)\n                        segment_has_repeated_header.append(False)  # First segment doesn't \"repeat\"\n                elif seg_idx == 1 and merge_headers is None:\n                    # Auto-detection: check if first row of second segment matches header\n                    has_header = segment_rows and header_row and segment_rows[0] == header_row\n                    segment_has_repeated_header.append(has_header)\n\n                    if has_header:\n                        merge_headers_enabled = True\n                        # Remove the detected repeated header from this segment\n                        segment_rows = segment_rows[1:]\n                        logger.debug(\n                            f\"    Auto-detected repeated header in segment {seg_idx+1}, removed\"\n                        )\n                        if not headers_warned:\n                            warnings.warn(\n                                \"Detected repeated headers in multi-page table. Merging by removing \"\n                                \"repeated headers from subsequent pages.\",\n                                UserWarning,\n                                stacklevel=2,\n                            )\n                            headers_warned = True\n                    else:\n                        merge_headers_enabled = False\n                        logger.debug(f\"    No repeated header detected in segment {seg_idx+1}\")\n                elif seg_idx &gt; 1:\n                    # Check consistency: all segments should have same pattern\n                    has_header = segment_rows and header_row and segment_rows[0] == header_row\n                    segment_has_repeated_header.append(has_header)\n\n                    # Remove header if merging is enabled and header is present\n                    if merge_headers_enabled and has_header:\n                        segment_rows = segment_rows[1:]\n                        logger.debug(f\"    Removed repeated header from segment {seg_idx+1}\")\n                elif seg_idx &gt; 0 and merge_headers_enabled:\n                    # Explicit merge_headers=True: remove headers from subsequent segments\n                    if segment_rows and header_row and segment_rows[0] == header_row:\n                        segment_rows = segment_rows[1:]\n                        logger.debug(f\"    Removed repeated header from segment {seg_idx+1}\")\n                        if not headers_warned:\n                            warnings.warn(\n                                \"Removing repeated headers from multi-page table during merge.\",\n                                UserWarning,\n                                stacklevel=2,\n                            )\n                            headers_warned = True\n\n                for row_idx, row in enumerate(segment_rows):\n                    should_merge = False\n\n                    if predicate is not None and aggregated_rows:\n                        if predicate_type == \"single_param\":\n                            # For single param: only call on first row of segment (row_idx == 0)\n                            # and pass the current row\n                            if row_idx == 0:\n                                should_merge = predicate(row)\n                        elif predicate_type == \"full_params\":\n                            # For full params: call with all arguments\n                            should_merge = predicate(aggregated_rows[-1], row, row_idx, segment)\n\n                    if should_merge:\n                        aggregated_rows[-1] = _default_merge(aggregated_rows[-1], row)\n                    else:\n                        aggregated_rows.append(row)\n\n                processed_segments += 1\n                logger.debug(\n                    f\"    Added {len(segment_rows)} rows (post-merge) from segment {seg_idx+1}\"\n                )\n\n            except Exception as e:\n                logger.error(f\"Error extracting table from segment {seg_idx+1}: {e}\", exc_info=True)\n                continue\n\n        # Check for inconsistent header patterns after processing all segments\n        if merge_headers is None and len(segment_has_repeated_header) &gt; 2:\n            # During auto-detection, check for consistency across all segments\n            expected_pattern = segment_has_repeated_header[1]  # Pattern from second segment\n            for seg_idx, has_header in enumerate(segment_has_repeated_header[2:], 2):\n                if has_header != expected_pattern:\n                    # Inconsistent pattern detected\n                    segments_with_headers = [\n                        i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if has_h\n                    ]\n                    segments_without_headers = [\n                        i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if not has_h\n                    ]\n                    raise ValueError(\n                        f\"Inconsistent header pattern in multi-page table: \"\n                        f\"segments {segments_with_headers} have repeated headers, \"\n                        f\"but segments {segments_without_headers} do not. \"\n                        f\"All segments must have the same header pattern for reliable merging.\"\n                    )\n\n        logger.info(\n            f\"Flow table extraction complete: {len(aggregated_rows)} total rows from {processed_segments}/{len(self.segments)} segments\"\n        )\n        return TableResult(aggregated_rows)\n\n    def analyze_layout(\n        self,\n        engine: Optional[str] = None,\n        options: Optional[Any] = None,\n        confidence: Optional[float] = None,\n        classes: Optional[List[str]] = None,\n        exclude_classes: Optional[List[str]] = None,\n        device: Optional[str] = None,\n        existing: str = \"replace\",\n        model_name: Optional[str] = None,\n        client: Optional[Any] = None,\n    ) -&gt; \"PhysicalElementCollection\":\n        \"\"\"\n        Analyze layout across all segments in the flow.\n\n        This method efficiently groups segments by their parent pages, runs layout analysis\n        only once per unique page, then filters results appropriately for each segment.\n        This avoids redundant analysis when multiple flow segments come from the same page.\n\n        Args:\n            engine: Name of the layout engine (e.g., 'yolo', 'tatr'). Uses manager's default if None.\n            options: Specific LayoutOptions object for advanced configuration.\n            confidence: Minimum confidence threshold.\n            classes: Specific classes to detect.\n            exclude_classes: Classes to exclude.\n            device: Device for inference.\n            existing: How to handle existing detected regions: 'replace' (default) or 'append'.\n            model_name: Optional model name for the engine.\n            client: Optional client for API-based engines.\n\n        Returns:\n            ElementCollection containing all detected Region objects from all segments.\n\n        Example:\n            Multi-page layout analysis:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n\n            # Create flow for first 3 pages\n            page_flow = Flow(\n                segments=pdf.pages[:3],\n                arrangement='vertical'\n            )\n\n            # Analyze layout across all pages (efficiently)\n            all_regions = page_flow.analyze_layout(engine='yolo')\n\n            # Find all tables across the flow\n            tables = all_regions.filter('region[type=table]')\n            ```\n        \"\"\"\n        from natural_pdf.elements.element_collection import ElementCollection\n\n        logger.info(\n            f\"Analyzing layout across Flow with {len(self.segments)} segments (engine: {engine or 'default'})\"\n        )\n\n        if not self.segments:\n            logger.warning(\"Flow has no segments, returning empty collection\")\n            return ElementCollection([])\n\n        # Step 1: Group segments by their parent pages to avoid redundant analysis\n        segments_by_page = {}  # Dict[Page, List[Segment]]\n\n        for i, segment in enumerate(self.segments):\n            # Determine the page for this segment\n            if hasattr(segment, \"analyze_layout\"):\n                # It's a Page object\n                page_obj = segment\n                segment_type = \"page\"\n            elif hasattr(segment, \"page\") and hasattr(segment.page, \"analyze_layout\"):\n                # It's a Region object\n                page_obj = segment.page\n                segment_type = \"region\"\n            else:\n                logger.warning(f\"Segment {i+1} does not support layout analysis, skipping\")\n                continue\n\n            if page_obj not in segments_by_page:\n                segments_by_page[page_obj] = []\n            segments_by_page[page_obj].append((segment, segment_type))\n\n        if not segments_by_page:\n            logger.warning(\"No segments with analyzable pages found\")\n            return ElementCollection([])\n\n        logger.debug(\n            f\"  Grouped {len(self.segments)} segments into {len(segments_by_page)} unique pages\"\n        )\n\n        # Step 2: Analyze each unique page only once\n        all_detected_regions: List[\"PhysicalRegion\"] = []\n        processed_pages = 0\n\n        for page_obj, page_segments in segments_by_page.items():\n            try:\n                logger.debug(\n                    f\"  Analyzing layout for page {getattr(page_obj, 'number', '?')} with {len(page_segments)} segments\"\n                )\n\n                # Run layout analysis once for this page\n                page_results = page_obj.analyze_layout(\n                    engine=engine,\n                    options=options,\n                    confidence=confidence,\n                    classes=classes,\n                    exclude_classes=exclude_classes,\n                    device=device,\n                    existing=existing,\n                    model_name=model_name,\n                    client=client,\n                )\n\n                # Extract regions from results\n                if hasattr(page_results, \"elements\"):\n                    # It's an ElementCollection\n                    page_regions = page_results.elements\n                elif isinstance(page_results, list):\n                    # It's a list of regions\n                    page_regions = page_results\n                else:\n                    logger.warning(\n                        f\"Page {getattr(page_obj, 'number', '?')} returned unexpected layout analysis result type: {type(page_results)}\"\n                    )\n                    continue\n\n                if not page_regions:\n                    logger.debug(\n                        f\"    No layout regions found on page {getattr(page_obj, 'number', '?')}\"\n                    )\n                    continue\n\n                # Step 3: For each segment on this page, collect relevant regions\n                segments_processed_on_page = 0\n                for segment, segment_type in page_segments:\n                    if segment_type == \"page\":\n                        # Full page segment: include all detected regions\n                        all_detected_regions.extend(page_regions)\n                        segments_processed_on_page += 1\n                        logger.debug(f\"    Added {len(page_regions)} regions for full-page segment\")\n\n                    elif segment_type == \"region\":\n                        # Region segment: filter to only intersecting regions\n                        intersecting_regions = []\n                        for region in page_regions:\n                            try:\n                                if segment.intersects(region):\n                                    intersecting_regions.append(region)\n                            except Exception as intersect_error:\n                                logger.debug(\n                                    f\"Error checking intersection for region: {intersect_error}\"\n                                )\n                                # Include the region anyway if intersection check fails\n                                intersecting_regions.append(region)\n\n                        all_detected_regions.extend(intersecting_regions)\n                        segments_processed_on_page += 1\n                        logger.debug(\n                            f\"    Added {len(intersecting_regions)} intersecting regions for region segment {segment.bbox}\"\n                        )\n\n                processed_pages += 1\n                logger.debug(\n                    f\"    Processed {segments_processed_on_page} segments on page {getattr(page_obj, 'number', '?')}\"\n                )\n\n            except Exception as e:\n                logger.error(\n                    f\"Error analyzing layout for page {getattr(page_obj, 'number', '?')}: {e}\",\n                    exc_info=True,\n                )\n                continue\n\n        # Step 4: Remove duplicates (can happen if multiple segments intersect the same region)\n        unique_regions = []\n        seen_region_ids = set()\n\n        for region in all_detected_regions:\n            # Create a unique identifier for this region (page + bbox)\n            region_id = (\n                getattr(region.page, \"index\", id(region.page)),\n                region.bbox if hasattr(region, \"bbox\") else id(region),\n            )\n\n            if region_id not in seen_region_ids:\n                unique_regions.append(region)\n                seen_region_ids.add(region_id)\n\n        dedupe_removed = len(all_detected_regions) - len(unique_regions)\n        if dedupe_removed &gt; 0:\n            logger.debug(f\"  Removed {dedupe_removed} duplicate regions\")\n\n        logger.info(\n            f\"Flow layout analysis complete: {len(unique_regions)} unique regions from {processed_pages} pages\"\n        )\n        return ElementCollection(unique_regions)\n\n    def _get_render_specs(\n        self,\n        mode: Literal[\"show\", \"render\"] = \"show\",\n        color: Optional[Union[str, Tuple[int, int, int]]] = None,\n        highlights: Optional[List[Dict[str, Any]]] = None,\n        crop: Union[bool, Literal[\"content\"]] = False,\n        crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n        label_prefix: Optional[str] = \"FlowSegment\",\n        **kwargs,\n    ) -&gt; List[RenderSpec]:\n        \"\"\"Get render specifications for this flow.\n\n        Args:\n            mode: Rendering mode - 'show' includes highlights, 'render' is clean\n            color: Color for highlighting segments in show mode\n            highlights: Additional highlight groups to show\n            crop: Whether to crop to segments\n            crop_bbox: Explicit crop bounds\n            label_prefix: Prefix for segment labels\n            **kwargs: Additional parameters\n\n        Returns:\n            List of RenderSpec objects, one per page with segments\n        \"\"\"\n        if not self.segments:\n            return []\n\n        # Group segments by their physical pages\n        segments_by_page = {}  # Dict[Page, List[PhysicalRegion]]\n\n        for i, segment in enumerate(self.segments):\n            # Get the page for this segment\n            if hasattr(segment, \"page\") and segment.page is not None:\n                # It's a Region, use its page\n                page_obj = segment.page\n                if page_obj not in segments_by_page:\n                    segments_by_page[page_obj] = []\n                segments_by_page[page_obj].append(segment)\n            elif (\n                hasattr(segment, \"index\")\n                and hasattr(segment, \"width\")\n                and hasattr(segment, \"height\")\n            ):\n                # It's a full Page object, create a full-page region for it\n                page_obj = segment\n                full_page_region = segment.region(0, 0, segment.width, segment.height)\n                if page_obj not in segments_by_page:\n                    segments_by_page[page_obj] = []\n                segments_by_page[page_obj].append(full_page_region)\n            else:\n                logger.warning(f\"Segment {i+1} has no identifiable page, skipping\")\n                continue\n\n        if not segments_by_page:\n            return []\n\n        # Create RenderSpec for each page\n        specs = []\n\n        # Sort pages by index for consistent output order\n        sorted_pages = sorted(\n            segments_by_page.keys(),\n            key=lambda p: p.index if hasattr(p, \"index\") else getattr(p, \"page_number\", 0),\n        )\n\n        for page_idx, page_obj in enumerate(sorted_pages):\n            segments_on_this_page = segments_by_page[page_obj]\n            if not segments_on_this_page:\n                continue\n\n            spec = RenderSpec(page=page_obj)\n\n            # Handle cropping\n            if crop_bbox:\n                spec.crop_bbox = crop_bbox\n            elif crop == \"content\" or crop is True:\n                # Calculate bounds of segments on this page\n                x_coords = []\n                y_coords = []\n                for segment in segments_on_this_page:\n                    if hasattr(segment, \"bbox\") and segment.bbox:\n                        x0, y0, x1, y1 = segment.bbox\n                        x_coords.extend([x0, x1])\n                        y_coords.extend([y0, y1])\n\n                if x_coords and y_coords:\n                    spec.crop_bbox = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))\n\n            # Add highlights in show mode\n            if mode == \"show\":\n                # Highlight segments\n                for i, segment in enumerate(segments_on_this_page):\n                    segment_label = None\n                    if label_prefix:\n                        # Create label for this segment\n                        global_segment_idx = None\n                        try:\n                            # Find the global index of this segment in the original flow\n                            global_segment_idx = self.segments.index(segment)\n                        except ValueError:\n                            # If it's a generated full-page region, find its source page\n                            for idx, orig_segment in enumerate(self.segments):\n                                if (\n                                    hasattr(orig_segment, \"index\")\n                                    and hasattr(segment, \"page\")\n                                    and orig_segment.index == segment.page.index\n                                ):\n                                    global_segment_idx = idx\n                                    break\n\n                        if global_segment_idx is not None:\n                            segment_label = f\"{label_prefix}_{global_segment_idx + 1}\"\n                        else:\n                            segment_label = f\"{label_prefix}_p{page_idx + 1}s{i + 1}\"\n\n                    spec.add_highlight(\n                        bbox=segment.bbox,\n                        polygon=segment.polygon if segment.has_polygon else None,\n                        color=color or \"blue\",\n                        label=segment_label,\n                    )\n\n                # Add additional highlight groups if provided\n                if highlights:\n                    for group in highlights:\n                        group_elements = group.get(\"elements\", [])\n                        group_color = group.get(\"color\", color)\n                        group_label = group.get(\"label\")\n\n                        for elem in group_elements:\n                            # Only add if element is on this page\n                            if hasattr(elem, \"page\") and elem.page == page_obj:\n                                spec.add_highlight(\n                                    element=elem, color=group_color, label=group_label\n                                )\n\n            specs.append(spec)\n\n        return specs\n\n    def _show_in_context(\n        self,\n        resolution: float,\n        width: Optional[int] = None,\n        stack_direction: str = \"vertical\",\n        stack_gap: int = 5,\n        stack_background_color: Tuple[int, int, int] = (255, 255, 255),\n        separator_color: Tuple[int, int, int] = (255, 0, 0),\n        separator_thickness: int = 2,\n        **kwargs,\n    ) -&gt; Optional[\"PIL_Image\"]:\n        \"\"\"\n        Show segments as cropped images stacked together with separators between segments.\n\n        Args:\n            resolution: Resolution in DPI for rendering segment images\n            width: Optional width for segment images\n            stack_direction: Direction to stack segments ('vertical' or 'horizontal')\n            stack_gap: Gap in pixels between segments\n            stack_background_color: RGB background color for the final image\n            separator_color: RGB color for separator lines between segments\n            separator_thickness: Thickness in pixels of separator lines\n            **kwargs: Additional arguments passed to segment rendering\n\n        Returns:\n            PIL Image with all segments stacked together\n        \"\"\"\n        from PIL import Image, ImageDraw\n\n        segment_images = []\n        segment_pages = []\n\n        # Determine stacking direction\n        final_stack_direction = stack_direction\n        if stack_direction == \"auto\":\n            final_stack_direction = self.arrangement\n\n        # Get cropped images for each segment\n        for i, segment in enumerate(self.segments):\n            # Get the page reference for this segment\n            if hasattr(segment, \"page\") and segment.page is not None:\n                segment_page = segment.page\n                # Get cropped image of the segment\n                # Use render() for clean image without highlights\n                segment_image = segment.render(\n                    resolution=resolution,\n                    crop=True,\n                    width=width,\n                    **kwargs,\n                )\n\n            elif (\n                hasattr(segment, \"index\")\n                and hasattr(segment, \"width\")\n                and hasattr(segment, \"height\")\n            ):\n                # It's a full Page object\n                segment_page = segment\n                # Use render() for clean image without highlights\n                segment_image = segment.render(resolution=resolution, width=width, **kwargs)\n            else:\n                raise ValueError(\n                    f\"Segment {i+1} has no identifiable page. Segment type: {type(segment)}, attributes: {dir(segment)}\"\n                )\n\n            if segment_image is not None:\n                segment_images.append(segment_image)\n                segment_pages.append(segment_page)\n            else:\n                logger.warning(f\"Segment {i+1} render() returned None, skipping\")\n\n        # Check if we have any valid images\n        if not segment_images:\n            logger.error(\"No valid segment images could be rendered\")\n            return None\n\n        # We should have at least one segment image by now (or an exception would have been raised)\n        if len(segment_images) == 1:\n            return segment_images[0]\n\n        # Calculate dimensions for the final stacked image\n        if final_stack_direction == \"vertical\":\n            # Stack vertically\n            final_width = max(img.width for img in segment_images)\n\n            # Calculate total height including gaps and separators\n            total_height = sum(img.height for img in segment_images)\n            total_height += (len(segment_images) - 1) * stack_gap\n\n            # Add separator thickness between all segments\n            num_separators = len(segment_images) - 1 if len(segment_images) &gt; 1 else 0\n            total_height += num_separators * separator_thickness\n\n            # Create the final image\n            final_image = Image.new(\"RGB\", (final_width, total_height), stack_background_color)\n            draw = ImageDraw.Draw(final_image)\n\n            current_y = 0\n\n            for i, img in enumerate(segment_images):\n                # Add separator line before each segment (except the first one)\n                if i &gt; 0:\n                    # Draw separator line\n                    draw.rectangle(\n                        [(0, current_y), (final_width, current_y + separator_thickness)],\n                        fill=separator_color,\n                    )\n                    current_y += separator_thickness\n\n                # Paste the segment image\n                paste_x = (final_width - img.width) // 2  # Center horizontally\n                final_image.paste(img, (paste_x, current_y))\n                current_y += img.height\n\n                # Add gap after segment (except for the last one)\n                if i &lt; len(segment_images) - 1:\n                    current_y += stack_gap\n\n            return final_image\n\n        elif final_stack_direction == \"horizontal\":\n            # Stack horizontally\n            final_height = max(img.height for img in segment_images)\n\n            # Calculate total width including gaps and separators\n            total_width = sum(img.width for img in segment_images)\n            total_width += (len(segment_images) - 1) * stack_gap\n\n            # Add separator thickness between all segments\n            num_separators = len(segment_images) - 1 if len(segment_images) &gt; 1 else 0\n            total_width += num_separators * separator_thickness\n\n            # Create the final image\n            final_image = Image.new(\"RGB\", (total_width, final_height), stack_background_color)\n            draw = ImageDraw.Draw(final_image)\n\n            current_x = 0\n\n            for i, img in enumerate(segment_images):\n                # Add separator line before each segment (except the first one)\n                if i &gt; 0:\n                    # Draw separator line\n                    draw.rectangle(\n                        [(current_x, 0), (current_x + separator_thickness, final_height)],\n                        fill=separator_color,\n                    )\n                    current_x += separator_thickness\n\n                # Paste the segment image\n                paste_y = (final_height - img.height) // 2  # Center vertically\n                final_image.paste(img, (current_x, paste_y))\n                current_x += img.width\n\n                # Add gap after segment (except for the last one)\n                if i &lt; len(segment_images) - 1:\n                    current_x += stack_gap\n\n            return final_image\n\n        else:\n            raise ValueError(\n                f\"Invalid stack_direction '{final_stack_direction}' for in_context. Must be 'vertical' or 'horizontal'.\"\n            )\n\n    # --- Helper methods for coordinate transformations and segment iteration ---\n    # These will be crucial for FlowElement's directional methods.\n\n    def get_segment_bounding_box_in_flow(\n        self, segment_index: int\n    ) -&gt; Optional[tuple[float, float, float, float]]:\n        \"\"\"\n        Calculates the conceptual bounding box of a segment within the flow's coordinate system.\n        This considers arrangement, alignment, and segment gaps.\n        (This is a placeholder for more complex logic if a true virtual coordinate system is needed)\n        For now, it might just return the physical segment's bbox if gaps are 0 and alignment is simple.\n        \"\"\"\n        if segment_index &lt; 0 or segment_index &gt;= len(self.segments):\n            return None\n\n        # This is a simplified version. A full implementation would calculate offsets.\n        # For now, we assume FlowElement directional logic handles segment traversal and uses physical coords.\n        # If we were to *draw* the flow or get a FlowRegion bbox that spans gaps, this would be critical.\n        # physical_segment = self.segments[segment_index]\n        # return physical_segment.bbox\n        raise NotImplementedError(\n            \"Calculating a segment's bbox *within the flow's virtual coordinate system* is not yet fully implemented.\"\n        )\n\n    def get_element_flow_coordinates(\n        self, physical_element: \"PhysicalElement\"\n    ) -&gt; Optional[tuple[float, float, float, float]]:\n        \"\"\"\n        Translates a physical element's coordinates into the flow's virtual coordinate system.\n        (Placeholder - very complex if segment_gap &gt; 0 or complex alignments)\n        \"\"\"\n        # For now, elements operate in their own physical coordinates. This method would be needed\n        # if FlowRegion.bbox or other operations needed to present a unified coordinate space.\n        # As per our discussion, elements *within* a FlowRegion retain original physical coordinates.\n        # So, this might not be strictly necessary for the current design's core functionality.\n        raise NotImplementedError(\n            \"Translating element coordinates to a unified flow coordinate system is not yet implemented.\"\n        )\n\n    def get_sections(\n        self,\n        start_elements=None,\n        end_elements=None,\n        new_section_on_page_break: bool = False,\n        include_boundaries: str = \"both\",\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Extract logical sections from the Flow based on *start* and *end* boundary\n        elements, mirroring the behaviour of PDF/PageCollection.get_sections().\n\n        This implementation is a thin wrapper that converts the Flow into a\n        temporary PageCollection (constructed from the unique pages that the\n        Flow spans) and then delegates the heavy\u2010lifting to that existing\n        implementation.  Any FlowElement / FlowElementCollection inputs are\n        automatically unwrapped to their underlying physical elements so that\n        PageCollection can work with them directly.\n\n        Args:\n            start_elements: Elements or selector string that mark the start of\n                sections (optional).\n            end_elements: Elements or selector string that mark the end of\n                sections (optional).\n            new_section_on_page_break: Whether to start a new section at page\n                boundaries (default: False).\n            include_boundaries: How to include boundary elements: 'start',\n                'end', 'both', or 'none' (default: 'both').\n\n        Returns:\n            ElementCollection of Region/FlowRegion objects representing the\n            extracted sections.\n        \"\"\"\n        # ------------------------------------------------------------------\n        # Unwrap FlowElement(-Collection) inputs and selector strings so we\n        # can reason about them generically.\n        # ------------------------------------------------------------------\n        from natural_pdf.flows.collections import FlowElementCollection\n        from natural_pdf.flows.element import FlowElement\n\n        def _unwrap(obj):\n            \"\"\"Convert Flow-specific wrappers to their underlying physical objects.\n\n            Keeps selector strings as-is; converts FlowElement to its physical\n            element; converts FlowElementCollection to list of physical\n            elements; passes through ElementCollection by taking .elements.\n            \"\"\"\n\n            if obj is None or isinstance(obj, str):\n                return obj\n\n            if isinstance(obj, FlowElement):\n                return obj.physical_object\n\n            if isinstance(obj, FlowElementCollection):\n                return [fe.physical_object for fe in obj.flow_elements]\n\n            if hasattr(obj, \"elements\"):\n                return obj.elements\n\n            if isinstance(obj, (list, tuple, set)):\n                out = []\n                for item in obj:\n                    if isinstance(item, FlowElement):\n                        out.append(item.physical_object)\n                    else:\n                        out.append(item)\n                return out\n\n            return obj  # Fallback \u2013 unknown type\n\n        start_elements_unwrapped = _unwrap(start_elements)\n        end_elements_unwrapped = _unwrap(end_elements)\n\n        # ------------------------------------------------------------------\n        # PRIMARY IMPLEMENTATION \u2013 operate on each Flow **segment region**\n        # independently so that sectioning happens *per-region*, not per page.\n        # ------------------------------------------------------------------\n        from natural_pdf.elements.element_collection import ElementCollection\n\n        aggregated_sections = []\n\n        # Helper to decide if an element lies inside a segment (Region)\n        def _element_in_segment(elem, segment_region):\n            try:\n                return segment_region.intersects(elem)  # Region method \u2013 robust\n            except Exception:\n                # Fallback to bounding-box containment checks\n                if not hasattr(elem, \"bbox\"):\n                    return False\n                ex0, etop, ex1, ebottom = elem.bbox\n                sx0, stop, sx1, sbottom = segment_region.bbox\n                return not (ex1 &lt; sx0 or ex0 &gt; sx1 or ebottom &lt; stop or etop &gt; sbottom)\n\n        for seg in self.segments:\n            # Each *seg* is guaranteed to be a Region (see _normalize_segments)\n\n            # Resolve segment-specific boundary arguments\n            seg_start_elems = None\n            seg_end_elems = None\n\n            # --- Handle selector strings ---\n            if isinstance(start_elements_unwrapped, str):\n                seg_start_elems = seg.find_all(start_elements_unwrapped).elements\n            elif start_elements_unwrapped is not None:\n                seg_start_elems = [\n                    e for e in start_elements_unwrapped if _element_in_segment(e, seg)\n                ]\n\n            if isinstance(end_elements_unwrapped, str):\n                seg_end_elems = seg.find_all(end_elements_unwrapped).elements\n            elif end_elements_unwrapped is not None:\n                seg_end_elems = [e for e in end_elements_unwrapped if _element_in_segment(e, seg)]\n\n            # Call Region.get_sections \u2013 this returns ElementCollection[Region]\n            seg_sections = seg.get_sections(\n                start_elements=seg_start_elems,\n                end_elements=seg_end_elems,\n                include_boundaries=include_boundaries,\n            )\n\n            if seg_sections:\n                aggregated_sections.extend(seg_sections.elements)\n\n            # Optionally, handle new_section_on_page_break \u2013 interpreted here as\n            # *new_section_on_segment_break*: if True and there were *no* explicit\n            # boundaries, treat the entire segment as a single section.\n            if (\n                new_section_on_page_break\n                and not seg_sections\n                and start_elements_unwrapped is None\n                and end_elements_unwrapped is None\n            ):\n                aggregated_sections.append(seg)\n\n        # ------------------------------------------------------------------\n        # CROSS-SEGMENT SECTION DETECTION: Check if we have boundaries that\n        # span multiple segments and create FlowRegions for those cases.\n        # ------------------------------------------------------------------\n\n        # If we have explicit start/end elements, check for cross-segment sections\n        if start_elements_unwrapped is not None and end_elements_unwrapped is not None:\n            # Find all start and end elements across all segments\n            all_start_elements = []\n            all_end_elements = []\n\n            # Map elements to their segments for tracking\n            element_to_segment = {}\n\n            for seg_idx, seg in enumerate(self.segments):\n                if isinstance(start_elements_unwrapped, str):\n                    seg_starts = seg.find_all(start_elements_unwrapped).elements\n                else:\n                    seg_starts = [\n                        e for e in start_elements_unwrapped if _element_in_segment(e, seg)\n                    ]\n\n                if isinstance(end_elements_unwrapped, str):\n                    seg_ends = seg.find_all(end_elements_unwrapped).elements\n                else:\n                    seg_ends = [e for e in end_elements_unwrapped if _element_in_segment(e, seg)]\n\n                for elem in seg_starts:\n                    all_start_elements.append((elem, seg_idx))\n                    element_to_segment[id(elem)] = seg_idx\n\n                for elem in seg_ends:\n                    all_end_elements.append((elem, seg_idx))\n                    element_to_segment[id(elem)] = seg_idx\n\n            # Sort by segment index, then by position within segment\n            all_start_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n            all_end_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n\n            # Look for cross-segment pairs (start in one segment, end in another)\n            cross_segment_sections = []\n            used_starts = set()\n            used_ends = set()\n\n            for start_elem, start_seg_idx in all_start_elements:\n                if id(start_elem) in used_starts:\n                    continue\n\n                # Find the next end element that comes after this start\n                matching_end = None\n                for end_elem, end_seg_idx in all_end_elements:\n                    if id(end_elem) in used_ends:\n                        continue\n\n                    # Check if this end comes after the start (by segment order or position)\n                    if end_seg_idx &gt; start_seg_idx or (\n                        end_seg_idx == start_seg_idx\n                        and (\n                            end_elem.top &gt; start_elem.top\n                            or (end_elem.top == start_elem.top and end_elem.x0 &gt;= start_elem.x0)\n                        )\n                    ):\n                        matching_end = (end_elem, end_seg_idx)\n                        break\n\n                if matching_end is not None:\n                    end_elem, end_seg_idx = matching_end\n\n                    # If start and end are in different segments, create FlowRegion\n                    if start_seg_idx != end_seg_idx:\n                        cross_segment_sections.append(\n                            (start_elem, start_seg_idx, end_elem, end_seg_idx)\n                        )\n                        used_starts.add(id(start_elem))\n                        used_ends.add(id(end_elem))\n\n            # Create FlowRegions for cross-segment sections\n            from natural_pdf.elements.region import Region\n            from natural_pdf.flows.element import FlowElement\n            from natural_pdf.flows.region import FlowRegion\n\n            for start_elem, start_seg_idx, end_elem, end_seg_idx in cross_segment_sections:\n                # Build constituent regions spanning from start segment to end segment\n                constituent_regions = []\n\n                # First segment: from start element to bottom\n                start_seg = self.segments[start_seg_idx]\n                first_region = Region(\n                    start_seg.page, (start_seg.x0, start_elem.top, start_seg.x1, start_seg.bottom)\n                )\n                constituent_regions.append(first_region)\n\n                # Middle segments: full segments\n                for seg_idx in range(start_seg_idx + 1, end_seg_idx):\n                    constituent_regions.append(self.segments[seg_idx])\n\n                # Last segment: from top to end element\n                if end_seg_idx != start_seg_idx:\n                    end_seg = self.segments[end_seg_idx]\n                    last_region = Region(\n                        end_seg.page, (end_seg.x0, end_seg.top, end_seg.x1, end_elem.bottom)\n                    )\n                    constituent_regions.append(last_region)\n\n                # Create FlowRegion\n                flow_element = FlowElement(physical_object=start_elem, flow=self)\n                flow_region = FlowRegion(\n                    flow=self,\n                    constituent_regions=constituent_regions,\n                    source_flow_element=flow_element,\n                    boundary_element_found=end_elem,\n                )\n\n                # Remove any single-segment sections that are now covered by this FlowRegion\n                # This prevents duplication of content\n                aggregated_sections = [\n                    s\n                    for s in aggregated_sections\n                    if not any(\n                        cr.intersects(s)\n                        for cr in constituent_regions\n                        if hasattr(cr, \"intersects\") and hasattr(s, \"intersects\")\n                    )\n                ]\n\n                aggregated_sections.append(flow_region)\n\n        # ------------------------------------------------------------------\n        # NEW APPROACH: First collect ALL boundary elements across all segments,\n        # then pair them up to create sections (either single-segment Regions\n        # or multi-segment FlowRegions).\n        # ------------------------------------------------------------------\n        from natural_pdf.elements.element_collection import ElementCollection\n        from natural_pdf.elements.region import Region\n        from natural_pdf.flows.element import FlowElement\n        from natural_pdf.flows.region import FlowRegion\n\n        # Helper to decide if an element lies inside a segment (Region)\n        def _element_in_segment(elem, segment_region):\n            try:\n                return segment_region.intersects(elem)  # Region method \u2013 robust\n            except Exception:\n                # Fallback to bounding-box containment checks\n                if not hasattr(elem, \"bbox\"):\n                    return False\n                ex0, etop, ex1, ebottom = elem.bbox\n                sx0, stop, sx1, sbottom = segment_region.bbox\n                return not (ex1 &lt; sx0 or ex0 &gt; sx1 or ebottom &lt; stop or etop &gt; sbottom)\n\n        # Collect ALL boundary elements across all segments with their segment indices\n        all_start_elements = []\n        all_end_elements = []\n\n        for seg_idx, seg in enumerate(self.segments):\n            # Find start elements in this segment\n            if isinstance(start_elements_unwrapped, str):\n                seg_starts = seg.find_all(start_elements_unwrapped).elements\n            elif start_elements_unwrapped is not None:\n                seg_starts = [e for e in start_elements_unwrapped if _element_in_segment(e, seg)]\n            else:\n                seg_starts = []\n\n            logger.debug(f\"\\n=== Processing segment {seg_idx} ===\")\n            logger.debug(f\"Segment bbox: {seg.bbox}\")\n            logger.debug(\n                f\"Segment page: {seg.page.number if hasattr(seg.page, 'number') else 'unknown'}\"\n            )\n\n            logger.debug(f\"Found {len(seg_starts)} start elements in segment {seg_idx}\")\n            for i, elem in enumerate(seg_starts):\n                logger.debug(\n                    f\"  Start {i}: bbox={elem.bbox}, text='{getattr(elem, 'text', 'N/A')[:50]}...'\"\n                )\n\n            # Find end elements in this segment\n            if isinstance(end_elements_unwrapped, str):\n                seg_ends = seg.find_all(end_elements_unwrapped).elements\n            elif end_elements_unwrapped is not None:\n                seg_ends = [e for e in end_elements_unwrapped if _element_in_segment(e, seg)]\n            else:\n                seg_ends = []\n\n            logger.debug(f\"Found {len(seg_ends)} end elements in segment {seg_idx}\")\n            for i, elem in enumerate(seg_ends):\n                logger.debug(\n                    f\"  End {i}: bbox={elem.bbox}, text='{getattr(elem, 'text', 'N/A')[:50]}...'\"\n                )\n\n            # Add to global lists with segment index\n            for elem in seg_starts:\n                all_start_elements.append((elem, seg_idx))\n            for elem in seg_ends:\n                all_end_elements.append((elem, seg_idx))\n\n        # Sort by flow order: segment index first, then position within segment\n        all_start_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n        all_end_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n\n        logger.debug(f\"\\n=== Total boundary elements found ===\")\n        logger.debug(f\"Total start elements: {len(all_start_elements)}\")\n        logger.debug(f\"Total end elements: {len(all_end_elements)}\")\n\n        # Pair up start and end elements to create sections\n        sections = []\n        used_starts = set()\n        used_ends = set()\n\n        for start_elem, start_seg_idx in all_start_elements:\n            if id(start_elem) in used_starts:\n                continue\n\n            logger.debug(f\"\\n--- Pairing start element from segment {start_seg_idx} ---\")\n            logger.debug(\n                f\"Start: bbox={start_elem.bbox}, text='{getattr(start_elem, 'text', 'N/A')[:30]}...'\"\n            )\n\n            # Find the next unused end element that comes after this start\n            matching_end = None\n            for end_elem, end_seg_idx in all_end_elements:\n                if id(end_elem) in used_ends:\n                    continue\n\n                # Check if this end comes after the start in flow order\n                if end_seg_idx &gt; start_seg_idx or (\n                    end_seg_idx == start_seg_idx\n                    and (\n                        end_elem.top &gt; start_elem.top\n                        or (end_elem.top == start_elem.top and end_elem.x0 &gt;= start_elem.x0)\n                    )\n                ):\n                    matching_end = (end_elem, end_seg_idx)\n                    break\n\n            if matching_end is not None:\n                end_elem, end_seg_idx = matching_end\n                used_starts.add(id(start_elem))\n                used_ends.add(id(end_elem))\n\n                logger.debug(f\"  Matched! Start seg={start_seg_idx}, End seg={end_seg_idx}\")\n\n                # Create section based on whether it spans segments\n                if start_seg_idx == end_seg_idx:\n                    # Single segment section - use Region.get_section_between\n                    seg = self.segments[start_seg_idx]\n                    section = seg.get_section_between(start_elem, end_elem, include_boundaries)\n                    sections.append(section)\n                    logger.debug(f\"  Created single-segment Region\")\n                else:\n                    # Multi-segment section - create FlowRegion\n                    logger.debug(\n                        f\"  Creating multi-segment FlowRegion spanning segments {start_seg_idx} to {end_seg_idx}\"\n                    )\n                    constituent_regions = []\n\n                    # First segment: from start element to bottom\n                    start_seg = self.segments[start_seg_idx]\n                    if include_boundaries in [\"start\", \"both\"]:\n                        first_top = start_elem.top\n                    else:\n                        first_top = start_elem.bottom\n                    first_region = Region(\n                        start_seg.page, (start_seg.x0, first_top, start_seg.x1, start_seg.bottom)\n                    )\n                    constituent_regions.append(first_region)\n\n                    # Middle segments: full segments\n                    for seg_idx in range(start_seg_idx + 1, end_seg_idx):\n                        constituent_regions.append(self.segments[seg_idx])\n\n                    # Last segment: from top to end element\n                    end_seg = self.segments[end_seg_idx]\n                    if include_boundaries in [\"end\", \"both\"]:\n                        last_bottom = end_elem.bottom\n                    else:\n                        last_bottom = end_elem.top\n                    last_region = Region(\n                        end_seg.page, (end_seg.x0, end_seg.top, end_seg.x1, last_bottom)\n                    )\n                    constituent_regions.append(last_region)\n\n                    # Create FlowRegion\n                    flow_element = FlowElement(physical_object=start_elem, flow=self)\n                    flow_region = FlowRegion(\n                        flow=self,\n                        constituent_regions=constituent_regions,\n                        source_flow_element=flow_element,\n                        boundary_element_found=end_elem,\n                    )\n                    sections.append(flow_region)\n\n        # Handle special cases when only start or only end elements are provided\n        if start_elements_unwrapped is not None and end_elements_unwrapped is None:\n            logger.debug(f\"\\n=== Handling start-only elements (no end elements provided) ===\")\n            for i, (start_elem, start_seg_idx) in enumerate(all_start_elements):\n                if id(start_elem) in used_starts:\n                    continue\n\n                # Find next start element\n                next_start = None\n                if i + 1 &lt; len(all_start_elements):\n                    next_start_elem, next_start_seg_idx = all_start_elements[i + 1]\n                    # Create section from this start to just before next start\n                    if start_seg_idx == next_start_seg_idx:\n                        # Same segment\n                        seg = self.segments[start_seg_idx]\n                        # Find element just before next start\n                        all_elems = seg.get_elements()\n                        all_elems.sort(key=lambda e: (e.top, e.x0))\n                        try:\n                            next_idx = all_elems.index(next_start_elem)\n                            if next_idx &gt; 0:\n                                end_elem = all_elems[next_idx - 1]\n                                section = seg.get_section_between(\n                                    start_elem, end_elem, include_boundaries\n                                )\n                                sections.append(section)\n                        except ValueError:\n                            pass\n                    elif next_start_seg_idx == start_seg_idx + 1:\n                        # Next start is in the immediately following segment in the flow\n                        # Create a FlowRegion that spans from current start to just before next start\n                        logger.debug(f\"  Next start is in next flow segment - creating FlowRegion\")\n\n                        constituent_regions = []\n\n                        # First segment: from start element to bottom\n                        start_seg = self.segments[start_seg_idx]\n                        if include_boundaries in [\"start\", \"both\"]:\n                            first_top = start_elem.top\n                        else:\n                            first_top = start_elem.bottom\n                        first_region = Region(\n                            start_seg.page,\n                            (start_seg.x0, first_top, start_seg.x1, start_seg.bottom),\n                        )\n                        constituent_regions.append(first_region)\n\n                        # Next segment: from top to just before next start\n                        next_seg = self.segments[next_start_seg_idx]\n                        # Find element just before next start in the next segment\n                        next_seg_elems = next_seg.get_elements()\n                        next_seg_elems.sort(key=lambda e: (e.top, e.x0))\n\n                        last_bottom = next_start_elem.top  # Default to just before the next start\n                        try:\n                            next_idx = next_seg_elems.index(next_start_elem)\n                            if next_idx &gt; 0:\n                                # Use the bottom of the element before next start\n                                prev_elem = next_seg_elems[next_idx - 1]\n                                last_bottom = prev_elem.bottom\n                        except ValueError:\n                            pass\n\n                        last_region = Region(\n                            next_seg.page, (next_seg.x0, next_seg.top, next_seg.x1, last_bottom)\n                        )\n                        constituent_regions.append(last_region)\n\n                        # Create FlowRegion\n                        flow_element = FlowElement(physical_object=start_elem, flow=self)\n                        flow_region = FlowRegion(\n                            flow=self,\n                            constituent_regions=constituent_regions,\n                            source_flow_element=flow_element,\n                            boundary_element_found=None,\n                        )\n                        sections.append(flow_region)\n                        logger.debug(\n                            f\"  Created FlowRegion with {len(constituent_regions)} constituent regions\"\n                        )\n                    else:\n                        # Next start is more than one segment away - just end at current segment\n                        start_seg = self.segments[start_seg_idx]\n                        if include_boundaries in [\"start\", \"both\"]:\n                            region_top = start_elem.top\n                        else:\n                            region_top = start_elem.bottom\n                        section = Region(\n                            start_seg.page,\n                            (start_seg.x0, region_top, start_seg.x1, start_seg.bottom),\n                        )\n                        sections.append(section)\n                        logger.debug(\n                            f\"  Next start is {next_start_seg_idx - start_seg_idx} segments away - ending at current segment\"\n                        )\n                else:\n                    # Last start element: section goes to end of flow\n                    # This could span multiple segments\n                    if start_seg_idx == len(self.segments) - 1:\n                        # Only in last segment\n                        seg = self.segments[start_seg_idx]\n                        if include_boundaries in [\"start\", \"both\"]:\n                            region_top = start_elem.top\n                        else:\n                            region_top = start_elem.bottom\n                        section = Region(seg.page, (seg.x0, region_top, seg.x1, seg.bottom))\n                        sections.append(section)\n                    else:\n                        # Spans to end of flow - create FlowRegion\n                        constituent_regions = []\n\n                        # First segment\n                        start_seg = self.segments[start_seg_idx]\n                        if include_boundaries in [\"start\", \"both\"]:\n                            first_top = start_elem.top\n                        else:\n                            first_top = start_elem.bottom\n                        first_region = Region(\n                            start_seg.page,\n                            (start_seg.x0, first_top, start_seg.x1, start_seg.bottom),\n                        )\n                        constituent_regions.append(first_region)\n\n                        # Remaining segments\n                        for seg_idx in range(start_seg_idx + 1, len(self.segments)):\n                            constituent_regions.append(self.segments[seg_idx])\n\n                        flow_element = FlowElement(physical_object=start_elem, flow=self)\n                        flow_region = FlowRegion(\n                            flow=self,\n                            constituent_regions=constituent_regions,\n                            source_flow_element=flow_element,\n                            boundary_element_found=None,\n                        )\n                        sections.append(flow_region)\n\n        # Handle new_section_on_page_break when no explicit boundaries\n        if (\n            new_section_on_page_break\n            and start_elements_unwrapped is None\n            and end_elements_unwrapped is None\n        ):\n            # Each segment becomes its own section\n            sections = list(self.segments)\n\n        # Sort sections by their position in the flow\n        def _section_sort_key(section):\n            if hasattr(section, \"constituent_regions\"):\n                # FlowRegion - use first constituent region\n                first_region = (\n                    section.constituent_regions[0] if section.constituent_regions else None\n                )\n                if first_region:\n                    # Find which segment this region belongs to\n                    for idx, seg in enumerate(self.segments):\n                        try:\n                            if seg.intersects(first_region):\n                                return (\n                                    idx,\n                                    getattr(first_region, \"top\", 0),\n                                    getattr(first_region, \"x0\", 0),\n                                )\n                        except:\n                            pass\n            else:\n                # Regular Region\n                for idx, seg in enumerate(self.segments):\n                    try:\n                        if seg.intersects(section):\n                            return (idx, getattr(section, \"top\", 0), getattr(section, \"x0\", 0))\n                    except:\n                        pass\n            return (float(\"inf\"), 0, 0)\n\n        sections.sort(key=_section_sort_key)\n\n        logger.debug(f\"\\n=== Section creation complete ===\")\n        logger.debug(f\"Total sections created: {len(sections)}\")\n        for i, section in enumerate(sections):\n            if hasattr(section, \"constituent_regions\"):\n                logger.debug(\n                    f\"Section {i}: FlowRegion with {len(section.constituent_regions)} constituent regions\"\n                )\n            else:\n                logger.debug(f\"Section {i}: Region with bbox={section.bbox}\")\n\n        return ElementCollection(sections)\n\n    def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n        \"\"\"\n        Create a highlight context for accumulating highlights.\n\n        This allows for clean syntax to show multiple highlight groups:\n\n        Example:\n            with flow.highlights() as h:\n                h.add(flow.find_all('table'), label='tables', color='blue')\n                h.add(flow.find_all('text:bold'), label='bold text', color='red')\n                h.show()\n\n        Or with automatic display:\n            with flow.highlights(show=True) as h:\n                h.add(flow.find_all('table'), label='tables')\n                h.add(flow.find_all('text:bold'), label='bold')\n                # Automatically shows when exiting the context\n\n        Args:\n            show: If True, automatically show highlights when exiting context\n\n        Returns:\n            HighlightContext for accumulating highlights\n        \"\"\"\n        from natural_pdf.core.highlighting_service import HighlightContext\n\n        return HighlightContext(self, show_on_exit=show)\n</code></pre>"},{"location":"api/#natural_pdf.Flow-functions","title":"Functions","text":"<code>natural_pdf.Flow.__init__(segments, arrangement, alignment='start', segment_gap=0.0)</code> <p>Initializes a Flow object.</p> <p>Parameters:</p> Name Type Description Default <code>segments</code> <code>Union[List[Union[Page, Region]], PageCollection]</code> <p>An ordered list of natural_pdf.core.page.Page or       natural_pdf.elements.region.Region objects that constitute the flow,       or a PageCollection containing pages.</p> required <code>arrangement</code> <code>Literal['vertical', 'horizontal']</code> <p>The primary direction of the flow.          - \"vertical\": Segments are stacked top-to-bottom.          - \"horizontal\": Segments are arranged left-to-right.</p> required <code>alignment</code> <code>Literal['start', 'center', 'end', 'top', 'left', 'bottom', 'right']</code> <p>How segments are aligned on their cross-axis if they have        differing dimensions. For a \"vertical\" arrangement:        - \"left\" (or \"start\"): Align left edges.        - \"center\": Align centers.        - \"right\" (or \"end\"): Align right edges.        For a \"horizontal\" arrangement:        - \"top\" (or \"start\"): Align top edges.        - \"center\": Align centers.        - \"bottom\" (or \"end\"): Align bottom edges.</p> <code>'start'</code> <code>segment_gap</code> <code>float</code> <p>The virtual gap (in PDF points) between segments.</p> <code>0.0</code> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def __init__(\n    self,\n    segments: Union[List[Union[\"Page\", \"PhysicalRegion\"]], \"PageCollection\"],\n    arrangement: Literal[\"vertical\", \"horizontal\"],\n    alignment: Literal[\"start\", \"center\", \"end\", \"top\", \"left\", \"bottom\", \"right\"] = \"start\",\n    segment_gap: float = 0.0,\n):\n    \"\"\"\n    Initializes a Flow object.\n\n    Args:\n        segments: An ordered list of natural_pdf.core.page.Page or\n                  natural_pdf.elements.region.Region objects that constitute the flow,\n                  or a PageCollection containing pages.\n        arrangement: The primary direction of the flow.\n                     - \"vertical\": Segments are stacked top-to-bottom.\n                     - \"horizontal\": Segments are arranged left-to-right.\n        alignment: How segments are aligned on their cross-axis if they have\n                   differing dimensions. For a \"vertical\" arrangement:\n                   - \"left\" (or \"start\"): Align left edges.\n                   - \"center\": Align centers.\n                   - \"right\" (or \"end\"): Align right edges.\n                   For a \"horizontal\" arrangement:\n                   - \"top\" (or \"start\"): Align top edges.\n                   - \"center\": Align centers.\n                   - \"bottom\" (or \"end\"): Align bottom edges.\n        segment_gap: The virtual gap (in PDF points) between segments.\n    \"\"\"\n    # Handle PageCollection input\n    if hasattr(segments, \"pages\"):  # It's a PageCollection\n        segments = list(segments.pages)\n\n    if not segments:\n        raise ValueError(\"Flow segments cannot be empty.\")\n    if arrangement not in [\"vertical\", \"horizontal\"]:\n        raise ValueError(\"Arrangement must be 'vertical' or 'horizontal'.\")\n\n    self.segments: List[\"PhysicalRegion\"] = self._normalize_segments(segments)\n    self.arrangement: Literal[\"vertical\", \"horizontal\"] = arrangement\n    self.alignment: Literal[\"start\", \"center\", \"end\", \"top\", \"left\", \"bottom\", \"right\"] = (\n        alignment\n    )\n    self.segment_gap: float = segment_gap\n\n    self._validate_alignment()\n</code></pre> <code>natural_pdf.Flow.analyze_layout(engine=None, options=None, confidence=None, classes=None, exclude_classes=None, device=None, existing='replace', model_name=None, client=None)</code> <p>Analyze layout across all segments in the flow.</p> <p>This method efficiently groups segments by their parent pages, runs layout analysis only once per unique page, then filters results appropriately for each segment. This avoids redundant analysis when multiple flow segments come from the same page.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Optional[str]</code> <p>Name of the layout engine (e.g., 'yolo', 'tatr'). Uses manager's default if None.</p> <code>None</code> <code>options</code> <code>Optional[Any]</code> <p>Specific LayoutOptions object for advanced configuration.</p> <code>None</code> <code>confidence</code> <code>Optional[float]</code> <p>Minimum confidence threshold.</p> <code>None</code> <code>classes</code> <code>Optional[List[str]]</code> <p>Specific classes to detect.</p> <code>None</code> <code>exclude_classes</code> <code>Optional[List[str]]</code> <p>Classes to exclude.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device for inference.</p> <code>None</code> <code>existing</code> <code>str</code> <p>How to handle existing detected regions: 'replace' (default) or 'append'.</p> <code>'replace'</code> <code>model_name</code> <code>Optional[str]</code> <p>Optional model name for the engine.</p> <code>None</code> <code>client</code> <code>Optional[Any]</code> <p>Optional client for API-based engines.</p> <code>None</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection containing all detected Region objects from all segments.</p> Example <p>Multi-page layout analysis: <pre><code>pdf = npdf.PDF(\"document.pdf\")\n\n# Create flow for first 3 pages\npage_flow = Flow(\n    segments=pdf.pages[:3],\n    arrangement='vertical'\n)\n\n# Analyze layout across all pages (efficiently)\nall_regions = page_flow.analyze_layout(engine='yolo')\n\n# Find all tables across the flow\ntables = all_regions.filter('region[type=table]')\n</code></pre></p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def analyze_layout(\n    self,\n    engine: Optional[str] = None,\n    options: Optional[Any] = None,\n    confidence: Optional[float] = None,\n    classes: Optional[List[str]] = None,\n    exclude_classes: Optional[List[str]] = None,\n    device: Optional[str] = None,\n    existing: str = \"replace\",\n    model_name: Optional[str] = None,\n    client: Optional[Any] = None,\n) -&gt; \"PhysicalElementCollection\":\n    \"\"\"\n    Analyze layout across all segments in the flow.\n\n    This method efficiently groups segments by their parent pages, runs layout analysis\n    only once per unique page, then filters results appropriately for each segment.\n    This avoids redundant analysis when multiple flow segments come from the same page.\n\n    Args:\n        engine: Name of the layout engine (e.g., 'yolo', 'tatr'). Uses manager's default if None.\n        options: Specific LayoutOptions object for advanced configuration.\n        confidence: Minimum confidence threshold.\n        classes: Specific classes to detect.\n        exclude_classes: Classes to exclude.\n        device: Device for inference.\n        existing: How to handle existing detected regions: 'replace' (default) or 'append'.\n        model_name: Optional model name for the engine.\n        client: Optional client for API-based engines.\n\n    Returns:\n        ElementCollection containing all detected Region objects from all segments.\n\n    Example:\n        Multi-page layout analysis:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n\n        # Create flow for first 3 pages\n        page_flow = Flow(\n            segments=pdf.pages[:3],\n            arrangement='vertical'\n        )\n\n        # Analyze layout across all pages (efficiently)\n        all_regions = page_flow.analyze_layout(engine='yolo')\n\n        # Find all tables across the flow\n        tables = all_regions.filter('region[type=table]')\n        ```\n    \"\"\"\n    from natural_pdf.elements.element_collection import ElementCollection\n\n    logger.info(\n        f\"Analyzing layout across Flow with {len(self.segments)} segments (engine: {engine or 'default'})\"\n    )\n\n    if not self.segments:\n        logger.warning(\"Flow has no segments, returning empty collection\")\n        return ElementCollection([])\n\n    # Step 1: Group segments by their parent pages to avoid redundant analysis\n    segments_by_page = {}  # Dict[Page, List[Segment]]\n\n    for i, segment in enumerate(self.segments):\n        # Determine the page for this segment\n        if hasattr(segment, \"analyze_layout\"):\n            # It's a Page object\n            page_obj = segment\n            segment_type = \"page\"\n        elif hasattr(segment, \"page\") and hasattr(segment.page, \"analyze_layout\"):\n            # It's a Region object\n            page_obj = segment.page\n            segment_type = \"region\"\n        else:\n            logger.warning(f\"Segment {i+1} does not support layout analysis, skipping\")\n            continue\n\n        if page_obj not in segments_by_page:\n            segments_by_page[page_obj] = []\n        segments_by_page[page_obj].append((segment, segment_type))\n\n    if not segments_by_page:\n        logger.warning(\"No segments with analyzable pages found\")\n        return ElementCollection([])\n\n    logger.debug(\n        f\"  Grouped {len(self.segments)} segments into {len(segments_by_page)} unique pages\"\n    )\n\n    # Step 2: Analyze each unique page only once\n    all_detected_regions: List[\"PhysicalRegion\"] = []\n    processed_pages = 0\n\n    for page_obj, page_segments in segments_by_page.items():\n        try:\n            logger.debug(\n                f\"  Analyzing layout for page {getattr(page_obj, 'number', '?')} with {len(page_segments)} segments\"\n            )\n\n            # Run layout analysis once for this page\n            page_results = page_obj.analyze_layout(\n                engine=engine,\n                options=options,\n                confidence=confidence,\n                classes=classes,\n                exclude_classes=exclude_classes,\n                device=device,\n                existing=existing,\n                model_name=model_name,\n                client=client,\n            )\n\n            # Extract regions from results\n            if hasattr(page_results, \"elements\"):\n                # It's an ElementCollection\n                page_regions = page_results.elements\n            elif isinstance(page_results, list):\n                # It's a list of regions\n                page_regions = page_results\n            else:\n                logger.warning(\n                    f\"Page {getattr(page_obj, 'number', '?')} returned unexpected layout analysis result type: {type(page_results)}\"\n                )\n                continue\n\n            if not page_regions:\n                logger.debug(\n                    f\"    No layout regions found on page {getattr(page_obj, 'number', '?')}\"\n                )\n                continue\n\n            # Step 3: For each segment on this page, collect relevant regions\n            segments_processed_on_page = 0\n            for segment, segment_type in page_segments:\n                if segment_type == \"page\":\n                    # Full page segment: include all detected regions\n                    all_detected_regions.extend(page_regions)\n                    segments_processed_on_page += 1\n                    logger.debug(f\"    Added {len(page_regions)} regions for full-page segment\")\n\n                elif segment_type == \"region\":\n                    # Region segment: filter to only intersecting regions\n                    intersecting_regions = []\n                    for region in page_regions:\n                        try:\n                            if segment.intersects(region):\n                                intersecting_regions.append(region)\n                        except Exception as intersect_error:\n                            logger.debug(\n                                f\"Error checking intersection for region: {intersect_error}\"\n                            )\n                            # Include the region anyway if intersection check fails\n                            intersecting_regions.append(region)\n\n                    all_detected_regions.extend(intersecting_regions)\n                    segments_processed_on_page += 1\n                    logger.debug(\n                        f\"    Added {len(intersecting_regions)} intersecting regions for region segment {segment.bbox}\"\n                    )\n\n            processed_pages += 1\n            logger.debug(\n                f\"    Processed {segments_processed_on_page} segments on page {getattr(page_obj, 'number', '?')}\"\n            )\n\n        except Exception as e:\n            logger.error(\n                f\"Error analyzing layout for page {getattr(page_obj, 'number', '?')}: {e}\",\n                exc_info=True,\n            )\n            continue\n\n    # Step 4: Remove duplicates (can happen if multiple segments intersect the same region)\n    unique_regions = []\n    seen_region_ids = set()\n\n    for region in all_detected_regions:\n        # Create a unique identifier for this region (page + bbox)\n        region_id = (\n            getattr(region.page, \"index\", id(region.page)),\n            region.bbox if hasattr(region, \"bbox\") else id(region),\n        )\n\n        if region_id not in seen_region_ids:\n            unique_regions.append(region)\n            seen_region_ids.add(region_id)\n\n    dedupe_removed = len(all_detected_regions) - len(unique_regions)\n    if dedupe_removed &gt; 0:\n        logger.debug(f\"  Removed {dedupe_removed} duplicate regions\")\n\n    logger.info(\n        f\"Flow layout analysis complete: {len(unique_regions)} unique regions from {processed_pages} pages\"\n    )\n    return ElementCollection(unique_regions)\n</code></pre> <code>natural_pdf.Flow.extract_table(method=None, table_settings=None, use_ocr=False, ocr_config=None, text_options=None, cell_extraction_func=None, show_progress=False, content_filter=None, stitch_rows=None, merge_headers=None)</code> <pre><code>extract_table(method: Optional[str] = None, table_settings: Optional[dict] = None, use_ocr: bool = False, ocr_config: Optional[dict] = None, text_options: Optional[dict] = None, cell_extraction_func: Optional[Any] = None, show_progress: bool = False, content_filter: Optional[Any] = None, stitch_rows: Callable[[List[Optional[str]]], bool] = None) -&gt; TableResult\n</code></pre><pre><code>extract_table(method: Optional[str] = None, table_settings: Optional[dict] = None, use_ocr: bool = False, ocr_config: Optional[dict] = None, text_options: Optional[dict] = None, cell_extraction_func: Optional[Any] = None, show_progress: bool = False, content_filter: Optional[Any] = None, stitch_rows: Callable[[List[Optional[str]], List[Optional[str]], int, Union[Page, PhysicalRegion]], bool] = None) -&gt; TableResult\n</code></pre> <p>Extract table data from all segments in the flow, combining results sequentially.</p> <p>This method extracts table data from each segment in flow order and combines the results into a single logical table. This is particularly useful for multi-page tables or tables that span across columns.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Optional[str]</code> <p>Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).</p> <code>None</code> <code>table_settings</code> <code>Optional[dict]</code> <p>Settings for pdfplumber table extraction.</p> <code>None</code> <code>use_ocr</code> <code>bool</code> <p>Whether to use OCR for text extraction (currently only applicable with 'tatr' method).</p> <code>False</code> <code>ocr_config</code> <code>Optional[dict]</code> <p>OCR configuration parameters.</p> <code>None</code> <code>text_options</code> <code>Optional[dict]</code> <p>Dictionary of options for the 'text' method.</p> <code>None</code> <code>cell_extraction_func</code> <code>Optional[Any]</code> <p>Optional callable function that takes a cell Region object                   and returns its string content. For 'text' method only.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>If True, display a progress bar during cell text extraction for the 'text' method.</p> <code>False</code> <code>content_filter</code> <code>Optional[Any]</code> <p>Optional content filter to apply during cell text extraction.</p> <code>None</code> <code>merge_headers</code> <code>Optional[bool]</code> <p>Whether to merge tables by removing repeated headers from subsequent segments. If None (default), auto-detects by checking if the first row of each segment matches the first row of the first segment. If segments have inconsistent header patterns (some repeat, others don't), raises ValueError. Useful for multi-page tables where headers repeat on each page.</p> <code>None</code> <code>stitch_rows</code> <code>Optional[Callable]</code> <p>Optional callable to determine when rows should be merged across          segment boundaries. Applied AFTER header removal if merge_headers          is enabled. Two overloaded signatures are supported:</p> <pre><code>     \u2022 func(current_row) -&gt; bool\n       Called only on the first row of each segment (after the first).\n       Return True to merge this first row with the last row from\n       the previous segment.\n\n     \u2022 func(prev_row, current_row, row_index, segment) -&gt; bool\n       Called for every row. Return True to merge current_row with\n       the previous row in the aggregated results.\n\n     When True is returned, rows are concatenated cell-by-cell.\n     This is useful for handling table rows split across page\n     boundaries or segments. If None, rows are never merged.\n</code></pre> <code>None</code> <p>Returns:</p> Type Description <code>TableResult</code> <p>TableResult object containing the aggregated table data from all segments.</p> Example <p>Multi-page table extraction: <pre><code>pdf = npdf.PDF(\"multi_page_table.pdf\")\n\n# Create flow for table spanning pages 2-4\ntable_flow = Flow(\n    segments=[pdf.pages[1], pdf.pages[2], pdf.pages[3]],\n    arrangement='vertical'\n)\n\n# Extract table as if it were continuous\ntable_data = table_flow.extract_table()\ndf = table_data.df  # Convert to pandas DataFrame\n\n# Custom row stitching - single parameter (simple case)\ntable_data = table_flow.extract_table(\n    stitch_rows=lambda row: row and not (row[0] or \"\").strip()\n)\n\n# Custom row stitching - full parameters (advanced case)\ntable_data = table_flow.extract_table(\n    stitch_rows=lambda prev, curr, idx, seg: idx == 0 and curr and not (curr[0] or \"\").strip()\n)\n</code></pre></p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def extract_table(\n    self,\n    method: Optional[str] = None,\n    table_settings: Optional[dict] = None,\n    use_ocr: bool = False,\n    ocr_config: Optional[dict] = None,\n    text_options: Optional[dict] = None,\n    cell_extraction_func: Optional[Any] = None,\n    show_progress: bool = False,\n    content_filter: Optional[Any] = None,\n    stitch_rows: Optional[Callable] = None,\n    merge_headers: Optional[bool] = None,\n) -&gt; TableResult:\n    \"\"\"\n    Extract table data from all segments in the flow, combining results sequentially.\n\n    This method extracts table data from each segment in flow order and combines\n    the results into a single logical table. This is particularly useful for\n    multi-page tables or tables that span across columns.\n\n    Args:\n        method: Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).\n        table_settings: Settings for pdfplumber table extraction.\n        use_ocr: Whether to use OCR for text extraction (currently only applicable with 'tatr' method).\n        ocr_config: OCR configuration parameters.\n        text_options: Dictionary of options for the 'text' method.\n        cell_extraction_func: Optional callable function that takes a cell Region object\n                              and returns its string content. For 'text' method only.\n        show_progress: If True, display a progress bar during cell text extraction for the 'text' method.\n        content_filter: Optional content filter to apply during cell text extraction.\n        merge_headers: Whether to merge tables by removing repeated headers from subsequent\n            segments. If None (default), auto-detects by checking if the first row\n            of each segment matches the first row of the first segment. If segments have\n            inconsistent header patterns (some repeat, others don't), raises ValueError.\n            Useful for multi-page tables where headers repeat on each page.\n        stitch_rows: Optional callable to determine when rows should be merged across\n                     segment boundaries. Applied AFTER header removal if merge_headers\n                     is enabled. Two overloaded signatures are supported:\n\n                     \u2022 func(current_row) -&gt; bool\n                       Called only on the first row of each segment (after the first).\n                       Return True to merge this first row with the last row from\n                       the previous segment.\n\n                     \u2022 func(prev_row, current_row, row_index, segment) -&gt; bool\n                       Called for every row. Return True to merge current_row with\n                       the previous row in the aggregated results.\n\n                     When True is returned, rows are concatenated cell-by-cell.\n                     This is useful for handling table rows split across page\n                     boundaries or segments. If None, rows are never merged.\n\n    Returns:\n        TableResult object containing the aggregated table data from all segments.\n\n    Example:\n        Multi-page table extraction:\n        ```python\n        pdf = npdf.PDF(\"multi_page_table.pdf\")\n\n        # Create flow for table spanning pages 2-4\n        table_flow = Flow(\n            segments=[pdf.pages[1], pdf.pages[2], pdf.pages[3]],\n            arrangement='vertical'\n        )\n\n        # Extract table as if it were continuous\n        table_data = table_flow.extract_table()\n        df = table_data.df  # Convert to pandas DataFrame\n\n        # Custom row stitching - single parameter (simple case)\n        table_data = table_flow.extract_table(\n            stitch_rows=lambda row: row and not (row[0] or \"\").strip()\n        )\n\n        # Custom row stitching - full parameters (advanced case)\n        table_data = table_flow.extract_table(\n            stitch_rows=lambda prev, curr, idx, seg: idx == 0 and curr and not (curr[0] or \"\").strip()\n        )\n        ```\n    \"\"\"\n    logger.info(\n        f\"Extracting table from Flow with {len(self.segments)} segments (method: {method or 'auto'})\"\n    )\n\n    if not self.segments:\n        logger.warning(\"Flow has no segments, returning empty table\")\n        return TableResult([])\n\n    # Resolve predicate and determine its signature\n    predicate: Optional[Callable] = None\n    predicate_type: str = \"none\"\n\n    if callable(stitch_rows):\n        import inspect\n\n        sig = inspect.signature(stitch_rows)\n        param_count = len(sig.parameters)\n\n        if param_count == 1:\n            predicate = stitch_rows\n            predicate_type = \"single_param\"\n        elif param_count == 4:\n            predicate = stitch_rows\n            predicate_type = \"full_params\"\n        else:\n            logger.warning(\n                f\"stitch_rows function has {param_count} parameters, expected 1 or 4. Ignoring.\"\n            )\n            predicate = None\n            predicate_type = \"none\"\n\n    def _default_merge(\n        prev_row: List[Optional[str]], cur_row: List[Optional[str]]\n    ) -&gt; List[Optional[str]]:\n        from itertools import zip_longest\n\n        merged: List[Optional[str]] = []\n        for p, c in zip_longest(prev_row, cur_row, fillvalue=\"\"):\n            if (p or \"\").strip() and (c or \"\").strip():\n                merged.append(f\"{p} {c}\".strip())\n            else:\n                merged.append((p or \"\") + (c or \"\"))\n        return merged\n\n    aggregated_rows: List[List[Optional[str]]] = []\n    processed_segments = 0\n    header_row: Optional[List[Optional[str]]] = None\n    merge_headers_enabled = False\n    headers_warned = False  # Track if we've already warned about dropping headers\n    segment_has_repeated_header = []  # Track which segments have repeated headers\n\n    for seg_idx, segment in enumerate(self.segments):\n        try:\n            logger.debug(f\"  Extracting table from segment {seg_idx+1}/{len(self.segments)}\")\n\n            segment_result = segment.extract_table(\n                method=method,\n                table_settings=table_settings.copy() if table_settings else None,\n                use_ocr=use_ocr,\n                ocr_config=ocr_config,\n                text_options=text_options.copy() if text_options else None,\n                cell_extraction_func=cell_extraction_func,\n                show_progress=show_progress,\n                content_filter=content_filter,\n            )\n\n            if not segment_result:\n                continue\n\n            if hasattr(segment_result, \"_rows\"):\n                segment_rows = list(segment_result._rows)\n            else:\n                segment_rows = list(segment_result)\n\n            if not segment_rows:\n                logger.debug(f\"    No table data found in segment {seg_idx+1}\")\n                continue\n\n            # Handle header detection and merging for multi-page tables\n            if seg_idx == 0:\n                # First segment: capture potential header row\n                if segment_rows:\n                    header_row = segment_rows[0]\n                    # Determine if we should merge headers\n                    if merge_headers is None:\n                        # Auto-detect: we'll check all subsequent segments\n                        merge_headers_enabled = False  # Will be determined later\n                    else:\n                        merge_headers_enabled = merge_headers\n                    # Track that first segment exists (for consistency checking)\n                    segment_has_repeated_header.append(False)  # First segment doesn't \"repeat\"\n            elif seg_idx == 1 and merge_headers is None:\n                # Auto-detection: check if first row of second segment matches header\n                has_header = segment_rows and header_row and segment_rows[0] == header_row\n                segment_has_repeated_header.append(has_header)\n\n                if has_header:\n                    merge_headers_enabled = True\n                    # Remove the detected repeated header from this segment\n                    segment_rows = segment_rows[1:]\n                    logger.debug(\n                        f\"    Auto-detected repeated header in segment {seg_idx+1}, removed\"\n                    )\n                    if not headers_warned:\n                        warnings.warn(\n                            \"Detected repeated headers in multi-page table. Merging by removing \"\n                            \"repeated headers from subsequent pages.\",\n                            UserWarning,\n                            stacklevel=2,\n                        )\n                        headers_warned = True\n                else:\n                    merge_headers_enabled = False\n                    logger.debug(f\"    No repeated header detected in segment {seg_idx+1}\")\n            elif seg_idx &gt; 1:\n                # Check consistency: all segments should have same pattern\n                has_header = segment_rows and header_row and segment_rows[0] == header_row\n                segment_has_repeated_header.append(has_header)\n\n                # Remove header if merging is enabled and header is present\n                if merge_headers_enabled and has_header:\n                    segment_rows = segment_rows[1:]\n                    logger.debug(f\"    Removed repeated header from segment {seg_idx+1}\")\n            elif seg_idx &gt; 0 and merge_headers_enabled:\n                # Explicit merge_headers=True: remove headers from subsequent segments\n                if segment_rows and header_row and segment_rows[0] == header_row:\n                    segment_rows = segment_rows[1:]\n                    logger.debug(f\"    Removed repeated header from segment {seg_idx+1}\")\n                    if not headers_warned:\n                        warnings.warn(\n                            \"Removing repeated headers from multi-page table during merge.\",\n                            UserWarning,\n                            stacklevel=2,\n                        )\n                        headers_warned = True\n\n            for row_idx, row in enumerate(segment_rows):\n                should_merge = False\n\n                if predicate is not None and aggregated_rows:\n                    if predicate_type == \"single_param\":\n                        # For single param: only call on first row of segment (row_idx == 0)\n                        # and pass the current row\n                        if row_idx == 0:\n                            should_merge = predicate(row)\n                    elif predicate_type == \"full_params\":\n                        # For full params: call with all arguments\n                        should_merge = predicate(aggregated_rows[-1], row, row_idx, segment)\n\n                if should_merge:\n                    aggregated_rows[-1] = _default_merge(aggregated_rows[-1], row)\n                else:\n                    aggregated_rows.append(row)\n\n            processed_segments += 1\n            logger.debug(\n                f\"    Added {len(segment_rows)} rows (post-merge) from segment {seg_idx+1}\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Error extracting table from segment {seg_idx+1}: {e}\", exc_info=True)\n            continue\n\n    # Check for inconsistent header patterns after processing all segments\n    if merge_headers is None and len(segment_has_repeated_header) &gt; 2:\n        # During auto-detection, check for consistency across all segments\n        expected_pattern = segment_has_repeated_header[1]  # Pattern from second segment\n        for seg_idx, has_header in enumerate(segment_has_repeated_header[2:], 2):\n            if has_header != expected_pattern:\n                # Inconsistent pattern detected\n                segments_with_headers = [\n                    i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if has_h\n                ]\n                segments_without_headers = [\n                    i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if not has_h\n                ]\n                raise ValueError(\n                    f\"Inconsistent header pattern in multi-page table: \"\n                    f\"segments {segments_with_headers} have repeated headers, \"\n                    f\"but segments {segments_without_headers} do not. \"\n                    f\"All segments must have the same header pattern for reliable merging.\"\n                )\n\n    logger.info(\n        f\"Flow table extraction complete: {len(aggregated_rows)} total rows from {processed_segments}/{len(self.segments)} segments\"\n    )\n    return TableResult(aggregated_rows)\n</code></pre> <code>natural_pdf.Flow.find(selector=None, *, text=None, apply_exclusions=True, regex=False, case=True, **kwargs)</code> <p>Finds the first element within the flow that matches the given selector or text criteria.</p> <p>Elements found are wrapped as FlowElement objects, anchored to this Flow.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for.</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to respect exclusion zones on the original pages/regions.</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether the text search uses regex.</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether the text search is case-sensitive.</p> <code>True</code> <code>**kwargs</code> <p>Additional filter parameters for the underlying find operation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[FlowElement]</code> <p>A FlowElement if a match is found, otherwise None.</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def find(\n    self,\n    selector: Optional[str] = None,\n    *,\n    text: Optional[str] = None,\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; Optional[\"FlowElement\"]:\n    \"\"\"\n    Finds the first element within the flow that matches the given selector or text criteria.\n\n    Elements found are wrapped as FlowElement objects, anchored to this Flow.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for.\n        apply_exclusions: Whether to respect exclusion zones on the original pages/regions.\n        regex: Whether the text search uses regex.\n        case: Whether the text search is case-sensitive.\n        **kwargs: Additional filter parameters for the underlying find operation.\n\n    Returns:\n        A FlowElement if a match is found, otherwise None.\n    \"\"\"\n    results = self.find_all(\n        selector=selector,\n        text=text,\n        apply_exclusions=apply_exclusions,\n        regex=regex,\n        case=case,\n        **kwargs,\n    )\n    return results.first if results else None\n</code></pre> <code>natural_pdf.Flow.find_all(selector=None, *, text=None, apply_exclusions=True, regex=False, case=True, **kwargs)</code> <p>Finds all elements within the flow that match the given selector or text criteria.</p> <p>This method efficiently groups segments by their parent pages, searches at the page level, then filters results appropriately for each segment. This ensures elements that intersect with flow segments (but aren't fully contained) are still found.</p> <p>Elements found are wrapped as FlowElement objects, anchored to this Flow, and returned in a FlowElementCollection.</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def find_all(\n    self,\n    selector: Optional[str] = None,\n    *,\n    text: Optional[str] = None,\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; \"FlowElementCollection\":\n    \"\"\"\n    Finds all elements within the flow that match the given selector or text criteria.\n\n    This method efficiently groups segments by their parent pages, searches at the page level,\n    then filters results appropriately for each segment. This ensures elements that intersect\n    with flow segments (but aren't fully contained) are still found.\n\n    Elements found are wrapped as FlowElement objects, anchored to this Flow,\n    and returned in a FlowElementCollection.\n    \"\"\"\n    from .collections import FlowElementCollection\n    from .element import FlowElement\n\n    # Step 1: Group segments by their parent pages (like in analyze_layout)\n    segments_by_page = {}  # Dict[Page, List[Segment]]\n\n    for i, segment in enumerate(self.segments):\n        # Determine the page for this segment - fix type detection\n        if hasattr(segment, \"page\") and hasattr(segment.page, \"find_all\"):\n            # It's a Region object (has a parent page)\n            page_obj = segment.page\n            segment_type = \"region\"\n        elif (\n            hasattr(segment, \"find_all\")\n            and hasattr(segment, \"width\")\n            and hasattr(segment, \"height\")\n            and not hasattr(segment, \"page\")\n        ):\n            # It's a Page object (has find_all but no parent page)\n            page_obj = segment\n            segment_type = \"page\"\n        else:\n            logger.warning(f\"Segment {i+1} does not support find_all, skipping\")\n            continue\n\n        if page_obj not in segments_by_page:\n            segments_by_page[page_obj] = []\n        segments_by_page[page_obj].append((segment, segment_type))\n\n    if not segments_by_page:\n        logger.warning(\"No segments with searchable pages found\")\n        return FlowElementCollection([])\n\n    # Step 2: Search each unique page only once\n    all_flow_elements: List[\"FlowElement\"] = []\n\n    for page_obj, page_segments in segments_by_page.items():\n        # Find all matching elements on this page\n        page_matches = page_obj.find_all(\n            selector=selector,\n            text=text,\n            apply_exclusions=apply_exclusions,\n            regex=regex,\n            case=case,\n            **kwargs,\n        )\n\n        if not page_matches:\n            continue\n\n        # Step 3: For each segment on this page, collect relevant elements\n        for segment, segment_type in page_segments:\n            if segment_type == \"page\":\n                # Full page segment: include all elements\n                for phys_elem in page_matches.elements:\n                    all_flow_elements.append(FlowElement(physical_object=phys_elem, flow=self))\n\n            elif segment_type == \"region\":\n                # Region segment: filter to only intersecting elements\n                for phys_elem in page_matches.elements:\n                    try:\n                        # Check if element intersects with this flow segment\n                        if segment.intersects(phys_elem):\n                            all_flow_elements.append(\n                                FlowElement(physical_object=phys_elem, flow=self)\n                            )\n                    except Exception as intersect_error:\n                        logger.debug(\n                            f\"Error checking intersection for element: {intersect_error}\"\n                        )\n                        # Include the element anyway if intersection check fails\n                        all_flow_elements.append(\n                            FlowElement(physical_object=phys_elem, flow=self)\n                        )\n\n    # Step 4: Remove duplicates (can happen if multiple segments intersect the same element)\n    unique_flow_elements = []\n    seen_element_ids = set()\n\n    for flow_elem in all_flow_elements:\n        # Create a unique identifier for the underlying physical element\n        phys_elem = flow_elem.physical_object\n        elem_id = (\n            (\n                getattr(phys_elem.page, \"index\", id(phys_elem.page))\n                if hasattr(phys_elem, \"page\")\n                else id(phys_elem)\n            ),\n            phys_elem.bbox if hasattr(phys_elem, \"bbox\") else id(phys_elem),\n        )\n\n        if elem_id not in seen_element_ids:\n            unique_flow_elements.append(flow_elem)\n            seen_element_ids.add(elem_id)\n\n    return FlowElementCollection(unique_flow_elements)\n</code></pre> <code>natural_pdf.Flow.get_element_flow_coordinates(physical_element)</code> <p>Translates a physical element's coordinates into the flow's virtual coordinate system. (Placeholder - very complex if segment_gap &gt; 0 or complex alignments)</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def get_element_flow_coordinates(\n    self, physical_element: \"PhysicalElement\"\n) -&gt; Optional[tuple[float, float, float, float]]:\n    \"\"\"\n    Translates a physical element's coordinates into the flow's virtual coordinate system.\n    (Placeholder - very complex if segment_gap &gt; 0 or complex alignments)\n    \"\"\"\n    # For now, elements operate in their own physical coordinates. This method would be needed\n    # if FlowRegion.bbox or other operations needed to present a unified coordinate space.\n    # As per our discussion, elements *within* a FlowRegion retain original physical coordinates.\n    # So, this might not be strictly necessary for the current design's core functionality.\n    raise NotImplementedError(\n        \"Translating element coordinates to a unified flow coordinate system is not yet implemented.\"\n    )\n</code></pre> <code>natural_pdf.Flow.get_sections(start_elements=None, end_elements=None, new_section_on_page_break=False, include_boundaries='both')</code> <p>Extract logical sections from the Flow based on start and end boundary elements, mirroring the behaviour of PDF/PageCollection.get_sections().</p> <p>This implementation is a thin wrapper that converts the Flow into a temporary PageCollection (constructed from the unique pages that the Flow spans) and then delegates the heavy\u2010lifting to that existing implementation.  Any FlowElement / FlowElementCollection inputs are automatically unwrapped to their underlying physical elements so that PageCollection can work with them directly.</p> <p>Parameters:</p> Name Type Description Default <code>start_elements</code> <p>Elements or selector string that mark the start of sections (optional).</p> <code>None</code> <code>end_elements</code> <p>Elements or selector string that mark the end of sections (optional).</p> <code>None</code> <code>new_section_on_page_break</code> <code>bool</code> <p>Whether to start a new section at page boundaries (default: False).</p> <code>False</code> <code>include_boundaries</code> <code>str</code> <p>How to include boundary elements: 'start', 'end', 'both', or 'none' (default: 'both').</p> <code>'both'</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection of Region/FlowRegion objects representing the</p> <code>ElementCollection</code> <p>extracted sections.</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def get_sections(\n    self,\n    start_elements=None,\n    end_elements=None,\n    new_section_on_page_break: bool = False,\n    include_boundaries: str = \"both\",\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Extract logical sections from the Flow based on *start* and *end* boundary\n    elements, mirroring the behaviour of PDF/PageCollection.get_sections().\n\n    This implementation is a thin wrapper that converts the Flow into a\n    temporary PageCollection (constructed from the unique pages that the\n    Flow spans) and then delegates the heavy\u2010lifting to that existing\n    implementation.  Any FlowElement / FlowElementCollection inputs are\n    automatically unwrapped to their underlying physical elements so that\n    PageCollection can work with them directly.\n\n    Args:\n        start_elements: Elements or selector string that mark the start of\n            sections (optional).\n        end_elements: Elements or selector string that mark the end of\n            sections (optional).\n        new_section_on_page_break: Whether to start a new section at page\n            boundaries (default: False).\n        include_boundaries: How to include boundary elements: 'start',\n            'end', 'both', or 'none' (default: 'both').\n\n    Returns:\n        ElementCollection of Region/FlowRegion objects representing the\n        extracted sections.\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Unwrap FlowElement(-Collection) inputs and selector strings so we\n    # can reason about them generically.\n    # ------------------------------------------------------------------\n    from natural_pdf.flows.collections import FlowElementCollection\n    from natural_pdf.flows.element import FlowElement\n\n    def _unwrap(obj):\n        \"\"\"Convert Flow-specific wrappers to their underlying physical objects.\n\n        Keeps selector strings as-is; converts FlowElement to its physical\n        element; converts FlowElementCollection to list of physical\n        elements; passes through ElementCollection by taking .elements.\n        \"\"\"\n\n        if obj is None or isinstance(obj, str):\n            return obj\n\n        if isinstance(obj, FlowElement):\n            return obj.physical_object\n\n        if isinstance(obj, FlowElementCollection):\n            return [fe.physical_object for fe in obj.flow_elements]\n\n        if hasattr(obj, \"elements\"):\n            return obj.elements\n\n        if isinstance(obj, (list, tuple, set)):\n            out = []\n            for item in obj:\n                if isinstance(item, FlowElement):\n                    out.append(item.physical_object)\n                else:\n                    out.append(item)\n            return out\n\n        return obj  # Fallback \u2013 unknown type\n\n    start_elements_unwrapped = _unwrap(start_elements)\n    end_elements_unwrapped = _unwrap(end_elements)\n\n    # ------------------------------------------------------------------\n    # PRIMARY IMPLEMENTATION \u2013 operate on each Flow **segment region**\n    # independently so that sectioning happens *per-region*, not per page.\n    # ------------------------------------------------------------------\n    from natural_pdf.elements.element_collection import ElementCollection\n\n    aggregated_sections = []\n\n    # Helper to decide if an element lies inside a segment (Region)\n    def _element_in_segment(elem, segment_region):\n        try:\n            return segment_region.intersects(elem)  # Region method \u2013 robust\n        except Exception:\n            # Fallback to bounding-box containment checks\n            if not hasattr(elem, \"bbox\"):\n                return False\n            ex0, etop, ex1, ebottom = elem.bbox\n            sx0, stop, sx1, sbottom = segment_region.bbox\n            return not (ex1 &lt; sx0 or ex0 &gt; sx1 or ebottom &lt; stop or etop &gt; sbottom)\n\n    for seg in self.segments:\n        # Each *seg* is guaranteed to be a Region (see _normalize_segments)\n\n        # Resolve segment-specific boundary arguments\n        seg_start_elems = None\n        seg_end_elems = None\n\n        # --- Handle selector strings ---\n        if isinstance(start_elements_unwrapped, str):\n            seg_start_elems = seg.find_all(start_elements_unwrapped).elements\n        elif start_elements_unwrapped is not None:\n            seg_start_elems = [\n                e for e in start_elements_unwrapped if _element_in_segment(e, seg)\n            ]\n\n        if isinstance(end_elements_unwrapped, str):\n            seg_end_elems = seg.find_all(end_elements_unwrapped).elements\n        elif end_elements_unwrapped is not None:\n            seg_end_elems = [e for e in end_elements_unwrapped if _element_in_segment(e, seg)]\n\n        # Call Region.get_sections \u2013 this returns ElementCollection[Region]\n        seg_sections = seg.get_sections(\n            start_elements=seg_start_elems,\n            end_elements=seg_end_elems,\n            include_boundaries=include_boundaries,\n        )\n\n        if seg_sections:\n            aggregated_sections.extend(seg_sections.elements)\n\n        # Optionally, handle new_section_on_page_break \u2013 interpreted here as\n        # *new_section_on_segment_break*: if True and there were *no* explicit\n        # boundaries, treat the entire segment as a single section.\n        if (\n            new_section_on_page_break\n            and not seg_sections\n            and start_elements_unwrapped is None\n            and end_elements_unwrapped is None\n        ):\n            aggregated_sections.append(seg)\n\n    # ------------------------------------------------------------------\n    # CROSS-SEGMENT SECTION DETECTION: Check if we have boundaries that\n    # span multiple segments and create FlowRegions for those cases.\n    # ------------------------------------------------------------------\n\n    # If we have explicit start/end elements, check for cross-segment sections\n    if start_elements_unwrapped is not None and end_elements_unwrapped is not None:\n        # Find all start and end elements across all segments\n        all_start_elements = []\n        all_end_elements = []\n\n        # Map elements to their segments for tracking\n        element_to_segment = {}\n\n        for seg_idx, seg in enumerate(self.segments):\n            if isinstance(start_elements_unwrapped, str):\n                seg_starts = seg.find_all(start_elements_unwrapped).elements\n            else:\n                seg_starts = [\n                    e for e in start_elements_unwrapped if _element_in_segment(e, seg)\n                ]\n\n            if isinstance(end_elements_unwrapped, str):\n                seg_ends = seg.find_all(end_elements_unwrapped).elements\n            else:\n                seg_ends = [e for e in end_elements_unwrapped if _element_in_segment(e, seg)]\n\n            for elem in seg_starts:\n                all_start_elements.append((elem, seg_idx))\n                element_to_segment[id(elem)] = seg_idx\n\n            for elem in seg_ends:\n                all_end_elements.append((elem, seg_idx))\n                element_to_segment[id(elem)] = seg_idx\n\n        # Sort by segment index, then by position within segment\n        all_start_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n        all_end_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n\n        # Look for cross-segment pairs (start in one segment, end in another)\n        cross_segment_sections = []\n        used_starts = set()\n        used_ends = set()\n\n        for start_elem, start_seg_idx in all_start_elements:\n            if id(start_elem) in used_starts:\n                continue\n\n            # Find the next end element that comes after this start\n            matching_end = None\n            for end_elem, end_seg_idx in all_end_elements:\n                if id(end_elem) in used_ends:\n                    continue\n\n                # Check if this end comes after the start (by segment order or position)\n                if end_seg_idx &gt; start_seg_idx or (\n                    end_seg_idx == start_seg_idx\n                    and (\n                        end_elem.top &gt; start_elem.top\n                        or (end_elem.top == start_elem.top and end_elem.x0 &gt;= start_elem.x0)\n                    )\n                ):\n                    matching_end = (end_elem, end_seg_idx)\n                    break\n\n            if matching_end is not None:\n                end_elem, end_seg_idx = matching_end\n\n                # If start and end are in different segments, create FlowRegion\n                if start_seg_idx != end_seg_idx:\n                    cross_segment_sections.append(\n                        (start_elem, start_seg_idx, end_elem, end_seg_idx)\n                    )\n                    used_starts.add(id(start_elem))\n                    used_ends.add(id(end_elem))\n\n        # Create FlowRegions for cross-segment sections\n        from natural_pdf.elements.region import Region\n        from natural_pdf.flows.element import FlowElement\n        from natural_pdf.flows.region import FlowRegion\n\n        for start_elem, start_seg_idx, end_elem, end_seg_idx in cross_segment_sections:\n            # Build constituent regions spanning from start segment to end segment\n            constituent_regions = []\n\n            # First segment: from start element to bottom\n            start_seg = self.segments[start_seg_idx]\n            first_region = Region(\n                start_seg.page, (start_seg.x0, start_elem.top, start_seg.x1, start_seg.bottom)\n            )\n            constituent_regions.append(first_region)\n\n            # Middle segments: full segments\n            for seg_idx in range(start_seg_idx + 1, end_seg_idx):\n                constituent_regions.append(self.segments[seg_idx])\n\n            # Last segment: from top to end element\n            if end_seg_idx != start_seg_idx:\n                end_seg = self.segments[end_seg_idx]\n                last_region = Region(\n                    end_seg.page, (end_seg.x0, end_seg.top, end_seg.x1, end_elem.bottom)\n                )\n                constituent_regions.append(last_region)\n\n            # Create FlowRegion\n            flow_element = FlowElement(physical_object=start_elem, flow=self)\n            flow_region = FlowRegion(\n                flow=self,\n                constituent_regions=constituent_regions,\n                source_flow_element=flow_element,\n                boundary_element_found=end_elem,\n            )\n\n            # Remove any single-segment sections that are now covered by this FlowRegion\n            # This prevents duplication of content\n            aggregated_sections = [\n                s\n                for s in aggregated_sections\n                if not any(\n                    cr.intersects(s)\n                    for cr in constituent_regions\n                    if hasattr(cr, \"intersects\") and hasattr(s, \"intersects\")\n                )\n            ]\n\n            aggregated_sections.append(flow_region)\n\n    # ------------------------------------------------------------------\n    # NEW APPROACH: First collect ALL boundary elements across all segments,\n    # then pair them up to create sections (either single-segment Regions\n    # or multi-segment FlowRegions).\n    # ------------------------------------------------------------------\n    from natural_pdf.elements.element_collection import ElementCollection\n    from natural_pdf.elements.region import Region\n    from natural_pdf.flows.element import FlowElement\n    from natural_pdf.flows.region import FlowRegion\n\n    # Helper to decide if an element lies inside a segment (Region)\n    def _element_in_segment(elem, segment_region):\n        try:\n            return segment_region.intersects(elem)  # Region method \u2013 robust\n        except Exception:\n            # Fallback to bounding-box containment checks\n            if not hasattr(elem, \"bbox\"):\n                return False\n            ex0, etop, ex1, ebottom = elem.bbox\n            sx0, stop, sx1, sbottom = segment_region.bbox\n            return not (ex1 &lt; sx0 or ex0 &gt; sx1 or ebottom &lt; stop or etop &gt; sbottom)\n\n    # Collect ALL boundary elements across all segments with their segment indices\n    all_start_elements = []\n    all_end_elements = []\n\n    for seg_idx, seg in enumerate(self.segments):\n        # Find start elements in this segment\n        if isinstance(start_elements_unwrapped, str):\n            seg_starts = seg.find_all(start_elements_unwrapped).elements\n        elif start_elements_unwrapped is not None:\n            seg_starts = [e for e in start_elements_unwrapped if _element_in_segment(e, seg)]\n        else:\n            seg_starts = []\n\n        logger.debug(f\"\\n=== Processing segment {seg_idx} ===\")\n        logger.debug(f\"Segment bbox: {seg.bbox}\")\n        logger.debug(\n            f\"Segment page: {seg.page.number if hasattr(seg.page, 'number') else 'unknown'}\"\n        )\n\n        logger.debug(f\"Found {len(seg_starts)} start elements in segment {seg_idx}\")\n        for i, elem in enumerate(seg_starts):\n            logger.debug(\n                f\"  Start {i}: bbox={elem.bbox}, text='{getattr(elem, 'text', 'N/A')[:50]}...'\"\n            )\n\n        # Find end elements in this segment\n        if isinstance(end_elements_unwrapped, str):\n            seg_ends = seg.find_all(end_elements_unwrapped).elements\n        elif end_elements_unwrapped is not None:\n            seg_ends = [e for e in end_elements_unwrapped if _element_in_segment(e, seg)]\n        else:\n            seg_ends = []\n\n        logger.debug(f\"Found {len(seg_ends)} end elements in segment {seg_idx}\")\n        for i, elem in enumerate(seg_ends):\n            logger.debug(\n                f\"  End {i}: bbox={elem.bbox}, text='{getattr(elem, 'text', 'N/A')[:50]}...'\"\n            )\n\n        # Add to global lists with segment index\n        for elem in seg_starts:\n            all_start_elements.append((elem, seg_idx))\n        for elem in seg_ends:\n            all_end_elements.append((elem, seg_idx))\n\n    # Sort by flow order: segment index first, then position within segment\n    all_start_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n    all_end_elements.sort(key=lambda x: (x[1], x[0].top, x[0].x0))\n\n    logger.debug(f\"\\n=== Total boundary elements found ===\")\n    logger.debug(f\"Total start elements: {len(all_start_elements)}\")\n    logger.debug(f\"Total end elements: {len(all_end_elements)}\")\n\n    # Pair up start and end elements to create sections\n    sections = []\n    used_starts = set()\n    used_ends = set()\n\n    for start_elem, start_seg_idx in all_start_elements:\n        if id(start_elem) in used_starts:\n            continue\n\n        logger.debug(f\"\\n--- Pairing start element from segment {start_seg_idx} ---\")\n        logger.debug(\n            f\"Start: bbox={start_elem.bbox}, text='{getattr(start_elem, 'text', 'N/A')[:30]}...'\"\n        )\n\n        # Find the next unused end element that comes after this start\n        matching_end = None\n        for end_elem, end_seg_idx in all_end_elements:\n            if id(end_elem) in used_ends:\n                continue\n\n            # Check if this end comes after the start in flow order\n            if end_seg_idx &gt; start_seg_idx or (\n                end_seg_idx == start_seg_idx\n                and (\n                    end_elem.top &gt; start_elem.top\n                    or (end_elem.top == start_elem.top and end_elem.x0 &gt;= start_elem.x0)\n                )\n            ):\n                matching_end = (end_elem, end_seg_idx)\n                break\n\n        if matching_end is not None:\n            end_elem, end_seg_idx = matching_end\n            used_starts.add(id(start_elem))\n            used_ends.add(id(end_elem))\n\n            logger.debug(f\"  Matched! Start seg={start_seg_idx}, End seg={end_seg_idx}\")\n\n            # Create section based on whether it spans segments\n            if start_seg_idx == end_seg_idx:\n                # Single segment section - use Region.get_section_between\n                seg = self.segments[start_seg_idx]\n                section = seg.get_section_between(start_elem, end_elem, include_boundaries)\n                sections.append(section)\n                logger.debug(f\"  Created single-segment Region\")\n            else:\n                # Multi-segment section - create FlowRegion\n                logger.debug(\n                    f\"  Creating multi-segment FlowRegion spanning segments {start_seg_idx} to {end_seg_idx}\"\n                )\n                constituent_regions = []\n\n                # First segment: from start element to bottom\n                start_seg = self.segments[start_seg_idx]\n                if include_boundaries in [\"start\", \"both\"]:\n                    first_top = start_elem.top\n                else:\n                    first_top = start_elem.bottom\n                first_region = Region(\n                    start_seg.page, (start_seg.x0, first_top, start_seg.x1, start_seg.bottom)\n                )\n                constituent_regions.append(first_region)\n\n                # Middle segments: full segments\n                for seg_idx in range(start_seg_idx + 1, end_seg_idx):\n                    constituent_regions.append(self.segments[seg_idx])\n\n                # Last segment: from top to end element\n                end_seg = self.segments[end_seg_idx]\n                if include_boundaries in [\"end\", \"both\"]:\n                    last_bottom = end_elem.bottom\n                else:\n                    last_bottom = end_elem.top\n                last_region = Region(\n                    end_seg.page, (end_seg.x0, end_seg.top, end_seg.x1, last_bottom)\n                )\n                constituent_regions.append(last_region)\n\n                # Create FlowRegion\n                flow_element = FlowElement(physical_object=start_elem, flow=self)\n                flow_region = FlowRegion(\n                    flow=self,\n                    constituent_regions=constituent_regions,\n                    source_flow_element=flow_element,\n                    boundary_element_found=end_elem,\n                )\n                sections.append(flow_region)\n\n    # Handle special cases when only start or only end elements are provided\n    if start_elements_unwrapped is not None and end_elements_unwrapped is None:\n        logger.debug(f\"\\n=== Handling start-only elements (no end elements provided) ===\")\n        for i, (start_elem, start_seg_idx) in enumerate(all_start_elements):\n            if id(start_elem) in used_starts:\n                continue\n\n            # Find next start element\n            next_start = None\n            if i + 1 &lt; len(all_start_elements):\n                next_start_elem, next_start_seg_idx = all_start_elements[i + 1]\n                # Create section from this start to just before next start\n                if start_seg_idx == next_start_seg_idx:\n                    # Same segment\n                    seg = self.segments[start_seg_idx]\n                    # Find element just before next start\n                    all_elems = seg.get_elements()\n                    all_elems.sort(key=lambda e: (e.top, e.x0))\n                    try:\n                        next_idx = all_elems.index(next_start_elem)\n                        if next_idx &gt; 0:\n                            end_elem = all_elems[next_idx - 1]\n                            section = seg.get_section_between(\n                                start_elem, end_elem, include_boundaries\n                            )\n                            sections.append(section)\n                    except ValueError:\n                        pass\n                elif next_start_seg_idx == start_seg_idx + 1:\n                    # Next start is in the immediately following segment in the flow\n                    # Create a FlowRegion that spans from current start to just before next start\n                    logger.debug(f\"  Next start is in next flow segment - creating FlowRegion\")\n\n                    constituent_regions = []\n\n                    # First segment: from start element to bottom\n                    start_seg = self.segments[start_seg_idx]\n                    if include_boundaries in [\"start\", \"both\"]:\n                        first_top = start_elem.top\n                    else:\n                        first_top = start_elem.bottom\n                    first_region = Region(\n                        start_seg.page,\n                        (start_seg.x0, first_top, start_seg.x1, start_seg.bottom),\n                    )\n                    constituent_regions.append(first_region)\n\n                    # Next segment: from top to just before next start\n                    next_seg = self.segments[next_start_seg_idx]\n                    # Find element just before next start in the next segment\n                    next_seg_elems = next_seg.get_elements()\n                    next_seg_elems.sort(key=lambda e: (e.top, e.x0))\n\n                    last_bottom = next_start_elem.top  # Default to just before the next start\n                    try:\n                        next_idx = next_seg_elems.index(next_start_elem)\n                        if next_idx &gt; 0:\n                            # Use the bottom of the element before next start\n                            prev_elem = next_seg_elems[next_idx - 1]\n                            last_bottom = prev_elem.bottom\n                    except ValueError:\n                        pass\n\n                    last_region = Region(\n                        next_seg.page, (next_seg.x0, next_seg.top, next_seg.x1, last_bottom)\n                    )\n                    constituent_regions.append(last_region)\n\n                    # Create FlowRegion\n                    flow_element = FlowElement(physical_object=start_elem, flow=self)\n                    flow_region = FlowRegion(\n                        flow=self,\n                        constituent_regions=constituent_regions,\n                        source_flow_element=flow_element,\n                        boundary_element_found=None,\n                    )\n                    sections.append(flow_region)\n                    logger.debug(\n                        f\"  Created FlowRegion with {len(constituent_regions)} constituent regions\"\n                    )\n                else:\n                    # Next start is more than one segment away - just end at current segment\n                    start_seg = self.segments[start_seg_idx]\n                    if include_boundaries in [\"start\", \"both\"]:\n                        region_top = start_elem.top\n                    else:\n                        region_top = start_elem.bottom\n                    section = Region(\n                        start_seg.page,\n                        (start_seg.x0, region_top, start_seg.x1, start_seg.bottom),\n                    )\n                    sections.append(section)\n                    logger.debug(\n                        f\"  Next start is {next_start_seg_idx - start_seg_idx} segments away - ending at current segment\"\n                    )\n            else:\n                # Last start element: section goes to end of flow\n                # This could span multiple segments\n                if start_seg_idx == len(self.segments) - 1:\n                    # Only in last segment\n                    seg = self.segments[start_seg_idx]\n                    if include_boundaries in [\"start\", \"both\"]:\n                        region_top = start_elem.top\n                    else:\n                        region_top = start_elem.bottom\n                    section = Region(seg.page, (seg.x0, region_top, seg.x1, seg.bottom))\n                    sections.append(section)\n                else:\n                    # Spans to end of flow - create FlowRegion\n                    constituent_regions = []\n\n                    # First segment\n                    start_seg = self.segments[start_seg_idx]\n                    if include_boundaries in [\"start\", \"both\"]:\n                        first_top = start_elem.top\n                    else:\n                        first_top = start_elem.bottom\n                    first_region = Region(\n                        start_seg.page,\n                        (start_seg.x0, first_top, start_seg.x1, start_seg.bottom),\n                    )\n                    constituent_regions.append(first_region)\n\n                    # Remaining segments\n                    for seg_idx in range(start_seg_idx + 1, len(self.segments)):\n                        constituent_regions.append(self.segments[seg_idx])\n\n                    flow_element = FlowElement(physical_object=start_elem, flow=self)\n                    flow_region = FlowRegion(\n                        flow=self,\n                        constituent_regions=constituent_regions,\n                        source_flow_element=flow_element,\n                        boundary_element_found=None,\n                    )\n                    sections.append(flow_region)\n\n    # Handle new_section_on_page_break when no explicit boundaries\n    if (\n        new_section_on_page_break\n        and start_elements_unwrapped is None\n        and end_elements_unwrapped is None\n    ):\n        # Each segment becomes its own section\n        sections = list(self.segments)\n\n    # Sort sections by their position in the flow\n    def _section_sort_key(section):\n        if hasattr(section, \"constituent_regions\"):\n            # FlowRegion - use first constituent region\n            first_region = (\n                section.constituent_regions[0] if section.constituent_regions else None\n            )\n            if first_region:\n                # Find which segment this region belongs to\n                for idx, seg in enumerate(self.segments):\n                    try:\n                        if seg.intersects(first_region):\n                            return (\n                                idx,\n                                getattr(first_region, \"top\", 0),\n                                getattr(first_region, \"x0\", 0),\n                            )\n                    except:\n                        pass\n        else:\n            # Regular Region\n            for idx, seg in enumerate(self.segments):\n                try:\n                    if seg.intersects(section):\n                        return (idx, getattr(section, \"top\", 0), getattr(section, \"x0\", 0))\n                except:\n                    pass\n        return (float(\"inf\"), 0, 0)\n\n    sections.sort(key=_section_sort_key)\n\n    logger.debug(f\"\\n=== Section creation complete ===\")\n    logger.debug(f\"Total sections created: {len(sections)}\")\n    for i, section in enumerate(sections):\n        if hasattr(section, \"constituent_regions\"):\n            logger.debug(\n                f\"Section {i}: FlowRegion with {len(section.constituent_regions)} constituent regions\"\n            )\n        else:\n            logger.debug(f\"Section {i}: Region with bbox={section.bbox}\")\n\n    return ElementCollection(sections)\n</code></pre> <code>natural_pdf.Flow.get_segment_bounding_box_in_flow(segment_index)</code> <p>Calculates the conceptual bounding box of a segment within the flow's coordinate system. This considers arrangement, alignment, and segment gaps. (This is a placeholder for more complex logic if a true virtual coordinate system is needed) For now, it might just return the physical segment's bbox if gaps are 0 and alignment is simple.</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def get_segment_bounding_box_in_flow(\n    self, segment_index: int\n) -&gt; Optional[tuple[float, float, float, float]]:\n    \"\"\"\n    Calculates the conceptual bounding box of a segment within the flow's coordinate system.\n    This considers arrangement, alignment, and segment gaps.\n    (This is a placeholder for more complex logic if a true virtual coordinate system is needed)\n    For now, it might just return the physical segment's bbox if gaps are 0 and alignment is simple.\n    \"\"\"\n    if segment_index &lt; 0 or segment_index &gt;= len(self.segments):\n        return None\n\n    # This is a simplified version. A full implementation would calculate offsets.\n    # For now, we assume FlowElement directional logic handles segment traversal and uses physical coords.\n    # If we were to *draw* the flow or get a FlowRegion bbox that spans gaps, this would be critical.\n    # physical_segment = self.segments[segment_index]\n    # return physical_segment.bbox\n    raise NotImplementedError(\n        \"Calculating a segment's bbox *within the flow's virtual coordinate system* is not yet fully implemented.\"\n    )\n</code></pre> <code>natural_pdf.Flow.highlights(show=False)</code> <p>Create a highlight context for accumulating highlights.</p> <p>This allows for clean syntax to show multiple highlight groups:</p> Example <p>with flow.highlights() as h:     h.add(flow.find_all('table'), label='tables', color='blue')     h.add(flow.find_all('text:bold'), label='bold text', color='red')     h.show()</p> Or with automatic display <p>with flow.highlights(show=True) as h:     h.add(flow.find_all('table'), label='tables')     h.add(flow.find_all('text:bold'), label='bold')     # Automatically shows when exiting the context</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>If True, automatically show highlights when exiting context</p> <code>False</code> <p>Returns:</p> Type Description <code>HighlightContext</code> <p>HighlightContext for accumulating highlights</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n    \"\"\"\n    Create a highlight context for accumulating highlights.\n\n    This allows for clean syntax to show multiple highlight groups:\n\n    Example:\n        with flow.highlights() as h:\n            h.add(flow.find_all('table'), label='tables', color='blue')\n            h.add(flow.find_all('text:bold'), label='bold text', color='red')\n            h.show()\n\n    Or with automatic display:\n        with flow.highlights(show=True) as h:\n            h.add(flow.find_all('table'), label='tables')\n            h.add(flow.find_all('text:bold'), label='bold')\n            # Automatically shows when exiting the context\n\n    Args:\n        show: If True, automatically show highlights when exiting context\n\n    Returns:\n        HighlightContext for accumulating highlights\n    \"\"\"\n    from natural_pdf.core.highlighting_service import HighlightContext\n\n    return HighlightContext(self, show_on_exit=show)\n</code></pre> <code>natural_pdf.Flow.show(*, resolution=None, width=None, color=None, labels=True, label_format=None, highlights=None, layout='stack', stack_direction='vertical', gap=5, columns=None, crop=False, crop_bbox=None, in_context=False, separator_color=None, separator_thickness=2, **kwargs)</code> <p>Generate a preview image with highlights.</p> <p>If in_context=True, shows segments as cropped images stacked together with separators between segments.</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>Optional[float]</code> <p>DPI for rendering (default from global settings)</p> <code>None</code> <code>width</code> <code>Optional[int]</code> <p>Target width in pixels (overrides resolution)</p> <code>None</code> <code>color</code> <code>Optional[Union[str, Tuple[int, int, int]]]</code> <p>Default highlight color</p> <code>None</code> <code>labels</code> <code>bool</code> <p>Whether to show labels for highlights</p> <code>True</code> <code>label_format</code> <code>Optional[str]</code> <p>Format string for labels</p> <code>None</code> <code>highlights</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Additional highlight groups to show</p> <code>None</code> <code>layout</code> <code>Literal['stack', 'grid', 'single']</code> <p>How to arrange multiple pages/regions</p> <code>'stack'</code> <code>stack_direction</code> <code>Literal['vertical', 'horizontal']</code> <p>Direction for stack layout</p> <code>'vertical'</code> <code>gap</code> <code>int</code> <p>Pixels between stacked images</p> <code>5</code> <code>columns</code> <code>Optional[int]</code> <p>Number of columns for grid layout</p> <code>None</code> <code>crop</code> <code>Union[bool, Literal['content']]</code> <p>Whether to crop</p> <code>False</code> <code>crop_bbox</code> <code>Optional[Tuple[float, float, float, float]]</code> <p>Explicit crop bounds</p> <code>None</code> <code>in_context</code> <code>bool</code> <p>If True, use special Flow visualization with separators</p> <code>False</code> <code>separator_color</code> <code>Optional[Tuple[int, int, int]]</code> <p>RGB color for separator lines (default: red)</p> <code>None</code> <code>separator_thickness</code> <code>int</code> <p>Thickness of separator lines</p> <code>2</code> <code>**kwargs</code> <p>Additional parameters passed to rendering</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Image]</code> <p>PIL Image object or None if nothing to render</p> Source code in <code>natural_pdf/flows/flow.py</code> <pre><code>def show(\n    self,\n    *,\n    # Basic rendering options\n    resolution: Optional[float] = None,\n    width: Optional[int] = None,\n    # Highlight options\n    color: Optional[Union[str, Tuple[int, int, int]]] = None,\n    labels: bool = True,\n    label_format: Optional[str] = None,\n    highlights: Optional[List[Dict[str, Any]]] = None,\n    # Layout options for multi-page/region\n    layout: Literal[\"stack\", \"grid\", \"single\"] = \"stack\",\n    stack_direction: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n    gap: int = 5,\n    columns: Optional[int] = None,  # For grid layout\n    # Cropping options\n    crop: Union[bool, Literal[\"content\"]] = False,\n    crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n    # Flow-specific options\n    in_context: bool = False,\n    separator_color: Optional[Tuple[int, int, int]] = None,\n    separator_thickness: int = 2,\n    **kwargs,\n) -&gt; Optional[\"PIL_Image\"]:\n    \"\"\"Generate a preview image with highlights.\n\n    If in_context=True, shows segments as cropped images stacked together\n    with separators between segments.\n\n    Args:\n        resolution: DPI for rendering (default from global settings)\n        width: Target width in pixels (overrides resolution)\n        color: Default highlight color\n        labels: Whether to show labels for highlights\n        label_format: Format string for labels\n        highlights: Additional highlight groups to show\n        layout: How to arrange multiple pages/regions\n        stack_direction: Direction for stack layout\n        gap: Pixels between stacked images\n        columns: Number of columns for grid layout\n        crop: Whether to crop\n        crop_bbox: Explicit crop bounds\n        in_context: If True, use special Flow visualization with separators\n        separator_color: RGB color for separator lines (default: red)\n        separator_thickness: Thickness of separator lines\n        **kwargs: Additional parameters passed to rendering\n\n    Returns:\n        PIL Image object or None if nothing to render\n    \"\"\"\n    if in_context:\n        # Use the special in_context visualization\n        return self._show_in_context(\n            resolution=resolution or 150,\n            width=width,\n            stack_direction=stack_direction,\n            stack_gap=gap,\n            separator_color=separator_color or (255, 0, 0),\n            separator_thickness=separator_thickness,\n            **kwargs,\n        )\n\n    # Otherwise use the standard show method\n    return super().show(\n        resolution=resolution,\n        width=width,\n        color=color,\n        labels=labels,\n        label_format=label_format,\n        highlights=highlights,\n        layout=layout,\n        stack_direction=stack_direction,\n        gap=gap,\n        columns=columns,\n        crop=crop,\n        crop_bbox=crop_bbox,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/#natural_pdf.FlowRegion","title":"<code>natural_pdf.FlowRegion</code>","text":"<p>               Bases: <code>Visualizable</code></p> <p>Represents a selected area within a Flow, potentially composed of multiple physical Region objects (constituent_regions) that might span across different original pages or disjoint physical regions defined in the Flow.</p> <p>A FlowRegion is the result of a directional operation (e.g., .below(), .above()) on a FlowElement.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>class FlowRegion(Visualizable):\n    \"\"\"\n    Represents a selected area within a Flow, potentially composed of multiple\n    physical Region objects (constituent_regions) that might span across\n    different original pages or disjoint physical regions defined in the Flow.\n\n    A FlowRegion is the result of a directional operation (e.g., .below(), .above())\n    on a FlowElement.\n    \"\"\"\n\n    def __init__(\n        self,\n        flow: \"Flow\",\n        constituent_regions: List[\"PhysicalRegion\"],\n        source_flow_element: \"FlowElement\",\n        boundary_element_found: Optional[\"PhysicalElement\"] = None,\n    ):\n        \"\"\"\n        Initializes a FlowRegion.\n\n        Args:\n            flow: The Flow instance this region belongs to.\n            constituent_regions: A list of physical natural_pdf.elements.region.Region\n                                 objects that make up this FlowRegion.\n            source_flow_element: The FlowElement that created this FlowRegion.\n            boundary_element_found: The physical element that stopped an 'until' search,\n                                    if applicable.\n        \"\"\"\n        self.flow: \"Flow\" = flow\n        self.constituent_regions: List[\"PhysicalRegion\"] = constituent_regions\n        self.source_flow_element: \"FlowElement\" = source_flow_element\n        self.boundary_element_found: Optional[\"PhysicalElement\"] = boundary_element_found\n\n        # Add attributes for grid building, similar to Region\n        self.source: Optional[str] = None\n        self.region_type: Optional[str] = None\n        self.metadata: Dict[str, Any] = {}\n\n        # Cache for expensive operations\n        self._cached_text: Optional[str] = None\n        self._cached_elements: Optional[\"ElementCollection\"] = None  # Stringized\n        self._cached_bbox: Optional[Tuple[float, float, float, float]] = None\n\n    def _get_highlighter(self):\n        \"\"\"Get the highlighting service from constituent regions.\"\"\"\n        if not self.constituent_regions:\n            raise RuntimeError(\"FlowRegion has no constituent regions to get highlighter from\")\n\n        # Get highlighter from first constituent region\n        first_region = self.constituent_regions[0]\n        if hasattr(first_region, \"_highlighter\"):\n            return first_region._highlighter\n        elif hasattr(first_region, \"page\") and hasattr(first_region.page, \"_highlighter\"):\n            return first_region.page._highlighter\n        else:\n            raise RuntimeError(\n                f\"Cannot find HighlightingService from FlowRegion constituent regions. \"\n                f\"First region type: {type(first_region).__name__}\"\n            )\n\n    def _get_render_specs(\n        self,\n        mode: Literal[\"show\", \"render\"] = \"show\",\n        color: Optional[Union[str, Tuple[int, int, int]]] = None,\n        highlights: Optional[List[Dict[str, Any]]] = None,\n        crop: Union[bool, Literal[\"content\"]] = False,\n        crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n        **kwargs,\n    ) -&gt; List[RenderSpec]:\n        \"\"\"Get render specifications for this flow region.\n\n        Args:\n            mode: Rendering mode - 'show' includes highlights, 'render' is clean\n            color: Color for highlighting this region in show mode\n            highlights: Additional highlight groups to show\n            crop: Whether to crop to constituent regions\n            crop_bbox: Explicit crop bounds\n            **kwargs: Additional parameters\n\n        Returns:\n            List of RenderSpec objects, one per page with constituent regions\n        \"\"\"\n        if not self.constituent_regions:\n            return []\n\n        # Group constituent regions by page\n        regions_by_page = {}\n        for region in self.constituent_regions:\n            if hasattr(region, \"page\") and region.page:\n                page = region.page\n                if page not in regions_by_page:\n                    regions_by_page[page] = []\n                regions_by_page[page].append(region)\n\n        if not regions_by_page:\n            return []\n\n        # Create RenderSpec for each page\n        specs = []\n        for page, page_regions in regions_by_page.items():\n            spec = RenderSpec(page=page)\n\n            # Handle cropping\n            if crop_bbox:\n                spec.crop_bbox = crop_bbox\n            elif crop == \"content\" or crop is True:\n                # Calculate bounds of regions on this page\n                x_coords = []\n                y_coords = []\n                for region in page_regions:\n                    if hasattr(region, \"bbox\") and region.bbox:\n                        x0, y0, x1, y1 = region.bbox\n                        x_coords.extend([x0, x1])\n                        y_coords.extend([y0, y1])\n\n                if x_coords and y_coords:\n                    spec.crop_bbox = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))\n\n            # Add highlights in show mode\n            if mode == \"show\":\n                # Highlight constituent regions\n                for i, region in enumerate(page_regions):\n                    # Label each part if multiple regions\n                    label = None\n                    if len(self.constituent_regions) &gt; 1:\n                        # Find global index\n                        try:\n                            global_idx = self.constituent_regions.index(region)\n                            label = f\"FlowPart_{global_idx + 1}\"\n                        except ValueError:\n                            label = f\"FlowPart_{i + 1}\"\n                    else:\n                        label = \"FlowRegion\"\n\n                    spec.add_highlight(\n                        bbox=region.bbox,\n                        polygon=region.polygon if region.has_polygon else None,\n                        color=color or \"fuchsia\",\n                        label=label,\n                    )\n\n                # Add additional highlight groups if provided\n                if highlights:\n                    for group in highlights:\n                        group_elements = group.get(\"elements\", [])\n                        group_color = group.get(\"color\", color)\n                        group_label = group.get(\"label\")\n\n                        for elem in group_elements:\n                            # Only add if element is on this page\n                            if hasattr(elem, \"page\") and elem.page == page:\n                                spec.add_highlight(\n                                    element=elem, color=group_color, label=group_label\n                                )\n\n            specs.append(spec)\n\n        return specs\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"\n        Dynamically proxy attribute access to the source FlowElement for safe attributes only.\n        Spatial methods (above, below, left, right) are explicitly implemented to prevent\n        silent failures and incorrect behavior.\n        \"\"\"\n        if name in self.__dict__:\n            return self.__dict__[name]\n\n        # List of methods that should NOT be proxied - they need proper FlowRegion implementation\n        spatial_methods = {\"above\", \"below\", \"left\", \"right\", \"to_region\"}\n\n        if name in spatial_methods:\n            raise AttributeError(\n                f\"'{self.__class__.__name__}' object has no attribute '{name}'. \"\n                f\"This method requires proper FlowRegion implementation to handle spatial relationships correctly.\"\n            )\n\n        # Only proxy safe attributes and methods\n        if self.source_flow_element is not None:\n            try:\n                attr = getattr(self.source_flow_element, name)\n                # Only proxy non-callable attributes and explicitly safe methods\n                if not callable(attr) or name in {\"page\", \"document\"}:  # Add safe methods as needed\n                    return attr\n                else:\n                    raise AttributeError(\n                        f\"Method '{name}' cannot be safely proxied from FlowElement to FlowRegion. \"\n                        f\"It may need explicit implementation.\"\n                    )\n            except AttributeError:\n                pass\n\n        raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n\n    @property\n    def bbox(self) -&gt; Optional[Tuple[float, float, float, float]]:\n        \"\"\"\n        The bounding box that encloses all constituent regions.\n        Calculated dynamically and cached.\n        \"\"\"\n        if self._cached_bbox is not None:\n            return self._cached_bbox\n        if not self.constituent_regions:\n            return None\n\n        # Use merge_bboxes from pdfplumber.utils.geometry to merge bboxes\n        # Extract bbox tuples from regions first\n        region_bboxes = [\n            region.bbox for region in self.constituent_regions if hasattr(region, \"bbox\")\n        ]\n        if not region_bboxes:\n            return None\n\n        self._cached_bbox = merge_bboxes(region_bboxes)\n        return self._cached_bbox\n\n    @property\n    def x0(self) -&gt; Optional[float]:\n        return self.bbox[0] if self.bbox else None\n\n    @property\n    def top(self) -&gt; Optional[float]:\n        return self.bbox[1] if self.bbox else None\n\n    @property\n    def x1(self) -&gt; Optional[float]:\n        return self.bbox[2] if self.bbox else None\n\n    @property\n    def bottom(self) -&gt; Optional[float]:\n        return self.bbox[3] if self.bbox else None\n\n    @property\n    def width(self) -&gt; Optional[float]:\n        return self.x1 - self.x0 if self.bbox else None\n\n    @property\n    def height(self) -&gt; Optional[float]:\n        return self.bottom - self.top if self.bbox else None\n\n    def extract_text(self, apply_exclusions: bool = True, **kwargs) -&gt; str:\n        \"\"\"\n        Extracts and concatenates text from all constituent physical regions.\n        The order of concatenation respects the flow's arrangement.\n\n        Args:\n            apply_exclusions: Whether to respect PDF exclusion zones within each\n                              constituent physical region during text extraction.\n            **kwargs: Additional arguments passed to the underlying extract_text method\n                      of each constituent region.\n\n        Returns:\n            The combined text content as a string.\n        \"\"\"\n        if (\n            self._cached_text is not None and apply_exclusions\n        ):  # Simple cache check, might need refinement if kwargs change behavior\n            return self._cached_text\n\n        if not self.constituent_regions:\n            return \"\"\n\n        texts: List[str] = []\n        # For now, simple concatenation. Order depends on how constituent_regions were added.\n        # The FlowElement._flow_direction method is responsible for ordering constituent_regions correctly.\n        for region in self.constituent_regions:\n            texts.append(region.extract_text(apply_exclusions=apply_exclusions, **kwargs))\n\n        # Join based on flow arrangement (e.g., newline for vertical, space for horizontal)\n        # This is a simplification; true layout-aware joining would be more complex.\n        joiner = (\n            \"\\n\" if self.flow.arrangement == \"vertical\" else \" \"\n        )  # TODO: Consider flow.segment_gap for proportional spacing between segments\n        extracted = joiner.join(t for t in texts if t)\n\n        if apply_exclusions:  # Only cache if standard exclusion behavior\n            self._cached_text = extracted\n        return extracted\n\n    def elements(self, apply_exclusions: bool = True) -&gt; \"ElementCollection\":  # Stringized return\n        \"\"\"\n        Collects all unique physical elements from all constituent physical regions.\n\n        Args:\n            apply_exclusions: Whether to respect PDF exclusion zones within each\n                              constituent physical region when gathering elements.\n\n        Returns:\n            An ElementCollection containing all unique elements.\n        \"\"\"\n        from natural_pdf.elements.element_collection import (\n            ElementCollection as RuntimeElementCollection,  # Local import\n        )\n\n        if self._cached_elements is not None and apply_exclusions:  # Simple cache check\n            return self._cached_elements\n\n        if not self.constituent_regions:\n            return RuntimeElementCollection([])\n\n        all_physical_elements: List[\"PhysicalElement\"] = []  # Stringized item type\n        seen_elements = (\n            set()\n        )  # To ensure uniqueness if elements are shared or duplicated by region definitions\n\n        for region in self.constituent_regions:\n            # Region.get_elements() returns a list, not ElementCollection\n            elements_in_region: List[\"PhysicalElement\"] = region.get_elements(\n                apply_exclusions=apply_exclusions\n            )\n            for elem in elements_in_region:\n                if elem not in seen_elements:  # Check for uniqueness based on object identity\n                    all_physical_elements.append(elem)\n                    seen_elements.add(elem)\n\n        # Basic reading order sort based on original page and coordinates.\n        def get_sort_key(phys_elem: \"PhysicalElement\"):  # Stringized param type\n            page_idx = -1\n            if hasattr(phys_elem, \"page\") and hasattr(phys_elem.page, \"index\"):\n                page_idx = phys_elem.page.index\n            return (page_idx, phys_elem.top, phys_elem.x0)\n\n        try:\n            sorted_physical_elements = sorted(all_physical_elements, key=get_sort_key)\n        except AttributeError:\n            logger.warning(\n                \"Could not sort elements in FlowRegion by reading order; some elements might be missing page, top or x0 attributes.\"\n            )\n            sorted_physical_elements = all_physical_elements\n\n        result_collection = RuntimeElementCollection(sorted_physical_elements)\n        if apply_exclusions:\n            self._cached_elements = result_collection\n        return result_collection\n\n    def find(\n        self, selector: Optional[str] = None, *, text: Optional[str] = None, **kwargs\n    ) -&gt; Optional[\"PhysicalElement\"]:  # Stringized\n        \"\"\"\n        Find the first element in flow order that matches the selector or text.\n\n        This implementation iterates through the constituent regions *in the order\n        they appear in ``self.constituent_regions`` (i.e. document flow order),\n        delegating the search to each region's own ``find`` method.  It therefore\n        avoids constructing a huge intermediate ElementCollection and returns as\n        soon as a match is found, which is substantially faster and ensures that\n        selectors such as 'table' work exactly as they do on an individual\n        Region.\n        \"\"\"\n        if not self.constituent_regions:\n            return None\n\n        for region in self.constituent_regions:\n            try:\n                result = region.find(selector=selector, text=text, **kwargs)\n                if result is not None:\n                    return result\n            except Exception as e:\n                logger.warning(\n                    f\"FlowRegion.find: error searching region {region}: {e}\",\n                    exc_info=False,\n                )\n        return None  # No match found\n\n    def find_all(\n        self, selector: Optional[str] = None, *, text: Optional[str] = None, **kwargs\n    ) -&gt; \"ElementCollection\":  # Stringized\n        \"\"\"\n        Find **all** elements across the constituent regions that match the given\n        selector or text.\n\n        Rather than first materialising *every* element in the FlowRegion (which\n        can be extremely slow for multi-page flows), this implementation simply\n        chains each region's native ``find_all`` call and concatenates their\n        results into a single ElementCollection while preserving flow order.\n        \"\"\"\n        from natural_pdf.elements.element_collection import (\n            ElementCollection as RuntimeElementCollection,\n        )\n\n        matched_elements = []  # type: List[\"PhysicalElement\"]\n\n        if not self.constituent_regions:\n            return RuntimeElementCollection([])\n\n        for region in self.constituent_regions:\n            try:\n                region_matches = region.find_all(selector=selector, text=text, **kwargs)\n                if region_matches:\n                    # ``region_matches`` is an ElementCollection \u2013 extend with its\n                    # underlying list so we don't create nested collections.\n                    matched_elements.extend(\n                        region_matches.elements\n                        if hasattr(region_matches, \"elements\")\n                        else list(region_matches)\n                    )\n            except Exception as e:\n                logger.warning(\n                    f\"FlowRegion.find_all: error searching region {region}: {e}\",\n                    exc_info=False,\n                )\n\n        return RuntimeElementCollection(matched_elements)\n\n    def highlight(\n        self, label: Optional[str] = None, color: Optional[Union[Tuple, str]] = None, **kwargs\n    ) -&gt; \"FlowRegion\":  # Stringized\n        \"\"\"\n        Highlights all constituent physical regions on their respective pages.\n\n        Args:\n            label: A base label for the highlights. Each constituent region might get an indexed label.\n            color: Color for the highlight.\n            **kwargs: Additional arguments for the underlying highlight method.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        if not self.constituent_regions:\n            return self\n\n        base_label = label if label else \"FlowRegionPart\"\n        for i, region in enumerate(self.constituent_regions):\n            current_label = (\n                f\"{base_label}_{i+1}\" if len(self.constituent_regions) &gt; 1 else base_label\n            )\n            region.highlight(label=current_label, color=color, **kwargs)\n        return self\n\n    def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n        \"\"\"\n        Create a highlight context for accumulating highlights.\n\n        This allows for clean syntax to show multiple highlight groups:\n\n        Example:\n            with flow_region.highlights() as h:\n                h.add(flow_region.find_all('table'), label='tables', color='blue')\n                h.add(flow_region.find_all('text:bold'), label='bold text', color='red')\n                h.show()\n\n        Or with automatic display:\n            with flow_region.highlights(show=True) as h:\n                h.add(flow_region.find_all('table'), label='tables')\n                h.add(flow_region.find_all('text:bold'), label='bold')\n                # Automatically shows when exiting the context\n\n        Args:\n            show: If True, automatically show highlights when exiting context\n\n        Returns:\n            HighlightContext for accumulating highlights\n        \"\"\"\n        from natural_pdf.core.highlighting_service import HighlightContext\n\n        return HighlightContext(self, show_on_exit=show)\n\n    def to_images(\n        self,\n        resolution: float = 150,\n        **kwargs,\n    ) -&gt; List[\"PIL_Image\"]:\n        \"\"\"\n        Generates and returns a list of cropped PIL Images,\n        one for each constituent physical region of this FlowRegion.\n        \"\"\"\n        if not self.constituent_regions:\n            logger.info(\"FlowRegion.to_images() called on an empty FlowRegion.\")\n            return []\n\n        cropped_images: List[\"PIL_Image\"] = []\n        for region_part in self.constituent_regions:\n            try:\n                # Use render() for clean image without highlights\n                img = region_part.render(resolution=resolution, crop=True, **kwargs)\n                if img:\n                    cropped_images.append(img)\n            except Exception as e:\n                logger.error(\n                    f\"Error generating image for constituent region {region_part.bbox}: {e}\",\n                    exc_info=True,\n                )\n\n        return cropped_images\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"&lt;FlowRegion constituents={len(self.constituent_regions)}, flow={self.flow}, \"\n            f\"source_bbox={self.source_flow_element.bbox if self.source_flow_element else 'N/A'}&gt;\"\n        )\n\n    def expand(\n        self,\n        left: float = 0,\n        right: float = 0,\n        top: float = 0,\n        bottom: float = 0,\n        width_factor: float = 1.0,\n        height_factor: float = 1.0,\n    ) -&gt; \"FlowRegion\":\n        \"\"\"\n        Create a new FlowRegion with all constituent regions expanded.\n\n        Args:\n            left: Amount to expand left edge (positive value expands leftwards)\n            right: Amount to expand right edge (positive value expands rightwards)\n            top: Amount to expand top edge (positive value expands upwards)\n            bottom: Amount to expand bottom edge (positive value expands downwards)\n            width_factor: Factor to multiply width by (applied after absolute expansion)\n            height_factor: Factor to multiply height by (applied after absolute expansion)\n\n        Returns:\n            New FlowRegion with expanded constituent regions\n        \"\"\"\n        if not self.constituent_regions:\n            return FlowRegion(\n                flow=self.flow,\n                constituent_regions=[],\n                source_flow_element=self.source_flow_element,\n                boundary_element_found=self.boundary_element_found,\n            )\n\n        expanded_regions = []\n        for idx, region in enumerate(self.constituent_regions):\n            # Determine which adjustments to apply based on flow arrangement\n            apply_left = left\n            apply_right = right\n            apply_top = top\n            apply_bottom = bottom\n\n            if self.flow.arrangement == \"vertical\":\n                # In a vertical flow, only the *first* region should react to `top`\n                # and only the *last* region should react to `bottom`.  This keeps\n                # the virtual contiguous area intact while allowing users to nudge\n                # the flow boundaries.\n                if idx != 0:\n                    apply_top = 0\n                if idx != len(self.constituent_regions) - 1:\n                    apply_bottom = 0\n                # left/right apply to every region (same column width change)\n            else:  # horizontal flow\n                # In a horizontal flow, only the first region reacts to `left`\n                # and only the last region reacts to `right`.\n                if idx != 0:\n                    apply_left = 0\n                if idx != len(self.constituent_regions) - 1:\n                    apply_right = 0\n                # top/bottom apply to every region in horizontal flows\n\n            # Skip no-op expansion to avoid extra Region objects\n            needs_expansion = (\n                any(\n                    v not in (0, 1.0)  # compare width/height factor logically later\n                    for v in (apply_left, apply_right, apply_top, apply_bottom)\n                )\n                or width_factor != 1.0\n                or height_factor != 1.0\n            )\n\n            try:\n                expanded_region = (\n                    region.expand(\n                        left=apply_left,\n                        right=apply_right,\n                        top=apply_top,\n                        bottom=apply_bottom,\n                        width_factor=width_factor,\n                        height_factor=height_factor,\n                    )\n                    if needs_expansion\n                    else region\n                )\n                expanded_regions.append(expanded_region)\n            except Exception as e:\n                logger.warning(\n                    f\"FlowRegion.expand: Error expanding constituent region {region.bbox}: {e}\",\n                    exc_info=False,\n                )\n                expanded_regions.append(region)\n\n        # Create new FlowRegion with expanded constituent regions\n        new_flow_region = FlowRegion(\n            flow=self.flow,\n            constituent_regions=expanded_regions,\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n        # Copy metadata\n        new_flow_region.source = self.source\n        new_flow_region.region_type = self.region_type\n        new_flow_region.metadata = self.metadata.copy()\n\n        # Clear caches since the regions have changed\n        new_flow_region._cached_text = None\n        new_flow_region._cached_elements = None\n        new_flow_region._cached_bbox = None\n\n        return new_flow_region\n\n    def above(\n        self,\n        height: Optional[float] = None,\n        width: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"FlowRegion\":\n        \"\"\"\n        Create a FlowRegion with regions above this FlowRegion.\n\n        For vertical flows: Only expands the topmost constituent region upward.\n        For horizontal flows: Expands all constituent regions upward.\n\n        Args:\n            height: Height of the region above, in points\n            width: Width mode - \"full\" for full page width or \"element\" for element width\n            include_source: Whether to include this FlowRegion in the result\n            until: Optional selector string to specify an upper boundary element\n            include_endpoint: Whether to include the boundary element in the region\n            **kwargs: Additional parameters\n\n        Returns:\n            New FlowRegion with regions above\n        \"\"\"\n        if not self.constituent_regions:\n            return FlowRegion(\n                flow=self.flow,\n                constituent_regions=[],\n                source_flow_element=self.source_flow_element,\n                boundary_element_found=self.boundary_element_found,\n            )\n\n        new_regions = []\n\n        if self.flow.arrangement == \"vertical\":\n            # For vertical flow, use FLOW ORDER (index 0 is earliest). Only expand the\n            # first constituent region in that order.\n            first_region = self.constituent_regions[0]\n            for idx, region in enumerate(self.constituent_regions):\n                if idx == 0:  # Only expand the first region (earliest in flow)\n                    above_region = region.above(\n                        height=height,\n                        width=\"element\",  # Keep original column width\n                        include_source=include_source,\n                        until=until,\n                        include_endpoint=include_endpoint,\n                        **kwargs,\n                    )\n                    new_regions.append(above_region)\n                elif include_source:\n                    new_regions.append(region)\n        else:  # horizontal flow\n            # For horizontal flow, expand all regions upward\n            for region in self.constituent_regions:\n                above_region = region.above(\n                    height=height,\n                    width=width,\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(above_region)\n\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=new_regions,\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    def below(\n        self,\n        height: Optional[float] = None,\n        width: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"FlowRegion\":\n        \"\"\"\n        Create a FlowRegion with regions below this FlowRegion.\n\n        For vertical flows: Only expands the bottommost constituent region downward.\n        For horizontal flows: Expands all constituent regions downward.\n\n        Args:\n            height: Height of the region below, in points\n            width: Width mode - \"full\" for full page width or \"element\" for element width\n            include_source: Whether to include this FlowRegion in the result\n            until: Optional selector string to specify a lower boundary element\n            include_endpoint: Whether to include the boundary element in the region\n            **kwargs: Additional parameters\n\n        Returns:\n            New FlowRegion with regions below\n        \"\"\"\n        if not self.constituent_regions:\n            return FlowRegion(\n                flow=self.flow,\n                constituent_regions=[],\n                source_flow_element=self.source_flow_element,\n                boundary_element_found=self.boundary_element_found,\n            )\n\n        new_regions = []\n\n        if self.flow.arrangement == \"vertical\":\n            # For vertical flow, expand only the LAST constituent region in flow order.\n            last_idx = len(self.constituent_regions) - 1\n            for idx, region in enumerate(self.constituent_regions):\n                if idx == last_idx:\n                    below_region = region.below(\n                        height=height,\n                        width=\"element\",\n                        include_source=include_source,\n                        until=until,\n                        include_endpoint=include_endpoint,\n                        **kwargs,\n                    )\n                    new_regions.append(below_region)\n                elif include_source:\n                    new_regions.append(region)\n        else:  # horizontal flow\n            # For horizontal flow, expand all regions downward\n            for region in self.constituent_regions:\n                below_region = region.below(\n                    height=height,\n                    width=width,\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(below_region)\n\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=new_regions,\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    def left(\n        self,\n        width: Optional[float] = None,\n        height: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"FlowRegion\":\n        \"\"\"\n        Create a FlowRegion with regions to the left of this FlowRegion.\n\n        For vertical flows: Expands all constituent regions leftward.\n        For horizontal flows: Only expands the leftmost constituent region leftward.\n\n        Args:\n            width: Width of the region to the left, in points\n            height: Height mode - \"full\" for full page height or \"element\" for element height\n            include_source: Whether to include this FlowRegion in the result\n            until: Optional selector string to specify a left boundary element\n            include_endpoint: Whether to include the boundary element in the region\n            **kwargs: Additional parameters\n\n        Returns:\n            New FlowRegion with regions to the left\n        \"\"\"\n        if not self.constituent_regions:\n            return FlowRegion(\n                flow=self.flow,\n                constituent_regions=[],\n                source_flow_element=self.source_flow_element,\n                boundary_element_found=self.boundary_element_found,\n            )\n\n        new_regions = []\n\n        if self.flow.arrangement == \"vertical\":\n            # For vertical flow, expand all regions leftward\n            for region in self.constituent_regions:\n                left_region = region.left(\n                    width=width,\n                    height=\"element\",\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(left_region)\n        else:  # horizontal flow\n            # For horizontal flow, only expand the leftmost region leftward\n            leftmost_region = min(self.constituent_regions, key=lambda r: r.x0)\n            for region in self.constituent_regions:\n                if region == leftmost_region:\n                    # Expand this region leftward\n                    left_region = region.left(\n                        width=width,\n                        height=\"element\",\n                        include_source=include_source,\n                        until=until,\n                        include_endpoint=include_endpoint,\n                        **kwargs,\n                    )\n                    new_regions.append(left_region)\n                elif include_source:\n                    # Include other regions unchanged if include_source is True\n                    new_regions.append(region)\n\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=new_regions,\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    def right(\n        self,\n        width: Optional[float] = None,\n        height: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"FlowRegion\":\n        \"\"\"\n        Create a FlowRegion with regions to the right of this FlowRegion.\n\n        For vertical flows: Expands all constituent regions rightward.\n        For horizontal flows: Only expands the rightmost constituent region rightward.\n\n        Args:\n            width: Width of the region to the right, in points\n            height: Height mode - \"full\" for full page height or \"element\" for element height\n            include_source: Whether to include this FlowRegion in the result\n            until: Optional selector string to specify a right boundary element\n            include_endpoint: Whether to include the boundary element in the region\n            **kwargs: Additional parameters\n\n        Returns:\n            New FlowRegion with regions to the right\n        \"\"\"\n        if not self.constituent_regions:\n            return FlowRegion(\n                flow=self.flow,\n                constituent_regions=[],\n                source_flow_element=self.source_flow_element,\n                boundary_element_found=self.boundary_element_found,\n            )\n\n        new_regions = []\n\n        if self.flow.arrangement == \"vertical\":\n            # For vertical flow, expand all regions rightward\n            for region in self.constituent_regions:\n                right_region = region.right(\n                    width=width,\n                    height=\"element\",\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(right_region)\n        else:  # horizontal flow\n            # For horizontal flow, only expand the rightmost region rightward\n            rightmost_region = max(self.constituent_regions, key=lambda r: r.x1)\n            for region in self.constituent_regions:\n                if region == rightmost_region:\n                    # Expand this region rightward\n                    right_region = region.right(\n                        width=width,\n                        height=\"element\",\n                        include_source=include_source,\n                        until=until,\n                        include_endpoint=include_endpoint,\n                        **kwargs,\n                    )\n                    new_regions.append(right_region)\n                elif include_source:\n                    # Include other regions unchanged if include_source is True\n                    new_regions.append(region)\n\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=new_regions,\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    def to_region(self) -&gt; \"FlowRegion\":\n        \"\"\"\n        Convert this FlowRegion to a region (returns a copy).\n        This is equivalent to calling expand() with no arguments.\n\n        Returns:\n            Copy of this FlowRegion\n        \"\"\"\n        return self.expand()\n\n    @property\n    def is_empty(self) -&gt; bool:\n        \"\"\"Checks if the FlowRegion contains no constituent regions or if all are empty.\"\"\"\n        if not self.constituent_regions:\n            return True\n        # A more robust check might see if extract_text() is empty and elements() is empty.\n        # For now, if it has regions, it's not considered empty by this simple check.\n        # User Point 4: FlowRegion can be empty (no text, no elements). This implies checking content.\n        try:\n            return not bool(self.extract_text(apply_exclusions=False).strip()) and not bool(\n                self.elements(apply_exclusions=False)\n            )\n        except Exception:\n            return True  # If error during check, assume empty to be safe\n\n    # ------------------------------------------------------------------\n    # Table extraction helpers (delegates to underlying physical regions)\n    # ------------------------------------------------------------------\n\n    def extract_table(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        use_ocr: bool = False,\n        ocr_config: Optional[dict] = None,\n        text_options: Optional[Dict] = None,\n        cell_extraction_func: Optional[Callable[[\"PhysicalRegion\"], Optional[str]]] = None,\n        show_progress: bool = False,\n        # Optional row-level merge predicate. If provided, it decides whether\n        # the current row (first row of a segment/page) should be merged with\n        # the previous one (to handle multi-page spill-overs).\n        stitch_rows: Optional[\n            Callable[[List[Optional[str]], List[Optional[str]], int, \"PhysicalRegion\"], bool]\n        ] = None,\n        merge_headers: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; TableResult:\n        \"\"\"Extracts a single logical table from the FlowRegion.\n\n        This is a convenience wrapper that iterates through the constituent\n        physical regions **in flow order**, calls their ``extract_table``\n        method, and concatenates the resulting rows.  It mirrors the public\n        interface of :pymeth:`natural_pdf.elements.region.Region.extract_table`.\n\n        Args:\n            method, table_settings, use_ocr, ocr_config, text_options, cell_extraction_func, show_progress:\n                Same as in :pymeth:`Region.extract_table` and are forwarded as-is\n                to each physical region.\n            merge_headers: Whether to merge tables by removing repeated headers from subsequent\n                pages/segments. If None (default), auto-detects by checking if the first row\n                of each segment matches the first row of the first segment. If segments have\n                inconsistent header patterns (some repeat, others don't), raises ValueError.\n                Useful for multi-page tables where headers repeat on each page.\n            **kwargs: Additional keyword arguments forwarded to the underlying\n                ``Region.extract_table`` implementation.\n\n        Returns:\n            A TableResult object containing the aggregated table data.  Rows returned from\n            consecutive constituent regions are appended in document order.  If\n            no tables are detected in any region, an empty TableResult is returned.\n\n        stitch_rows parameter:\n            Controls whether the first rows of subsequent segments/regions should be merged\n            into the previous row (to handle spill-over across page breaks).\n            Applied AFTER header removal if merge_headers is enabled.\n\n            \u2022 None (default) \u2013 no merging (behaviour identical to previous versions).\n            \u2022 Callable \u2013 custom predicate taking\n                   (prev_row, cur_row, row_idx_in_segment, segment_object) \u2192 bool.\n               Return True to merge `cur_row` into `prev_row` (default column-wise merge is used).\n        \"\"\"\n\n        if table_settings is None:\n            table_settings = {}\n        if text_options is None:\n            text_options = {}\n\n        if not self.constituent_regions:\n            return TableResult([])\n\n        # Resolve stitch_rows predicate -------------------------------------------------------\n        predicate: Optional[\n            Callable[[List[Optional[str]], List[Optional[str]], int, \"PhysicalRegion\"], bool]\n        ] = (stitch_rows if callable(stitch_rows) else None)\n\n        def _default_merge(\n            prev_row: List[Optional[str]], cur_row: List[Optional[str]]\n        ) -&gt; List[Optional[str]]:\n            \"\"\"Column-wise merge \u2013 concatenates non-empty strings with a space.\"\"\"\n            from itertools import zip_longest\n\n            merged: List[Optional[str]] = []\n            for p, c in zip_longest(prev_row, cur_row, fillvalue=\"\"):\n                if (p or \"\").strip() and (c or \"\").strip():\n                    merged.append(f\"{p} {c}\".strip())\n                else:\n                    merged.append((p or \"\") + (c or \"\"))\n            return merged\n\n        aggregated_rows: List[List[Optional[str]]] = []\n        header_row: Optional[List[Optional[str]]] = None\n        merge_headers_enabled = False\n        headers_warned = False  # Track if we've already warned about dropping headers\n        segment_has_repeated_header = []  # Track which segments have repeated headers\n\n        for region_idx, region in enumerate(self.constituent_regions):\n            try:\n                region_result = region.extract_table(\n                    method=method,\n                    table_settings=table_settings.copy(),  # Avoid side-effects\n                    use_ocr=use_ocr,\n                    ocr_config=ocr_config,\n                    text_options=text_options.copy(),\n                    cell_extraction_func=cell_extraction_func,\n                    show_progress=show_progress,\n                    **kwargs,\n                )\n\n                # Convert result to list of rows\n                if not region_result:\n                    continue\n\n                if isinstance(region_result, TableResult):\n                    segment_rows = list(region_result)\n                else:\n                    segment_rows = list(region_result)\n\n                # Handle header detection and merging for multi-page tables\n                if region_idx == 0:\n                    # First segment: capture potential header row\n                    if segment_rows:\n                        header_row = segment_rows[0]\n                        # Determine if we should merge headers\n                        if merge_headers is None:\n                            # Auto-detect: we'll check all subsequent segments\n                            merge_headers_enabled = False  # Will be determined later\n                        else:\n                            merge_headers_enabled = merge_headers\n                        # Track that first segment exists (for consistency checking)\n                        segment_has_repeated_header.append(False)  # First segment doesn't \"repeat\"\n                elif region_idx == 1 and merge_headers is None:\n                    # Auto-detection: check if first row of second segment matches header\n                    has_header = segment_rows and header_row and segment_rows[0] == header_row\n                    segment_has_repeated_header.append(has_header)\n\n                    if has_header:\n                        merge_headers_enabled = True\n                        # Remove the detected repeated header from this segment\n                        segment_rows = segment_rows[1:]\n                        if not headers_warned:\n                            warnings.warn(\n                                \"Detected repeated headers in multi-page table. Merging by removing \"\n                                \"repeated headers from subsequent pages.\",\n                                UserWarning,\n                                stacklevel=2,\n                            )\n                            headers_warned = True\n                    else:\n                        merge_headers_enabled = False\n                elif region_idx &gt; 1:\n                    # Check consistency: all segments should have same pattern\n                    has_header = segment_rows and header_row and segment_rows[0] == header_row\n                    segment_has_repeated_header.append(has_header)\n\n                    # Remove header if merging is enabled and header is present\n                    if merge_headers_enabled and has_header:\n                        segment_rows = segment_rows[1:]\n                elif region_idx &gt; 0 and merge_headers_enabled:\n                    # Explicit merge_headers=True: remove headers from subsequent segments\n                    if segment_rows and header_row and segment_rows[0] == header_row:\n                        segment_rows = segment_rows[1:]\n                        if not headers_warned:\n                            warnings.warn(\n                                \"Removing repeated headers from multi-page table during merge.\",\n                                UserWarning,\n                                stacklevel=2,\n                            )\n                            headers_warned = True\n\n                # Process remaining rows with stitch_rows logic\n                for row_idx, row in enumerate(segment_rows):\n                    if (\n                        predicate is not None\n                        and aggregated_rows\n                        and predicate(aggregated_rows[-1], row, row_idx, region)\n                    ):\n                        # Merge with previous row\n                        aggregated_rows[-1] = _default_merge(aggregated_rows[-1], row)\n                    else:\n                        aggregated_rows.append(row)\n            except Exception as e:\n                logger.error(\n                    f\"FlowRegion.extract_table: Error extracting table from constituent region {region}: {e}\",\n                    exc_info=True,\n                )\n\n        # Check for inconsistent header patterns after processing all segments\n        if merge_headers is None and len(segment_has_repeated_header) &gt; 2:\n            # During auto-detection, check for consistency across all segments\n            expected_pattern = segment_has_repeated_header[1]  # Pattern from second segment\n            for seg_idx, has_header in enumerate(segment_has_repeated_header[2:], 2):\n                if has_header != expected_pattern:\n                    # Inconsistent pattern detected\n                    segments_with_headers = [\n                        i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if has_h\n                    ]\n                    segments_without_headers = [\n                        i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if not has_h\n                    ]\n                    raise ValueError(\n                        f\"Inconsistent header pattern in multi-page table: \"\n                        f\"segments {segments_with_headers} have repeated headers, \"\n                        f\"but segments {segments_without_headers} do not. \"\n                        f\"All segments must have the same header pattern for reliable merging.\"\n                    )\n\n        return TableResult(aggregated_rows)\n\n    def extract_tables(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        **kwargs,\n    ) -&gt; List[List[List[Optional[str]]]]:\n        \"\"\"Extract **all** tables from the FlowRegion.\n\n        This simply chains :pymeth:`Region.extract_tables` over each physical\n        region and concatenates their results, preserving flow order.\n\n        Args:\n            method, table_settings: Forwarded to underlying ``Region.extract_tables``.\n            **kwargs: Additional keyword arguments forwarded.\n\n        Returns:\n            A list where each item is a full table (list of rows).  The order of\n            tables follows the order of the constituent regions in the flow.\n        \"\"\"\n\n        if table_settings is None:\n            table_settings = {}\n\n        if not self.constituent_regions:\n            return []\n\n        all_tables: List[List[List[Optional[str]]]] = []\n\n        for region in self.constituent_regions:\n            try:\n                region_tables = region.extract_tables(\n                    method=method,\n                    table_settings=table_settings.copy(),\n                    **kwargs,\n                )\n                # ``region_tables`` is a list (possibly empty).\n                if region_tables:\n                    all_tables.extend(region_tables)\n            except Exception as e:\n                logger.error(\n                    f\"FlowRegion.extract_tables: Error extracting tables from constituent region {region}: {e}\",\n                    exc_info=True,\n                )\n\n        return all_tables\n\n    @property\n    def normalized_type(self) -&gt; Optional[str]:\n        \"\"\"\n        Return the normalized type for selector compatibility.\n        This allows FlowRegion to be found by selectors like 'table'.\n        \"\"\"\n        if self.region_type:\n            # Convert region_type to normalized format (replace spaces with underscores, lowercase)\n            return self.region_type.lower().replace(\" \", \"_\")\n        return None\n\n    @property\n    def type(self) -&gt; Optional[str]:\n        \"\"\"\n        Return the type attribute for selector compatibility.\n        This is an alias for normalized_type.\n        \"\"\"\n        return self.normalized_type\n\n    def get_highlight_specs(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Get highlight specifications for all constituent regions.\n\n        This implements the highlighting protocol for FlowRegions, returning\n        specs for each constituent region so they can be highlighted on their\n        respective pages.\n\n        Returns:\n            List of highlight specification dictionaries, one for each\n            constituent region.\n        \"\"\"\n        specs = []\n\n        for region in self.constituent_regions:\n            if not hasattr(region, \"page\") or region.page is None:\n                continue\n\n            if not hasattr(region, \"bbox\") or region.bbox is None:\n                continue\n\n            spec = {\n                \"page\": region.page,\n                \"page_index\": region.page.index if hasattr(region.page, \"index\") else 0,\n                \"bbox\": region.bbox,\n                \"element\": region,  # Reference to the constituent region\n            }\n\n            # Add polygon if available\n            if hasattr(region, \"polygon\") and hasattr(region, \"has_polygon\") and region.has_polygon:\n                spec[\"polygon\"] = region.polygon\n\n            specs.append(spec)\n\n        return specs\n</code></pre>"},{"location":"api/#natural_pdf.FlowRegion-attributes","title":"Attributes","text":"<code>natural_pdf.FlowRegion.bbox</code> <code>property</code> <p>The bounding box that encloses all constituent regions. Calculated dynamically and cached.</p> <code>natural_pdf.FlowRegion.is_empty</code> <code>property</code> <p>Checks if the FlowRegion contains no constituent regions or if all are empty.</p> <code>natural_pdf.FlowRegion.normalized_type</code> <code>property</code> <p>Return the normalized type for selector compatibility. This allows FlowRegion to be found by selectors like 'table'.</p> <code>natural_pdf.FlowRegion.type</code> <code>property</code> <p>Return the type attribute for selector compatibility. This is an alias for normalized_type.</p>"},{"location":"api/#natural_pdf.FlowRegion-functions","title":"Functions","text":"<code>natural_pdf.FlowRegion.__getattr__(name)</code> <p>Dynamically proxy attribute access to the source FlowElement for safe attributes only. Spatial methods (above, below, left, right) are explicitly implemented to prevent silent failures and incorrect behavior.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Dynamically proxy attribute access to the source FlowElement for safe attributes only.\n    Spatial methods (above, below, left, right) are explicitly implemented to prevent\n    silent failures and incorrect behavior.\n    \"\"\"\n    if name in self.__dict__:\n        return self.__dict__[name]\n\n    # List of methods that should NOT be proxied - they need proper FlowRegion implementation\n    spatial_methods = {\"above\", \"below\", \"left\", \"right\", \"to_region\"}\n\n    if name in spatial_methods:\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}'. \"\n            f\"This method requires proper FlowRegion implementation to handle spatial relationships correctly.\"\n        )\n\n    # Only proxy safe attributes and methods\n    if self.source_flow_element is not None:\n        try:\n            attr = getattr(self.source_flow_element, name)\n            # Only proxy non-callable attributes and explicitly safe methods\n            if not callable(attr) or name in {\"page\", \"document\"}:  # Add safe methods as needed\n                return attr\n            else:\n                raise AttributeError(\n                    f\"Method '{name}' cannot be safely proxied from FlowElement to FlowRegion. \"\n                    f\"It may need explicit implementation.\"\n                )\n        except AttributeError:\n            pass\n\n    raise AttributeError(f\"'{self.__class__.__name__}' object has no attribute '{name}'\")\n</code></pre> <code>natural_pdf.FlowRegion.__init__(flow, constituent_regions, source_flow_element, boundary_element_found=None)</code> <p>Initializes a FlowRegion.</p> <p>Parameters:</p> Name Type Description Default <code>flow</code> <code>Flow</code> <p>The Flow instance this region belongs to.</p> required <code>constituent_regions</code> <code>List[Region]</code> <p>A list of physical natural_pdf.elements.region.Region                  objects that make up this FlowRegion.</p> required <code>source_flow_element</code> <code>FlowElement</code> <p>The FlowElement that created this FlowRegion.</p> required <code>boundary_element_found</code> <code>Optional[Element]</code> <p>The physical element that stopped an 'until' search,                     if applicable.</p> <code>None</code> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def __init__(\n    self,\n    flow: \"Flow\",\n    constituent_regions: List[\"PhysicalRegion\"],\n    source_flow_element: \"FlowElement\",\n    boundary_element_found: Optional[\"PhysicalElement\"] = None,\n):\n    \"\"\"\n    Initializes a FlowRegion.\n\n    Args:\n        flow: The Flow instance this region belongs to.\n        constituent_regions: A list of physical natural_pdf.elements.region.Region\n                             objects that make up this FlowRegion.\n        source_flow_element: The FlowElement that created this FlowRegion.\n        boundary_element_found: The physical element that stopped an 'until' search,\n                                if applicable.\n    \"\"\"\n    self.flow: \"Flow\" = flow\n    self.constituent_regions: List[\"PhysicalRegion\"] = constituent_regions\n    self.source_flow_element: \"FlowElement\" = source_flow_element\n    self.boundary_element_found: Optional[\"PhysicalElement\"] = boundary_element_found\n\n    # Add attributes for grid building, similar to Region\n    self.source: Optional[str] = None\n    self.region_type: Optional[str] = None\n    self.metadata: Dict[str, Any] = {}\n\n    # Cache for expensive operations\n    self._cached_text: Optional[str] = None\n    self._cached_elements: Optional[\"ElementCollection\"] = None  # Stringized\n    self._cached_bbox: Optional[Tuple[float, float, float, float]] = None\n</code></pre> <code>natural_pdf.FlowRegion.above(height=None, width='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Create a FlowRegion with regions above this FlowRegion.</p> <p>For vertical flows: Only expands the topmost constituent region upward. For horizontal flows: Expands all constituent regions upward.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>Optional[float]</code> <p>Height of the region above, in points</p> <code>None</code> <code>width</code> <code>str</code> <p>Width mode - \"full\" for full page width or \"element\" for element width</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this FlowRegion in the result</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify an upper boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>FlowRegion</code> <p>New FlowRegion with regions above</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def above(\n    self,\n    height: Optional[float] = None,\n    width: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"FlowRegion\":\n    \"\"\"\n    Create a FlowRegion with regions above this FlowRegion.\n\n    For vertical flows: Only expands the topmost constituent region upward.\n    For horizontal flows: Expands all constituent regions upward.\n\n    Args:\n        height: Height of the region above, in points\n        width: Width mode - \"full\" for full page width or \"element\" for element width\n        include_source: Whether to include this FlowRegion in the result\n        until: Optional selector string to specify an upper boundary element\n        include_endpoint: Whether to include the boundary element in the region\n        **kwargs: Additional parameters\n\n    Returns:\n        New FlowRegion with regions above\n    \"\"\"\n    if not self.constituent_regions:\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=[],\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    new_regions = []\n\n    if self.flow.arrangement == \"vertical\":\n        # For vertical flow, use FLOW ORDER (index 0 is earliest). Only expand the\n        # first constituent region in that order.\n        first_region = self.constituent_regions[0]\n        for idx, region in enumerate(self.constituent_regions):\n            if idx == 0:  # Only expand the first region (earliest in flow)\n                above_region = region.above(\n                    height=height,\n                    width=\"element\",  # Keep original column width\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(above_region)\n            elif include_source:\n                new_regions.append(region)\n    else:  # horizontal flow\n        # For horizontal flow, expand all regions upward\n        for region in self.constituent_regions:\n            above_region = region.above(\n                height=height,\n                width=width,\n                include_source=include_source,\n                until=until,\n                include_endpoint=include_endpoint,\n                **kwargs,\n            )\n            new_regions.append(above_region)\n\n    return FlowRegion(\n        flow=self.flow,\n        constituent_regions=new_regions,\n        source_flow_element=self.source_flow_element,\n        boundary_element_found=self.boundary_element_found,\n    )\n</code></pre> <code>natural_pdf.FlowRegion.below(height=None, width='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Create a FlowRegion with regions below this FlowRegion.</p> <p>For vertical flows: Only expands the bottommost constituent region downward. For horizontal flows: Expands all constituent regions downward.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>Optional[float]</code> <p>Height of the region below, in points</p> <code>None</code> <code>width</code> <code>str</code> <p>Width mode - \"full\" for full page width or \"element\" for element width</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this FlowRegion in the result</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify a lower boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>FlowRegion</code> <p>New FlowRegion with regions below</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def below(\n    self,\n    height: Optional[float] = None,\n    width: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"FlowRegion\":\n    \"\"\"\n    Create a FlowRegion with regions below this FlowRegion.\n\n    For vertical flows: Only expands the bottommost constituent region downward.\n    For horizontal flows: Expands all constituent regions downward.\n\n    Args:\n        height: Height of the region below, in points\n        width: Width mode - \"full\" for full page width or \"element\" for element width\n        include_source: Whether to include this FlowRegion in the result\n        until: Optional selector string to specify a lower boundary element\n        include_endpoint: Whether to include the boundary element in the region\n        **kwargs: Additional parameters\n\n    Returns:\n        New FlowRegion with regions below\n    \"\"\"\n    if not self.constituent_regions:\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=[],\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    new_regions = []\n\n    if self.flow.arrangement == \"vertical\":\n        # For vertical flow, expand only the LAST constituent region in flow order.\n        last_idx = len(self.constituent_regions) - 1\n        for idx, region in enumerate(self.constituent_regions):\n            if idx == last_idx:\n                below_region = region.below(\n                    height=height,\n                    width=\"element\",\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(below_region)\n            elif include_source:\n                new_regions.append(region)\n    else:  # horizontal flow\n        # For horizontal flow, expand all regions downward\n        for region in self.constituent_regions:\n            below_region = region.below(\n                height=height,\n                width=width,\n                include_source=include_source,\n                until=until,\n                include_endpoint=include_endpoint,\n                **kwargs,\n            )\n            new_regions.append(below_region)\n\n    return FlowRegion(\n        flow=self.flow,\n        constituent_regions=new_regions,\n        source_flow_element=self.source_flow_element,\n        boundary_element_found=self.boundary_element_found,\n    )\n</code></pre> <code>natural_pdf.FlowRegion.elements(apply_exclusions=True)</code> <p>Collects all unique physical elements from all constituent physical regions.</p> <p>Parameters:</p> Name Type Description Default <code>apply_exclusions</code> <code>bool</code> <p>Whether to respect PDF exclusion zones within each               constituent physical region when gathering elements.</p> <code>True</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>An ElementCollection containing all unique elements.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def elements(self, apply_exclusions: bool = True) -&gt; \"ElementCollection\":  # Stringized return\n    \"\"\"\n    Collects all unique physical elements from all constituent physical regions.\n\n    Args:\n        apply_exclusions: Whether to respect PDF exclusion zones within each\n                          constituent physical region when gathering elements.\n\n    Returns:\n        An ElementCollection containing all unique elements.\n    \"\"\"\n    from natural_pdf.elements.element_collection import (\n        ElementCollection as RuntimeElementCollection,  # Local import\n    )\n\n    if self._cached_elements is not None and apply_exclusions:  # Simple cache check\n        return self._cached_elements\n\n    if not self.constituent_regions:\n        return RuntimeElementCollection([])\n\n    all_physical_elements: List[\"PhysicalElement\"] = []  # Stringized item type\n    seen_elements = (\n        set()\n    )  # To ensure uniqueness if elements are shared or duplicated by region definitions\n\n    for region in self.constituent_regions:\n        # Region.get_elements() returns a list, not ElementCollection\n        elements_in_region: List[\"PhysicalElement\"] = region.get_elements(\n            apply_exclusions=apply_exclusions\n        )\n        for elem in elements_in_region:\n            if elem not in seen_elements:  # Check for uniqueness based on object identity\n                all_physical_elements.append(elem)\n                seen_elements.add(elem)\n\n    # Basic reading order sort based on original page and coordinates.\n    def get_sort_key(phys_elem: \"PhysicalElement\"):  # Stringized param type\n        page_idx = -1\n        if hasattr(phys_elem, \"page\") and hasattr(phys_elem.page, \"index\"):\n            page_idx = phys_elem.page.index\n        return (page_idx, phys_elem.top, phys_elem.x0)\n\n    try:\n        sorted_physical_elements = sorted(all_physical_elements, key=get_sort_key)\n    except AttributeError:\n        logger.warning(\n            \"Could not sort elements in FlowRegion by reading order; some elements might be missing page, top or x0 attributes.\"\n        )\n        sorted_physical_elements = all_physical_elements\n\n    result_collection = RuntimeElementCollection(sorted_physical_elements)\n    if apply_exclusions:\n        self._cached_elements = result_collection\n    return result_collection\n</code></pre> <code>natural_pdf.FlowRegion.expand(left=0, right=0, top=0, bottom=0, width_factor=1.0, height_factor=1.0)</code> <p>Create a new FlowRegion with all constituent regions expanded.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>float</code> <p>Amount to expand left edge (positive value expands leftwards)</p> <code>0</code> <code>right</code> <code>float</code> <p>Amount to expand right edge (positive value expands rightwards)</p> <code>0</code> <code>top</code> <code>float</code> <p>Amount to expand top edge (positive value expands upwards)</p> <code>0</code> <code>bottom</code> <code>float</code> <p>Amount to expand bottom edge (positive value expands downwards)</p> <code>0</code> <code>width_factor</code> <code>float</code> <p>Factor to multiply width by (applied after absolute expansion)</p> <code>1.0</code> <code>height_factor</code> <code>float</code> <p>Factor to multiply height by (applied after absolute expansion)</p> <code>1.0</code> <p>Returns:</p> Type Description <code>FlowRegion</code> <p>New FlowRegion with expanded constituent regions</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def expand(\n    self,\n    left: float = 0,\n    right: float = 0,\n    top: float = 0,\n    bottom: float = 0,\n    width_factor: float = 1.0,\n    height_factor: float = 1.0,\n) -&gt; \"FlowRegion\":\n    \"\"\"\n    Create a new FlowRegion with all constituent regions expanded.\n\n    Args:\n        left: Amount to expand left edge (positive value expands leftwards)\n        right: Amount to expand right edge (positive value expands rightwards)\n        top: Amount to expand top edge (positive value expands upwards)\n        bottom: Amount to expand bottom edge (positive value expands downwards)\n        width_factor: Factor to multiply width by (applied after absolute expansion)\n        height_factor: Factor to multiply height by (applied after absolute expansion)\n\n    Returns:\n        New FlowRegion with expanded constituent regions\n    \"\"\"\n    if not self.constituent_regions:\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=[],\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    expanded_regions = []\n    for idx, region in enumerate(self.constituent_regions):\n        # Determine which adjustments to apply based on flow arrangement\n        apply_left = left\n        apply_right = right\n        apply_top = top\n        apply_bottom = bottom\n\n        if self.flow.arrangement == \"vertical\":\n            # In a vertical flow, only the *first* region should react to `top`\n            # and only the *last* region should react to `bottom`.  This keeps\n            # the virtual contiguous area intact while allowing users to nudge\n            # the flow boundaries.\n            if idx != 0:\n                apply_top = 0\n            if idx != len(self.constituent_regions) - 1:\n                apply_bottom = 0\n            # left/right apply to every region (same column width change)\n        else:  # horizontal flow\n            # In a horizontal flow, only the first region reacts to `left`\n            # and only the last region reacts to `right`.\n            if idx != 0:\n                apply_left = 0\n            if idx != len(self.constituent_regions) - 1:\n                apply_right = 0\n            # top/bottom apply to every region in horizontal flows\n\n        # Skip no-op expansion to avoid extra Region objects\n        needs_expansion = (\n            any(\n                v not in (0, 1.0)  # compare width/height factor logically later\n                for v in (apply_left, apply_right, apply_top, apply_bottom)\n            )\n            or width_factor != 1.0\n            or height_factor != 1.0\n        )\n\n        try:\n            expanded_region = (\n                region.expand(\n                    left=apply_left,\n                    right=apply_right,\n                    top=apply_top,\n                    bottom=apply_bottom,\n                    width_factor=width_factor,\n                    height_factor=height_factor,\n                )\n                if needs_expansion\n                else region\n            )\n            expanded_regions.append(expanded_region)\n        except Exception as e:\n            logger.warning(\n                f\"FlowRegion.expand: Error expanding constituent region {region.bbox}: {e}\",\n                exc_info=False,\n            )\n            expanded_regions.append(region)\n\n    # Create new FlowRegion with expanded constituent regions\n    new_flow_region = FlowRegion(\n        flow=self.flow,\n        constituent_regions=expanded_regions,\n        source_flow_element=self.source_flow_element,\n        boundary_element_found=self.boundary_element_found,\n    )\n\n    # Copy metadata\n    new_flow_region.source = self.source\n    new_flow_region.region_type = self.region_type\n    new_flow_region.metadata = self.metadata.copy()\n\n    # Clear caches since the regions have changed\n    new_flow_region._cached_text = None\n    new_flow_region._cached_elements = None\n    new_flow_region._cached_bbox = None\n\n    return new_flow_region\n</code></pre> <code>natural_pdf.FlowRegion.extract_table(method=None, table_settings=None, use_ocr=False, ocr_config=None, text_options=None, cell_extraction_func=None, show_progress=False, stitch_rows=None, merge_headers=None, **kwargs)</code> <p>Extracts a single logical table from the FlowRegion.</p> <p>This is a convenience wrapper that iterates through the constituent physical regions in flow order, calls their <code>extract_table</code> method, and concatenates the resulting rows.  It mirrors the public interface of :pymeth:<code>natural_pdf.elements.region.Region.extract_table</code>.</p> <p>Parameters:</p> Name Type Description Default <code>method,</code> <code>(table_settings, use_ocr, ocr_config, text_options, cell_extraction_func, show_progress)</code> <p>Same as in :pymeth:<code>Region.extract_table</code> and are forwarded as-is to each physical region.</p> required <code>merge_headers</code> <code>Optional[bool]</code> <p>Whether to merge tables by removing repeated headers from subsequent pages/segments. If None (default), auto-detects by checking if the first row of each segment matches the first row of the first segment. If segments have inconsistent header patterns (some repeat, others don't), raises ValueError. Useful for multi-page tables where headers repeat on each page.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to the underlying <code>Region.extract_table</code> implementation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TableResult</code> <p>A TableResult object containing the aggregated table data.  Rows returned from</p> <code>TableResult</code> <p>consecutive constituent regions are appended in document order.  If</p> <code>TableResult</code> <p>no tables are detected in any region, an empty TableResult is returned.</p> stitch_rows parameter <p>Controls whether the first rows of subsequent segments/regions should be merged into the previous row (to handle spill-over across page breaks). Applied AFTER header removal if merge_headers is enabled.</p> <p>\u2022 None (default) \u2013 no merging (behaviour identical to previous versions). \u2022 Callable \u2013 custom predicate taking        (prev_row, cur_row, row_idx_in_segment, segment_object) \u2192 bool.    Return True to merge <code>cur_row</code> into <code>prev_row</code> (default column-wise merge is used).</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def extract_table(\n    self,\n    method: Optional[str] = None,\n    table_settings: Optional[dict] = None,\n    use_ocr: bool = False,\n    ocr_config: Optional[dict] = None,\n    text_options: Optional[Dict] = None,\n    cell_extraction_func: Optional[Callable[[\"PhysicalRegion\"], Optional[str]]] = None,\n    show_progress: bool = False,\n    # Optional row-level merge predicate. If provided, it decides whether\n    # the current row (first row of a segment/page) should be merged with\n    # the previous one (to handle multi-page spill-overs).\n    stitch_rows: Optional[\n        Callable[[List[Optional[str]], List[Optional[str]], int, \"PhysicalRegion\"], bool]\n    ] = None,\n    merge_headers: Optional[bool] = None,\n    **kwargs,\n) -&gt; TableResult:\n    \"\"\"Extracts a single logical table from the FlowRegion.\n\n    This is a convenience wrapper that iterates through the constituent\n    physical regions **in flow order**, calls their ``extract_table``\n    method, and concatenates the resulting rows.  It mirrors the public\n    interface of :pymeth:`natural_pdf.elements.region.Region.extract_table`.\n\n    Args:\n        method, table_settings, use_ocr, ocr_config, text_options, cell_extraction_func, show_progress:\n            Same as in :pymeth:`Region.extract_table` and are forwarded as-is\n            to each physical region.\n        merge_headers: Whether to merge tables by removing repeated headers from subsequent\n            pages/segments. If None (default), auto-detects by checking if the first row\n            of each segment matches the first row of the first segment. If segments have\n            inconsistent header patterns (some repeat, others don't), raises ValueError.\n            Useful for multi-page tables where headers repeat on each page.\n        **kwargs: Additional keyword arguments forwarded to the underlying\n            ``Region.extract_table`` implementation.\n\n    Returns:\n        A TableResult object containing the aggregated table data.  Rows returned from\n        consecutive constituent regions are appended in document order.  If\n        no tables are detected in any region, an empty TableResult is returned.\n\n    stitch_rows parameter:\n        Controls whether the first rows of subsequent segments/regions should be merged\n        into the previous row (to handle spill-over across page breaks).\n        Applied AFTER header removal if merge_headers is enabled.\n\n        \u2022 None (default) \u2013 no merging (behaviour identical to previous versions).\n        \u2022 Callable \u2013 custom predicate taking\n               (prev_row, cur_row, row_idx_in_segment, segment_object) \u2192 bool.\n           Return True to merge `cur_row` into `prev_row` (default column-wise merge is used).\n    \"\"\"\n\n    if table_settings is None:\n        table_settings = {}\n    if text_options is None:\n        text_options = {}\n\n    if not self.constituent_regions:\n        return TableResult([])\n\n    # Resolve stitch_rows predicate -------------------------------------------------------\n    predicate: Optional[\n        Callable[[List[Optional[str]], List[Optional[str]], int, \"PhysicalRegion\"], bool]\n    ] = (stitch_rows if callable(stitch_rows) else None)\n\n    def _default_merge(\n        prev_row: List[Optional[str]], cur_row: List[Optional[str]]\n    ) -&gt; List[Optional[str]]:\n        \"\"\"Column-wise merge \u2013 concatenates non-empty strings with a space.\"\"\"\n        from itertools import zip_longest\n\n        merged: List[Optional[str]] = []\n        for p, c in zip_longest(prev_row, cur_row, fillvalue=\"\"):\n            if (p or \"\").strip() and (c or \"\").strip():\n                merged.append(f\"{p} {c}\".strip())\n            else:\n                merged.append((p or \"\") + (c or \"\"))\n        return merged\n\n    aggregated_rows: List[List[Optional[str]]] = []\n    header_row: Optional[List[Optional[str]]] = None\n    merge_headers_enabled = False\n    headers_warned = False  # Track if we've already warned about dropping headers\n    segment_has_repeated_header = []  # Track which segments have repeated headers\n\n    for region_idx, region in enumerate(self.constituent_regions):\n        try:\n            region_result = region.extract_table(\n                method=method,\n                table_settings=table_settings.copy(),  # Avoid side-effects\n                use_ocr=use_ocr,\n                ocr_config=ocr_config,\n                text_options=text_options.copy(),\n                cell_extraction_func=cell_extraction_func,\n                show_progress=show_progress,\n                **kwargs,\n            )\n\n            # Convert result to list of rows\n            if not region_result:\n                continue\n\n            if isinstance(region_result, TableResult):\n                segment_rows = list(region_result)\n            else:\n                segment_rows = list(region_result)\n\n            # Handle header detection and merging for multi-page tables\n            if region_idx == 0:\n                # First segment: capture potential header row\n                if segment_rows:\n                    header_row = segment_rows[0]\n                    # Determine if we should merge headers\n                    if merge_headers is None:\n                        # Auto-detect: we'll check all subsequent segments\n                        merge_headers_enabled = False  # Will be determined later\n                    else:\n                        merge_headers_enabled = merge_headers\n                    # Track that first segment exists (for consistency checking)\n                    segment_has_repeated_header.append(False)  # First segment doesn't \"repeat\"\n            elif region_idx == 1 and merge_headers is None:\n                # Auto-detection: check if first row of second segment matches header\n                has_header = segment_rows and header_row and segment_rows[0] == header_row\n                segment_has_repeated_header.append(has_header)\n\n                if has_header:\n                    merge_headers_enabled = True\n                    # Remove the detected repeated header from this segment\n                    segment_rows = segment_rows[1:]\n                    if not headers_warned:\n                        warnings.warn(\n                            \"Detected repeated headers in multi-page table. Merging by removing \"\n                            \"repeated headers from subsequent pages.\",\n                            UserWarning,\n                            stacklevel=2,\n                        )\n                        headers_warned = True\n                else:\n                    merge_headers_enabled = False\n            elif region_idx &gt; 1:\n                # Check consistency: all segments should have same pattern\n                has_header = segment_rows and header_row and segment_rows[0] == header_row\n                segment_has_repeated_header.append(has_header)\n\n                # Remove header if merging is enabled and header is present\n                if merge_headers_enabled and has_header:\n                    segment_rows = segment_rows[1:]\n            elif region_idx &gt; 0 and merge_headers_enabled:\n                # Explicit merge_headers=True: remove headers from subsequent segments\n                if segment_rows and header_row and segment_rows[0] == header_row:\n                    segment_rows = segment_rows[1:]\n                    if not headers_warned:\n                        warnings.warn(\n                            \"Removing repeated headers from multi-page table during merge.\",\n                            UserWarning,\n                            stacklevel=2,\n                        )\n                        headers_warned = True\n\n            # Process remaining rows with stitch_rows logic\n            for row_idx, row in enumerate(segment_rows):\n                if (\n                    predicate is not None\n                    and aggregated_rows\n                    and predicate(aggregated_rows[-1], row, row_idx, region)\n                ):\n                    # Merge with previous row\n                    aggregated_rows[-1] = _default_merge(aggregated_rows[-1], row)\n                else:\n                    aggregated_rows.append(row)\n        except Exception as e:\n            logger.error(\n                f\"FlowRegion.extract_table: Error extracting table from constituent region {region}: {e}\",\n                exc_info=True,\n            )\n\n    # Check for inconsistent header patterns after processing all segments\n    if merge_headers is None and len(segment_has_repeated_header) &gt; 2:\n        # During auto-detection, check for consistency across all segments\n        expected_pattern = segment_has_repeated_header[1]  # Pattern from second segment\n        for seg_idx, has_header in enumerate(segment_has_repeated_header[2:], 2):\n            if has_header != expected_pattern:\n                # Inconsistent pattern detected\n                segments_with_headers = [\n                    i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if has_h\n                ]\n                segments_without_headers = [\n                    i for i, has_h in enumerate(segment_has_repeated_header[1:], 1) if not has_h\n                ]\n                raise ValueError(\n                    f\"Inconsistent header pattern in multi-page table: \"\n                    f\"segments {segments_with_headers} have repeated headers, \"\n                    f\"but segments {segments_without_headers} do not. \"\n                    f\"All segments must have the same header pattern for reliable merging.\"\n                )\n\n    return TableResult(aggregated_rows)\n</code></pre> <code>natural_pdf.FlowRegion.extract_tables(method=None, table_settings=None, **kwargs)</code> <p>Extract all tables from the FlowRegion.</p> <p>This simply chains :pymeth:<code>Region.extract_tables</code> over each physical region and concatenates their results, preserving flow order.</p> <p>Parameters:</p> Name Type Description Default <code>method,</code> <code>table_settings</code> <p>Forwarded to underlying <code>Region.extract_tables</code>.</p> required <code>**kwargs</code> <p>Additional keyword arguments forwarded.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[List[List[Optional[str]]]]</code> <p>A list where each item is a full table (list of rows).  The order of</p> <code>List[List[List[Optional[str]]]]</code> <p>tables follows the order of the constituent regions in the flow.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def extract_tables(\n    self,\n    method: Optional[str] = None,\n    table_settings: Optional[dict] = None,\n    **kwargs,\n) -&gt; List[List[List[Optional[str]]]]:\n    \"\"\"Extract **all** tables from the FlowRegion.\n\n    This simply chains :pymeth:`Region.extract_tables` over each physical\n    region and concatenates their results, preserving flow order.\n\n    Args:\n        method, table_settings: Forwarded to underlying ``Region.extract_tables``.\n        **kwargs: Additional keyword arguments forwarded.\n\n    Returns:\n        A list where each item is a full table (list of rows).  The order of\n        tables follows the order of the constituent regions in the flow.\n    \"\"\"\n\n    if table_settings is None:\n        table_settings = {}\n\n    if not self.constituent_regions:\n        return []\n\n    all_tables: List[List[List[Optional[str]]]] = []\n\n    for region in self.constituent_regions:\n        try:\n            region_tables = region.extract_tables(\n                method=method,\n                table_settings=table_settings.copy(),\n                **kwargs,\n            )\n            # ``region_tables`` is a list (possibly empty).\n            if region_tables:\n                all_tables.extend(region_tables)\n        except Exception as e:\n            logger.error(\n                f\"FlowRegion.extract_tables: Error extracting tables from constituent region {region}: {e}\",\n                exc_info=True,\n            )\n\n    return all_tables\n</code></pre> <code>natural_pdf.FlowRegion.extract_text(apply_exclusions=True, **kwargs)</code> <p>Extracts and concatenates text from all constituent physical regions. The order of concatenation respects the flow's arrangement.</p> <p>Parameters:</p> Name Type Description Default <code>apply_exclusions</code> <code>bool</code> <p>Whether to respect PDF exclusion zones within each               constituent physical region during text extraction.</p> <code>True</code> <code>**kwargs</code> <p>Additional arguments passed to the underlying extract_text method       of each constituent region.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The combined text content as a string.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def extract_text(self, apply_exclusions: bool = True, **kwargs) -&gt; str:\n    \"\"\"\n    Extracts and concatenates text from all constituent physical regions.\n    The order of concatenation respects the flow's arrangement.\n\n    Args:\n        apply_exclusions: Whether to respect PDF exclusion zones within each\n                          constituent physical region during text extraction.\n        **kwargs: Additional arguments passed to the underlying extract_text method\n                  of each constituent region.\n\n    Returns:\n        The combined text content as a string.\n    \"\"\"\n    if (\n        self._cached_text is not None and apply_exclusions\n    ):  # Simple cache check, might need refinement if kwargs change behavior\n        return self._cached_text\n\n    if not self.constituent_regions:\n        return \"\"\n\n    texts: List[str] = []\n    # For now, simple concatenation. Order depends on how constituent_regions were added.\n    # The FlowElement._flow_direction method is responsible for ordering constituent_regions correctly.\n    for region in self.constituent_regions:\n        texts.append(region.extract_text(apply_exclusions=apply_exclusions, **kwargs))\n\n    # Join based on flow arrangement (e.g., newline for vertical, space for horizontal)\n    # This is a simplification; true layout-aware joining would be more complex.\n    joiner = (\n        \"\\n\" if self.flow.arrangement == \"vertical\" else \" \"\n    )  # TODO: Consider flow.segment_gap for proportional spacing between segments\n    extracted = joiner.join(t for t in texts if t)\n\n    if apply_exclusions:  # Only cache if standard exclusion behavior\n        self._cached_text = extracted\n    return extracted\n</code></pre> <code>natural_pdf.FlowRegion.find(selector=None, *, text=None, **kwargs)</code> <p>Find the first element in flow order that matches the selector or text.</p> <p>This implementation iterates through the constituent regions *in the order they appear in <code>self.constituent_regions</code> (i.e. document flow order), delegating the search to each region's own <code>find</code> method.  It therefore avoids constructing a huge intermediate ElementCollection and returns as soon as a match is found, which is substantially faster and ensures that selectors such as 'table' work exactly as they do on an individual Region.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def find(\n    self, selector: Optional[str] = None, *, text: Optional[str] = None, **kwargs\n) -&gt; Optional[\"PhysicalElement\"]:  # Stringized\n    \"\"\"\n    Find the first element in flow order that matches the selector or text.\n\n    This implementation iterates through the constituent regions *in the order\n    they appear in ``self.constituent_regions`` (i.e. document flow order),\n    delegating the search to each region's own ``find`` method.  It therefore\n    avoids constructing a huge intermediate ElementCollection and returns as\n    soon as a match is found, which is substantially faster and ensures that\n    selectors such as 'table' work exactly as they do on an individual\n    Region.\n    \"\"\"\n    if not self.constituent_regions:\n        return None\n\n    for region in self.constituent_regions:\n        try:\n            result = region.find(selector=selector, text=text, **kwargs)\n            if result is not None:\n                return result\n        except Exception as e:\n            logger.warning(\n                f\"FlowRegion.find: error searching region {region}: {e}\",\n                exc_info=False,\n            )\n    return None  # No match found\n</code></pre> <code>natural_pdf.FlowRegion.find_all(selector=None, *, text=None, **kwargs)</code> <p>Find all elements across the constituent regions that match the given selector or text.</p> <p>Rather than first materialising every element in the FlowRegion (which can be extremely slow for multi-page flows), this implementation simply chains each region's native <code>find_all</code> call and concatenates their results into a single ElementCollection while preserving flow order.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def find_all(\n    self, selector: Optional[str] = None, *, text: Optional[str] = None, **kwargs\n) -&gt; \"ElementCollection\":  # Stringized\n    \"\"\"\n    Find **all** elements across the constituent regions that match the given\n    selector or text.\n\n    Rather than first materialising *every* element in the FlowRegion (which\n    can be extremely slow for multi-page flows), this implementation simply\n    chains each region's native ``find_all`` call and concatenates their\n    results into a single ElementCollection while preserving flow order.\n    \"\"\"\n    from natural_pdf.elements.element_collection import (\n        ElementCollection as RuntimeElementCollection,\n    )\n\n    matched_elements = []  # type: List[\"PhysicalElement\"]\n\n    if not self.constituent_regions:\n        return RuntimeElementCollection([])\n\n    for region in self.constituent_regions:\n        try:\n            region_matches = region.find_all(selector=selector, text=text, **kwargs)\n            if region_matches:\n                # ``region_matches`` is an ElementCollection \u2013 extend with its\n                # underlying list so we don't create nested collections.\n                matched_elements.extend(\n                    region_matches.elements\n                    if hasattr(region_matches, \"elements\")\n                    else list(region_matches)\n                )\n        except Exception as e:\n            logger.warning(\n                f\"FlowRegion.find_all: error searching region {region}: {e}\",\n                exc_info=False,\n            )\n\n    return RuntimeElementCollection(matched_elements)\n</code></pre> <code>natural_pdf.FlowRegion.get_highlight_specs()</code> <p>Get highlight specifications for all constituent regions.</p> <p>This implements the highlighting protocol for FlowRegions, returning specs for each constituent region so they can be highlighted on their respective pages.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of highlight specification dictionaries, one for each</p> <code>List[Dict[str, Any]]</code> <p>constituent region.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def get_highlight_specs(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Get highlight specifications for all constituent regions.\n\n    This implements the highlighting protocol for FlowRegions, returning\n    specs for each constituent region so they can be highlighted on their\n    respective pages.\n\n    Returns:\n        List of highlight specification dictionaries, one for each\n        constituent region.\n    \"\"\"\n    specs = []\n\n    for region in self.constituent_regions:\n        if not hasattr(region, \"page\") or region.page is None:\n            continue\n\n        if not hasattr(region, \"bbox\") or region.bbox is None:\n            continue\n\n        spec = {\n            \"page\": region.page,\n            \"page_index\": region.page.index if hasattr(region.page, \"index\") else 0,\n            \"bbox\": region.bbox,\n            \"element\": region,  # Reference to the constituent region\n        }\n\n        # Add polygon if available\n        if hasattr(region, \"polygon\") and hasattr(region, \"has_polygon\") and region.has_polygon:\n            spec[\"polygon\"] = region.polygon\n\n        specs.append(spec)\n\n    return specs\n</code></pre> <code>natural_pdf.FlowRegion.highlight(label=None, color=None, **kwargs)</code> <p>Highlights all constituent physical regions on their respective pages.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Optional[str]</code> <p>A base label for the highlights. Each constituent region might get an indexed label.</p> <code>None</code> <code>color</code> <code>Optional[Union[Tuple, str]]</code> <p>Color for the highlight.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for the underlying highlight method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>FlowRegion</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def highlight(\n    self, label: Optional[str] = None, color: Optional[Union[Tuple, str]] = None, **kwargs\n) -&gt; \"FlowRegion\":  # Stringized\n    \"\"\"\n    Highlights all constituent physical regions on their respective pages.\n\n    Args:\n        label: A base label for the highlights. Each constituent region might get an indexed label.\n        color: Color for the highlight.\n        **kwargs: Additional arguments for the underlying highlight method.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    if not self.constituent_regions:\n        return self\n\n    base_label = label if label else \"FlowRegionPart\"\n    for i, region in enumerate(self.constituent_regions):\n        current_label = (\n            f\"{base_label}_{i+1}\" if len(self.constituent_regions) &gt; 1 else base_label\n        )\n        region.highlight(label=current_label, color=color, **kwargs)\n    return self\n</code></pre> <code>natural_pdf.FlowRegion.highlights(show=False)</code> <p>Create a highlight context for accumulating highlights.</p> <p>This allows for clean syntax to show multiple highlight groups:</p> Example <p>with flow_region.highlights() as h:     h.add(flow_region.find_all('table'), label='tables', color='blue')     h.add(flow_region.find_all('text:bold'), label='bold text', color='red')     h.show()</p> Or with automatic display <p>with flow_region.highlights(show=True) as h:     h.add(flow_region.find_all('table'), label='tables')     h.add(flow_region.find_all('text:bold'), label='bold')     # Automatically shows when exiting the context</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>If True, automatically show highlights when exiting context</p> <code>False</code> <p>Returns:</p> Type Description <code>HighlightContext</code> <p>HighlightContext for accumulating highlights</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n    \"\"\"\n    Create a highlight context for accumulating highlights.\n\n    This allows for clean syntax to show multiple highlight groups:\n\n    Example:\n        with flow_region.highlights() as h:\n            h.add(flow_region.find_all('table'), label='tables', color='blue')\n            h.add(flow_region.find_all('text:bold'), label='bold text', color='red')\n            h.show()\n\n    Or with automatic display:\n        with flow_region.highlights(show=True) as h:\n            h.add(flow_region.find_all('table'), label='tables')\n            h.add(flow_region.find_all('text:bold'), label='bold')\n            # Automatically shows when exiting the context\n\n    Args:\n        show: If True, automatically show highlights when exiting context\n\n    Returns:\n        HighlightContext for accumulating highlights\n    \"\"\"\n    from natural_pdf.core.highlighting_service import HighlightContext\n\n    return HighlightContext(self, show_on_exit=show)\n</code></pre> <code>natural_pdf.FlowRegion.left(width=None, height='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Create a FlowRegion with regions to the left of this FlowRegion.</p> <p>For vertical flows: Expands all constituent regions leftward. For horizontal flows: Only expands the leftmost constituent region leftward.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[float]</code> <p>Width of the region to the left, in points</p> <code>None</code> <code>height</code> <code>str</code> <p>Height mode - \"full\" for full page height or \"element\" for element height</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this FlowRegion in the result</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify a left boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>FlowRegion</code> <p>New FlowRegion with regions to the left</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def left(\n    self,\n    width: Optional[float] = None,\n    height: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"FlowRegion\":\n    \"\"\"\n    Create a FlowRegion with regions to the left of this FlowRegion.\n\n    For vertical flows: Expands all constituent regions leftward.\n    For horizontal flows: Only expands the leftmost constituent region leftward.\n\n    Args:\n        width: Width of the region to the left, in points\n        height: Height mode - \"full\" for full page height or \"element\" for element height\n        include_source: Whether to include this FlowRegion in the result\n        until: Optional selector string to specify a left boundary element\n        include_endpoint: Whether to include the boundary element in the region\n        **kwargs: Additional parameters\n\n    Returns:\n        New FlowRegion with regions to the left\n    \"\"\"\n    if not self.constituent_regions:\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=[],\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    new_regions = []\n\n    if self.flow.arrangement == \"vertical\":\n        # For vertical flow, expand all regions leftward\n        for region in self.constituent_regions:\n            left_region = region.left(\n                width=width,\n                height=\"element\",\n                include_source=include_source,\n                until=until,\n                include_endpoint=include_endpoint,\n                **kwargs,\n            )\n            new_regions.append(left_region)\n    else:  # horizontal flow\n        # For horizontal flow, only expand the leftmost region leftward\n        leftmost_region = min(self.constituent_regions, key=lambda r: r.x0)\n        for region in self.constituent_regions:\n            if region == leftmost_region:\n                # Expand this region leftward\n                left_region = region.left(\n                    width=width,\n                    height=\"element\",\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(left_region)\n            elif include_source:\n                # Include other regions unchanged if include_source is True\n                new_regions.append(region)\n\n    return FlowRegion(\n        flow=self.flow,\n        constituent_regions=new_regions,\n        source_flow_element=self.source_flow_element,\n        boundary_element_found=self.boundary_element_found,\n    )\n</code></pre> <code>natural_pdf.FlowRegion.right(width=None, height='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Create a FlowRegion with regions to the right of this FlowRegion.</p> <p>For vertical flows: Expands all constituent regions rightward. For horizontal flows: Only expands the rightmost constituent region rightward.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[float]</code> <p>Width of the region to the right, in points</p> <code>None</code> <code>height</code> <code>str</code> <p>Height mode - \"full\" for full page height or \"element\" for element height</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this FlowRegion in the result</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify a right boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>FlowRegion</code> <p>New FlowRegion with regions to the right</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def right(\n    self,\n    width: Optional[float] = None,\n    height: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"FlowRegion\":\n    \"\"\"\n    Create a FlowRegion with regions to the right of this FlowRegion.\n\n    For vertical flows: Expands all constituent regions rightward.\n    For horizontal flows: Only expands the rightmost constituent region rightward.\n\n    Args:\n        width: Width of the region to the right, in points\n        height: Height mode - \"full\" for full page height or \"element\" for element height\n        include_source: Whether to include this FlowRegion in the result\n        until: Optional selector string to specify a right boundary element\n        include_endpoint: Whether to include the boundary element in the region\n        **kwargs: Additional parameters\n\n    Returns:\n        New FlowRegion with regions to the right\n    \"\"\"\n    if not self.constituent_regions:\n        return FlowRegion(\n            flow=self.flow,\n            constituent_regions=[],\n            source_flow_element=self.source_flow_element,\n            boundary_element_found=self.boundary_element_found,\n        )\n\n    new_regions = []\n\n    if self.flow.arrangement == \"vertical\":\n        # For vertical flow, expand all regions rightward\n        for region in self.constituent_regions:\n            right_region = region.right(\n                width=width,\n                height=\"element\",\n                include_source=include_source,\n                until=until,\n                include_endpoint=include_endpoint,\n                **kwargs,\n            )\n            new_regions.append(right_region)\n    else:  # horizontal flow\n        # For horizontal flow, only expand the rightmost region rightward\n        rightmost_region = max(self.constituent_regions, key=lambda r: r.x1)\n        for region in self.constituent_regions:\n            if region == rightmost_region:\n                # Expand this region rightward\n                right_region = region.right(\n                    width=width,\n                    height=\"element\",\n                    include_source=include_source,\n                    until=until,\n                    include_endpoint=include_endpoint,\n                    **kwargs,\n                )\n                new_regions.append(right_region)\n            elif include_source:\n                # Include other regions unchanged if include_source is True\n                new_regions.append(region)\n\n    return FlowRegion(\n        flow=self.flow,\n        constituent_regions=new_regions,\n        source_flow_element=self.source_flow_element,\n        boundary_element_found=self.boundary_element_found,\n    )\n</code></pre> <code>natural_pdf.FlowRegion.to_images(resolution=150, **kwargs)</code> <p>Generates and returns a list of cropped PIL Images, one for each constituent physical region of this FlowRegion.</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def to_images(\n    self,\n    resolution: float = 150,\n    **kwargs,\n) -&gt; List[\"PIL_Image\"]:\n    \"\"\"\n    Generates and returns a list of cropped PIL Images,\n    one for each constituent physical region of this FlowRegion.\n    \"\"\"\n    if not self.constituent_regions:\n        logger.info(\"FlowRegion.to_images() called on an empty FlowRegion.\")\n        return []\n\n    cropped_images: List[\"PIL_Image\"] = []\n    for region_part in self.constituent_regions:\n        try:\n            # Use render() for clean image without highlights\n            img = region_part.render(resolution=resolution, crop=True, **kwargs)\n            if img:\n                cropped_images.append(img)\n        except Exception as e:\n            logger.error(\n                f\"Error generating image for constituent region {region_part.bbox}: {e}\",\n                exc_info=True,\n            )\n\n    return cropped_images\n</code></pre> <code>natural_pdf.FlowRegion.to_region()</code> <p>Convert this FlowRegion to a region (returns a copy). This is equivalent to calling expand() with no arguments.</p> <p>Returns:</p> Type Description <code>FlowRegion</code> <p>Copy of this FlowRegion</p> Source code in <code>natural_pdf/flows/region.py</code> <pre><code>def to_region(self) -&gt; \"FlowRegion\":\n    \"\"\"\n    Convert this FlowRegion to a region (returns a copy).\n    This is equivalent to calling expand() with no arguments.\n\n    Returns:\n        Copy of this FlowRegion\n    \"\"\"\n    return self.expand()\n</code></pre>"},{"location":"api/#natural_pdf.Guides","title":"<code>natural_pdf.Guides</code>","text":"<p>Manages vertical and horizontal guide lines for table extraction and layout analysis.</p> <p>Guides are collections of coordinates that can be used to define table boundaries, column positions, or general layout structures. They can be created through various detection methods or manually specified.</p> <p>Attributes:</p> Name Type Description <code>verticals</code> <p>List of x-coordinates for vertical guide lines</p> <code>horizontals</code> <p>List of y-coordinates for horizontal guide lines</p> <code>context</code> <p>Optional Page/Region that these guides relate to</p> <code>bounds</code> <p>Optional bounding box (x0, y0, x1, y1) for relative coordinate conversion</p> <code>snap_behavior</code> <p>How to handle failed snapping operations ('warn', 'ignore', 'raise')</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>class Guides:\n    \"\"\"\n    Manages vertical and horizontal guide lines for table extraction and layout analysis.\n\n    Guides are collections of coordinates that can be used to define table boundaries,\n    column positions, or general layout structures. They can be created through various\n    detection methods or manually specified.\n\n    Attributes:\n        verticals: List of x-coordinates for vertical guide lines\n        horizontals: List of y-coordinates for horizontal guide lines\n        context: Optional Page/Region that these guides relate to\n        bounds: Optional bounding box (x0, y0, x1, y1) for relative coordinate conversion\n        snap_behavior: How to handle failed snapping operations ('warn', 'ignore', 'raise')\n    \"\"\"\n\n    def __init__(\n        self,\n        verticals: Optional[Union[List[float], \"Page\", \"Region\", \"FlowRegion\"]] = None,\n        horizontals: Optional[List[float]] = None,\n        context: Optional[Union[\"Page\", \"Region\", \"FlowRegion\"]] = None,\n        bounds: Optional[Tuple[float, float, float, float]] = None,\n        relative: bool = False,\n        snap_behavior: Literal[\"raise\", \"warn\", \"ignore\"] = \"warn\",\n    ):\n        \"\"\"\n        Initialize a Guides object.\n\n        Args:\n            verticals: List of x-coordinates for vertical guides, or a Page/Region/FlowRegion as context\n            horizontals: List of y-coordinates for horizontal guides\n            context: Page, Region, or FlowRegion object these guides were created from\n            bounds: Bounding box (x0, top, x1, bottom) if context not provided\n            relative: Whether coordinates are relative (0-1) or absolute\n            snap_behavior: How to handle snapping conflicts ('raise', 'warn', or 'ignore')\n        \"\"\"\n        # Handle Guides(page) or Guides(flow_region) shorthand\n        if (\n            verticals is not None\n            and not isinstance(verticals, (list, tuple))\n            and horizontals is None\n            and context is None\n        ):\n            # First argument is a page/region/flow_region, not coordinates\n            context = verticals\n            verticals = None\n\n        self.context = context\n        self.bounds = bounds\n        self.relative = relative\n        self.snap_behavior = snap_behavior\n\n        # Check if we're dealing with a FlowRegion\n        self.is_flow_region = hasattr(context, \"constituent_regions\")\n\n        # If FlowRegion, we'll store guides per constituent region\n        if self.is_flow_region:\n            self._flow_guides: Dict[\"Region\", Tuple[List[float], List[float]]] = {}\n            # For unified view across all regions\n            self._unified_vertical: List[Tuple[float, \"Region\"]] = []\n            self._unified_horizontal: List[Tuple[float, \"Region\"]] = []\n            # Cache for sorted unique coordinates\n            self._vertical_cache: Optional[List[float]] = None\n            self._horizontal_cache: Optional[List[float]] = None\n\n        # Initialize with GuidesList instances\n        self._vertical = GuidesList(self, \"vertical\", sorted([float(x) for x in (verticals or [])]))\n        self._horizontal = GuidesList(\n            self, \"horizontal\", sorted([float(y) for y in (horizontals or [])])\n        )\n\n        # Determine bounds from context if needed\n        if self.bounds is None and self.context is not None:\n            if hasattr(self.context, \"bbox\"):\n                self.bounds = self.context.bbox\n            elif hasattr(self.context, \"x0\"):\n                self.bounds = (\n                    self.context.x0,\n                    self.context.top,\n                    self.context.x1,\n                    self.context.bottom,\n                )\n\n        # Convert relative to absolute if needed\n        if self.relative and self.bounds:\n            x0, top, x1, bottom = self.bounds\n            width = x1 - x0\n            height = bottom - top\n\n            self._vertical.data = [x0 + v * width for v in self._vertical]\n            self._horizontal.data = [top + h * height for h in self._horizontal]\n            self.relative = False\n\n    @property\n    def vertical(self) -&gt; GuidesList:\n        \"\"\"Get vertical guide coordinates.\"\"\"\n        if self.is_flow_region and self._vertical_cache is not None:\n            # Return cached unified view\n            self._vertical.data = self._vertical_cache\n        elif self.is_flow_region and self._unified_vertical:\n            # Build unified view from flow guides\n            all_verticals = []\n            for coord, region in self._unified_vertical:\n                all_verticals.append(coord)\n            # Remove duplicates and sort\n            self._vertical_cache = sorted(list(set(all_verticals)))\n            self._vertical.data = self._vertical_cache\n        return self._vertical\n\n    @vertical.setter\n    def vertical(self, value: Union[List[float], \"Guides\", None]):\n        \"\"\"Set vertical guides from a list of coordinates or another Guides object.\"\"\"\n        if self.is_flow_region:\n            # Invalidate cache when setting new values\n            self._vertical_cache = None\n\n        if value is None:\n            self._vertical.data = []\n        elif isinstance(value, Guides):\n            # Extract vertical coordinates from another Guides object\n            self._vertical.data = sorted([float(x) for x in value.vertical])\n        elif isinstance(value, str):\n            # Explicitly reject strings to avoid confusing iteration over characters\n            raise TypeError(\n                f\"vertical cannot be a string, got '{value}'. Use a list of coordinates or Guides object.\"\n            )\n        elif hasattr(value, \"__iter__\"):\n            # Handle list/tuple of coordinates\n            try:\n                self._vertical.data = sorted([float(x) for x in value])\n            except (ValueError, TypeError) as e:\n                raise TypeError(f\"vertical must contain numeric values, got {value}: {e}\")\n        else:\n            raise TypeError(f\"vertical must be a list, Guides object, or None, got {type(value)}\")\n\n    @property\n    def horizontal(self) -&gt; GuidesList:\n        \"\"\"Get horizontal guide coordinates.\"\"\"\n        if self.is_flow_region and self._horizontal_cache is not None:\n            # Return cached unified view\n            self._horizontal.data = self._horizontal_cache\n        elif self.is_flow_region and self._unified_horizontal:\n            # Build unified view from flow guides\n            all_horizontals = []\n            for coord, region in self._unified_horizontal:\n                all_horizontals.append(coord)\n            # Remove duplicates and sort\n            self._horizontal_cache = sorted(list(set(all_horizontals)))\n            self._horizontal.data = self._horizontal_cache\n        return self._horizontal\n\n    @horizontal.setter\n    def horizontal(self, value: Union[List[float], \"Guides\", None]):\n        \"\"\"Set horizontal guides from a list of coordinates or another Guides object.\"\"\"\n        if self.is_flow_region:\n            # Invalidate cache when setting new values\n            self._horizontal_cache = None\n\n        if value is None:\n            self._horizontal.data = []\n        elif isinstance(value, Guides):\n            # Extract horizontal coordinates from another Guides object\n            self._horizontal.data = sorted([float(y) for y in value.horizontal])\n        elif isinstance(value, str):\n            # Explicitly reject strings\n            raise TypeError(\n                f\"horizontal cannot be a string, got '{value}'. Use a list of coordinates or Guides object.\"\n            )\n        elif hasattr(value, \"__iter__\"):\n            # Handle list/tuple of coordinates\n            try:\n                self._horizontal.data = sorted([float(y) for y in value])\n            except (ValueError, TypeError) as e:\n                raise TypeError(f\"horizontal must contain numeric values, got {value}: {e}\")\n        else:\n            raise TypeError(f\"horizontal must be a list, Guides object, or None, got {type(value)}\")\n\n    def _get_context_bounds(self) -&gt; Optional[Tuple[float, float, float, float]]:\n        \"\"\"Get bounds from context if available.\"\"\"\n        if self.context is None:\n            return None\n\n        if hasattr(self.context, \"bbox\"):\n            return self.context.bbox\n        elif hasattr(self.context, \"x0\") and hasattr(self.context, \"top\"):\n            return (self.context.x0, self.context.top, self.context.x1, self.context.bottom)\n        elif hasattr(self.context, \"width\") and hasattr(self.context, \"height\"):\n            return (0, 0, self.context.width, self.context.height)\n        return None\n\n    # -------------------------------------------------------------------------\n    # Factory Methods\n    # -------------------------------------------------------------------------\n\n    @classmethod\n    def divide(\n        cls,\n        obj: Union[\"Page\", \"Region\", Tuple[float, float, float, float]],\n        n: Optional[int] = None,\n        cols: Optional[int] = None,\n        rows: Optional[int] = None,\n        axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Create guides by evenly dividing an object.\n\n        Args:\n            obj: Object to divide (Page, Region, or bbox tuple)\n            n: Number of divisions (creates n+1 guides). Used if cols/rows not specified.\n            cols: Number of columns (creates cols+1 vertical guides)\n            rows: Number of rows (creates rows+1 horizontal guides)\n            axis: Which axis to divide along\n\n        Returns:\n            New Guides object with evenly spaced lines\n\n        Examples:\n            # Divide into 3 columns\n            guides = Guides.divide(page, cols=3)\n\n            # Divide into 5 rows\n            guides = Guides.divide(region, rows=5)\n\n            # Divide both axes\n            guides = Guides.divide(page, cols=3, rows=5)\n        \"\"\"\n        # Extract bounds from object\n        if isinstance(obj, tuple) and len(obj) == 4:\n            bounds = obj\n            context = None\n        else:\n            context = obj\n            if hasattr(obj, \"bbox\"):\n                bounds = obj.bbox\n            elif hasattr(obj, \"x0\"):\n                bounds = (obj.x0, obj.top, obj.x1, obj.bottom)\n            else:\n                bounds = (0, 0, obj.width, obj.height)\n\n        x0, y0, x1, y1 = bounds\n        verticals = []\n        horizontals = []\n\n        # Handle vertical guides\n        if axis in (\"vertical\", \"both\"):\n            n_vertical = cols + 1 if cols is not None else (n + 1 if n is not None else 0)\n            if n_vertical &gt; 0:\n                for i in range(n_vertical):\n                    x = x0 + (x1 - x0) * i / (n_vertical - 1)\n                    verticals.append(float(x))\n\n        # Handle horizontal guides\n        if axis in (\"horizontal\", \"both\"):\n            n_horizontal = rows + 1 if rows is not None else (n + 1 if n is not None else 0)\n            if n_horizontal &gt; 0:\n                for i in range(n_horizontal):\n                    y = y0 + (y1 - y0) * i / (n_horizontal - 1)\n                    horizontals.append(float(y))\n\n        return cls(verticals=verticals, horizontals=horizontals, context=context, bounds=bounds)\n\n    @classmethod\n    def from_lines(\n        cls,\n        obj: Union[\"Page\", \"Region\", \"FlowRegion\"],\n        axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n        threshold: Union[float, str] = \"auto\",\n        source_label: Optional[str] = None,\n        max_lines_h: Optional[int] = None,\n        max_lines_v: Optional[int] = None,\n        outer: bool = False,\n        detection_method: str = \"pixels\",\n        resolution: int = 192,\n        **detect_kwargs,\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Create guides from detected line elements.\n\n        Args:\n            obj: Page, Region, or FlowRegion to detect lines from\n            axis: Which orientations to detect\n            threshold: Detection threshold ('auto' or float 0.0-1.0) - used for pixel detection\n            source_label: Filter for line source (vector method) or label for detected lines (pixel method)\n            max_lines_h: Maximum number of horizontal lines to keep\n            max_lines_v: Maximum number of vertical lines to keep\n            outer: Whether to add outer boundary guides\n            detection_method: 'vector' (use existing LineElements) or 'pixels' (detect from image)\n            resolution: DPI for pixel-based detection (default: 192)\n            **detect_kwargs: Additional parameters for pixel-based detection:\n                - min_gap_h: Minimum gap between horizontal lines (pixels)\n                - min_gap_v: Minimum gap between vertical lines (pixels)\n                - binarization_method: 'adaptive' or 'otsu'\n                - morph_op_h/v: Morphological operations ('open', 'close', 'none')\n                - smoothing_sigma_h/v: Gaussian smoothing sigma\n                - method: 'projection' (default) or 'lsd' (requires opencv)\n\n        Returns:\n            New Guides object with detected line positions\n        \"\"\"\n        # Handle FlowRegion\n        if hasattr(obj, \"constituent_regions\"):\n            guides = cls(context=obj)\n\n            # Process each constituent region\n            for region in obj.constituent_regions:\n                # Create guides for this specific region\n                region_guides = cls.from_lines(\n                    region,\n                    axis=axis,\n                    threshold=threshold,\n                    source_label=source_label,\n                    max_lines_h=max_lines_h,\n                    max_lines_v=max_lines_v,\n                    outer=outer,\n                    detection_method=detection_method,\n                    resolution=resolution,\n                    **detect_kwargs,\n                )\n\n                # Store in flow guides\n                guides._flow_guides[region] = (\n                    list(region_guides.vertical),\n                    list(region_guides.horizontal),\n                )\n\n                # Add to unified view\n                for v in region_guides.vertical:\n                    guides._unified_vertical.append((v, region))\n                for h in region_guides.horizontal:\n                    guides._unified_horizontal.append((h, region))\n\n            # Invalidate caches to force rebuild on next access\n            guides._vertical_cache = None\n            guides._horizontal_cache = None\n\n            return guides\n\n        # Original single-region logic follows...\n        # Get bounds for potential outer guides\n        if hasattr(obj, \"bbox\"):\n            bounds = obj.bbox\n        elif hasattr(obj, \"x0\"):\n            bounds = (obj.x0, obj.top, obj.x1, obj.bottom)\n        elif hasattr(obj, \"width\"):\n            bounds = (0, 0, obj.width, obj.height)\n        else:\n            bounds = None\n\n        verticals = []\n        horizontals = []\n\n        if detection_method == \"pixels\":\n            # Use pixel-based line detection\n            if not hasattr(obj, \"detect_lines\"):\n                raise ValueError(f\"Object {obj} does not support pixel-based line detection\")\n\n            # Set up detection parameters\n            detect_params = {\n                \"resolution\": resolution,\n                \"source_label\": source_label or \"guides_detection\",\n                \"horizontal\": axis in (\"horizontal\", \"both\"),\n                \"vertical\": axis in (\"vertical\", \"both\"),\n                \"replace\": True,  # Replace any existing lines with this source\n                \"method\": detect_kwargs.get(\"method\", \"projection\"),\n            }\n\n            # Handle threshold parameter\n            if threshold == \"auto\" and detection_method == \"vector\":\n                # Auto mode: use very low thresholds with max_lines constraints\n                detect_params[\"peak_threshold_h\"] = 0.0\n                detect_params[\"peak_threshold_v\"] = 0.0\n                detect_params[\"max_lines_h\"] = max_lines_h\n                detect_params[\"max_lines_v\"] = max_lines_v\n            if threshold == \"auto\" and detection_method == \"pixels\":\n                detect_params[\"peak_threshold_h\"] = 0.5\n                detect_params[\"peak_threshold_v\"] = 0.5\n                detect_params[\"max_lines_h\"] = max_lines_h\n                detect_params[\"max_lines_v\"] = max_lines_v\n            else:\n                # Fixed threshold mode\n                detect_params[\"peak_threshold_h\"] = (\n                    float(threshold) if axis in (\"horizontal\", \"both\") else 1.0\n                )\n                detect_params[\"peak_threshold_v\"] = (\n                    float(threshold) if axis in (\"vertical\", \"both\") else 1.0\n                )\n                detect_params[\"max_lines_h\"] = max_lines_h\n                detect_params[\"max_lines_v\"] = max_lines_v\n\n            # Add any additional detection parameters\n            for key in [\n                \"min_gap_h\",\n                \"min_gap_v\",\n                \"binarization_method\",\n                \"adaptive_thresh_block_size\",\n                \"adaptive_thresh_C_val\",\n                \"morph_op_h\",\n                \"morph_kernel_h\",\n                \"morph_op_v\",\n                \"morph_kernel_v\",\n                \"smoothing_sigma_h\",\n                \"smoothing_sigma_v\",\n                \"peak_width_rel_height\",\n            ]:\n                if key in detect_kwargs:\n                    detect_params[key] = detect_kwargs[key]\n\n            # Perform the detection\n            obj.detect_lines(**detect_params)\n\n            # Now get the detected lines and use them\n            if hasattr(obj, \"lines\"):\n                lines = obj.lines\n            elif hasattr(obj, \"find_all\"):\n                lines = obj.find_all(\"line\")\n            else:\n                lines = []\n\n            # Filter by the source we just used\n\n            lines = [\n                l for l in lines if getattr(l, \"source\", None) == detect_params[\"source_label\"]\n            ]\n\n        else:  # detection_method == 'vector' (default)\n            # Get existing lines from the object\n            if hasattr(obj, \"lines\"):\n                lines = obj.lines\n            elif hasattr(obj, \"find_all\"):\n                lines = obj.find_all(\"line\")\n            else:\n                logger.warning(f\"Object {obj} has no lines or find_all method\")\n                lines = []\n\n            # Filter by source if specified\n            if source_label:\n                lines = [l for l in lines if getattr(l, \"source\", None) == source_label]\n\n        # Process lines (same logic for both methods)\n        # Separate lines by orientation and collect with metadata for ranking\n        h_line_data = []  # (y_coord, length, line_obj)\n        v_line_data = []  # (x_coord, length, line_obj)\n\n        for line in lines:\n            if hasattr(line, \"is_horizontal\") and hasattr(line, \"is_vertical\"):\n                if line.is_horizontal and axis in (\"horizontal\", \"both\"):\n                    # Use the midpoint y-coordinate for horizontal lines\n                    y = (line.top + line.bottom) / 2\n                    # Calculate line length for ranking\n                    length = getattr(\n                        line, \"width\", abs(getattr(line, \"x1\", 0) - getattr(line, \"x0\", 0))\n                    )\n                    h_line_data.append((y, length, line))\n                elif line.is_vertical and axis in (\"vertical\", \"both\"):\n                    # Use the midpoint x-coordinate for vertical lines\n                    x = (line.x0 + line.x1) / 2\n                    # Calculate line length for ranking\n                    length = getattr(\n                        line, \"height\", abs(getattr(line, \"bottom\", 0) - getattr(line, \"top\", 0))\n                    )\n                    v_line_data.append((x, length, line))\n\n        # Process horizontal lines\n        if max_lines_h is not None and h_line_data:\n            # Sort by length (longer lines are typically more significant)\n            h_line_data.sort(key=lambda x: x[1], reverse=True)\n            # Take the top N by length\n            selected_h = h_line_data[:max_lines_h]\n            # Extract just the coordinates and sort by position\n            horizontals = sorted([coord for coord, _, _ in selected_h])\n            logger.debug(\n                f\"Selected {len(horizontals)} horizontal lines from {len(h_line_data)} candidates\"\n            )\n        else:\n            # Use all horizontal lines (original behavior)\n            horizontals = [coord for coord, _, _ in h_line_data]\n            horizontals = sorted(list(set(horizontals)))\n\n        # Process vertical lines\n        if max_lines_v is not None and v_line_data:\n            # Sort by length (longer lines are typically more significant)\n            v_line_data.sort(key=lambda x: x[1], reverse=True)\n            # Take the top N by length\n            selected_v = v_line_data[:max_lines_v]\n            # Extract just the coordinates and sort by position\n            verticals = sorted([coord for coord, _, _ in selected_v])\n            logger.debug(\n                f\"Selected {len(verticals)} vertical lines from {len(v_line_data)} candidates\"\n            )\n        else:\n            # Use all vertical lines (original behavior)\n            verticals = [coord for coord, _, _ in v_line_data]\n            verticals = sorted(list(set(verticals)))\n\n        # Add outer guides if requested\n        if outer and bounds:\n            if axis in (\"vertical\", \"both\"):\n                if not verticals or verticals[0] &gt; bounds[0]:\n                    verticals.insert(0, bounds[0])  # x0\n                if not verticals or verticals[-1] &lt; bounds[2]:\n                    verticals.append(bounds[2])  # x1\n            if axis in (\"horizontal\", \"both\"):\n                if not horizontals or horizontals[0] &gt; bounds[1]:\n                    horizontals.insert(0, bounds[1])  # y0\n                if not horizontals or horizontals[-1] &lt; bounds[3]:\n                    horizontals.append(bounds[3])  # y1\n\n        # Remove duplicates and sort again\n        verticals = sorted(list(set(verticals)))\n        horizontals = sorted(list(set(horizontals)))\n\n        return cls(verticals=verticals, horizontals=horizontals, context=obj, bounds=bounds)\n\n    @classmethod\n    def from_content(\n        cls,\n        obj: Union[\"Page\", \"Region\", \"FlowRegion\"],\n        axis: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n        markers: Union[str, List[str], \"ElementCollection\", None] = None,\n        align: Literal[\"left\", \"right\", \"center\", \"between\"] = \"left\",\n        outer: bool = True,\n        tolerance: float = 5,\n        apply_exclusions: bool = True,\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Create guides based on text content positions.\n\n        Args:\n            obj: Page, Region, or FlowRegion to search for content\n            axis: Whether to create vertical or horizontal guides\n            markers: Content to search for. Can be:\n                - str: single selector (e.g., 'text:contains(\"Name\")') or literal text\n                - List[str]: list of selectors or literal text strings\n                - ElementCollection: collection of elements to extract text from\n                - None: no markers\n            align: Where to place guides relative to found text\n            outer: Whether to add guides at the boundaries\n            tolerance: Maximum distance to search for text\n            apply_exclusions: Whether to apply exclusion zones when searching for text\n\n        Returns:\n            New Guides object aligned to text content\n        \"\"\"\n        # Handle FlowRegion\n        if hasattr(obj, \"constituent_regions\"):\n            guides = cls(context=obj)\n\n            # Process each constituent region\n            for region in obj.constituent_regions:\n                # Create guides for this specific region\n                region_guides = cls.from_content(\n                    region,\n                    axis=axis,\n                    markers=markers,\n                    align=align,\n                    outer=outer,\n                    tolerance=tolerance,\n                    apply_exclusions=apply_exclusions,\n                )\n\n                # Store in flow guides\n                guides._flow_guides[region] = (\n                    list(region_guides.vertical),\n                    list(region_guides.horizontal),\n                )\n\n                # Add to unified view\n                for v in region_guides.vertical:\n                    guides._unified_vertical.append((v, region))\n                for h in region_guides.horizontal:\n                    guides._unified_horizontal.append((h, region))\n\n            # Invalidate caches\n            guides._vertical_cache = None\n            guides._horizontal_cache = None\n\n            return guides\n\n        # Original single-region logic follows...\n        guides_coords = []\n        bounds = None\n\n        # Get bounds from object\n        if hasattr(obj, \"bbox\"):\n            bounds = obj.bbox\n        elif hasattr(obj, \"x0\"):\n            bounds = (obj.x0, obj.top, obj.x1, obj.bottom)\n        elif hasattr(obj, \"width\"):\n            bounds = (0, 0, obj.width, obj.height)\n\n        # Normalize markers to list of text strings\n        marker_texts = _normalize_markers(markers, obj)\n\n        # Find each marker and determine guide position\n        for marker in marker_texts:\n            if hasattr(obj, \"find\"):\n                element = obj.find(f'text:contains(\"{marker}\")', apply_exclusions=apply_exclusions)\n                if element:\n                    if axis == \"vertical\":\n                        if align == \"left\":\n                            guides_coords.append(element.x0)\n                        elif align == \"right\":\n                            guides_coords.append(element.x1)\n                        elif align == \"center\":\n                            guides_coords.append((element.x0 + element.x1) / 2)\n                        elif align == \"between\":\n                            # For between, collect left edges for processing later\n                            guides_coords.append(element.x0)\n                    else:  # horizontal\n                        if align == \"left\":  # top for horizontal\n                            guides_coords.append(element.top)\n                        elif align == \"right\":  # bottom for horizontal\n                            guides_coords.append(element.bottom)\n                        elif align == \"center\":\n                            guides_coords.append((element.top + element.bottom) / 2)\n                        elif align == \"between\":\n                            # For between, collect top edges for processing later\n                            guides_coords.append(element.top)\n\n        # Handle 'between' alignment - find midpoints between adjacent markers\n        if align == \"between\" and len(guides_coords) &gt;= 2:\n            # We need to get the right and left edges of each marker\n            marker_bounds = []\n            for marker in marker_texts:\n                if hasattr(obj, \"find\"):\n                    element = obj.find(\n                        f'text:contains(\"{marker}\")', apply_exclusions=apply_exclusions\n                    )\n                    if element:\n                        if axis == \"vertical\":\n                            marker_bounds.append((element.x0, element.x1))\n                        else:  # horizontal\n                            marker_bounds.append((element.top, element.bottom))\n\n            # Sort markers by their left edge (or top edge for horizontal)\n            marker_bounds.sort(key=lambda x: x[0])\n\n            # Create guides at midpoints between adjacent markers\n            between_coords = []\n            for i in range(len(marker_bounds) - 1):\n                # Midpoint between right edge of current marker and left edge of next marker\n                right_edge_current = marker_bounds[i][1]\n                left_edge_next = marker_bounds[i + 1][0]\n                midpoint = (right_edge_current + left_edge_next) / 2\n                between_coords.append(midpoint)\n\n            guides_coords = between_coords\n\n        # Add outer guides if requested\n        if outer and bounds:\n            if axis == \"vertical\":\n                guides_coords.insert(0, bounds[0])  # x0\n                guides_coords.append(bounds[2])  # x1\n            else:\n                guides_coords.insert(0, bounds[1])  # y0\n                guides_coords.append(bounds[3])  # y1\n\n        # Remove duplicates and sort\n        guides_coords = sorted(list(set(guides_coords)))\n\n        # Create guides object\n        if axis == \"vertical\":\n            return cls(verticals=guides_coords, context=obj, bounds=bounds)\n        else:\n            return cls(horizontals=guides_coords, context=obj, bounds=bounds)\n\n    @classmethod\n    def from_whitespace(\n        cls,\n        obj: Union[\"Page\", \"Region\", \"FlowRegion\"],\n        axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n        min_gap: float = 10,\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Create guides by detecting whitespace gaps.\n\n        Args:\n            obj: Page or Region to analyze\n            min_gap: Minimum gap size to consider as whitespace\n            axis: Which axes to analyze for gaps\n\n        Returns:\n            New Guides object positioned at whitespace gaps\n        \"\"\"\n        # This is a placeholder - would need sophisticated gap detection\n        logger.info(\"Whitespace detection not yet implemented, using divide instead\")\n        return cls.divide(obj, n=3, axis=axis)\n\n    @classmethod\n    def new(cls, context: Optional[Union[\"Page\", \"Region\"]] = None) -&gt; \"Guides\":\n        \"\"\"\n        Create a new empty Guides object, optionally with a context.\n\n        This provides a clean way to start building guides through chaining:\n        guides = Guides.new(page).add_content(axis='vertical', markers=[...])\n\n        Args:\n            context: Optional Page or Region to use as default context for operations\n\n        Returns:\n            New empty Guides object\n        \"\"\"\n        return cls(verticals=[], horizontals=[], context=context)\n\n    # -------------------------------------------------------------------------\n    # Manipulation Methods\n    # -------------------------------------------------------------------------\n\n    def snap_to_whitespace(\n        self,\n        axis: str = \"vertical\",\n        min_gap: float = 10.0,\n        detection_method: str = \"pixels\",  # 'pixels' or 'text'\n        threshold: Union[\n            float, str\n        ] = \"auto\",  # threshold for what counts as a trough (0.0-1.0) or 'auto'\n        on_no_snap: str = \"warn\",\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Snap guides to nearby whitespace gaps (troughs) using optimal assignment.\n        Modifies this Guides object in place.\n\n        Args:\n            axis: Direction to snap ('vertical' or 'horizontal')\n            min_gap: Minimum gap size to consider as a valid trough\n            detection_method: Method for detecting troughs:\n                            'pixels' - use pixel-based density analysis (default)\n                            'text' - use text element spacing analysis\n            threshold: Threshold for what counts as a trough:\n                      - float (0.0-1.0): areas with this fraction or less of max density count as troughs\n                      - 'auto': automatically find threshold that creates enough troughs for guides\n            on_no_snap: Action when snapping fails ('warn', 'ignore', 'raise')\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        if not self.context:\n            logger.warning(\"No context available for whitespace detection\")\n            return self\n\n        # Handle FlowRegion case - collect all text elements across regions\n        if self.is_flow_region:\n            all_text_elements = []\n            region_bounds = {}\n\n            for region in self.context.constituent_regions:\n                # Get text elements from this region\n                if hasattr(region, \"find_all\"):\n                    try:\n                        text_elements = region.find_all(\"text\", apply_exclusions=False)\n                        elements = (\n                            text_elements.elements\n                            if hasattr(text_elements, \"elements\")\n                            else text_elements\n                        )\n                        all_text_elements.extend(elements)\n\n                        # Store bounds for each region\n                        if hasattr(region, \"bbox\"):\n                            region_bounds[region] = region.bbox\n                        elif hasattr(region, \"x0\"):\n                            region_bounds[region] = (\n                                region.x0,\n                                region.top,\n                                region.x1,\n                                region.bottom,\n                            )\n                    except Exception as e:\n                        logger.warning(f\"Error getting text elements from region: {e}\")\n\n            if not all_text_elements:\n                logger.warning(\n                    \"No text elements found across flow regions for whitespace detection\"\n                )\n                return self\n\n            # Find whitespace gaps across all regions\n            if axis == \"vertical\":\n                gaps = self._find_vertical_whitespace_gaps(all_text_elements, min_gap, threshold)\n                # Get all vertical guides across regions\n                all_guides = []\n                guide_to_region_map = {}  # Map guide coordinate to its original list of regions\n                for coord, region in self._unified_vertical:\n                    all_guides.append(coord)\n                    guide_to_region_map.setdefault(coord, []).append(region)\n\n                if gaps and all_guides:\n                    # Keep a copy of original guides to maintain mapping\n                    original_guides = all_guides.copy()\n\n                    # Snap guides to gaps\n                    self._snap_guides_to_gaps(all_guides, gaps, axis)\n\n                    # Update the unified view with snapped positions\n                    self._unified_vertical = []\n                    for i, new_coord in enumerate(all_guides):\n                        # Find the original region for this guide using the original position\n                        original_coord = original_guides[i]\n                        # A guide might be associated with multiple regions, add them all\n                        regions = guide_to_region_map.get(original_coord, [])\n                        for region in regions:\n                            self._unified_vertical.append((new_coord, region))\n\n                    # Update individual region guides\n                    for region in self._flow_guides:\n                        region_verticals = []\n                        for coord, r in self._unified_vertical:\n                            if r == region:\n                                region_verticals.append(coord)\n                        self._flow_guides[region] = (\n                            sorted(list(set(region_verticals))),  # Deduplicate here\n                            self._flow_guides[region][1],\n                        )\n\n                    # Invalidate cache\n                    self._vertical_cache = None\n\n            elif axis == \"horizontal\":\n                gaps = self._find_horizontal_whitespace_gaps(all_text_elements, min_gap, threshold)\n                # Get all horizontal guides across regions\n                all_guides = []\n                guide_to_region_map = {}  # Map guide coordinate to its original list of regions\n                for coord, region in self._unified_horizontal:\n                    all_guides.append(coord)\n                    guide_to_region_map.setdefault(coord, []).append(region)\n\n                if gaps and all_guides:\n                    # Keep a copy of original guides to maintain mapping\n                    original_guides = all_guides.copy()\n\n                    # Snap guides to gaps\n                    self._snap_guides_to_gaps(all_guides, gaps, axis)\n\n                    # Update the unified view with snapped positions\n                    self._unified_horizontal = []\n                    for i, new_coord in enumerate(all_guides):\n                        # Find the original region for this guide using the original position\n                        original_coord = original_guides[i]\n                        regions = guide_to_region_map.get(original_coord, [])\n                        for region in regions:\n                            self._unified_horizontal.append((new_coord, region))\n\n                    # Update individual region guides\n                    for region in self._flow_guides:\n                        region_horizontals = []\n                        for coord, r in self._unified_horizontal:\n                            if r == region:\n                                region_horizontals.append(coord)\n                        self._flow_guides[region] = (\n                            self._flow_guides[region][0],\n                            sorted(list(set(region_horizontals))),  # Deduplicate here\n                        )\n\n                    # Invalidate cache\n                    self._horizontal_cache = None\n\n            else:\n                raise ValueError(\"axis must be 'vertical' or 'horizontal'\")\n\n            return self\n\n        # Original single-region logic\n        # Get elements for trough detection\n        text_elements = self._get_text_elements()\n        if not text_elements:\n            logger.warning(\"No text elements found for whitespace detection\")\n            return self\n\n        if axis == \"vertical\":\n            gaps = self._find_vertical_whitespace_gaps(text_elements, min_gap, threshold)\n            if gaps:\n                self._snap_guides_to_gaps(self.vertical.data, gaps, axis)\n        elif axis == \"horizontal\":\n            gaps = self._find_horizontal_whitespace_gaps(text_elements, min_gap, threshold)\n            if gaps:\n                self._snap_guides_to_gaps(self.horizontal.data, gaps, axis)\n        else:\n            raise ValueError(\"axis must be 'vertical' or 'horizontal'\")\n\n        # Ensure all coordinates are Python floats (not numpy types)\n        self.vertical.data[:] = [float(x) for x in self.vertical.data]\n        self.horizontal.data[:] = [float(y) for y in self.horizontal.data]\n\n        return self\n\n    def shift(\n        self, index: int, offset: float, axis: Literal[\"vertical\", \"horizontal\"] = \"vertical\"\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Move a specific guide by a offset amount.\n\n        Args:\n            index: Index of the guide to move\n            offset: Amount to move (positive = right/down)\n            axis: Which guide list to modify\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        if axis == \"vertical\":\n            if 0 &lt;= index &lt; len(self.vertical):\n                self.vertical[index] += offset\n                self.vertical = sorted(self.vertical)\n            else:\n                logger.warning(f\"Vertical guide index {index} out of range\")\n        else:\n            if 0 &lt;= index &lt; len(self.horizontal):\n                self.horizontal[index] += offset\n                self.horizontal = sorted(self.horizontal)\n            else:\n                logger.warning(f\"Horizontal guide index {index} out of range\")\n\n        return self\n\n    def add_vertical(self, x: float) -&gt; \"Guides\":\n        \"\"\"Add a vertical guide at the specified x-coordinate.\"\"\"\n        self.vertical.append(x)\n        self.vertical = sorted(self.vertical)\n        return self\n\n    def add_horizontal(self, y: float) -&gt; \"Guides\":\n        \"\"\"Add a horizontal guide at the specified y-coordinate.\"\"\"\n        self.horizontal.append(y)\n        self.horizontal = sorted(self.horizontal)\n        return self\n\n    def remove_vertical(self, index: int) -&gt; \"Guides\":\n        \"\"\"Remove a vertical guide by index.\"\"\"\n        if 0 &lt;= index &lt; len(self.vertical):\n            self.vertical.pop(index)\n        return self\n\n    def remove_horizontal(self, index: int) -&gt; \"Guides\":\n        \"\"\"Remove a horizontal guide by index.\"\"\"\n        if 0 &lt;= index &lt; len(self.horizontal):\n            self.horizontal.pop(index)\n        return self\n\n    # -------------------------------------------------------------------------\n    # Operations\n    # -------------------------------------------------------------------------\n\n    def __add__(self, other: \"Guides\") -&gt; \"Guides\":\n        \"\"\"\n        Combine two guide sets.\n\n        Returns:\n            New Guides object with combined coordinates\n        \"\"\"\n        # Combine and deduplicate coordinates, ensuring Python floats\n        combined_verticals = sorted([float(x) for x in set(self.vertical + other.vertical)])\n        combined_horizontals = sorted([float(y) for y in set(self.horizontal + other.horizontal)])\n\n        # Handle FlowRegion context merging\n        new_context = self.context or other.context\n\n        # If both are flow regions, we might need a more complex merge,\n        # but for now, just picking one context is sufficient.\n\n        # Create the new Guides object\n        new_guides = Guides(\n            verticals=combined_verticals,\n            horizontals=combined_horizontals,\n            context=new_context,\n            bounds=self.bounds or other.bounds,\n        )\n\n        # If the new context is a FlowRegion, we need to rebuild the flow-related state\n        if new_guides.is_flow_region:\n            # Re-initialize flow guides from both sources\n            # This is a simplification; a true merge would be more complex.\n            # For now, we combine the flow_guides dictionaries.\n            if hasattr(self, \"_flow_guides\"):\n                new_guides._flow_guides.update(self._flow_guides)\n            if hasattr(other, \"_flow_guides\"):\n                new_guides._flow_guides.update(other._flow_guides)\n\n            # Re-initialize unified views\n            if hasattr(self, \"_unified_vertical\"):\n                new_guides._unified_vertical.extend(self._unified_vertical)\n            if hasattr(other, \"_unified_vertical\"):\n                new_guides._unified_vertical.extend(other._unified_vertical)\n\n            if hasattr(self, \"_unified_horizontal\"):\n                new_guides._unified_horizontal.extend(self._unified_horizontal)\n            if hasattr(other, \"_unified_horizontal\"):\n                new_guides._unified_horizontal.extend(other._unified_horizontal)\n\n            # Invalidate caches to force rebuild\n            new_guides._vertical_cache = None\n            new_guides._horizontal_cache = None\n\n        return new_guides\n\n    def show(self, on=None, **kwargs):\n        \"\"\"\n        Display the guides overlaid on a page or region.\n\n        Args:\n            on: Page, Region, PIL Image, or string to display guides on.\n                If None, uses self.context (the object guides were created from).\n                If string 'page', uses the page from self.context.\n            **kwargs: Additional arguments passed to to_image() if applicable.\n\n        Returns:\n            PIL Image with guides drawn on it.\n        \"\"\"\n        # Handle FlowRegion case\n        if self.is_flow_region and (on is None or on == self.context):\n            if not self._flow_guides:\n                raise ValueError(\"No guides to show for FlowRegion\")\n\n            # Get stacking parameters from kwargs or use defaults\n            stack_direction = kwargs.get(\"stack_direction\", \"vertical\")\n            stack_gap = kwargs.get(\"stack_gap\", 5)\n            stack_background_color = kwargs.get(\"stack_background_color\", (255, 255, 255))\n\n            # First, render all constituent regions without guides to get base images\n            base_images = []\n            region_infos = []  # Store region info for guide coordinate mapping\n\n            for region in self.context.constituent_regions:\n                try:\n                    # Render region without guides using new system\n                    if hasattr(region, \"render\"):\n                        img = region.render(\n                            resolution=kwargs.get(\"resolution\", 150),\n                            width=kwargs.get(\"width\", None),\n                            crop=True,  # Always crop regions to their bounds\n                        )\n                    else:\n                        # Fallback to old method\n                        img = region.render(**kwargs)\n                    if img:\n                        base_images.append(img)\n\n                        # Calculate scaling factors for this region\n                        scale_x = img.width / region.width\n                        scale_y = img.height / region.height\n\n                        region_infos.append(\n                            {\n                                \"region\": region,\n                                \"img_width\": img.width,\n                                \"img_height\": img.height,\n                                \"scale_x\": scale_x,\n                                \"scale_y\": scale_y,\n                                \"pdf_x0\": region.x0,\n                                \"pdf_top\": region.top,\n                                \"pdf_x1\": region.x1,\n                                \"pdf_bottom\": region.bottom,\n                            }\n                        )\n                except Exception as e:\n                    logger.warning(f\"Failed to render region: {e}\")\n\n            if not base_images:\n                raise ValueError(\"Failed to render any images for FlowRegion\")\n\n            # Calculate final canvas size based on stacking direction\n            if stack_direction == \"vertical\":\n                final_width = max(img.width for img in base_images)\n                final_height = (\n                    sum(img.height for img in base_images) + (len(base_images) - 1) * stack_gap\n                )\n            else:  # horizontal\n                final_width = (\n                    sum(img.width for img in base_images) + (len(base_images) - 1) * stack_gap\n                )\n                final_height = max(img.height for img in base_images)\n\n            # Create unified canvas\n            canvas = Image.new(\"RGB\", (final_width, final_height), stack_background_color)\n            draw = ImageDraw.Draw(canvas)\n\n            # Paste base images and track positions\n            region_positions = []  # (region_info, paste_x, paste_y)\n\n            if stack_direction == \"vertical\":\n                current_y = 0\n                for i, (img, info) in enumerate(zip(base_images, region_infos)):\n                    paste_x = (final_width - img.width) // 2  # Center horizontally\n                    canvas.paste(img, (paste_x, current_y))\n                    region_positions.append((info, paste_x, current_y))\n                    current_y += img.height + stack_gap\n            else:  # horizontal\n                current_x = 0\n                for i, (img, info) in enumerate(zip(base_images, region_infos)):\n                    paste_y = (final_height - img.height) // 2  # Center vertically\n                    canvas.paste(img, (current_x, paste_y))\n                    region_positions.append((info, current_x, paste_y))\n                    current_x += img.width + stack_gap\n\n            # Now draw guides on the unified canvas\n            # Draw vertical guides (blue) - these extend through the full canvas height\n            for v_coord in self.vertical:\n                # Find which region(s) this guide intersects\n                for info, paste_x, paste_y in region_positions:\n                    if info[\"pdf_x0\"] &lt;= v_coord &lt;= info[\"pdf_x1\"]:\n                        # This guide is within this region's x-bounds\n                        # Convert PDF coordinate to pixel coordinate relative to the region\n                        adjusted_x = v_coord - info[\"pdf_x0\"]\n                        pixel_x = adjusted_x * info[\"scale_x\"] + paste_x\n\n                        # Draw full-height line on canvas (not clipped to region)\n                        if 0 &lt;= pixel_x &lt;= final_width:\n                            x_pixel = int(pixel_x)\n                            draw.line(\n                                [(x_pixel, 0), (x_pixel, final_height - 1)],\n                                fill=(0, 0, 255, 200),\n                                width=2,\n                            )\n                        break  # Only draw once per guide\n\n            # Draw horizontal guides (red) - these extend through the full canvas width\n            for h_coord in self.horizontal:\n                # Find which region(s) this guide intersects\n                for info, paste_x, paste_y in region_positions:\n                    if info[\"pdf_top\"] &lt;= h_coord &lt;= info[\"pdf_bottom\"]:\n                        # This guide is within this region's y-bounds\n                        # Convert PDF coordinate to pixel coordinate relative to the region\n                        adjusted_y = h_coord - info[\"pdf_top\"]\n                        pixel_y = adjusted_y * info[\"scale_y\"] + paste_y\n\n                        # Draw full-width line on canvas (not clipped to region)\n                        if 0 &lt;= pixel_y &lt;= final_height:\n                            y_pixel = int(pixel_y)\n                            draw.line(\n                                [(0, y_pixel), (final_width - 1, y_pixel)],\n                                fill=(255, 0, 0, 200),\n                                width=2,\n                            )\n                        break  # Only draw once per guide\n\n            return canvas\n\n        # Original single-region logic follows...\n        # Determine what to display guides on\n        target = on if on is not None else self.context\n\n        # Handle string shortcuts\n        if isinstance(target, str):\n            if target == \"page\":\n                if hasattr(self.context, \"page\"):\n                    target = self.context.page\n                elif hasattr(self.context, \"_page\"):\n                    target = self.context._page\n                else:\n                    raise ValueError(\"Cannot resolve 'page' - context has no page attribute\")\n            else:\n                raise ValueError(f\"Unknown string target: {target}. Only 'page' is supported.\")\n\n        if target is None:\n            raise ValueError(\"No target specified and no context available for guides display\")\n\n        # Prepare kwargs for image generation\n        image_kwargs = {}\n\n        # Extract only the parameters that the new render() method accepts\n        if \"resolution\" in kwargs:\n            image_kwargs[\"resolution\"] = kwargs[\"resolution\"]\n        if \"width\" in kwargs:\n            image_kwargs[\"width\"] = kwargs[\"width\"]\n        if \"crop\" in kwargs:\n            image_kwargs[\"crop\"] = kwargs[\"crop\"]\n\n        # If target is a region-like object, crop to just that region\n        if hasattr(target, \"bbox\") and hasattr(target, \"page\"):\n            # This is likely a Region\n            image_kwargs[\"crop\"] = True\n\n        # Get base image\n        if hasattr(target, \"render\"):\n            # Use the new unified rendering system\n            img = target.render(**image_kwargs)\n        elif hasattr(target, \"render\"):\n            # Fallback to old method if available\n            img = target.render(**image_kwargs)\n        elif hasattr(target, \"mode\") and hasattr(target, \"size\"):\n            # It's already a PIL Image\n            img = target\n        else:\n            raise ValueError(f\"Object {target} does not support render() and is not a PIL Image\")\n\n        if img is None:\n            raise ValueError(\"Failed to generate base image\")\n\n        # Create a copy to draw on\n        img = img.copy()\n        draw = ImageDraw.Draw(img)\n\n        # Determine scale factor for coordinate conversion\n        if (\n            hasattr(target, \"width\")\n            and hasattr(target, \"height\")\n            and not (hasattr(target, \"mode\") and hasattr(target, \"size\"))\n        ):\n            # target is a PDF object (Page/Region) with PDF coordinates\n            scale_x = img.width / target.width\n            scale_y = img.height / target.height\n\n            # If we're showing guides on a region, we need to adjust coordinates\n            # to be relative to the region's origin\n            if hasattr(target, \"bbox\") and hasattr(target, \"page\"):\n                # This is a Region - adjust guide coordinates to be relative to region\n                region_x0, region_top = target.x0, target.top\n            else:\n                # This is a Page - no adjustment needed\n                region_x0, region_top = 0, 0\n        else:\n            # target is already an image, no scaling needed\n            scale_x = 1.0\n            scale_y = 1.0\n            region_x0, region_top = 0, 0\n\n        # Draw vertical guides (blue)\n        for x_coord in self.vertical:\n            # Adjust coordinate if we're showing on a region\n            adjusted_x = x_coord - region_x0\n            pixel_x = adjusted_x * scale_x\n            # Ensure guides at the edge are still visible by clamping to valid range\n            if 0 &lt;= pixel_x &lt;= img.width - 1:\n                x_pixel = int(min(pixel_x, img.width - 1))\n                draw.line([(x_pixel, 0), (x_pixel, img.height - 1)], fill=(0, 0, 255, 200), width=2)\n\n        # Draw horizontal guides (red)\n        for y_coord in self.horizontal:\n            # Adjust coordinate if we're showing on a region\n            adjusted_y = y_coord - region_top\n            pixel_y = adjusted_y * scale_y\n            # Ensure guides at the edge are still visible by clamping to valid range\n            if 0 &lt;= pixel_y &lt;= img.height - 1:\n                y_pixel = int(min(pixel_y, img.height - 1))\n                draw.line([(0, y_pixel), (img.width - 1, y_pixel)], fill=(255, 0, 0, 200), width=2)\n\n        return img\n\n    # -------------------------------------------------------------------------\n    # Utility Methods\n    # -------------------------------------------------------------------------\n\n    def get_cells(self) -&gt; List[Tuple[float, float, float, float]]:\n        \"\"\"\n        Get all cell bounding boxes from guide intersections.\n\n        Returns:\n            List of (x0, y0, x1, y1) tuples for each cell\n        \"\"\"\n        cells = []\n\n        # Create cells from guide intersections\n        for i in range(len(self.vertical) - 1):\n            for j in range(len(self.horizontal) - 1):\n                x0 = self.vertical[i]\n                x1 = self.vertical[i + 1]\n                y0 = self.horizontal[j]\n                y1 = self.horizontal[j + 1]\n                cells.append((x0, y0, x1, y1))\n\n        return cells\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Convert to dictionary format suitable for pdfplumber table_settings.\n\n        Returns:\n            Dictionary with explicit_vertical_lines and explicit_horizontal_lines\n        \"\"\"\n        return {\n            \"explicit_vertical_lines\": self.vertical,\n            \"explicit_horizontal_lines\": self.horizontal,\n        }\n\n    def to_relative(self) -&gt; \"Guides\":\n        \"\"\"\n        Convert absolute coordinates to relative (0-1) coordinates.\n\n        Returns:\n            New Guides object with relative coordinates\n        \"\"\"\n        if self.relative:\n            return self  # Already relative\n\n        if not self.bounds:\n            raise ValueError(\"Cannot convert to relative without bounds\")\n\n        x0, y0, x1, y1 = self.bounds\n        width = x1 - x0\n        height = y1 - y0\n\n        rel_verticals = [(x - x0) / width for x in self.vertical]\n        rel_horizontals = [(y - y0) / height for y in self.horizontal]\n\n        return Guides(\n            verticals=rel_verticals,\n            horizontals=rel_horizontals,\n            context=self.context,\n            bounds=(0, 0, 1, 1),\n            relative=True,\n        )\n\n    def to_absolute(self, bounds: Tuple[float, float, float, float]) -&gt; \"Guides\":\n        \"\"\"\n        Convert relative coordinates to absolute coordinates.\n\n        Args:\n            bounds: Target bounding box (x0, y0, x1, y1)\n\n        Returns:\n            New Guides object with absolute coordinates\n        \"\"\"\n        if not self.relative:\n            return self  # Already absolute\n\n        x0, y0, x1, y1 = bounds\n        width = x1 - x0\n        height = y1 - y0\n\n        abs_verticals = [x0 + x * width for x in self.vertical]\n        abs_horizontals = [y0 + y * height for y in self.horizontal]\n\n        return Guides(\n            verticals=abs_verticals,\n            horizontals=abs_horizontals,\n            context=self.context,\n            bounds=bounds,\n            relative=False,\n        )\n\n    @property\n    def n_rows(self) -&gt; int:\n        \"\"\"Number of rows defined by horizontal guides.\"\"\"\n        return max(0, len(self.horizontal) - 1)\n\n    @property\n    def n_cols(self) -&gt; int:\n        \"\"\"Number of columns defined by vertical guides.\"\"\"\n        return max(0, len(self.vertical) - 1)\n\n    def _handle_snap_failure(self, message: str):\n        \"\"\"Handle cases where snapping cannot be performed.\"\"\"\n        if hasattr(self, \"on_no_snap\"):\n            if self.on_no_snap == \"warn\":\n                logger.warning(message)\n            elif self.on_no_snap == \"raise\":\n                raise ValueError(message)\n            # 'ignore' case: do nothing\n        else:\n            logger.warning(message)  # Default behavior\n\n    def _find_vertical_whitespace_gaps(\n        self, text_elements, min_gap: float, threshold: Union[float, str] = \"auto\"\n    ) -&gt; List[Tuple[float, float]]:\n        \"\"\"\n        Find vertical whitespace gaps using bbox-based density analysis.\n        Returns list of (start, end) tuples representing trough ranges.\n        \"\"\"\n        if not self.bounds:\n            return []\n\n        x0, _, x1, _ = self.bounds\n        width_pixels = int(x1 - x0)\n\n        if width_pixels &lt;= 0:\n            return []\n\n        # Create density histogram: count bbox overlaps per x-coordinate\n        density = np.zeros(width_pixels)\n\n        for element in text_elements:\n            if not hasattr(element, \"x0\") or not hasattr(element, \"x1\"):\n                continue\n\n            # Clip coordinates to bounds\n            elem_x0 = max(x0, element.x0) - x0\n            elem_x1 = min(x1, element.x1) - x0\n\n            if elem_x1 &gt; elem_x0:\n                start_px = int(elem_x0)\n                end_px = int(elem_x1)\n                density[start_px:end_px] += 1\n\n        if density.max() == 0:\n            return []\n\n        # Determine the threshold value\n        if threshold == \"auto\":\n            # Auto mode: try different thresholds with step 0.05 until we have enough troughs\n            guides_needing_troughs = len(\n                [g for i, g in enumerate(self.vertical) if 0 &lt; i &lt; len(self.vertical) - 1]\n            )\n            if guides_needing_troughs == 0:\n                threshold_val = 0.5  # Default when no guides need placement\n            else:\n                threshold_val = None\n                for test_threshold in np.arange(0.1, 1.0, 0.05):\n                    test_gaps = self._find_gaps_with_threshold(density, test_threshold, min_gap, x0)\n                    if len(test_gaps) &gt;= guides_needing_troughs:\n                        threshold_val = test_threshold\n                        logger.debug(\n                            f\"Auto threshold found: {test_threshold:.2f} (found {len(test_gaps)} troughs for {guides_needing_troughs} guides)\"\n                        )\n                        break\n\n                if threshold_val is None:\n                    threshold_val = 0.8  # Fallback to permissive threshold\n                    logger.debug(f\"Auto threshold fallback to {threshold_val}\")\n        else:\n            # Fixed threshold mode\n            if not isinstance(threshold, (int, float)) or not (0.0 &lt;= threshold &lt;= 1.0):\n                raise ValueError(\"threshold must be a number between 0.0 and 1.0, or 'auto'\")\n            threshold_val = float(threshold)\n\n        return self._find_gaps_with_threshold(density, threshold_val, min_gap, x0)\n\n    def _find_gaps_with_threshold(self, density, threshold_val, min_gap, x0):\n        \"\"\"Helper method to find gaps given a specific threshold value.\"\"\"\n        max_density = density.max()\n        threshold_density = threshold_val * max_density\n\n        # Smooth the density for better trough detection\n        from scipy.ndimage import gaussian_filter1d\n\n        smoothed_density = gaussian_filter1d(density.astype(float), sigma=1.0)\n\n        # Find regions below threshold\n        below_threshold = smoothed_density &lt;= threshold_density\n\n        # Find contiguous regions\n        from scipy.ndimage import label as nd_label\n\n        labeled_regions, num_regions = nd_label(below_threshold)\n\n        gaps = []\n        for region_id in range(1, num_regions + 1):\n            region_mask = labeled_regions == region_id\n            region_indices = np.where(region_mask)[0]\n\n            if len(region_indices) == 0:\n                continue\n\n            start_px = region_indices[0]\n            end_px = region_indices[-1] + 1\n\n            # Convert back to PDF coordinates\n            start_pdf = x0 + start_px\n            end_pdf = x0 + end_px\n\n            # Check minimum gap size\n            if end_pdf - start_pdf &gt;= min_gap:\n                gaps.append((start_pdf, end_pdf))\n\n        return gaps\n\n    def _find_horizontal_whitespace_gaps(\n        self, text_elements, min_gap: float, threshold: Union[float, str] = \"auto\"\n    ) -&gt; List[Tuple[float, float]]:\n        \"\"\"\n        Find horizontal whitespace gaps using bbox-based density analysis.\n        Returns list of (start, end) tuples representing trough ranges.\n        \"\"\"\n        if not self.bounds:\n            return []\n\n        _, y0, _, y1 = self.bounds\n        height_pixels = int(y1 - y0)\n\n        if height_pixels &lt;= 0:\n            return []\n\n        # Create density histogram: count bbox overlaps per y-coordinate\n        density = np.zeros(height_pixels)\n\n        for element in text_elements:\n            if not hasattr(element, \"top\") or not hasattr(element, \"bottom\"):\n                continue\n\n            # Clip coordinates to bounds\n            elem_top = max(y0, element.top) - y0\n            elem_bottom = min(y1, element.bottom) - y0\n\n            if elem_bottom &gt; elem_top:\n                start_px = int(elem_top)\n                end_px = int(elem_bottom)\n                density[start_px:end_px] += 1\n\n        if density.max() == 0:\n            return []\n\n        # Determine the threshold value (same logic as vertical)\n        if threshold == \"auto\":\n            guides_needing_troughs = len(\n                [g for i, g in enumerate(self.horizontal) if 0 &lt; i &lt; len(self.horizontal) - 1]\n            )\n            if guides_needing_troughs == 0:\n                threshold_val = 0.5  # Default when no guides need placement\n            else:\n                threshold_val = None\n                for test_threshold in np.arange(0.1, 1.0, 0.05):\n                    test_gaps = self._find_gaps_with_threshold_horizontal(\n                        density, test_threshold, min_gap, y0\n                    )\n                    if len(test_gaps) &gt;= guides_needing_troughs:\n                        threshold_val = test_threshold\n                        logger.debug(\n                            f\"Auto threshold found: {test_threshold:.2f} (found {len(test_gaps)} troughs for {guides_needing_troughs} guides)\"\n                        )\n                        break\n\n                if threshold_val is None:\n                    threshold_val = 0.8  # Fallback to permissive threshold\n                    logger.debug(f\"Auto threshold fallback to {threshold_val}\")\n        else:\n            # Fixed threshold mode\n            if not isinstance(threshold, (int, float)) or not (0.0 &lt;= threshold &lt;= 1.0):\n                raise ValueError(\"threshold must be a number between 0.0 and 1.0, or 'auto'\")\n            threshold_val = float(threshold)\n\n        return self._find_gaps_with_threshold_horizontal(density, threshold_val, min_gap, y0)\n\n    def _find_gaps_with_threshold_horizontal(self, density, threshold_val, min_gap, y0):\n        \"\"\"Helper method to find horizontal gaps given a specific threshold value.\"\"\"\n        max_density = density.max()\n        threshold_density = threshold_val * max_density\n\n        # Smooth the density for better trough detection\n        from scipy.ndimage import gaussian_filter1d\n\n        smoothed_density = gaussian_filter1d(density.astype(float), sigma=1.0)\n\n        # Find regions below threshold\n        below_threshold = smoothed_density &lt;= threshold_density\n\n        # Find contiguous regions\n        from scipy.ndimage import label as nd_label\n\n        labeled_regions, num_regions = nd_label(below_threshold)\n\n        gaps = []\n        for region_id in range(1, num_regions + 1):\n            region_mask = labeled_regions == region_id\n            region_indices = np.where(region_mask)[0]\n\n            if len(region_indices) == 0:\n                continue\n\n            start_px = region_indices[0]\n            end_px = region_indices[-1] + 1\n\n            # Convert back to PDF coordinates\n            start_pdf = y0 + start_px\n            end_pdf = y0 + end_px\n\n            # Check minimum gap size\n            if end_pdf - start_pdf &gt;= min_gap:\n                gaps.append((start_pdf, end_pdf))\n\n        return gaps\n\n    def _find_vertical_element_gaps(\n        self, text_elements, min_gap: float\n    ) -&gt; List[Tuple[float, float]]:\n        \"\"\"\n        Find vertical whitespace gaps using text element spacing analysis.\n        Returns list of (start, end) tuples representing trough ranges.\n        \"\"\"\n        if not self.bounds or not text_elements:\n            return []\n\n        x0, _, x1, _ = self.bounds\n\n        # Get all element right and left edges\n        element_edges = []\n        for element in text_elements:\n            if not hasattr(element, \"x0\") or not hasattr(element, \"x1\"):\n                continue\n            # Only include elements that overlap vertically with our bounds\n            if hasattr(element, \"top\") and hasattr(element, \"bottom\"):\n                if element.bottom &lt; self.bounds[1] or element.top &gt; self.bounds[3]:\n                    continue\n            element_edges.extend([element.x0, element.x1])\n\n        if not element_edges:\n            return []\n\n        # Sort edges and find gaps\n        element_edges = sorted(set(element_edges))\n\n        trough_ranges = []\n        for i in range(len(element_edges) - 1):\n            gap_start = element_edges[i]\n            gap_end = element_edges[i + 1]\n            gap_width = gap_end - gap_start\n\n            if gap_width &gt;= min_gap:\n                # Check if this gap actually contains no text (is empty space)\n                gap_has_text = False\n                for element in text_elements:\n                    if (\n                        hasattr(element, \"x0\")\n                        and hasattr(element, \"x1\")\n                        and element.x0 &lt; gap_end\n                        and element.x1 &gt; gap_start\n                    ):\n                        gap_has_text = True\n                        break\n\n                if not gap_has_text:\n                    trough_ranges.append((gap_start, gap_end))\n\n        return trough_ranges\n\n    def _find_horizontal_element_gaps(\n        self, text_elements, min_gap: float\n    ) -&gt; List[Tuple[float, float]]:\n        \"\"\"\n        Find horizontal whitespace gaps using text element spacing analysis.\n        Returns list of (start, end) tuples representing trough ranges.\n        \"\"\"\n        if not self.bounds or not text_elements:\n            return []\n\n        _, y0, _, y1 = self.bounds\n\n        # Get all element top and bottom edges\n        element_edges = []\n        for element in text_elements:\n            if not hasattr(element, \"top\") or not hasattr(element, \"bottom\"):\n                continue\n            # Only include elements that overlap horizontally with our bounds\n            if hasattr(element, \"x0\") and hasattr(element, \"x1\"):\n                if element.x1 &lt; self.bounds[0] or element.x0 &gt; self.bounds[2]:\n                    continue\n            element_edges.extend([element.top, element.bottom])\n\n        if not element_edges:\n            return []\n\n        # Sort edges and find gaps\n        element_edges = sorted(set(element_edges))\n\n        trough_ranges = []\n        for i in range(len(element_edges) - 1):\n            gap_start = element_edges[i]\n            gap_end = element_edges[i + 1]\n            gap_width = gap_end - gap_start\n\n            if gap_width &gt;= min_gap:\n                # Check if this gap actually contains no text (is empty space)\n                gap_has_text = False\n                for element in text_elements:\n                    if (\n                        hasattr(element, \"top\")\n                        and hasattr(element, \"bottom\")\n                        and element.top &lt; gap_end\n                        and element.bottom &gt; gap_start\n                    ):\n                        gap_has_text = True\n                        break\n\n                if not gap_has_text:\n                    trough_ranges.append((gap_start, gap_end))\n\n        return trough_ranges\n\n    def _optimal_guide_assignment(\n        self, guides: List[float], trough_ranges: List[Tuple[float, float]]\n    ) -&gt; Dict[int, int]:\n        \"\"\"\n        Assign guides to trough ranges using the user's desired logic:\n        - Guides already in a trough stay put\n        - Only guides NOT in any trough get moved to available troughs\n        - Prefer closest assignment for guides that need to move\n        \"\"\"\n        if not guides or not trough_ranges:\n            return {}\n\n        assignments = {}\n\n        # Step 1: Identify which guides are already in troughs\n        guides_in_troughs = set()\n        for i, guide_pos in enumerate(guides):\n            for trough_start, trough_end in trough_ranges:\n                if trough_start &lt;= guide_pos &lt;= trough_end:\n                    guides_in_troughs.add(i)\n                    logger.debug(\n                        f\"Guide {i} (pos {guide_pos:.1f}) is already in trough ({trough_start:.1f}-{trough_end:.1f}), keeping in place\"\n                    )\n                    break\n\n        # Step 2: Identify which troughs are already occupied\n        occupied_troughs = set()\n        for i in guides_in_troughs:\n            guide_pos = guides[i]\n            for j, (trough_start, trough_end) in enumerate(trough_ranges):\n                if trough_start &lt;= guide_pos &lt;= trough_end:\n                    occupied_troughs.add(j)\n                    break\n\n        # Step 3: Find guides that need reassignment (not in any trough)\n        guides_to_move = []\n        for i, guide_pos in enumerate(guides):\n            if i not in guides_in_troughs:\n                guides_to_move.append(i)\n                logger.debug(\n                    f\"Guide {i} (pos {guide_pos:.1f}) is NOT in any trough, needs reassignment\"\n                )\n\n        # Step 4: Find available troughs (not occupied by existing guides)\n        available_troughs = []\n        for j, (trough_start, trough_end) in enumerate(trough_ranges):\n            if j not in occupied_troughs:\n                available_troughs.append(j)\n                logger.debug(f\"Trough {j} ({trough_start:.1f}-{trough_end:.1f}) is available\")\n\n        # Step 5: Assign guides to move to closest available troughs\n        if guides_to_move and available_troughs:\n            # Calculate distances for all combinations\n            distances = []\n            for guide_idx in guides_to_move:\n                guide_pos = guides[guide_idx]\n                for trough_idx in available_troughs:\n                    trough_start, trough_end = trough_ranges[trough_idx]\n                    trough_center = (trough_start + trough_end) / 2\n                    distance = abs(guide_pos - trough_center)\n                    distances.append((distance, guide_idx, trough_idx))\n\n            # Sort by distance and assign greedily\n            distances.sort()\n            used_troughs = set()\n\n            for distance, guide_idx, trough_idx in distances:\n                if guide_idx not in assignments and trough_idx not in used_troughs:\n                    assignments[guide_idx] = trough_idx\n                    used_troughs.add(trough_idx)\n                    logger.debug(\n                        f\"Assigned guide {guide_idx} (pos {guides[guide_idx]:.1f}) to trough {trough_idx} (distance: {distance:.1f})\"\n                    )\n\n        logger.debug(f\"Final assignments: {assignments}\")\n        return assignments\n\n    def _snap_guides_to_gaps(self, guides: List[float], gaps: List[Tuple[float, float]], axis: str):\n        \"\"\"\n        Snap guides to nearby gaps using optimal assignment.\n        Only moves guides that are NOT already in a trough.\n        \"\"\"\n        if not guides or not gaps:\n            return\n\n        logger.debug(f\"Snapping {len(guides)} {axis} guides to {len(gaps)} trough ranges\")\n        for i, (start, end) in enumerate(gaps):\n            center = (start + end) / 2\n            logger.debug(f\"  Trough {i}: {start:.1f} to {end:.1f} (center: {center:.1f})\")\n\n        # Get optimal assignments\n        assignments = self._optimal_guide_assignment(guides, gaps)\n\n        # Apply assignments (modify guides list in-place)\n        for guide_idx, trough_idx in assignments.items():\n            trough_start, trough_end = gaps[trough_idx]\n            new_pos = (trough_start + trough_end) / 2  # Move to trough center\n            old_pos = guides[guide_idx]\n            guides[guide_idx] = new_pos\n            logger.info(f\"Snapped {axis} guide from {old_pos:.1f} to {new_pos:.1f}\")\n\n    def build_grid(\n        self,\n        target: Optional[Union[\"Page\", \"Region\"]] = None,\n        source: str = \"guides\",\n        cell_padding: float = 0.5,\n        include_outer_boundaries: bool = False,\n        *,\n        multi_page: Literal[\"auto\", True, False] = \"auto\",\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Create table structure (table, rows, columns, cells) from guide coordinates.\n\n        Args:\n            target: Page or Region to create regions on (uses self.context if None)\n            source: Source label for created regions (for identification)\n            cell_padding: Internal padding for cell regions in points\n            include_outer_boundaries: Whether to add boundaries at edges if missing\n            multi_page: Controls multi-region table creation for FlowRegions.\n                - \"auto\": (default) Creates a unified grid if there are multiple regions or guides span pages.\n                - True: Forces creation of a unified multi-region grid.\n                - False: Creates separate grids for each region.\n\n        Returns:\n            Dictionary with 'counts' and 'regions' created.\n        \"\"\"\n        # Dispatch to appropriate implementation based on context and flags\n        if self.is_flow_region:\n            # Check if we should create a unified multi-region grid\n            has_multiple_regions = len(self.context.constituent_regions) &gt; 1\n            spans_pages = self._spans_pages()\n\n            # Create unified grid if:\n            # - multi_page is explicitly True, OR\n            # - multi_page is \"auto\" AND (spans pages OR has multiple regions)\n            if multi_page is True or (\n                multi_page == \"auto\" and (spans_pages or has_multiple_regions)\n            ):\n                return self._build_grid_multi_page(\n                    source=source,\n                    cell_padding=cell_padding,\n                    include_outer_boundaries=include_outer_boundaries,\n                )\n            else:\n                # Single region FlowRegion or multi_page=False: create separate tables per region\n                total_counts = {\"table\": 0, \"rows\": 0, \"columns\": 0, \"cells\": 0}\n                all_regions = {\"table\": [], \"rows\": [], \"columns\": [], \"cells\": []}\n\n                for region in self.context.constituent_regions:\n                    if region in self._flow_guides:\n                        verticals, horizontals = self._flow_guides[region]\n\n                        region_guides = Guides(\n                            verticals=verticals, horizontals=horizontals, context=region\n                        )\n\n                        try:\n                            result = region_guides._build_grid_single_page(\n                                target=region,\n                                source=source,\n                                cell_padding=cell_padding,\n                                include_outer_boundaries=include_outer_boundaries,\n                            )\n\n                            for key in total_counts:\n                                total_counts[key] += result[\"counts\"][key]\n\n                            if result[\"regions\"][\"table\"]:\n                                all_regions[\"table\"].append(result[\"regions\"][\"table\"])\n                            all_regions[\"rows\"].extend(result[\"regions\"][\"rows\"])\n                            all_regions[\"columns\"].extend(result[\"regions\"][\"columns\"])\n                            all_regions[\"cells\"].extend(result[\"regions\"][\"cells\"])\n\n                        except Exception as e:\n                            logger.warning(f\"Failed to build grid on region: {e}\")\n\n                logger.info(\n                    f\"Created {total_counts['table']} tables, {total_counts['rows']} rows, \"\n                    f\"{total_counts['columns']} columns, and {total_counts['cells']} cells \"\n                    f\"from guides across {len(self._flow_guides)} regions\"\n                )\n\n                return {\"counts\": total_counts, \"regions\": all_regions}\n\n        # Fallback for single page/region\n        return self._build_grid_single_page(\n            target=target,\n            source=source,\n            cell_padding=cell_padding,\n            include_outer_boundaries=include_outer_boundaries,\n        )\n\n    def _build_grid_multi_page(\n        self,\n        source: str,\n        cell_padding: float,\n        include_outer_boundaries: bool,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Builds a single, coherent grid across multiple regions of a FlowRegion.\n\n        Creates physical Region objects for each constituent region with _fragment\n        region types (e.g., table_column_fragment), then stitches them into logical\n        FlowRegion objects. Both are registered with pages, but the fragment types\n        allow easy differentiation:\n        - find_all('table_column') returns only logical columns\n        - find_all('table_column_fragment') returns only physical fragments\n        \"\"\"\n        from natural_pdf.flows.region import FlowRegion\n\n        if not self.is_flow_region or not hasattr(self.context, \"flow\") or not self.context.flow:\n            raise ValueError(\"Multi-page grid building requires a FlowRegion with a valid Flow.\")\n\n        # Determine flow orientation to guide stitching\n        orientation = self._get_flow_orientation()\n\n        # Phase 1: Build physical grid on each page, clipping guides to that page's region\n        results_by_region = []\n        unified_verticals = self.vertical.data\n        unified_horizontals = self.horizontal.data\n\n        for region in self.context.constituent_regions:\n            bounds = region.bbox\n            if not bounds:\n                continue\n\n            # Clip unified guides to the current region's bounds\n            clipped_verticals = [v for v in unified_verticals if bounds[0] &lt;= v &lt;= bounds[2]]\n            clipped_horizontals = [h for h in unified_horizontals if bounds[1] &lt;= h &lt;= bounds[3]]\n\n            # Ensure the region's own boundaries are included to close off cells at page breaks\n            clipped_verticals = sorted(list(set([bounds[0], bounds[2]] + clipped_verticals)))\n            clipped_horizontals = sorted(list(set([bounds[1], bounds[3]] + clipped_horizontals)))\n\n            if len(clipped_verticals) &lt; 2 or len(clipped_horizontals) &lt; 2:\n                continue  # Not enough guides to form a cell\n\n            region_guides = Guides(\n                verticals=clipped_verticals,\n                horizontals=clipped_horizontals,\n                context=region,\n            )\n\n            grid_parts = region_guides._build_grid_single_page(\n                target=region,\n                source=source,\n                cell_padding=cell_padding,\n                include_outer_boundaries=False,  # Boundaries are already handled\n            )\n\n            if grid_parts[\"counts\"][\"table\"] &gt; 0:\n                # Mark physical regions as fragments by updating their region_type\n                # This happens before stitching into logical FlowRegions\n                if len(self.context.constituent_regions) &gt; 1:\n                    # Update region types to indicate these are fragments\n                    if grid_parts[\"regions\"][\"table\"]:\n                        grid_parts[\"regions\"][\"table\"].region_type = \"table_fragment\"\n                        grid_parts[\"regions\"][\"table\"].metadata[\"is_fragment\"] = True\n\n                    for row in grid_parts[\"regions\"][\"rows\"]:\n                        row.region_type = \"table_row_fragment\"\n                        row.metadata[\"is_fragment\"] = True\n\n                    for col in grid_parts[\"regions\"][\"columns\"]:\n                        col.region_type = \"table_column_fragment\"\n                        col.metadata[\"is_fragment\"] = True\n\n                    for cell in grid_parts[\"regions\"][\"cells\"]:\n                        cell.region_type = \"table_cell_fragment\"\n                        cell.metadata[\"is_fragment\"] = True\n\n                results_by_region.append(grid_parts)\n\n        if not results_by_region:\n            return {\n                \"counts\": {\"table\": 0, \"rows\": 0, \"columns\": 0, \"cells\": 0},\n                \"regions\": {\"table\": None, \"rows\": [], \"columns\": [], \"cells\": []},\n            }\n\n        # Phase 2: Stitch physical regions into logical FlowRegions based on orientation\n        flow = self.context.flow\n\n        # The overall table is always a FlowRegion\n        physical_tables = [res[\"regions\"][\"table\"] for res in results_by_region]\n        multi_page_table = FlowRegion(\n            flow=flow, constituent_regions=physical_tables, source_flow_element=None\n        )\n        multi_page_table.source = source\n        multi_page_table.region_type = \"table\"\n        multi_page_table.metadata.update(\n            {\"is_multi_page\": True, \"num_rows\": self.n_rows, \"num_cols\": self.n_cols}\n        )\n\n        # Initialize final region collections\n        final_rows = []\n        final_cols = []\n        final_cells = []\n\n        orientation = self._get_flow_orientation()\n\n        if orientation == \"vertical\":\n            # Start with all rows &amp; cells from the first page's grid\n            if results_by_region:\n                # Make copies to modify\n                page_rows = [res[\"regions\"][\"rows\"] for res in results_by_region]\n                page_cells = [res[\"regions\"][\"cells\"] for res in results_by_region]\n\n                # Iterate through page breaks to merge split rows/cells\n                for i in range(len(results_by_region) - 1):\n                    region_A = self.context.constituent_regions[i]\n\n                    # Check if a guide exists at the boundary\n                    is_break_bounded = any(\n                        abs(h - region_A.bottom) &lt; 0.1 for h in self.horizontal.data\n                    )\n\n                    if not is_break_bounded and page_rows[i] and page_rows[i + 1]:\n                        # No guide at break -&gt; merge last row of A with first row of B\n                        last_row_A = page_rows[i].pop(-1)\n                        first_row_B = page_rows[i + 1].pop(0)\n\n                        merged_row = FlowRegion(\n                            flow, [last_row_A, first_row_B], source_flow_element=None\n                        )\n                        merged_row.source = source\n                        merged_row.region_type = \"table_row\"\n                        merged_row.metadata.update(\n                            {\n                                \"row_index\": last_row_A.metadata.get(\"row_index\"),\n                                \"is_multi_page\": True,\n                            }\n                        )\n                        page_rows[i].append(merged_row)  # Add merged row back in place of A's last\n\n                        # Merge the corresponding cells using explicit row/col indices\n                        last_row_idx = last_row_A.metadata.get(\"row_index\")\n                        first_row_idx = first_row_B.metadata.get(\"row_index\")\n\n                        # Cells belonging to those rows\n                        last_cells_A = [\n                            c for c in page_cells[i] if c.metadata.get(\"row_index\") == last_row_idx\n                        ]\n                        first_cells_B = [\n                            c\n                            for c in page_cells[i + 1]\n                            if c.metadata.get(\"row_index\") == first_row_idx\n                        ]\n\n                        # Remove them from their page lists\n                        page_cells[i] = [\n                            c for c in page_cells[i] if c.metadata.get(\"row_index\") != last_row_idx\n                        ]\n                        page_cells[i + 1] = [\n                            c\n                            for c in page_cells[i + 1]\n                            if c.metadata.get(\"row_index\") != first_row_idx\n                        ]\n\n                        # Sort both lists by column index to keep alignment stable\n                        last_cells_A.sort(key=lambda c: c.metadata.get(\"col_index\", 0))\n                        first_cells_B.sort(key=lambda c: c.metadata.get(\"col_index\", 0))\n\n                        # Pair-wise merge\n                        for cell_A, cell_B in zip(last_cells_A, first_cells_B):\n                            merged_cell = FlowRegion(\n                                flow, [cell_A, cell_B], source_flow_element=None\n                            )\n                            merged_cell.source = source\n                            merged_cell.region_type = \"table_cell\"\n                            merged_cell.metadata.update(\n                                {\n                                    \"row_index\": cell_A.metadata.get(\"row_index\"),\n                                    \"col_index\": cell_A.metadata.get(\"col_index\"),\n                                    \"is_multi_page\": True,\n                                }\n                            )\n                            page_cells[i].append(merged_cell)\n\n                # Flatten the potentially modified lists of rows and cells\n                final_rows = [row for rows_list in page_rows for row in rows_list]\n                final_cells = [cell for cells_list in page_cells for cell in cells_list]\n\n                # Stitch columns, which always span vertically\n                physical_cols_by_index = zip(\n                    *(res[\"regions\"][\"columns\"] for res in results_by_region)\n                )\n                for j, physical_cols in enumerate(physical_cols_by_index):\n                    col_fr = FlowRegion(\n                        flow=flow, constituent_regions=list(physical_cols), source_flow_element=None\n                    )\n                    col_fr.source = source\n                    col_fr.region_type = \"table_column\"\n                    col_fr.metadata.update({\"col_index\": j, \"is_multi_page\": True})\n                    final_cols.append(col_fr)\n\n        elif orientation == \"horizontal\":\n            # Symmetric logic for horizontal flow (not fully implemented here for brevity)\n            # This would merge last column of A with first column of B if no vertical guide exists\n            logger.warning(\"Horizontal table stitching not fully implemented.\")\n            final_rows = [row for res in results_by_region for row in res[\"regions\"][\"rows\"]]\n            final_cols = [col for res in results_by_region for col in res[\"regions\"][\"columns\"]]\n            final_cells = [cell for res in results_by_region for cell in res[\"regions\"][\"cells\"]]\n\n        else:  # Unknown orientation, just flatten everything\n            final_rows = [row for res in results_by_region for row in res[\"regions\"][\"rows\"]]\n            final_cols = [col for res in results_by_region for col in res[\"regions\"][\"columns\"]]\n            final_cells = [cell for res in results_by_region for cell in res[\"regions\"][\"cells\"]]\n\n        # SMART PAGE-LEVEL REGISTRY: Remove individual tables and replace with multi-page table\n        # This ensures that page.find('table') finds the logical multi-page table, not fragments\n        constituent_pages = set()\n        for region in self.context.constituent_regions:\n            if hasattr(region, \"page\") and hasattr(region.page, \"_element_mgr\"):\n                constituent_pages.add(region.page)\n\n        # Register the logical multi-page table with all constituent pages\n        # Note: Physical table fragments are already registered with region_type=\"table_fragment\"\n        for page in constituent_pages:\n            try:\n                page._element_mgr.add_element(multi_page_table, element_type=\"regions\")\n                logger.debug(f\"Registered multi-page table with page {page.page_number}\")\n\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to register multi-page table with page {page.page_number}: {e}\"\n                )\n\n        # SMART PAGE-LEVEL REGISTRY: Register logical FlowRegion elements.\n        # Physical fragments are already registered with their pages with _fragment region types,\n        # so users can differentiate between logical regions and physical fragments.\n        for page in constituent_pages:\n            try:\n                # Register all logical rows with this page\n                for row in final_rows:\n                    page._element_mgr.add_element(row, element_type=\"regions\")\n\n                # Register all logical columns with this page\n                for col in final_cols:\n                    page._element_mgr.add_element(col, element_type=\"regions\")\n\n                # Register all logical cells with this page\n                for cell in final_cells:\n                    page._element_mgr.add_element(cell, element_type=\"regions\")\n\n            except Exception as e:\n                logger.warning(f\"Failed to register multi-region table elements with page: {e}\")\n\n        final_counts = {\n            \"table\": 1,\n            \"rows\": len(final_rows),\n            \"columns\": len(final_cols),\n            \"cells\": len(final_cells),\n        }\n        final_regions = {\n            \"table\": multi_page_table,\n            \"rows\": final_rows,\n            \"columns\": final_cols,\n            \"cells\": final_cells,\n        }\n\n        logger.info(\n            f\"Created 1 multi-page table, {final_counts['rows']} logical rows, \"\n            f\"{final_counts['columns']} logical columns from guides and registered with all constituent pages\"\n        )\n\n        return {\"counts\": final_counts, \"regions\": final_regions}\n\n    def _build_grid_single_page(\n        self,\n        target: Optional[Union[\"Page\", \"Region\"]] = None,\n        source: str = \"guides\",\n        cell_padding: float = 0.5,\n        include_outer_boundaries: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Private method to create table structure on a single page or region.\n        (Refactored from the original public build_grid method).\n        \"\"\"\n        # This method now only handles a single page/region context.\n        # Looping for FlowRegions is handled by the public `build_grid` method.\n\n        # Original single-region logic follows...\n        target_obj = target or self.context\n        if not target_obj:\n            raise ValueError(\"No target object available. Provide target parameter or context.\")\n\n        # Get the page for creating regions\n        if hasattr(target_obj, \"x0\") and hasattr(\n            target_obj, \"top\"\n        ):  # Region (has bbox coordinates)\n            page = target_obj._page\n            origin_x, origin_y = target_obj.x0, target_obj.top\n            context_width, context_height = target_obj.width, target_obj.height\n        elif hasattr(target_obj, \"_element_mgr\") or hasattr(target_obj, \"width\"):  # Page\n            page = target_obj\n            origin_x, origin_y = 0.0, 0.0\n            context_width, context_height = page.width, page.height\n        else:\n            raise ValueError(f\"Target object {target_obj} is not a Page or Region\")\n\n        element_manager = page._element_mgr\n\n        # Setup boundaries\n        row_boundaries = list(self.horizontal)\n        col_boundaries = list(self.vertical)\n\n        # Add outer boundaries if requested and missing\n        if include_outer_boundaries:\n            if not row_boundaries or row_boundaries[0] &gt; origin_y:\n                row_boundaries.insert(0, origin_y)\n            if not row_boundaries or row_boundaries[-1] &lt; origin_y + context_height:\n                row_boundaries.append(origin_y + context_height)\n\n            if not col_boundaries or col_boundaries[0] &gt; origin_x:\n                col_boundaries.insert(0, origin_x)\n            if not col_boundaries or col_boundaries[-1] &lt; origin_x + context_width:\n                col_boundaries.append(origin_x + context_width)\n\n        # Remove duplicates and sort\n        row_boundaries = sorted(list(set(row_boundaries)))\n        col_boundaries = sorted(list(set(col_boundaries)))\n\n        # ------------------------------------------------------------------\n        # Clean-up: remove any previously created grid regions (table, rows,\n        # columns, cells) that were generated by the same `source` label and\n        # overlap the area we are about to populate.  This prevents the page's\n        # `ElementManager` from accumulating stale/duplicate regions when the\n        # user rebuilds the grid multiple times.\n        # ------------------------------------------------------------------\n        try:\n            # Bounding box of the grid we are about to create\n            if row_boundaries and col_boundaries:\n                grid_bbox = (\n                    col_boundaries[0],  # x0\n                    row_boundaries[0],  # top\n                    col_boundaries[-1],  # x1\n                    row_boundaries[-1],  # bottom\n                )\n\n                def _bbox_overlap(b1, b2):\n                    \"\"\"Return True if two (x0, top, x1, bottom) bboxes overlap.\"\"\"\n                    return not (\n                        b1[2] &lt;= b2[0]  # b1 right \u2264 b2 left\n                        or b1[0] &gt;= b2[2]  # b1 left \u2265 b2 right\n                        or b1[3] &lt;= b2[1]  # b1 bottom \u2264 b2 top\n                        or b1[1] &gt;= b2[3]  # b1 top \u2265 b2 bottom\n                    )\n\n                # Collect existing regions that match the source &amp; region types\n                regions_to_remove = [\n                    r\n                    for r in element_manager.regions\n                    if getattr(r, \"source\", None) == source\n                    and getattr(r, \"region_type\", None)\n                    in {\"table\", \"table_row\", \"table_column\", \"table_cell\"}\n                    and hasattr(r, \"bbox\")\n                    and _bbox_overlap(r.bbox, grid_bbox)\n                ]\n\n                for r in regions_to_remove:\n                    element_manager.remove_element(r, element_type=\"regions\")\n\n                if regions_to_remove:\n                    logger.debug(\n                        f\"Removed {len(regions_to_remove)} existing grid region(s) prior to rebuild\"\n                    )\n        except Exception as cleanup_err:  # pragma: no cover \u2013 cleanup must never crash\n            logger.warning(f\"Grid cleanup failed: {cleanup_err}\")\n\n        logger.debug(\n            f\"Building grid with {len(row_boundaries)} row and {len(col_boundaries)} col boundaries\"\n        )\n\n        # Track creation counts and regions\n        counts = {\"table\": 0, \"rows\": 0, \"columns\": 0, \"cells\": 0}\n        created_regions = {\"table\": None, \"rows\": [], \"columns\": [], \"cells\": []}\n\n        # Create overall table region\n        if len(row_boundaries) &gt;= 2 and len(col_boundaries) &gt;= 2:\n            table_region = page.create_region(\n                col_boundaries[0], row_boundaries[0], col_boundaries[-1], row_boundaries[-1]\n            )\n            table_region.source = source\n            table_region.region_type = \"table\"\n            table_region.normalized_type = \"table\"\n            table_region.metadata.update(\n                {\n                    \"source_guides\": True,\n                    \"num_rows\": len(row_boundaries) - 1,\n                    \"num_cols\": len(col_boundaries) - 1,\n                    \"boundaries\": {\"rows\": row_boundaries, \"cols\": col_boundaries},\n                }\n            )\n            element_manager.add_element(table_region, element_type=\"regions\")\n            counts[\"table\"] = 1\n            created_regions[\"table\"] = table_region\n\n        # Create row regions\n        if len(row_boundaries) &gt;= 2 and len(col_boundaries) &gt;= 2:\n            for i in range(len(row_boundaries) - 1):\n                row_region = page.create_region(\n                    col_boundaries[0], row_boundaries[i], col_boundaries[-1], row_boundaries[i + 1]\n                )\n                row_region.source = source\n                row_region.region_type = \"table_row\"\n                row_region.normalized_type = \"table_row\"\n                row_region.metadata.update({\"row_index\": i, \"source_guides\": True})\n                element_manager.add_element(row_region, element_type=\"regions\")\n                counts[\"rows\"] += 1\n                created_regions[\"rows\"].append(row_region)\n\n        # Create column regions\n        if len(col_boundaries) &gt;= 2 and len(row_boundaries) &gt;= 2:\n            for j in range(len(col_boundaries) - 1):\n                col_region = page.create_region(\n                    col_boundaries[j], row_boundaries[0], col_boundaries[j + 1], row_boundaries[-1]\n                )\n                col_region.source = source\n                col_region.region_type = \"table_column\"\n                col_region.normalized_type = \"table_column\"\n                col_region.metadata.update({\"col_index\": j, \"source_guides\": True})\n                element_manager.add_element(col_region, element_type=\"regions\")\n                counts[\"columns\"] += 1\n                created_regions[\"columns\"].append(col_region)\n\n        # Create cell regions\n        if len(row_boundaries) &gt;= 2 and len(col_boundaries) &gt;= 2:\n            for i in range(len(row_boundaries) - 1):\n                for j in range(len(col_boundaries) - 1):\n                    # Apply padding\n                    cell_x0 = col_boundaries[j] + cell_padding\n                    cell_top = row_boundaries[i] + cell_padding\n                    cell_x1 = col_boundaries[j + 1] - cell_padding\n                    cell_bottom = row_boundaries[i + 1] - cell_padding\n\n                    # Skip invalid cells\n                    if cell_x1 &lt;= cell_x0 or cell_bottom &lt;= cell_top:\n                        continue\n\n                    cell_region = page.create_region(cell_x0, cell_top, cell_x1, cell_bottom)\n                    cell_region.source = source\n                    cell_region.region_type = \"table_cell\"\n                    cell_region.normalized_type = \"table_cell\"\n                    cell_region.metadata.update(\n                        {\n                            \"row_index\": i,\n                            \"col_index\": j,\n                            \"source_guides\": True,\n                            \"original_boundaries\": {\n                                \"left\": col_boundaries[j],\n                                \"top\": row_boundaries[i],\n                                \"right\": col_boundaries[j + 1],\n                                \"bottom\": row_boundaries[i + 1],\n                            },\n                        }\n                    )\n                    element_manager.add_element(cell_region, element_type=\"regions\")\n                    counts[\"cells\"] += 1\n                    created_regions[\"cells\"].append(cell_region)\n\n        logger.info(\n            f\"Created {counts['table']} table, {counts['rows']} rows, \"\n            f\"{counts['columns']} columns, and {counts['cells']} cells from guides\"\n        )\n\n        return {\"counts\": counts, \"regions\": created_regions}\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the guides.\"\"\"\n        return (\n            f\"Guides(verticals={len(self.vertical)}, \"\n            f\"horizontals={len(self.horizontal)}, \"\n            f\"cells={len(self.get_cells())})\"\n        )\n\n    def _get_text_elements(self):\n        \"\"\"Get text elements from the context.\"\"\"\n        if not self.context:\n            return []\n\n        # Handle FlowRegion context\n        if self.is_flow_region:\n            all_text_elements = []\n            for region in self.context.constituent_regions:\n                if hasattr(region, \"find_all\"):\n                    try:\n                        text_elements = region.find_all(\"text\", apply_exclusions=False)\n                        elements = (\n                            text_elements.elements\n                            if hasattr(text_elements, \"elements\")\n                            else text_elements\n                        )\n                        all_text_elements.extend(elements)\n                    except Exception as e:\n                        logger.warning(f\"Error getting text elements from region: {e}\")\n            return all_text_elements\n\n        # Original single-region logic\n        # Get text elements from the context\n        if hasattr(self.context, \"find_all\"):\n            try:\n                text_elements = self.context.find_all(\"text\", apply_exclusions=False)\n                return (\n                    text_elements.elements if hasattr(text_elements, \"elements\") else text_elements\n                )\n            except Exception as e:\n                logger.warning(f\"Error getting text elements: {e}\")\n                return []\n        else:\n            logger.warning(\"Context does not support text element search\")\n            return []\n\n    def _spans_pages(self) -&gt; bool:\n        \"\"\"Check if any guides are defined across multiple pages in a FlowRegion.\"\"\"\n        if not self.is_flow_region:\n            return False\n\n        # Check vertical guides\n        v_guide_pages = {}\n        for coord, region in self._unified_vertical:\n            v_guide_pages.setdefault(coord, set()).add(region.page.page_number)\n\n        for pages in v_guide_pages.values():\n            if len(pages) &gt; 1:\n                return True\n\n        # Check horizontal guides\n        h_guide_pages = {}\n        for coord, region in self._unified_horizontal:\n            h_guide_pages.setdefault(coord, set()).add(region.page.page_number)\n\n        for pages in h_guide_pages.values():\n            if len(pages) &gt; 1:\n                return True\n\n        return False\n\n    # -------------------------------------------------------------------------\n    # Instance methods for fluent chaining (avoid name conflicts with class methods)\n    # -------------------------------------------------------------------------\n\n    def add_content(\n        self,\n        axis: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n        markers: Union[str, List[str], \"ElementCollection\", None] = None,\n        obj: Optional[Union[\"Page\", \"Region\"]] = None,\n        align: Literal[\"left\", \"right\", \"center\", \"between\"] = \"left\",\n        outer: bool = True,\n        tolerance: float = 5,\n        apply_exclusions: bool = True,\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Instance method: Add guides from content, allowing chaining.\n        This allows: Guides.new(page).add_content(axis='vertical', markers=[...])\n\n        Args:\n            axis: Which axis to create guides for\n            markers: Content to search for. Can be:\n                - str: single selector or literal text\n                - List[str]: list of selectors or literal text strings\n                - ElementCollection: collection of elements to extract text from\n                - None: no markers\n            obj: Page or Region to search (uses self.context if None)\n            align: How to align guides relative to found elements\n            outer: Whether to add outer boundary guides\n            tolerance: Tolerance for snapping to element edges\n            apply_exclusions: Whether to apply exclusion zones when searching for text\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Use provided object or fall back to stored context\n        target_obj = obj or self.context\n        if target_obj is None:\n            raise ValueError(\"No object provided and no context available\")\n\n        # Create new guides using the class method\n        new_guides = Guides.from_content(\n            obj=target_obj,\n            axis=axis,\n            markers=markers,\n            align=align,\n            outer=outer,\n            tolerance=tolerance,\n            apply_exclusions=apply_exclusions,\n        )\n\n        # Add the appropriate coordinates to this object\n        if axis == \"vertical\":\n            self.vertical = list(set(self.vertical + new_guides.vertical))\n        else:\n            self.horizontal = list(set(self.horizontal + new_guides.horizontal))\n\n        return self\n\n    def add_lines(\n        self,\n        axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n        obj: Optional[Union[\"Page\", \"Region\"]] = None,\n        threshold: Union[float, str] = \"auto\",\n        source_label: Optional[str] = None,\n        max_lines_h: Optional[int] = None,\n        max_lines_v: Optional[int] = None,\n        outer: bool = False,\n        detection_method: str = \"vector\",\n        resolution: int = 192,\n        **detect_kwargs,\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Instance method: Add guides from lines, allowing chaining.\n        This allows: Guides.new(page).add_lines(axis='horizontal')\n\n        Args:\n            axis: Which axis to detect lines for\n            obj: Page or Region to search (uses self.context if None)\n            threshold: Line detection threshold ('auto' or float 0.0-1.0)\n            source_label: Filter lines by source label (vector) or label for detected lines (pixels)\n            max_lines_h: Maximum horizontal lines to use\n            max_lines_v: Maximum vertical lines to use\n            outer: Whether to add outer boundary guides\n            detection_method: 'vector' (use existing LineElements) or 'pixels' (detect from image)\n            resolution: DPI for pixel-based detection (default: 192)\n            **detect_kwargs: Additional parameters for pixel detection (see from_lines)\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Use provided object or fall back to stored context\n        target_obj = obj or self.context\n        if target_obj is None:\n            raise ValueError(\"No object provided and no context available\")\n\n        # Create new guides using the class method\n        new_guides = Guides.from_lines(\n            obj=target_obj,\n            axis=axis,\n            threshold=threshold,\n            source_label=source_label,\n            max_lines_h=max_lines_h,\n            max_lines_v=max_lines_v,\n            outer=outer,\n            detection_method=detection_method,\n            resolution=resolution,\n            **detect_kwargs,\n        )\n\n        # Add the appropriate coordinates to this object\n        if axis in (\"vertical\", \"both\"):\n            self.vertical = list(set(self.vertical + new_guides.vertical))\n        if axis in (\"horizontal\", \"both\"):\n            self.horizontal = list(set(self.horizontal + new_guides.horizontal))\n\n        return self\n\n    def add_whitespace(\n        self,\n        axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n        obj: Optional[Union[\"Page\", \"Region\"]] = None,\n        min_gap: float = 10,\n    ) -&gt; \"Guides\":\n        \"\"\"\n        Instance method: Add guides from whitespace, allowing chaining.\n        This allows: Guides.new(page).add_whitespace(axis='both')\n\n        Args:\n            axis: Which axis to create guides for\n            obj: Page or Region to search (uses self.context if None)\n            min_gap: Minimum gap size to consider\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Use provided object or fall back to stored context\n        target_obj = obj or self.context\n        if target_obj is None:\n            raise ValueError(\"No object provided and no context available\")\n\n        # Create new guides using the class method\n        new_guides = Guides.from_whitespace(obj=target_obj, axis=axis, min_gap=min_gap)\n\n        # Add the appropriate coordinates to this object\n        if axis in (\"vertical\", \"both\"):\n            self.vertical = list(set(self.vertical + new_guides.vertical))\n        if axis in (\"horizontal\", \"both\"):\n            self.horizontal = list(set(self.horizontal + new_guides.horizontal))\n\n        return self\n\n    def extract_table(\n        self,\n        target: Optional[Union[\"Page\", \"Region\"]] = None,\n        source: str = \"guides_temp\",\n        cell_padding: float = 0.5,\n        include_outer_boundaries: bool = False,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        use_ocr: bool = False,\n        ocr_config: Optional[dict] = None,\n        text_options: Optional[Dict] = None,\n        cell_extraction_func: Optional[Callable[[\"Region\"], Optional[str]]] = None,\n        show_progress: bool = False,\n        content_filter: Optional[Union[str, Callable[[str], bool], List[str]]] = None,\n        *,\n        multi_page: Literal[\"auto\", True, False] = \"auto\",\n    ) -&gt; \"TableResult\":\n        \"\"\"\n        Extract table data directly from guides without leaving temporary regions.\n\n        This method:\n        1. Creates table structure using build_grid()\n        2. Extracts table data from the created table region\n        3. Cleans up all temporary regions\n        4. Returns the TableResult\n\n        Args:\n            target: Page or Region to create regions on (uses self.context if None)\n            source: Source label for temporary regions (will be cleaned up)\n            cell_padding: Internal padding for cell regions in points\n            include_outer_boundaries: Whether to add boundaries at edges if missing\n            method: Table extraction method ('tatr', 'pdfplumber', 'text', etc.)\n            table_settings: Settings for pdfplumber table extraction\n            use_ocr: Whether to use OCR for text extraction\n            ocr_config: OCR configuration parameters\n            text_options: Dictionary of options for the 'text' method\n            cell_extraction_func: Optional callable for custom cell text extraction\n            show_progress: Controls progress bar for text method\n            content_filter: Content filtering function or patterns\n            multi_page: Controls multi-region table creation for FlowRegions\n\n        Returns:\n            TableResult: Extracted table data\n\n        Raises:\n            ValueError: If no table region is created from the guides\n\n        Example:\n            ```python\n            from natural_pdf.analyzers import Guides\n\n            # Create guides from detected lines\n            guides = Guides.from_lines(page, source_label=\"detected\")\n\n            # Extract table directly - no temporary regions left behind\n            table_data = guides.extract_table()\n\n            # Convert to pandas DataFrame\n            df = table_data.to_df()\n            ```\n        \"\"\"\n        target_obj = target or self.context\n        if not target_obj:\n            raise ValueError(\"No target object available. Provide target parameter or context.\")\n\n        # Get the page for cleanup later\n        if hasattr(target_obj, \"x0\") and hasattr(target_obj, \"top\"):  # Region\n            page = target_obj._page\n            element_manager = page._element_mgr\n        elif hasattr(target_obj, \"_element_mgr\"):  # Page\n            page = target_obj\n            element_manager = page._element_mgr\n        else:\n            raise ValueError(f\"Target object {target_obj} is not a Page or Region\")\n\n        try:\n            # Step 1: Build grid structure (creates temporary regions)\n            grid_result = self.build_grid(\n                target=target_obj,\n                source=source,\n                cell_padding=cell_padding,\n                include_outer_boundaries=include_outer_boundaries,\n                multi_page=multi_page,\n            )\n\n            # Step 2: Get the table region and extract table data\n            table_region = grid_result[\"regions\"][\"table\"]\n            if table_region is None:\n                raise ValueError(\n                    \"No table region was created from the guides. Check that you have both vertical and horizontal guides.\"\n                )\n\n            # Handle multi-page case where table_region might be a list\n            if isinstance(table_region, list):\n                if not table_region:\n                    raise ValueError(\"No table regions were created from the guides.\")\n                # Use the first table region for extraction\n                table_region = table_region[0]\n\n            # Step 3: Extract table data using the region's extract_table method\n            table_result = table_region.extract_table(\n                method=method,\n                table_settings=table_settings,\n                use_ocr=use_ocr,\n                ocr_config=ocr_config,\n                text_options=text_options,\n                cell_extraction_func=cell_extraction_func,\n                show_progress=show_progress,\n                content_filter=content_filter,\n            )\n\n            return table_result\n\n        finally:\n            # Step 4: Clean up all temporary regions created by build_grid\n            # This ensures no regions are left behind regardless of success/failure\n            try:\n                regions_to_remove = [\n                    r\n                    for r in element_manager.regions\n                    if getattr(r, \"source\", None) == source\n                    and getattr(r, \"region_type\", None)\n                    in {\"table\", \"table_row\", \"table_column\", \"table_cell\"}\n                ]\n\n                for region in regions_to_remove:\n                    element_manager.remove_element(region, element_type=\"regions\")\n\n                if regions_to_remove:\n                    logger.debug(f\"Cleaned up {len(regions_to_remove)} temporary regions\")\n\n            except Exception as cleanup_err:\n                logger.warning(f\"Failed to clean up temporary regions: {cleanup_err}\")\n\n    def _get_flow_orientation(self) -&gt; Literal[\"vertical\", \"horizontal\", \"unknown\"]:\n        \"\"\"Determines if a FlowRegion's constituent parts are arranged vertically or horizontally.\"\"\"\n        if not self.is_flow_region or len(self.context.constituent_regions) &lt; 2:\n            return \"unknown\"\n\n        r1 = self.context.constituent_regions[0]\n        r2 = self.context.constituent_regions[1]  # Compare first two regions\n\n        if not r1.bbox or not r2.bbox:\n            return \"unknown\"\n\n        # Calculate non-overlapping distances.\n        # This determines the primary direction of separation.\n        x_dist = max(0, max(r1.x0, r2.x0) - min(r1.x1, r2.x1))\n        y_dist = max(0, max(r1.top, r2.top) - min(r1.bottom, r2.bottom))\n\n        if y_dist &gt; x_dist:\n            return \"vertical\"\n        else:\n            return \"horizontal\"\n</code></pre>"},{"location":"api/#natural_pdf.Guides-attributes","title":"Attributes","text":"<code>natural_pdf.Guides.horizontal</code> <code>property</code> <code>writable</code> <p>Get horizontal guide coordinates.</p> <code>natural_pdf.Guides.n_cols</code> <code>property</code> <p>Number of columns defined by vertical guides.</p> <code>natural_pdf.Guides.n_rows</code> <code>property</code> <p>Number of rows defined by horizontal guides.</p> <code>natural_pdf.Guides.vertical</code> <code>property</code> <code>writable</code> <p>Get vertical guide coordinates.</p>"},{"location":"api/#natural_pdf.Guides-functions","title":"Functions","text":"<code>natural_pdf.Guides.__add__(other)</code> <p>Combine two guide sets.</p> <p>Returns:</p> Type Description <code>Guides</code> <p>New Guides object with combined coordinates</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def __add__(self, other: \"Guides\") -&gt; \"Guides\":\n    \"\"\"\n    Combine two guide sets.\n\n    Returns:\n        New Guides object with combined coordinates\n    \"\"\"\n    # Combine and deduplicate coordinates, ensuring Python floats\n    combined_verticals = sorted([float(x) for x in set(self.vertical + other.vertical)])\n    combined_horizontals = sorted([float(y) for y in set(self.horizontal + other.horizontal)])\n\n    # Handle FlowRegion context merging\n    new_context = self.context or other.context\n\n    # If both are flow regions, we might need a more complex merge,\n    # but for now, just picking one context is sufficient.\n\n    # Create the new Guides object\n    new_guides = Guides(\n        verticals=combined_verticals,\n        horizontals=combined_horizontals,\n        context=new_context,\n        bounds=self.bounds or other.bounds,\n    )\n\n    # If the new context is a FlowRegion, we need to rebuild the flow-related state\n    if new_guides.is_flow_region:\n        # Re-initialize flow guides from both sources\n        # This is a simplification; a true merge would be more complex.\n        # For now, we combine the flow_guides dictionaries.\n        if hasattr(self, \"_flow_guides\"):\n            new_guides._flow_guides.update(self._flow_guides)\n        if hasattr(other, \"_flow_guides\"):\n            new_guides._flow_guides.update(other._flow_guides)\n\n        # Re-initialize unified views\n        if hasattr(self, \"_unified_vertical\"):\n            new_guides._unified_vertical.extend(self._unified_vertical)\n        if hasattr(other, \"_unified_vertical\"):\n            new_guides._unified_vertical.extend(other._unified_vertical)\n\n        if hasattr(self, \"_unified_horizontal\"):\n            new_guides._unified_horizontal.extend(self._unified_horizontal)\n        if hasattr(other, \"_unified_horizontal\"):\n            new_guides._unified_horizontal.extend(other._unified_horizontal)\n\n        # Invalidate caches to force rebuild\n        new_guides._vertical_cache = None\n        new_guides._horizontal_cache = None\n\n    return new_guides\n</code></pre> <code>natural_pdf.Guides.__init__(verticals=None, horizontals=None, context=None, bounds=None, relative=False, snap_behavior='warn')</code> <p>Initialize a Guides object.</p> <p>Parameters:</p> Name Type Description Default <code>verticals</code> <code>Optional[Union[List[float], Page, Region, FlowRegion]]</code> <p>List of x-coordinates for vertical guides, or a Page/Region/FlowRegion as context</p> <code>None</code> <code>horizontals</code> <code>Optional[List[float]]</code> <p>List of y-coordinates for horizontal guides</p> <code>None</code> <code>context</code> <code>Optional[Union[Page, Region, FlowRegion]]</code> <p>Page, Region, or FlowRegion object these guides were created from</p> <code>None</code> <code>bounds</code> <code>Optional[Tuple[float, float, float, float]]</code> <p>Bounding box (x0, top, x1, bottom) if context not provided</p> <code>None</code> <code>relative</code> <code>bool</code> <p>Whether coordinates are relative (0-1) or absolute</p> <code>False</code> <code>snap_behavior</code> <code>Literal['raise', 'warn', 'ignore']</code> <p>How to handle snapping conflicts ('raise', 'warn', or 'ignore')</p> <code>'warn'</code> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def __init__(\n    self,\n    verticals: Optional[Union[List[float], \"Page\", \"Region\", \"FlowRegion\"]] = None,\n    horizontals: Optional[List[float]] = None,\n    context: Optional[Union[\"Page\", \"Region\", \"FlowRegion\"]] = None,\n    bounds: Optional[Tuple[float, float, float, float]] = None,\n    relative: bool = False,\n    snap_behavior: Literal[\"raise\", \"warn\", \"ignore\"] = \"warn\",\n):\n    \"\"\"\n    Initialize a Guides object.\n\n    Args:\n        verticals: List of x-coordinates for vertical guides, or a Page/Region/FlowRegion as context\n        horizontals: List of y-coordinates for horizontal guides\n        context: Page, Region, or FlowRegion object these guides were created from\n        bounds: Bounding box (x0, top, x1, bottom) if context not provided\n        relative: Whether coordinates are relative (0-1) or absolute\n        snap_behavior: How to handle snapping conflicts ('raise', 'warn', or 'ignore')\n    \"\"\"\n    # Handle Guides(page) or Guides(flow_region) shorthand\n    if (\n        verticals is not None\n        and not isinstance(verticals, (list, tuple))\n        and horizontals is None\n        and context is None\n    ):\n        # First argument is a page/region/flow_region, not coordinates\n        context = verticals\n        verticals = None\n\n    self.context = context\n    self.bounds = bounds\n    self.relative = relative\n    self.snap_behavior = snap_behavior\n\n    # Check if we're dealing with a FlowRegion\n    self.is_flow_region = hasattr(context, \"constituent_regions\")\n\n    # If FlowRegion, we'll store guides per constituent region\n    if self.is_flow_region:\n        self._flow_guides: Dict[\"Region\", Tuple[List[float], List[float]]] = {}\n        # For unified view across all regions\n        self._unified_vertical: List[Tuple[float, \"Region\"]] = []\n        self._unified_horizontal: List[Tuple[float, \"Region\"]] = []\n        # Cache for sorted unique coordinates\n        self._vertical_cache: Optional[List[float]] = None\n        self._horizontal_cache: Optional[List[float]] = None\n\n    # Initialize with GuidesList instances\n    self._vertical = GuidesList(self, \"vertical\", sorted([float(x) for x in (verticals or [])]))\n    self._horizontal = GuidesList(\n        self, \"horizontal\", sorted([float(y) for y in (horizontals or [])])\n    )\n\n    # Determine bounds from context if needed\n    if self.bounds is None and self.context is not None:\n        if hasattr(self.context, \"bbox\"):\n            self.bounds = self.context.bbox\n        elif hasattr(self.context, \"x0\"):\n            self.bounds = (\n                self.context.x0,\n                self.context.top,\n                self.context.x1,\n                self.context.bottom,\n            )\n\n    # Convert relative to absolute if needed\n    if self.relative and self.bounds:\n        x0, top, x1, bottom = self.bounds\n        width = x1 - x0\n        height = bottom - top\n\n        self._vertical.data = [x0 + v * width for v in self._vertical]\n        self._horizontal.data = [top + h * height for h in self._horizontal]\n        self.relative = False\n</code></pre> <code>natural_pdf.Guides.__repr__()</code> <p>String representation of the guides.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the guides.\"\"\"\n    return (\n        f\"Guides(verticals={len(self.vertical)}, \"\n        f\"horizontals={len(self.horizontal)}, \"\n        f\"cells={len(self.get_cells())})\"\n    )\n</code></pre> <code>natural_pdf.Guides.add_content(axis='vertical', markers=None, obj=None, align='left', outer=True, tolerance=5, apply_exclusions=True)</code> <p>Instance method: Add guides from content, allowing chaining. This allows: Guides.new(page).add_content(axis='vertical', markers=[...])</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Literal['vertical', 'horizontal']</code> <p>Which axis to create guides for</p> <code>'vertical'</code> <code>markers</code> <code>Union[str, List[str], ElementCollection, None]</code> <p>Content to search for. Can be: - str: single selector or literal text - List[str]: list of selectors or literal text strings - ElementCollection: collection of elements to extract text from - None: no markers</p> <code>None</code> <code>obj</code> <code>Optional[Union[Page, Region]]</code> <p>Page or Region to search (uses self.context if None)</p> <code>None</code> <code>align</code> <code>Literal['left', 'right', 'center', 'between']</code> <p>How to align guides relative to found elements</p> <code>'left'</code> <code>outer</code> <code>bool</code> <p>Whether to add outer boundary guides</p> <code>True</code> <code>tolerance</code> <code>float</code> <p>Tolerance for snapping to element edges</p> <code>5</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to apply exclusion zones when searching for text</p> <code>True</code> <p>Returns:</p> Type Description <code>Guides</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def add_content(\n    self,\n    axis: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n    markers: Union[str, List[str], \"ElementCollection\", None] = None,\n    obj: Optional[Union[\"Page\", \"Region\"]] = None,\n    align: Literal[\"left\", \"right\", \"center\", \"between\"] = \"left\",\n    outer: bool = True,\n    tolerance: float = 5,\n    apply_exclusions: bool = True,\n) -&gt; \"Guides\":\n    \"\"\"\n    Instance method: Add guides from content, allowing chaining.\n    This allows: Guides.new(page).add_content(axis='vertical', markers=[...])\n\n    Args:\n        axis: Which axis to create guides for\n        markers: Content to search for. Can be:\n            - str: single selector or literal text\n            - List[str]: list of selectors or literal text strings\n            - ElementCollection: collection of elements to extract text from\n            - None: no markers\n        obj: Page or Region to search (uses self.context if None)\n        align: How to align guides relative to found elements\n        outer: Whether to add outer boundary guides\n        tolerance: Tolerance for snapping to element edges\n        apply_exclusions: Whether to apply exclusion zones when searching for text\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Use provided object or fall back to stored context\n    target_obj = obj or self.context\n    if target_obj is None:\n        raise ValueError(\"No object provided and no context available\")\n\n    # Create new guides using the class method\n    new_guides = Guides.from_content(\n        obj=target_obj,\n        axis=axis,\n        markers=markers,\n        align=align,\n        outer=outer,\n        tolerance=tolerance,\n        apply_exclusions=apply_exclusions,\n    )\n\n    # Add the appropriate coordinates to this object\n    if axis == \"vertical\":\n        self.vertical = list(set(self.vertical + new_guides.vertical))\n    else:\n        self.horizontal = list(set(self.horizontal + new_guides.horizontal))\n\n    return self\n</code></pre> <code>natural_pdf.Guides.add_horizontal(y)</code> <p>Add a horizontal guide at the specified y-coordinate.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def add_horizontal(self, y: float) -&gt; \"Guides\":\n    \"\"\"Add a horizontal guide at the specified y-coordinate.\"\"\"\n    self.horizontal.append(y)\n    self.horizontal = sorted(self.horizontal)\n    return self\n</code></pre> <code>natural_pdf.Guides.add_lines(axis='both', obj=None, threshold='auto', source_label=None, max_lines_h=None, max_lines_v=None, outer=False, detection_method='vector', resolution=192, **detect_kwargs)</code> <p>Instance method: Add guides from lines, allowing chaining. This allows: Guides.new(page).add_lines(axis='horizontal')</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Literal['vertical', 'horizontal', 'both']</code> <p>Which axis to detect lines for</p> <code>'both'</code> <code>obj</code> <code>Optional[Union[Page, Region]]</code> <p>Page or Region to search (uses self.context if None)</p> <code>None</code> <code>threshold</code> <code>Union[float, str]</code> <p>Line detection threshold ('auto' or float 0.0-1.0)</p> <code>'auto'</code> <code>source_label</code> <code>Optional[str]</code> <p>Filter lines by source label (vector) or label for detected lines (pixels)</p> <code>None</code> <code>max_lines_h</code> <code>Optional[int]</code> <p>Maximum horizontal lines to use</p> <code>None</code> <code>max_lines_v</code> <code>Optional[int]</code> <p>Maximum vertical lines to use</p> <code>None</code> <code>outer</code> <code>bool</code> <p>Whether to add outer boundary guides</p> <code>False</code> <code>detection_method</code> <code>str</code> <p>'vector' (use existing LineElements) or 'pixels' (detect from image)</p> <code>'vector'</code> <code>resolution</code> <code>int</code> <p>DPI for pixel-based detection (default: 192)</p> <code>192</code> <code>**detect_kwargs</code> <p>Additional parameters for pixel detection (see from_lines)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Guides</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def add_lines(\n    self,\n    axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n    obj: Optional[Union[\"Page\", \"Region\"]] = None,\n    threshold: Union[float, str] = \"auto\",\n    source_label: Optional[str] = None,\n    max_lines_h: Optional[int] = None,\n    max_lines_v: Optional[int] = None,\n    outer: bool = False,\n    detection_method: str = \"vector\",\n    resolution: int = 192,\n    **detect_kwargs,\n) -&gt; \"Guides\":\n    \"\"\"\n    Instance method: Add guides from lines, allowing chaining.\n    This allows: Guides.new(page).add_lines(axis='horizontal')\n\n    Args:\n        axis: Which axis to detect lines for\n        obj: Page or Region to search (uses self.context if None)\n        threshold: Line detection threshold ('auto' or float 0.0-1.0)\n        source_label: Filter lines by source label (vector) or label for detected lines (pixels)\n        max_lines_h: Maximum horizontal lines to use\n        max_lines_v: Maximum vertical lines to use\n        outer: Whether to add outer boundary guides\n        detection_method: 'vector' (use existing LineElements) or 'pixels' (detect from image)\n        resolution: DPI for pixel-based detection (default: 192)\n        **detect_kwargs: Additional parameters for pixel detection (see from_lines)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Use provided object or fall back to stored context\n    target_obj = obj or self.context\n    if target_obj is None:\n        raise ValueError(\"No object provided and no context available\")\n\n    # Create new guides using the class method\n    new_guides = Guides.from_lines(\n        obj=target_obj,\n        axis=axis,\n        threshold=threshold,\n        source_label=source_label,\n        max_lines_h=max_lines_h,\n        max_lines_v=max_lines_v,\n        outer=outer,\n        detection_method=detection_method,\n        resolution=resolution,\n        **detect_kwargs,\n    )\n\n    # Add the appropriate coordinates to this object\n    if axis in (\"vertical\", \"both\"):\n        self.vertical = list(set(self.vertical + new_guides.vertical))\n    if axis in (\"horizontal\", \"both\"):\n        self.horizontal = list(set(self.horizontal + new_guides.horizontal))\n\n    return self\n</code></pre> <code>natural_pdf.Guides.add_vertical(x)</code> <p>Add a vertical guide at the specified x-coordinate.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def add_vertical(self, x: float) -&gt; \"Guides\":\n    \"\"\"Add a vertical guide at the specified x-coordinate.\"\"\"\n    self.vertical.append(x)\n    self.vertical = sorted(self.vertical)\n    return self\n</code></pre> <code>natural_pdf.Guides.add_whitespace(axis='both', obj=None, min_gap=10)</code> <p>Instance method: Add guides from whitespace, allowing chaining. This allows: Guides.new(page).add_whitespace(axis='both')</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Literal['vertical', 'horizontal', 'both']</code> <p>Which axis to create guides for</p> <code>'both'</code> <code>obj</code> <code>Optional[Union[Page, Region]]</code> <p>Page or Region to search (uses self.context if None)</p> <code>None</code> <code>min_gap</code> <code>float</code> <p>Minimum gap size to consider</p> <code>10</code> <p>Returns:</p> Type Description <code>Guides</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def add_whitespace(\n    self,\n    axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n    obj: Optional[Union[\"Page\", \"Region\"]] = None,\n    min_gap: float = 10,\n) -&gt; \"Guides\":\n    \"\"\"\n    Instance method: Add guides from whitespace, allowing chaining.\n    This allows: Guides.new(page).add_whitespace(axis='both')\n\n    Args:\n        axis: Which axis to create guides for\n        obj: Page or Region to search (uses self.context if None)\n        min_gap: Minimum gap size to consider\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Use provided object or fall back to stored context\n    target_obj = obj or self.context\n    if target_obj is None:\n        raise ValueError(\"No object provided and no context available\")\n\n    # Create new guides using the class method\n    new_guides = Guides.from_whitespace(obj=target_obj, axis=axis, min_gap=min_gap)\n\n    # Add the appropriate coordinates to this object\n    if axis in (\"vertical\", \"both\"):\n        self.vertical = list(set(self.vertical + new_guides.vertical))\n    if axis in (\"horizontal\", \"both\"):\n        self.horizontal = list(set(self.horizontal + new_guides.horizontal))\n\n    return self\n</code></pre> <code>natural_pdf.Guides.build_grid(target=None, source='guides', cell_padding=0.5, include_outer_boundaries=False, *, multi_page='auto')</code> <p>Create table structure (table, rows, columns, cells) from guide coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[Union[Page, Region]]</code> <p>Page or Region to create regions on (uses self.context if None)</p> <code>None</code> <code>source</code> <code>str</code> <p>Source label for created regions (for identification)</p> <code>'guides'</code> <code>cell_padding</code> <code>float</code> <p>Internal padding for cell regions in points</p> <code>0.5</code> <code>include_outer_boundaries</code> <code>bool</code> <p>Whether to add boundaries at edges if missing</p> <code>False</code> <code>multi_page</code> <code>Literal['auto', True, False]</code> <p>Controls multi-region table creation for FlowRegions. - \"auto\": (default) Creates a unified grid if there are multiple regions or guides span pages. - True: Forces creation of a unified multi-region grid. - False: Creates separate grids for each region.</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with 'counts' and 'regions' created.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def build_grid(\n    self,\n    target: Optional[Union[\"Page\", \"Region\"]] = None,\n    source: str = \"guides\",\n    cell_padding: float = 0.5,\n    include_outer_boundaries: bool = False,\n    *,\n    multi_page: Literal[\"auto\", True, False] = \"auto\",\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Create table structure (table, rows, columns, cells) from guide coordinates.\n\n    Args:\n        target: Page or Region to create regions on (uses self.context if None)\n        source: Source label for created regions (for identification)\n        cell_padding: Internal padding for cell regions in points\n        include_outer_boundaries: Whether to add boundaries at edges if missing\n        multi_page: Controls multi-region table creation for FlowRegions.\n            - \"auto\": (default) Creates a unified grid if there are multiple regions or guides span pages.\n            - True: Forces creation of a unified multi-region grid.\n            - False: Creates separate grids for each region.\n\n    Returns:\n        Dictionary with 'counts' and 'regions' created.\n    \"\"\"\n    # Dispatch to appropriate implementation based on context and flags\n    if self.is_flow_region:\n        # Check if we should create a unified multi-region grid\n        has_multiple_regions = len(self.context.constituent_regions) &gt; 1\n        spans_pages = self._spans_pages()\n\n        # Create unified grid if:\n        # - multi_page is explicitly True, OR\n        # - multi_page is \"auto\" AND (spans pages OR has multiple regions)\n        if multi_page is True or (\n            multi_page == \"auto\" and (spans_pages or has_multiple_regions)\n        ):\n            return self._build_grid_multi_page(\n                source=source,\n                cell_padding=cell_padding,\n                include_outer_boundaries=include_outer_boundaries,\n            )\n        else:\n            # Single region FlowRegion or multi_page=False: create separate tables per region\n            total_counts = {\"table\": 0, \"rows\": 0, \"columns\": 0, \"cells\": 0}\n            all_regions = {\"table\": [], \"rows\": [], \"columns\": [], \"cells\": []}\n\n            for region in self.context.constituent_regions:\n                if region in self._flow_guides:\n                    verticals, horizontals = self._flow_guides[region]\n\n                    region_guides = Guides(\n                        verticals=verticals, horizontals=horizontals, context=region\n                    )\n\n                    try:\n                        result = region_guides._build_grid_single_page(\n                            target=region,\n                            source=source,\n                            cell_padding=cell_padding,\n                            include_outer_boundaries=include_outer_boundaries,\n                        )\n\n                        for key in total_counts:\n                            total_counts[key] += result[\"counts\"][key]\n\n                        if result[\"regions\"][\"table\"]:\n                            all_regions[\"table\"].append(result[\"regions\"][\"table\"])\n                        all_regions[\"rows\"].extend(result[\"regions\"][\"rows\"])\n                        all_regions[\"columns\"].extend(result[\"regions\"][\"columns\"])\n                        all_regions[\"cells\"].extend(result[\"regions\"][\"cells\"])\n\n                    except Exception as e:\n                        logger.warning(f\"Failed to build grid on region: {e}\")\n\n            logger.info(\n                f\"Created {total_counts['table']} tables, {total_counts['rows']} rows, \"\n                f\"{total_counts['columns']} columns, and {total_counts['cells']} cells \"\n                f\"from guides across {len(self._flow_guides)} regions\"\n            )\n\n            return {\"counts\": total_counts, \"regions\": all_regions}\n\n    # Fallback for single page/region\n    return self._build_grid_single_page(\n        target=target,\n        source=source,\n        cell_padding=cell_padding,\n        include_outer_boundaries=include_outer_boundaries,\n    )\n</code></pre> <code>natural_pdf.Guides.divide(obj, n=None, cols=None, rows=None, axis='both')</code> <code>classmethod</code> <p>Create guides by evenly dividing an object.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Page, Region, Tuple[float, float, float, float]]</code> <p>Object to divide (Page, Region, or bbox tuple)</p> required <code>n</code> <code>Optional[int]</code> <p>Number of divisions (creates n+1 guides). Used if cols/rows not specified.</p> <code>None</code> <code>cols</code> <code>Optional[int]</code> <p>Number of columns (creates cols+1 vertical guides)</p> <code>None</code> <code>rows</code> <code>Optional[int]</code> <p>Number of rows (creates rows+1 horizontal guides)</p> <code>None</code> <code>axis</code> <code>Literal['vertical', 'horizontal', 'both']</code> <p>Which axis to divide along</p> <code>'both'</code> <p>Returns:</p> Type Description <code>Guides</code> <p>New Guides object with evenly spaced lines</p> <p>Examples:</p> <code>natural_pdf.Guides.extract_table(target=None, source='guides_temp', cell_padding=0.5, include_outer_boundaries=False, method=None, table_settings=None, use_ocr=False, ocr_config=None, text_options=None, cell_extraction_func=None, show_progress=False, content_filter=None, *, multi_page='auto')</code> <p>Extract table data directly from guides without leaving temporary regions.</p> <p>This method: 1. Creates table structure using build_grid() 2. Extracts table data from the created table region 3. Cleans up all temporary regions 4. Returns the TableResult</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Optional[Union[Page, Region]]</code> <p>Page or Region to create regions on (uses self.context if None)</p> <code>None</code> <code>source</code> <code>str</code> <p>Source label for temporary regions (will be cleaned up)</p> <code>'guides_temp'</code> <code>cell_padding</code> <code>float</code> <p>Internal padding for cell regions in points</p> <code>0.5</code> <code>include_outer_boundaries</code> <code>bool</code> <p>Whether to add boundaries at edges if missing</p> <code>False</code> <code>method</code> <code>Optional[str]</code> <p>Table extraction method ('tatr', 'pdfplumber', 'text', etc.)</p> <code>None</code> <code>table_settings</code> <code>Optional[dict]</code> <p>Settings for pdfplumber table extraction</p> <code>None</code> <code>use_ocr</code> <code>bool</code> <p>Whether to use OCR for text extraction</p> <code>False</code> <code>ocr_config</code> <code>Optional[dict]</code> <p>OCR configuration parameters</p> <code>None</code> <code>text_options</code> <code>Optional[Dict]</code> <p>Dictionary of options for the 'text' method</p> <code>None</code> <code>cell_extraction_func</code> <code>Optional[Callable[[Region], Optional[str]]]</code> <p>Optional callable for custom cell text extraction</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Controls progress bar for text method</p> <code>False</code> <code>content_filter</code> <code>Optional[Union[str, Callable[[str], bool], List[str]]]</code> <p>Content filtering function or patterns</p> <code>None</code> <code>multi_page</code> <code>Literal['auto', True, False]</code> <p>Controls multi-region table creation for FlowRegions</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>TableResult</code> <code>TableResult</code> <p>Extracted table data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no table region is created from the guides</p> Example <pre><code>from natural_pdf.analyzers import Guides\n\n# Create guides from detected lines\nguides = Guides.from_lines(page, source_label=\"detected\")\n\n# Extract table directly - no temporary regions left behind\ntable_data = guides.extract_table()\n\n# Convert to pandas DataFrame\ndf = table_data.to_df()\n</code></pre> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def extract_table(\n    self,\n    target: Optional[Union[\"Page\", \"Region\"]] = None,\n    source: str = \"guides_temp\",\n    cell_padding: float = 0.5,\n    include_outer_boundaries: bool = False,\n    method: Optional[str] = None,\n    table_settings: Optional[dict] = None,\n    use_ocr: bool = False,\n    ocr_config: Optional[dict] = None,\n    text_options: Optional[Dict] = None,\n    cell_extraction_func: Optional[Callable[[\"Region\"], Optional[str]]] = None,\n    show_progress: bool = False,\n    content_filter: Optional[Union[str, Callable[[str], bool], List[str]]] = None,\n    *,\n    multi_page: Literal[\"auto\", True, False] = \"auto\",\n) -&gt; \"TableResult\":\n    \"\"\"\n    Extract table data directly from guides without leaving temporary regions.\n\n    This method:\n    1. Creates table structure using build_grid()\n    2. Extracts table data from the created table region\n    3. Cleans up all temporary regions\n    4. Returns the TableResult\n\n    Args:\n        target: Page or Region to create regions on (uses self.context if None)\n        source: Source label for temporary regions (will be cleaned up)\n        cell_padding: Internal padding for cell regions in points\n        include_outer_boundaries: Whether to add boundaries at edges if missing\n        method: Table extraction method ('tatr', 'pdfplumber', 'text', etc.)\n        table_settings: Settings for pdfplumber table extraction\n        use_ocr: Whether to use OCR for text extraction\n        ocr_config: OCR configuration parameters\n        text_options: Dictionary of options for the 'text' method\n        cell_extraction_func: Optional callable for custom cell text extraction\n        show_progress: Controls progress bar for text method\n        content_filter: Content filtering function or patterns\n        multi_page: Controls multi-region table creation for FlowRegions\n\n    Returns:\n        TableResult: Extracted table data\n\n    Raises:\n        ValueError: If no table region is created from the guides\n\n    Example:\n        ```python\n        from natural_pdf.analyzers import Guides\n\n        # Create guides from detected lines\n        guides = Guides.from_lines(page, source_label=\"detected\")\n\n        # Extract table directly - no temporary regions left behind\n        table_data = guides.extract_table()\n\n        # Convert to pandas DataFrame\n        df = table_data.to_df()\n        ```\n    \"\"\"\n    target_obj = target or self.context\n    if not target_obj:\n        raise ValueError(\"No target object available. Provide target parameter or context.\")\n\n    # Get the page for cleanup later\n    if hasattr(target_obj, \"x0\") and hasattr(target_obj, \"top\"):  # Region\n        page = target_obj._page\n        element_manager = page._element_mgr\n    elif hasattr(target_obj, \"_element_mgr\"):  # Page\n        page = target_obj\n        element_manager = page._element_mgr\n    else:\n        raise ValueError(f\"Target object {target_obj} is not a Page or Region\")\n\n    try:\n        # Step 1: Build grid structure (creates temporary regions)\n        grid_result = self.build_grid(\n            target=target_obj,\n            source=source,\n            cell_padding=cell_padding,\n            include_outer_boundaries=include_outer_boundaries,\n            multi_page=multi_page,\n        )\n\n        # Step 2: Get the table region and extract table data\n        table_region = grid_result[\"regions\"][\"table\"]\n        if table_region is None:\n            raise ValueError(\n                \"No table region was created from the guides. Check that you have both vertical and horizontal guides.\"\n            )\n\n        # Handle multi-page case where table_region might be a list\n        if isinstance(table_region, list):\n            if not table_region:\n                raise ValueError(\"No table regions were created from the guides.\")\n            # Use the first table region for extraction\n            table_region = table_region[0]\n\n        # Step 3: Extract table data using the region's extract_table method\n        table_result = table_region.extract_table(\n            method=method,\n            table_settings=table_settings,\n            use_ocr=use_ocr,\n            ocr_config=ocr_config,\n            text_options=text_options,\n            cell_extraction_func=cell_extraction_func,\n            show_progress=show_progress,\n            content_filter=content_filter,\n        )\n\n        return table_result\n\n    finally:\n        # Step 4: Clean up all temporary regions created by build_grid\n        # This ensures no regions are left behind regardless of success/failure\n        try:\n            regions_to_remove = [\n                r\n                for r in element_manager.regions\n                if getattr(r, \"source\", None) == source\n                and getattr(r, \"region_type\", None)\n                in {\"table\", \"table_row\", \"table_column\", \"table_cell\"}\n            ]\n\n            for region in regions_to_remove:\n                element_manager.remove_element(region, element_type=\"regions\")\n\n            if regions_to_remove:\n                logger.debug(f\"Cleaned up {len(regions_to_remove)} temporary regions\")\n\n        except Exception as cleanup_err:\n            logger.warning(f\"Failed to clean up temporary regions: {cleanup_err}\")\n</code></pre> <code>natural_pdf.Guides.from_content(obj, axis='vertical', markers=None, align='left', outer=True, tolerance=5, apply_exclusions=True)</code> <code>classmethod</code> <p>Create guides based on text content positions.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Page, Region, FlowRegion]</code> <p>Page, Region, or FlowRegion to search for content</p> required <code>axis</code> <code>Literal['vertical', 'horizontal']</code> <p>Whether to create vertical or horizontal guides</p> <code>'vertical'</code> <code>markers</code> <code>Union[str, List[str], ElementCollection, None]</code> <p>Content to search for. Can be: - str: single selector (e.g., 'text:contains(\"Name\")') or literal text - List[str]: list of selectors or literal text strings - ElementCollection: collection of elements to extract text from - None: no markers</p> <code>None</code> <code>align</code> <code>Literal['left', 'right', 'center', 'between']</code> <p>Where to place guides relative to found text</p> <code>'left'</code> <code>outer</code> <code>bool</code> <p>Whether to add guides at the boundaries</p> <code>True</code> <code>tolerance</code> <code>float</code> <p>Maximum distance to search for text</p> <code>5</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to apply exclusion zones when searching for text</p> <code>True</code> <p>Returns:</p> Type Description <code>Guides</code> <p>New Guides object aligned to text content</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>@classmethod\ndef from_content(\n    cls,\n    obj: Union[\"Page\", \"Region\", \"FlowRegion\"],\n    axis: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n    markers: Union[str, List[str], \"ElementCollection\", None] = None,\n    align: Literal[\"left\", \"right\", \"center\", \"between\"] = \"left\",\n    outer: bool = True,\n    tolerance: float = 5,\n    apply_exclusions: bool = True,\n) -&gt; \"Guides\":\n    \"\"\"\n    Create guides based on text content positions.\n\n    Args:\n        obj: Page, Region, or FlowRegion to search for content\n        axis: Whether to create vertical or horizontal guides\n        markers: Content to search for. Can be:\n            - str: single selector (e.g., 'text:contains(\"Name\")') or literal text\n            - List[str]: list of selectors or literal text strings\n            - ElementCollection: collection of elements to extract text from\n            - None: no markers\n        align: Where to place guides relative to found text\n        outer: Whether to add guides at the boundaries\n        tolerance: Maximum distance to search for text\n        apply_exclusions: Whether to apply exclusion zones when searching for text\n\n    Returns:\n        New Guides object aligned to text content\n    \"\"\"\n    # Handle FlowRegion\n    if hasattr(obj, \"constituent_regions\"):\n        guides = cls(context=obj)\n\n        # Process each constituent region\n        for region in obj.constituent_regions:\n            # Create guides for this specific region\n            region_guides = cls.from_content(\n                region,\n                axis=axis,\n                markers=markers,\n                align=align,\n                outer=outer,\n                tolerance=tolerance,\n                apply_exclusions=apply_exclusions,\n            )\n\n            # Store in flow guides\n            guides._flow_guides[region] = (\n                list(region_guides.vertical),\n                list(region_guides.horizontal),\n            )\n\n            # Add to unified view\n            for v in region_guides.vertical:\n                guides._unified_vertical.append((v, region))\n            for h in region_guides.horizontal:\n                guides._unified_horizontal.append((h, region))\n\n        # Invalidate caches\n        guides._vertical_cache = None\n        guides._horizontal_cache = None\n\n        return guides\n\n    # Original single-region logic follows...\n    guides_coords = []\n    bounds = None\n\n    # Get bounds from object\n    if hasattr(obj, \"bbox\"):\n        bounds = obj.bbox\n    elif hasattr(obj, \"x0\"):\n        bounds = (obj.x0, obj.top, obj.x1, obj.bottom)\n    elif hasattr(obj, \"width\"):\n        bounds = (0, 0, obj.width, obj.height)\n\n    # Normalize markers to list of text strings\n    marker_texts = _normalize_markers(markers, obj)\n\n    # Find each marker and determine guide position\n    for marker in marker_texts:\n        if hasattr(obj, \"find\"):\n            element = obj.find(f'text:contains(\"{marker}\")', apply_exclusions=apply_exclusions)\n            if element:\n                if axis == \"vertical\":\n                    if align == \"left\":\n                        guides_coords.append(element.x0)\n                    elif align == \"right\":\n                        guides_coords.append(element.x1)\n                    elif align == \"center\":\n                        guides_coords.append((element.x0 + element.x1) / 2)\n                    elif align == \"between\":\n                        # For between, collect left edges for processing later\n                        guides_coords.append(element.x0)\n                else:  # horizontal\n                    if align == \"left\":  # top for horizontal\n                        guides_coords.append(element.top)\n                    elif align == \"right\":  # bottom for horizontal\n                        guides_coords.append(element.bottom)\n                    elif align == \"center\":\n                        guides_coords.append((element.top + element.bottom) / 2)\n                    elif align == \"between\":\n                        # For between, collect top edges for processing later\n                        guides_coords.append(element.top)\n\n    # Handle 'between' alignment - find midpoints between adjacent markers\n    if align == \"between\" and len(guides_coords) &gt;= 2:\n        # We need to get the right and left edges of each marker\n        marker_bounds = []\n        for marker in marker_texts:\n            if hasattr(obj, \"find\"):\n                element = obj.find(\n                    f'text:contains(\"{marker}\")', apply_exclusions=apply_exclusions\n                )\n                if element:\n                    if axis == \"vertical\":\n                        marker_bounds.append((element.x0, element.x1))\n                    else:  # horizontal\n                        marker_bounds.append((element.top, element.bottom))\n\n        # Sort markers by their left edge (or top edge for horizontal)\n        marker_bounds.sort(key=lambda x: x[0])\n\n        # Create guides at midpoints between adjacent markers\n        between_coords = []\n        for i in range(len(marker_bounds) - 1):\n            # Midpoint between right edge of current marker and left edge of next marker\n            right_edge_current = marker_bounds[i][1]\n            left_edge_next = marker_bounds[i + 1][0]\n            midpoint = (right_edge_current + left_edge_next) / 2\n            between_coords.append(midpoint)\n\n        guides_coords = between_coords\n\n    # Add outer guides if requested\n    if outer and bounds:\n        if axis == \"vertical\":\n            guides_coords.insert(0, bounds[0])  # x0\n            guides_coords.append(bounds[2])  # x1\n        else:\n            guides_coords.insert(0, bounds[1])  # y0\n            guides_coords.append(bounds[3])  # y1\n\n    # Remove duplicates and sort\n    guides_coords = sorted(list(set(guides_coords)))\n\n    # Create guides object\n    if axis == \"vertical\":\n        return cls(verticals=guides_coords, context=obj, bounds=bounds)\n    else:\n        return cls(horizontals=guides_coords, context=obj, bounds=bounds)\n</code></pre> <code>natural_pdf.Guides.from_lines(obj, axis='both', threshold='auto', source_label=None, max_lines_h=None, max_lines_v=None, outer=False, detection_method='pixels', resolution=192, **detect_kwargs)</code> <code>classmethod</code> <p>Create guides from detected line elements.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Page, Region, FlowRegion]</code> <p>Page, Region, or FlowRegion to detect lines from</p> required <code>axis</code> <code>Literal['vertical', 'horizontal', 'both']</code> <p>Which orientations to detect</p> <code>'both'</code> <code>threshold</code> <code>Union[float, str]</code> <p>Detection threshold ('auto' or float 0.0-1.0) - used for pixel detection</p> <code>'auto'</code> <code>source_label</code> <code>Optional[str]</code> <p>Filter for line source (vector method) or label for detected lines (pixel method)</p> <code>None</code> <code>max_lines_h</code> <code>Optional[int]</code> <p>Maximum number of horizontal lines to keep</p> <code>None</code> <code>max_lines_v</code> <code>Optional[int]</code> <p>Maximum number of vertical lines to keep</p> <code>None</code> <code>outer</code> <code>bool</code> <p>Whether to add outer boundary guides</p> <code>False</code> <code>detection_method</code> <code>str</code> <p>'vector' (use existing LineElements) or 'pixels' (detect from image)</p> <code>'pixels'</code> <code>resolution</code> <code>int</code> <p>DPI for pixel-based detection (default: 192)</p> <code>192</code> <code>**detect_kwargs</code> <p>Additional parameters for pixel-based detection: - min_gap_h: Minimum gap between horizontal lines (pixels) - min_gap_v: Minimum gap between vertical lines (pixels) - binarization_method: 'adaptive' or 'otsu' - morph_op_h/v: Morphological operations ('open', 'close', 'none') - smoothing_sigma_h/v: Gaussian smoothing sigma - method: 'projection' (default) or 'lsd' (requires opencv)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Guides</code> <p>New Guides object with detected line positions</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>@classmethod\ndef from_lines(\n    cls,\n    obj: Union[\"Page\", \"Region\", \"FlowRegion\"],\n    axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n    threshold: Union[float, str] = \"auto\",\n    source_label: Optional[str] = None,\n    max_lines_h: Optional[int] = None,\n    max_lines_v: Optional[int] = None,\n    outer: bool = False,\n    detection_method: str = \"pixels\",\n    resolution: int = 192,\n    **detect_kwargs,\n) -&gt; \"Guides\":\n    \"\"\"\n    Create guides from detected line elements.\n\n    Args:\n        obj: Page, Region, or FlowRegion to detect lines from\n        axis: Which orientations to detect\n        threshold: Detection threshold ('auto' or float 0.0-1.0) - used for pixel detection\n        source_label: Filter for line source (vector method) or label for detected lines (pixel method)\n        max_lines_h: Maximum number of horizontal lines to keep\n        max_lines_v: Maximum number of vertical lines to keep\n        outer: Whether to add outer boundary guides\n        detection_method: 'vector' (use existing LineElements) or 'pixels' (detect from image)\n        resolution: DPI for pixel-based detection (default: 192)\n        **detect_kwargs: Additional parameters for pixel-based detection:\n            - min_gap_h: Minimum gap between horizontal lines (pixels)\n            - min_gap_v: Minimum gap between vertical lines (pixels)\n            - binarization_method: 'adaptive' or 'otsu'\n            - morph_op_h/v: Morphological operations ('open', 'close', 'none')\n            - smoothing_sigma_h/v: Gaussian smoothing sigma\n            - method: 'projection' (default) or 'lsd' (requires opencv)\n\n    Returns:\n        New Guides object with detected line positions\n    \"\"\"\n    # Handle FlowRegion\n    if hasattr(obj, \"constituent_regions\"):\n        guides = cls(context=obj)\n\n        # Process each constituent region\n        for region in obj.constituent_regions:\n            # Create guides for this specific region\n            region_guides = cls.from_lines(\n                region,\n                axis=axis,\n                threshold=threshold,\n                source_label=source_label,\n                max_lines_h=max_lines_h,\n                max_lines_v=max_lines_v,\n                outer=outer,\n                detection_method=detection_method,\n                resolution=resolution,\n                **detect_kwargs,\n            )\n\n            # Store in flow guides\n            guides._flow_guides[region] = (\n                list(region_guides.vertical),\n                list(region_guides.horizontal),\n            )\n\n            # Add to unified view\n            for v in region_guides.vertical:\n                guides._unified_vertical.append((v, region))\n            for h in region_guides.horizontal:\n                guides._unified_horizontal.append((h, region))\n\n        # Invalidate caches to force rebuild on next access\n        guides._vertical_cache = None\n        guides._horizontal_cache = None\n\n        return guides\n\n    # Original single-region logic follows...\n    # Get bounds for potential outer guides\n    if hasattr(obj, \"bbox\"):\n        bounds = obj.bbox\n    elif hasattr(obj, \"x0\"):\n        bounds = (obj.x0, obj.top, obj.x1, obj.bottom)\n    elif hasattr(obj, \"width\"):\n        bounds = (0, 0, obj.width, obj.height)\n    else:\n        bounds = None\n\n    verticals = []\n    horizontals = []\n\n    if detection_method == \"pixels\":\n        # Use pixel-based line detection\n        if not hasattr(obj, \"detect_lines\"):\n            raise ValueError(f\"Object {obj} does not support pixel-based line detection\")\n\n        # Set up detection parameters\n        detect_params = {\n            \"resolution\": resolution,\n            \"source_label\": source_label or \"guides_detection\",\n            \"horizontal\": axis in (\"horizontal\", \"both\"),\n            \"vertical\": axis in (\"vertical\", \"both\"),\n            \"replace\": True,  # Replace any existing lines with this source\n            \"method\": detect_kwargs.get(\"method\", \"projection\"),\n        }\n\n        # Handle threshold parameter\n        if threshold == \"auto\" and detection_method == \"vector\":\n            # Auto mode: use very low thresholds with max_lines constraints\n            detect_params[\"peak_threshold_h\"] = 0.0\n            detect_params[\"peak_threshold_v\"] = 0.0\n            detect_params[\"max_lines_h\"] = max_lines_h\n            detect_params[\"max_lines_v\"] = max_lines_v\n        if threshold == \"auto\" and detection_method == \"pixels\":\n            detect_params[\"peak_threshold_h\"] = 0.5\n            detect_params[\"peak_threshold_v\"] = 0.5\n            detect_params[\"max_lines_h\"] = max_lines_h\n            detect_params[\"max_lines_v\"] = max_lines_v\n        else:\n            # Fixed threshold mode\n            detect_params[\"peak_threshold_h\"] = (\n                float(threshold) if axis in (\"horizontal\", \"both\") else 1.0\n            )\n            detect_params[\"peak_threshold_v\"] = (\n                float(threshold) if axis in (\"vertical\", \"both\") else 1.0\n            )\n            detect_params[\"max_lines_h\"] = max_lines_h\n            detect_params[\"max_lines_v\"] = max_lines_v\n\n        # Add any additional detection parameters\n        for key in [\n            \"min_gap_h\",\n            \"min_gap_v\",\n            \"binarization_method\",\n            \"adaptive_thresh_block_size\",\n            \"adaptive_thresh_C_val\",\n            \"morph_op_h\",\n            \"morph_kernel_h\",\n            \"morph_op_v\",\n            \"morph_kernel_v\",\n            \"smoothing_sigma_h\",\n            \"smoothing_sigma_v\",\n            \"peak_width_rel_height\",\n        ]:\n            if key in detect_kwargs:\n                detect_params[key] = detect_kwargs[key]\n\n        # Perform the detection\n        obj.detect_lines(**detect_params)\n\n        # Now get the detected lines and use them\n        if hasattr(obj, \"lines\"):\n            lines = obj.lines\n        elif hasattr(obj, \"find_all\"):\n            lines = obj.find_all(\"line\")\n        else:\n            lines = []\n\n        # Filter by the source we just used\n\n        lines = [\n            l for l in lines if getattr(l, \"source\", None) == detect_params[\"source_label\"]\n        ]\n\n    else:  # detection_method == 'vector' (default)\n        # Get existing lines from the object\n        if hasattr(obj, \"lines\"):\n            lines = obj.lines\n        elif hasattr(obj, \"find_all\"):\n            lines = obj.find_all(\"line\")\n        else:\n            logger.warning(f\"Object {obj} has no lines or find_all method\")\n            lines = []\n\n        # Filter by source if specified\n        if source_label:\n            lines = [l for l in lines if getattr(l, \"source\", None) == source_label]\n\n    # Process lines (same logic for both methods)\n    # Separate lines by orientation and collect with metadata for ranking\n    h_line_data = []  # (y_coord, length, line_obj)\n    v_line_data = []  # (x_coord, length, line_obj)\n\n    for line in lines:\n        if hasattr(line, \"is_horizontal\") and hasattr(line, \"is_vertical\"):\n            if line.is_horizontal and axis in (\"horizontal\", \"both\"):\n                # Use the midpoint y-coordinate for horizontal lines\n                y = (line.top + line.bottom) / 2\n                # Calculate line length for ranking\n                length = getattr(\n                    line, \"width\", abs(getattr(line, \"x1\", 0) - getattr(line, \"x0\", 0))\n                )\n                h_line_data.append((y, length, line))\n            elif line.is_vertical and axis in (\"vertical\", \"both\"):\n                # Use the midpoint x-coordinate for vertical lines\n                x = (line.x0 + line.x1) / 2\n                # Calculate line length for ranking\n                length = getattr(\n                    line, \"height\", abs(getattr(line, \"bottom\", 0) - getattr(line, \"top\", 0))\n                )\n                v_line_data.append((x, length, line))\n\n    # Process horizontal lines\n    if max_lines_h is not None and h_line_data:\n        # Sort by length (longer lines are typically more significant)\n        h_line_data.sort(key=lambda x: x[1], reverse=True)\n        # Take the top N by length\n        selected_h = h_line_data[:max_lines_h]\n        # Extract just the coordinates and sort by position\n        horizontals = sorted([coord for coord, _, _ in selected_h])\n        logger.debug(\n            f\"Selected {len(horizontals)} horizontal lines from {len(h_line_data)} candidates\"\n        )\n    else:\n        # Use all horizontal lines (original behavior)\n        horizontals = [coord for coord, _, _ in h_line_data]\n        horizontals = sorted(list(set(horizontals)))\n\n    # Process vertical lines\n    if max_lines_v is not None and v_line_data:\n        # Sort by length (longer lines are typically more significant)\n        v_line_data.sort(key=lambda x: x[1], reverse=True)\n        # Take the top N by length\n        selected_v = v_line_data[:max_lines_v]\n        # Extract just the coordinates and sort by position\n        verticals = sorted([coord for coord, _, _ in selected_v])\n        logger.debug(\n            f\"Selected {len(verticals)} vertical lines from {len(v_line_data)} candidates\"\n        )\n    else:\n        # Use all vertical lines (original behavior)\n        verticals = [coord for coord, _, _ in v_line_data]\n        verticals = sorted(list(set(verticals)))\n\n    # Add outer guides if requested\n    if outer and bounds:\n        if axis in (\"vertical\", \"both\"):\n            if not verticals or verticals[0] &gt; bounds[0]:\n                verticals.insert(0, bounds[0])  # x0\n            if not verticals or verticals[-1] &lt; bounds[2]:\n                verticals.append(bounds[2])  # x1\n        if axis in (\"horizontal\", \"both\"):\n            if not horizontals or horizontals[0] &gt; bounds[1]:\n                horizontals.insert(0, bounds[1])  # y0\n            if not horizontals or horizontals[-1] &lt; bounds[3]:\n                horizontals.append(bounds[3])  # y1\n\n    # Remove duplicates and sort again\n    verticals = sorted(list(set(verticals)))\n    horizontals = sorted(list(set(horizontals)))\n\n    return cls(verticals=verticals, horizontals=horizontals, context=obj, bounds=bounds)\n</code></pre> <code>natural_pdf.Guides.from_whitespace(obj, axis='both', min_gap=10)</code> <code>classmethod</code> <p>Create guides by detecting whitespace gaps.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Union[Page, Region, FlowRegion]</code> <p>Page or Region to analyze</p> required <code>min_gap</code> <code>float</code> <p>Minimum gap size to consider as whitespace</p> <code>10</code> <code>axis</code> <code>Literal['vertical', 'horizontal', 'both']</code> <p>Which axes to analyze for gaps</p> <code>'both'</code> <p>Returns:</p> Type Description <code>Guides</code> <p>New Guides object positioned at whitespace gaps</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>@classmethod\ndef from_whitespace(\n    cls,\n    obj: Union[\"Page\", \"Region\", \"FlowRegion\"],\n    axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n    min_gap: float = 10,\n) -&gt; \"Guides\":\n    \"\"\"\n    Create guides by detecting whitespace gaps.\n\n    Args:\n        obj: Page or Region to analyze\n        min_gap: Minimum gap size to consider as whitespace\n        axis: Which axes to analyze for gaps\n\n    Returns:\n        New Guides object positioned at whitespace gaps\n    \"\"\"\n    # This is a placeholder - would need sophisticated gap detection\n    logger.info(\"Whitespace detection not yet implemented, using divide instead\")\n    return cls.divide(obj, n=3, axis=axis)\n</code></pre> <code>natural_pdf.Guides.get_cells()</code> <p>Get all cell bounding boxes from guide intersections.</p> <p>Returns:</p> Type Description <code>List[Tuple[float, float, float, float]]</code> <p>List of (x0, y0, x1, y1) tuples for each cell</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def get_cells(self) -&gt; List[Tuple[float, float, float, float]]:\n    \"\"\"\n    Get all cell bounding boxes from guide intersections.\n\n    Returns:\n        List of (x0, y0, x1, y1) tuples for each cell\n    \"\"\"\n    cells = []\n\n    # Create cells from guide intersections\n    for i in range(len(self.vertical) - 1):\n        for j in range(len(self.horizontal) - 1):\n            x0 = self.vertical[i]\n            x1 = self.vertical[i + 1]\n            y0 = self.horizontal[j]\n            y1 = self.horizontal[j + 1]\n            cells.append((x0, y0, x1, y1))\n\n    return cells\n</code></pre> <code>natural_pdf.Guides.new(context=None)</code> <code>classmethod</code> <p>Create a new empty Guides object, optionally with a context.</p> <p>This provides a clean way to start building guides through chaining: guides = Guides.new(page).add_content(axis='vertical', markers=[...])</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[Union[Page, Region]]</code> <p>Optional Page or Region to use as default context for operations</p> <code>None</code> <p>Returns:</p> Type Description <code>Guides</code> <p>New empty Guides object</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>@classmethod\ndef new(cls, context: Optional[Union[\"Page\", \"Region\"]] = None) -&gt; \"Guides\":\n    \"\"\"\n    Create a new empty Guides object, optionally with a context.\n\n    This provides a clean way to start building guides through chaining:\n    guides = Guides.new(page).add_content(axis='vertical', markers=[...])\n\n    Args:\n        context: Optional Page or Region to use as default context for operations\n\n    Returns:\n        New empty Guides object\n    \"\"\"\n    return cls(verticals=[], horizontals=[], context=context)\n</code></pre> <code>natural_pdf.Guides.remove_horizontal(index)</code> <p>Remove a horizontal guide by index.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def remove_horizontal(self, index: int) -&gt; \"Guides\":\n    \"\"\"Remove a horizontal guide by index.\"\"\"\n    if 0 &lt;= index &lt; len(self.horizontal):\n        self.horizontal.pop(index)\n    return self\n</code></pre> <code>natural_pdf.Guides.remove_vertical(index)</code> <p>Remove a vertical guide by index.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def remove_vertical(self, index: int) -&gt; \"Guides\":\n    \"\"\"Remove a vertical guide by index.\"\"\"\n    if 0 &lt;= index &lt; len(self.vertical):\n        self.vertical.pop(index)\n    return self\n</code></pre> <code>natural_pdf.Guides.shift(index, offset, axis='vertical')</code> <p>Move a specific guide by a offset amount.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the guide to move</p> required <code>offset</code> <code>float</code> <p>Amount to move (positive = right/down)</p> required <code>axis</code> <code>Literal['vertical', 'horizontal']</code> <p>Which guide list to modify</p> <code>'vertical'</code> <p>Returns:</p> Type Description <code>Guides</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def shift(\n    self, index: int, offset: float, axis: Literal[\"vertical\", \"horizontal\"] = \"vertical\"\n) -&gt; \"Guides\":\n    \"\"\"\n    Move a specific guide by a offset amount.\n\n    Args:\n        index: Index of the guide to move\n        offset: Amount to move (positive = right/down)\n        axis: Which guide list to modify\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    if axis == \"vertical\":\n        if 0 &lt;= index &lt; len(self.vertical):\n            self.vertical[index] += offset\n            self.vertical = sorted(self.vertical)\n        else:\n            logger.warning(f\"Vertical guide index {index} out of range\")\n    else:\n        if 0 &lt;= index &lt; len(self.horizontal):\n            self.horizontal[index] += offset\n            self.horizontal = sorted(self.horizontal)\n        else:\n            logger.warning(f\"Horizontal guide index {index} out of range\")\n\n    return self\n</code></pre> <code>natural_pdf.Guides.show(on=None, **kwargs)</code> <p>Display the guides overlaid on a page or region.</p> <p>Parameters:</p> Name Type Description Default <code>on</code> <p>Page, Region, PIL Image, or string to display guides on. If None, uses self.context (the object guides were created from). If string 'page', uses the page from self.context.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to to_image() if applicable.</p> <code>{}</code> <p>Returns:</p> Type Description <p>PIL Image with guides drawn on it.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def show(self, on=None, **kwargs):\n    \"\"\"\n    Display the guides overlaid on a page or region.\n\n    Args:\n        on: Page, Region, PIL Image, or string to display guides on.\n            If None, uses self.context (the object guides were created from).\n            If string 'page', uses the page from self.context.\n        **kwargs: Additional arguments passed to to_image() if applicable.\n\n    Returns:\n        PIL Image with guides drawn on it.\n    \"\"\"\n    # Handle FlowRegion case\n    if self.is_flow_region and (on is None or on == self.context):\n        if not self._flow_guides:\n            raise ValueError(\"No guides to show for FlowRegion\")\n\n        # Get stacking parameters from kwargs or use defaults\n        stack_direction = kwargs.get(\"stack_direction\", \"vertical\")\n        stack_gap = kwargs.get(\"stack_gap\", 5)\n        stack_background_color = kwargs.get(\"stack_background_color\", (255, 255, 255))\n\n        # First, render all constituent regions without guides to get base images\n        base_images = []\n        region_infos = []  # Store region info for guide coordinate mapping\n\n        for region in self.context.constituent_regions:\n            try:\n                # Render region without guides using new system\n                if hasattr(region, \"render\"):\n                    img = region.render(\n                        resolution=kwargs.get(\"resolution\", 150),\n                        width=kwargs.get(\"width\", None),\n                        crop=True,  # Always crop regions to their bounds\n                    )\n                else:\n                    # Fallback to old method\n                    img = region.render(**kwargs)\n                if img:\n                    base_images.append(img)\n\n                    # Calculate scaling factors for this region\n                    scale_x = img.width / region.width\n                    scale_y = img.height / region.height\n\n                    region_infos.append(\n                        {\n                            \"region\": region,\n                            \"img_width\": img.width,\n                            \"img_height\": img.height,\n                            \"scale_x\": scale_x,\n                            \"scale_y\": scale_y,\n                            \"pdf_x0\": region.x0,\n                            \"pdf_top\": region.top,\n                            \"pdf_x1\": region.x1,\n                            \"pdf_bottom\": region.bottom,\n                        }\n                    )\n            except Exception as e:\n                logger.warning(f\"Failed to render region: {e}\")\n\n        if not base_images:\n            raise ValueError(\"Failed to render any images for FlowRegion\")\n\n        # Calculate final canvas size based on stacking direction\n        if stack_direction == \"vertical\":\n            final_width = max(img.width for img in base_images)\n            final_height = (\n                sum(img.height for img in base_images) + (len(base_images) - 1) * stack_gap\n            )\n        else:  # horizontal\n            final_width = (\n                sum(img.width for img in base_images) + (len(base_images) - 1) * stack_gap\n            )\n            final_height = max(img.height for img in base_images)\n\n        # Create unified canvas\n        canvas = Image.new(\"RGB\", (final_width, final_height), stack_background_color)\n        draw = ImageDraw.Draw(canvas)\n\n        # Paste base images and track positions\n        region_positions = []  # (region_info, paste_x, paste_y)\n\n        if stack_direction == \"vertical\":\n            current_y = 0\n            for i, (img, info) in enumerate(zip(base_images, region_infos)):\n                paste_x = (final_width - img.width) // 2  # Center horizontally\n                canvas.paste(img, (paste_x, current_y))\n                region_positions.append((info, paste_x, current_y))\n                current_y += img.height + stack_gap\n        else:  # horizontal\n            current_x = 0\n            for i, (img, info) in enumerate(zip(base_images, region_infos)):\n                paste_y = (final_height - img.height) // 2  # Center vertically\n                canvas.paste(img, (current_x, paste_y))\n                region_positions.append((info, current_x, paste_y))\n                current_x += img.width + stack_gap\n\n        # Now draw guides on the unified canvas\n        # Draw vertical guides (blue) - these extend through the full canvas height\n        for v_coord in self.vertical:\n            # Find which region(s) this guide intersects\n            for info, paste_x, paste_y in region_positions:\n                if info[\"pdf_x0\"] &lt;= v_coord &lt;= info[\"pdf_x1\"]:\n                    # This guide is within this region's x-bounds\n                    # Convert PDF coordinate to pixel coordinate relative to the region\n                    adjusted_x = v_coord - info[\"pdf_x0\"]\n                    pixel_x = adjusted_x * info[\"scale_x\"] + paste_x\n\n                    # Draw full-height line on canvas (not clipped to region)\n                    if 0 &lt;= pixel_x &lt;= final_width:\n                        x_pixel = int(pixel_x)\n                        draw.line(\n                            [(x_pixel, 0), (x_pixel, final_height - 1)],\n                            fill=(0, 0, 255, 200),\n                            width=2,\n                        )\n                    break  # Only draw once per guide\n\n        # Draw horizontal guides (red) - these extend through the full canvas width\n        for h_coord in self.horizontal:\n            # Find which region(s) this guide intersects\n            for info, paste_x, paste_y in region_positions:\n                if info[\"pdf_top\"] &lt;= h_coord &lt;= info[\"pdf_bottom\"]:\n                    # This guide is within this region's y-bounds\n                    # Convert PDF coordinate to pixel coordinate relative to the region\n                    adjusted_y = h_coord - info[\"pdf_top\"]\n                    pixel_y = adjusted_y * info[\"scale_y\"] + paste_y\n\n                    # Draw full-width line on canvas (not clipped to region)\n                    if 0 &lt;= pixel_y &lt;= final_height:\n                        y_pixel = int(pixel_y)\n                        draw.line(\n                            [(0, y_pixel), (final_width - 1, y_pixel)],\n                            fill=(255, 0, 0, 200),\n                            width=2,\n                        )\n                    break  # Only draw once per guide\n\n        return canvas\n\n    # Original single-region logic follows...\n    # Determine what to display guides on\n    target = on if on is not None else self.context\n\n    # Handle string shortcuts\n    if isinstance(target, str):\n        if target == \"page\":\n            if hasattr(self.context, \"page\"):\n                target = self.context.page\n            elif hasattr(self.context, \"_page\"):\n                target = self.context._page\n            else:\n                raise ValueError(\"Cannot resolve 'page' - context has no page attribute\")\n        else:\n            raise ValueError(f\"Unknown string target: {target}. Only 'page' is supported.\")\n\n    if target is None:\n        raise ValueError(\"No target specified and no context available for guides display\")\n\n    # Prepare kwargs for image generation\n    image_kwargs = {}\n\n    # Extract only the parameters that the new render() method accepts\n    if \"resolution\" in kwargs:\n        image_kwargs[\"resolution\"] = kwargs[\"resolution\"]\n    if \"width\" in kwargs:\n        image_kwargs[\"width\"] = kwargs[\"width\"]\n    if \"crop\" in kwargs:\n        image_kwargs[\"crop\"] = kwargs[\"crop\"]\n\n    # If target is a region-like object, crop to just that region\n    if hasattr(target, \"bbox\") and hasattr(target, \"page\"):\n        # This is likely a Region\n        image_kwargs[\"crop\"] = True\n\n    # Get base image\n    if hasattr(target, \"render\"):\n        # Use the new unified rendering system\n        img = target.render(**image_kwargs)\n    elif hasattr(target, \"render\"):\n        # Fallback to old method if available\n        img = target.render(**image_kwargs)\n    elif hasattr(target, \"mode\") and hasattr(target, \"size\"):\n        # It's already a PIL Image\n        img = target\n    else:\n        raise ValueError(f\"Object {target} does not support render() and is not a PIL Image\")\n\n    if img is None:\n        raise ValueError(\"Failed to generate base image\")\n\n    # Create a copy to draw on\n    img = img.copy()\n    draw = ImageDraw.Draw(img)\n\n    # Determine scale factor for coordinate conversion\n    if (\n        hasattr(target, \"width\")\n        and hasattr(target, \"height\")\n        and not (hasattr(target, \"mode\") and hasattr(target, \"size\"))\n    ):\n        # target is a PDF object (Page/Region) with PDF coordinates\n        scale_x = img.width / target.width\n        scale_y = img.height / target.height\n\n        # If we're showing guides on a region, we need to adjust coordinates\n        # to be relative to the region's origin\n        if hasattr(target, \"bbox\") and hasattr(target, \"page\"):\n            # This is a Region - adjust guide coordinates to be relative to region\n            region_x0, region_top = target.x0, target.top\n        else:\n            # This is a Page - no adjustment needed\n            region_x0, region_top = 0, 0\n    else:\n        # target is already an image, no scaling needed\n        scale_x = 1.0\n        scale_y = 1.0\n        region_x0, region_top = 0, 0\n\n    # Draw vertical guides (blue)\n    for x_coord in self.vertical:\n        # Adjust coordinate if we're showing on a region\n        adjusted_x = x_coord - region_x0\n        pixel_x = adjusted_x * scale_x\n        # Ensure guides at the edge are still visible by clamping to valid range\n        if 0 &lt;= pixel_x &lt;= img.width - 1:\n            x_pixel = int(min(pixel_x, img.width - 1))\n            draw.line([(x_pixel, 0), (x_pixel, img.height - 1)], fill=(0, 0, 255, 200), width=2)\n\n    # Draw horizontal guides (red)\n    for y_coord in self.horizontal:\n        # Adjust coordinate if we're showing on a region\n        adjusted_y = y_coord - region_top\n        pixel_y = adjusted_y * scale_y\n        # Ensure guides at the edge are still visible by clamping to valid range\n        if 0 &lt;= pixel_y &lt;= img.height - 1:\n            y_pixel = int(min(pixel_y, img.height - 1))\n            draw.line([(0, y_pixel), (img.width - 1, y_pixel)], fill=(255, 0, 0, 200), width=2)\n\n    return img\n</code></pre> <code>natural_pdf.Guides.snap_to_whitespace(axis='vertical', min_gap=10.0, detection_method='pixels', threshold='auto', on_no_snap='warn')</code> <p>Snap guides to nearby whitespace gaps (troughs) using optimal assignment. Modifies this Guides object in place.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>str</code> <p>Direction to snap ('vertical' or 'horizontal')</p> <code>'vertical'</code> <code>min_gap</code> <code>float</code> <p>Minimum gap size to consider as a valid trough</p> <code>10.0</code> <code>detection_method</code> <code>str</code> <p>Method for detecting troughs:             'pixels' - use pixel-based density analysis (default)             'text' - use text element spacing analysis</p> <code>'pixels'</code> <code>threshold</code> <code>Union[float, str]</code> <p>Threshold for what counts as a trough:       - float (0.0-1.0): areas with this fraction or less of max density count as troughs       - 'auto': automatically find threshold that creates enough troughs for guides</p> <code>'auto'</code> <code>on_no_snap</code> <code>str</code> <p>Action when snapping fails ('warn', 'ignore', 'raise')</p> <code>'warn'</code> <p>Returns:</p> Type Description <code>Guides</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def snap_to_whitespace(\n    self,\n    axis: str = \"vertical\",\n    min_gap: float = 10.0,\n    detection_method: str = \"pixels\",  # 'pixels' or 'text'\n    threshold: Union[\n        float, str\n    ] = \"auto\",  # threshold for what counts as a trough (0.0-1.0) or 'auto'\n    on_no_snap: str = \"warn\",\n) -&gt; \"Guides\":\n    \"\"\"\n    Snap guides to nearby whitespace gaps (troughs) using optimal assignment.\n    Modifies this Guides object in place.\n\n    Args:\n        axis: Direction to snap ('vertical' or 'horizontal')\n        min_gap: Minimum gap size to consider as a valid trough\n        detection_method: Method for detecting troughs:\n                        'pixels' - use pixel-based density analysis (default)\n                        'text' - use text element spacing analysis\n        threshold: Threshold for what counts as a trough:\n                  - float (0.0-1.0): areas with this fraction or less of max density count as troughs\n                  - 'auto': automatically find threshold that creates enough troughs for guides\n        on_no_snap: Action when snapping fails ('warn', 'ignore', 'raise')\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    if not self.context:\n        logger.warning(\"No context available for whitespace detection\")\n        return self\n\n    # Handle FlowRegion case - collect all text elements across regions\n    if self.is_flow_region:\n        all_text_elements = []\n        region_bounds = {}\n\n        for region in self.context.constituent_regions:\n            # Get text elements from this region\n            if hasattr(region, \"find_all\"):\n                try:\n                    text_elements = region.find_all(\"text\", apply_exclusions=False)\n                    elements = (\n                        text_elements.elements\n                        if hasattr(text_elements, \"elements\")\n                        else text_elements\n                    )\n                    all_text_elements.extend(elements)\n\n                    # Store bounds for each region\n                    if hasattr(region, \"bbox\"):\n                        region_bounds[region] = region.bbox\n                    elif hasattr(region, \"x0\"):\n                        region_bounds[region] = (\n                            region.x0,\n                            region.top,\n                            region.x1,\n                            region.bottom,\n                        )\n                except Exception as e:\n                    logger.warning(f\"Error getting text elements from region: {e}\")\n\n        if not all_text_elements:\n            logger.warning(\n                \"No text elements found across flow regions for whitespace detection\"\n            )\n            return self\n\n        # Find whitespace gaps across all regions\n        if axis == \"vertical\":\n            gaps = self._find_vertical_whitespace_gaps(all_text_elements, min_gap, threshold)\n            # Get all vertical guides across regions\n            all_guides = []\n            guide_to_region_map = {}  # Map guide coordinate to its original list of regions\n            for coord, region in self._unified_vertical:\n                all_guides.append(coord)\n                guide_to_region_map.setdefault(coord, []).append(region)\n\n            if gaps and all_guides:\n                # Keep a copy of original guides to maintain mapping\n                original_guides = all_guides.copy()\n\n                # Snap guides to gaps\n                self._snap_guides_to_gaps(all_guides, gaps, axis)\n\n                # Update the unified view with snapped positions\n                self._unified_vertical = []\n                for i, new_coord in enumerate(all_guides):\n                    # Find the original region for this guide using the original position\n                    original_coord = original_guides[i]\n                    # A guide might be associated with multiple regions, add them all\n                    regions = guide_to_region_map.get(original_coord, [])\n                    for region in regions:\n                        self._unified_vertical.append((new_coord, region))\n\n                # Update individual region guides\n                for region in self._flow_guides:\n                    region_verticals = []\n                    for coord, r in self._unified_vertical:\n                        if r == region:\n                            region_verticals.append(coord)\n                    self._flow_guides[region] = (\n                        sorted(list(set(region_verticals))),  # Deduplicate here\n                        self._flow_guides[region][1],\n                    )\n\n                # Invalidate cache\n                self._vertical_cache = None\n\n        elif axis == \"horizontal\":\n            gaps = self._find_horizontal_whitespace_gaps(all_text_elements, min_gap, threshold)\n            # Get all horizontal guides across regions\n            all_guides = []\n            guide_to_region_map = {}  # Map guide coordinate to its original list of regions\n            for coord, region in self._unified_horizontal:\n                all_guides.append(coord)\n                guide_to_region_map.setdefault(coord, []).append(region)\n\n            if gaps and all_guides:\n                # Keep a copy of original guides to maintain mapping\n                original_guides = all_guides.copy()\n\n                # Snap guides to gaps\n                self._snap_guides_to_gaps(all_guides, gaps, axis)\n\n                # Update the unified view with snapped positions\n                self._unified_horizontal = []\n                for i, new_coord in enumerate(all_guides):\n                    # Find the original region for this guide using the original position\n                    original_coord = original_guides[i]\n                    regions = guide_to_region_map.get(original_coord, [])\n                    for region in regions:\n                        self._unified_horizontal.append((new_coord, region))\n\n                # Update individual region guides\n                for region in self._flow_guides:\n                    region_horizontals = []\n                    for coord, r in self._unified_horizontal:\n                        if r == region:\n                            region_horizontals.append(coord)\n                    self._flow_guides[region] = (\n                        self._flow_guides[region][0],\n                        sorted(list(set(region_horizontals))),  # Deduplicate here\n                    )\n\n                # Invalidate cache\n                self._horizontal_cache = None\n\n        else:\n            raise ValueError(\"axis must be 'vertical' or 'horizontal'\")\n\n        return self\n\n    # Original single-region logic\n    # Get elements for trough detection\n    text_elements = self._get_text_elements()\n    if not text_elements:\n        logger.warning(\"No text elements found for whitespace detection\")\n        return self\n\n    if axis == \"vertical\":\n        gaps = self._find_vertical_whitespace_gaps(text_elements, min_gap, threshold)\n        if gaps:\n            self._snap_guides_to_gaps(self.vertical.data, gaps, axis)\n    elif axis == \"horizontal\":\n        gaps = self._find_horizontal_whitespace_gaps(text_elements, min_gap, threshold)\n        if gaps:\n            self._snap_guides_to_gaps(self.horizontal.data, gaps, axis)\n    else:\n        raise ValueError(\"axis must be 'vertical' or 'horizontal'\")\n\n    # Ensure all coordinates are Python floats (not numpy types)\n    self.vertical.data[:] = [float(x) for x in self.vertical.data]\n    self.horizontal.data[:] = [float(y) for y in self.horizontal.data]\n\n    return self\n</code></pre> <code>natural_pdf.Guides.to_absolute(bounds)</code> <p>Convert relative coordinates to absolute coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>bounds</code> <code>Tuple[float, float, float, float]</code> <p>Target bounding box (x0, y0, x1, y1)</p> required <p>Returns:</p> Type Description <code>Guides</code> <p>New Guides object with absolute coordinates</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def to_absolute(self, bounds: Tuple[float, float, float, float]) -&gt; \"Guides\":\n    \"\"\"\n    Convert relative coordinates to absolute coordinates.\n\n    Args:\n        bounds: Target bounding box (x0, y0, x1, y1)\n\n    Returns:\n        New Guides object with absolute coordinates\n    \"\"\"\n    if not self.relative:\n        return self  # Already absolute\n\n    x0, y0, x1, y1 = bounds\n    width = x1 - x0\n    height = y1 - y0\n\n    abs_verticals = [x0 + x * width for x in self.vertical]\n    abs_horizontals = [y0 + y * height for y in self.horizontal]\n\n    return Guides(\n        verticals=abs_verticals,\n        horizontals=abs_horizontals,\n        context=self.context,\n        bounds=bounds,\n        relative=False,\n    )\n</code></pre> <code>natural_pdf.Guides.to_dict()</code> <p>Convert to dictionary format suitable for pdfplumber table_settings.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with explicit_vertical_lines and explicit_horizontal_lines</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert to dictionary format suitable for pdfplumber table_settings.\n\n    Returns:\n        Dictionary with explicit_vertical_lines and explicit_horizontal_lines\n    \"\"\"\n    return {\n        \"explicit_vertical_lines\": self.vertical,\n        \"explicit_horizontal_lines\": self.horizontal,\n    }\n</code></pre> <code>natural_pdf.Guides.to_relative()</code> <p>Convert absolute coordinates to relative (0-1) coordinates.</p> <p>Returns:</p> Type Description <code>Guides</code> <p>New Guides object with relative coordinates</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>def to_relative(self) -&gt; \"Guides\":\n    \"\"\"\n    Convert absolute coordinates to relative (0-1) coordinates.\n\n    Returns:\n        New Guides object with relative coordinates\n    \"\"\"\n    if self.relative:\n        return self  # Already relative\n\n    if not self.bounds:\n        raise ValueError(\"Cannot convert to relative without bounds\")\n\n    x0, y0, x1, y1 = self.bounds\n    width = x1 - x0\n    height = y1 - y0\n\n    rel_verticals = [(x - x0) / width for x in self.vertical]\n    rel_horizontals = [(y - y0) / height for y in self.horizontal]\n\n    return Guides(\n        verticals=rel_verticals,\n        horizontals=rel_horizontals,\n        context=self.context,\n        bounds=(0, 0, 1, 1),\n        relative=True,\n    )\n</code></pre>"},{"location":"api/#natural_pdf.Guides.divide--divide-into-3-columns","title":"Divide into 3 columns","text":"<p>guides = Guides.divide(page, cols=3)</p>"},{"location":"api/#natural_pdf.Guides.divide--divide-into-5-rows","title":"Divide into 5 rows","text":"<p>guides = Guides.divide(region, rows=5)</p>"},{"location":"api/#natural_pdf.Guides.divide--divide-both-axes","title":"Divide both axes","text":"<p>guides = Guides.divide(page, cols=3, rows=5)</p> Source code in <code>natural_pdf/analyzers/guides.py</code> <pre><code>@classmethod\ndef divide(\n    cls,\n    obj: Union[\"Page\", \"Region\", Tuple[float, float, float, float]],\n    n: Optional[int] = None,\n    cols: Optional[int] = None,\n    rows: Optional[int] = None,\n    axis: Literal[\"vertical\", \"horizontal\", \"both\"] = \"both\",\n) -&gt; \"Guides\":\n    \"\"\"\n    Create guides by evenly dividing an object.\n\n    Args:\n        obj: Object to divide (Page, Region, or bbox tuple)\n        n: Number of divisions (creates n+1 guides). Used if cols/rows not specified.\n        cols: Number of columns (creates cols+1 vertical guides)\n        rows: Number of rows (creates rows+1 horizontal guides)\n        axis: Which axis to divide along\n\n    Returns:\n        New Guides object with evenly spaced lines\n\n    Examples:\n        # Divide into 3 columns\n        guides = Guides.divide(page, cols=3)\n\n        # Divide into 5 rows\n        guides = Guides.divide(region, rows=5)\n\n        # Divide both axes\n        guides = Guides.divide(page, cols=3, rows=5)\n    \"\"\"\n    # Extract bounds from object\n    if isinstance(obj, tuple) and len(obj) == 4:\n        bounds = obj\n        context = None\n    else:\n        context = obj\n        if hasattr(obj, \"bbox\"):\n            bounds = obj.bbox\n        elif hasattr(obj, \"x0\"):\n            bounds = (obj.x0, obj.top, obj.x1, obj.bottom)\n        else:\n            bounds = (0, 0, obj.width, obj.height)\n\n    x0, y0, x1, y1 = bounds\n    verticals = []\n    horizontals = []\n\n    # Handle vertical guides\n    if axis in (\"vertical\", \"both\"):\n        n_vertical = cols + 1 if cols is not None else (n + 1 if n is not None else 0)\n        if n_vertical &gt; 0:\n            for i in range(n_vertical):\n                x = x0 + (x1 - x0) * i / (n_vertical - 1)\n                verticals.append(float(x))\n\n    # Handle horizontal guides\n    if axis in (\"horizontal\", \"both\"):\n        n_horizontal = rows + 1 if rows is not None else (n + 1 if n is not None else 0)\n        if n_horizontal &gt; 0:\n            for i in range(n_horizontal):\n                y = y0 + (y1 - y0) * i / (n_horizontal - 1)\n                horizontals.append(float(y))\n\n    return cls(verticals=verticals, horizontals=horizontals, context=context, bounds=bounds)\n</code></pre>"},{"location":"api/#natural_pdf.Options","title":"<code>natural_pdf.Options</code>","text":"<p>Global options for natural-pdf, similar to pandas options.</p> Source code in <code>natural_pdf/__init__.py</code> <pre><code>class Options:\n    \"\"\"Global options for natural-pdf, similar to pandas options.\"\"\"\n\n    def __init__(self):\n        # Image rendering defaults\n        self.image = ConfigSection(width=None, resolution=150)\n\n        # OCR defaults\n        self.ocr = ConfigSection(engine=\"easyocr\", languages=[\"en\"], min_confidence=0.5)\n\n        # Text extraction defaults (empty for now)\n        self.text = ConfigSection()\n</code></pre>"},{"location":"api/#natural_pdf.PDF","title":"<code>natural_pdf.PDF</code>","text":"<p>               Bases: <code>TextMixin</code>, <code>ExtractionMixin</code>, <code>ExportMixin</code>, <code>ClassificationMixin</code>, <code>Visualizable</code></p> <p>Enhanced PDF wrapper built on top of pdfplumber.</p> <p>This class provides a fluent interface for working with PDF documents, with improved selection, navigation, and extraction capabilities. It integrates OCR, layout analysis, and AI-powered data extraction features while maintaining compatibility with the underlying pdfplumber API.</p> <p>The PDF class supports loading from files, URLs, or streams, and provides spatial navigation, element selection with CSS-like selectors, and advanced document processing workflows including multi-page content flows.</p> <p>Attributes:</p> Name Type Description <code>pages</code> <code>PageCollection</code> <p>Lazy-loaded list of Page objects for document pages.</p> <code>path</code> <p>Resolved path to the PDF file or source identifier.</p> <code>source_path</code> <p>Original path, URL, or stream identifier provided during initialization.</p> <code>highlighter</code> <p>Service for rendering highlighted visualizations of document content.</p> Example <p>Basic usage: <pre><code>import natural_pdf as npdf\n\npdf = npdf.PDF(\"document.pdf\")\npage = pdf.pages[0]\ntext_elements = page.find_all('text:contains(\"Summary\")')\n</code></pre></p> <p>Advanced usage with OCR: <pre><code>pdf = npdf.PDF(\"scanned_document.pdf\")\npdf.apply_ocr(engine=\"easyocr\", resolution=144)\ntables = pdf.pages[0].find_all('table')\n</code></pre></p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>class PDF(TextMixin, ExtractionMixin, ExportMixin, ClassificationMixin, Visualizable):\n    \"\"\"Enhanced PDF wrapper built on top of pdfplumber.\n\n    This class provides a fluent interface for working with PDF documents,\n    with improved selection, navigation, and extraction capabilities. It integrates\n    OCR, layout analysis, and AI-powered data extraction features while maintaining\n    compatibility with the underlying pdfplumber API.\n\n    The PDF class supports loading from files, URLs, or streams, and provides\n    spatial navigation, element selection with CSS-like selectors, and advanced\n    document processing workflows including multi-page content flows.\n\n    Attributes:\n        pages: Lazy-loaded list of Page objects for document pages.\n        path: Resolved path to the PDF file or source identifier.\n        source_path: Original path, URL, or stream identifier provided during initialization.\n        highlighter: Service for rendering highlighted visualizations of document content.\n\n    Example:\n        Basic usage:\n        ```python\n        import natural_pdf as npdf\n\n        pdf = npdf.PDF(\"document.pdf\")\n        page = pdf.pages[0]\n        text_elements = page.find_all('text:contains(\"Summary\")')\n        ```\n\n        Advanced usage with OCR:\n        ```python\n        pdf = npdf.PDF(\"scanned_document.pdf\")\n        pdf.apply_ocr(engine=\"easyocr\", resolution=144)\n        tables = pdf.pages[0].find_all('table')\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        path_or_url_or_stream,\n        reading_order: bool = True,\n        font_attrs: Optional[List[str]] = None,\n        keep_spaces: bool = True,\n        text_tolerance: Optional[dict] = None,\n        auto_text_tolerance: bool = True,\n        text_layer: bool = True,\n    ):\n        \"\"\"Initialize the enhanced PDF object.\n\n        Args:\n            path_or_url_or_stream: Path to the PDF file (str/Path), a URL (str),\n                or a file-like object (stream). URLs must start with 'http://' or 'https://'.\n            reading_order: If True, use natural reading order for text extraction.\n                Defaults to True.\n            font_attrs: List of font attributes for grouping characters into words.\n                Common attributes include ['fontname', 'size']. Defaults to None.\n            keep_spaces: If True, include spaces in word elements during text extraction.\n                Defaults to True.\n            text_tolerance: PDFplumber-style tolerance settings for text grouping.\n                Dictionary with keys like 'x_tolerance', 'y_tolerance'. Defaults to None.\n            auto_text_tolerance: If True, automatically scale text tolerance based on\n                font size and document characteristics. Defaults to True.\n            text_layer: If True, preserve existing text layer from the PDF. If False,\n                removes all existing text elements during initialization, useful for\n                OCR-only workflows. Defaults to True.\n\n        Raises:\n            TypeError: If path_or_url_or_stream is not a valid type.\n            IOError: If the PDF file cannot be opened or read.\n            ValueError: If URL download fails.\n\n        Example:\n            ```python\n            # From file path\n            pdf = npdf.PDF(\"document.pdf\")\n\n            # From URL\n            pdf = npdf.PDF(\"https://example.com/document.pdf\")\n\n            # From stream\n            with open(\"document.pdf\", \"rb\") as f:\n                pdf = npdf.PDF(f)\n\n            # With custom settings\n            pdf = npdf.PDF(\"document.pdf\",\n                          reading_order=False,\n                          text_layer=False,  # For OCR-only processing\n                          font_attrs=['fontname', 'size', 'flags'])\n            ```\n        \"\"\"\n        self._original_path_or_stream = path_or_url_or_stream\n        self._temp_file = None\n        self._resolved_path = None\n        self._is_stream = False\n        self._text_layer = text_layer\n        stream_to_open = None\n\n        if hasattr(path_or_url_or_stream, \"read\"):  # Check if it's file-like\n            logger.info(\"Initializing PDF from in-memory stream.\")\n            self._is_stream = True\n            self._resolved_path = None  # No resolved file path for streams\n            self.source_path = \"&lt;stream&gt;\"  # Identifier for source\n            self.path = self.source_path  # Use source identifier as path for streams\n            stream_to_open = path_or_url_or_stream\n            try:\n                if hasattr(path_or_url_or_stream, \"read\"):\n                    # If caller provided an in-memory binary stream, capture bytes for potential re-export\n                    current_pos = path_or_url_or_stream.tell()\n                    path_or_url_or_stream.seek(0)\n                    self._original_bytes = path_or_url_or_stream.read()\n                    path_or_url_or_stream.seek(current_pos)\n            except Exception:\n                pass\n        elif isinstance(path_or_url_or_stream, (str, Path)):\n            path_or_url = str(path_or_url_or_stream)\n            self.source_path = path_or_url  # Store original path/URL as source\n            is_url = path_or_url.startswith(\"http://\") or path_or_url.startswith(\"https://\")\n\n            if is_url:\n                logger.info(f\"Downloading PDF from URL: {path_or_url}\")\n                try:\n                    with urllib.request.urlopen(path_or_url) as response:\n                        data = response.read()\n                    # Load directly into an in-memory buffer \u2014 no temp file needed\n                    buffer = io.BytesIO(data)\n                    buffer.seek(0)\n                    self._temp_file = None  # No on-disk temp file\n                    self._resolved_path = path_or_url  # For repr / get_id purposes\n                    stream_to_open = buffer  # pdfplumber accepts file-like objects\n                except Exception as e:\n                    logger.error(f\"Failed to download PDF from URL: {e}\")\n                    raise ValueError(f\"Failed to download PDF from URL: {e}\")\n            else:\n                self._resolved_path = str(Path(path_or_url).resolve())  # Resolve local paths\n                stream_to_open = self._resolved_path\n            self.path = self._resolved_path  # Use resolved path for file-based PDFs\n        else:\n            raise TypeError(\n                f\"Invalid input type: {type(path_or_url_or_stream)}. \"\n                f\"Expected path (str/Path), URL (str), or file-like object.\"\n            )\n\n        logger.info(f\"Opening PDF source: {self.source_path}\")\n        logger.debug(\n            f\"Parameters: reading_order={reading_order}, font_attrs={font_attrs}, keep_spaces={keep_spaces}\"\n        )\n\n        try:\n            self._pdf = pdfplumber.open(stream_to_open)\n        except Exception as e:\n            logger.error(f\"Failed to open PDF: {e}\", exc_info=True)\n            self.close()  # Attempt cleanup if opening fails\n            raise IOError(f\"Failed to open PDF source: {self.source_path}\") from e\n\n        # Store configuration used for initialization\n        self._reading_order = reading_order\n        self._config = {\"keep_spaces\": keep_spaces}\n        self._font_attrs = font_attrs\n\n        self._ocr_manager = OCRManager() if OCRManager else None\n        self._layout_manager = LayoutManager() if LayoutManager else None\n        self.highlighter = HighlightingService(self)\n        # self._classification_manager_instance = ClassificationManager() # Removed this line\n        self._manager_registry = {}\n\n        # Lazily instantiate pages only when accessed\n        self._pages = _LazyPageList(\n            self, self._pdf, font_attrs=font_attrs, load_text=self._text_layer\n        )\n\n        self._element_cache = {}\n        self._exclusions = []\n        self._regions = []\n\n        logger.info(f\"PDF '{self.source_path}' initialized with {len(self._pages)} pages.\")\n\n        self._initialize_managers()\n        self._initialize_highlighter()\n\n        # Remove text layer if requested\n        if not self._text_layer:\n            logger.info(\"Removing text layer as requested (text_layer=False)\")\n            # Text layer is not loaded when text_layer=False, so no need to remove\n            pass\n\n        # Analysis results accessed via self.analyses property (see below)\n\n        # --- Automatic cleanup when object is garbage-collected ---\n        self._finalizer = weakref.finalize(\n            self,\n            PDF._finalize_cleanup,\n            self._pdf,\n            getattr(self, \"_temp_file\", None),\n            getattr(self, \"_is_stream\", False),\n        )\n\n        # --- Text tolerance settings ------------------------------------\n        # Users can pass pdfplumber-style keys (x_tolerance, x_tolerance_ratio,\n        # y_tolerance, etc.) via *text_tolerance*.  We also keep a flag that\n        # enables automatic tolerance scaling when explicit values are not\n        # supplied.\n        self._config[\"auto_text_tolerance\"] = bool(auto_text_tolerance)\n        if text_tolerance:\n            # Only copy recognised primitives (numbers / None); ignore junk.\n            allowed = {\n                \"x_tolerance\",\n                \"x_tolerance_ratio\",\n                \"y_tolerance\",\n                \"keep_blank_chars\",  # passthrough convenience\n            }\n            for k, v in text_tolerance.items():\n                if k in allowed:\n                    self._config[k] = v\n\n    def _initialize_managers(self):\n        \"\"\"Set up manager factories for lazy instantiation.\"\"\"\n        # Store factories/classes for each manager key\n        self._manager_factories = dict(DEFAULT_MANAGERS)\n        self._managers = {}  # Will hold instantiated managers\n\n    def get_manager(self, key: str) -&gt; Any:\n        \"\"\"Retrieve a manager instance by its key, instantiating it lazily if needed.\n\n        Managers are specialized components that handle specific functionality like\n        classification, structured data extraction, or OCR processing. They are\n        instantiated on-demand to minimize memory usage and startup time.\n\n        Args:\n            key: The manager key to retrieve. Common keys include 'classification'\n                and 'structured_data'.\n\n        Returns:\n            The manager instance for the specified key.\n\n        Raises:\n            KeyError: If no manager is registered for the given key.\n            RuntimeError: If the manager failed to initialize.\n\n        Example:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n            classification_mgr = pdf.get_manager('classification')\n            structured_data_mgr = pdf.get_manager('structured_data')\n            ```\n        \"\"\"\n        # Check if already instantiated\n        if key in self._managers:\n            manager_instance = self._managers[key]\n            if manager_instance is None:\n                raise RuntimeError(f\"Manager '{key}' failed to initialize previously.\")\n            return manager_instance\n\n        # Not instantiated yet: get factory/class\n        if not hasattr(self, \"_manager_factories\") or key not in self._manager_factories:\n            raise KeyError(\n                f\"No manager registered for key '{key}'. Available: {list(getattr(self, '_manager_factories', {}).keys())}\"\n            )\n        factory_or_class = self._manager_factories[key]\n        try:\n            resolved = factory_or_class\n            # If it's a callable that's not a class, call it to get the class/instance\n            if not isinstance(resolved, type) and callable(resolved):\n                resolved = resolved()\n            # If it's a class, instantiate it\n            if isinstance(resolved, type):\n                instance = resolved()\n            else:\n                instance = resolved  # Already an instance\n            self._managers[key] = instance\n            return instance\n        except Exception as e:\n            logger.error(f\"Failed to initialize manager for key '{key}': {e}\")\n            self._managers[key] = None\n            raise RuntimeError(f\"Manager '{key}' failed to initialize: {e}\") from e\n\n    def _initialize_highlighter(self):\n        pass\n\n    @property\n    def metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"Access PDF metadata as a dictionary.\n\n        Returns document metadata such as title, author, creation date, and other\n        properties embedded in the PDF file. The exact keys available depend on\n        what metadata was included when the PDF was created.\n\n        Returns:\n            Dictionary containing PDF metadata. Common keys include 'Title',\n            'Author', 'Subject', 'Creator', 'Producer', 'CreationDate', and\n            'ModDate'. May be empty if no metadata is available.\n\n        Example:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n            print(pdf.metadata.get('Title', 'No title'))\n            print(f\"Created: {pdf.metadata.get('CreationDate')}\")\n            ```\n        \"\"\"\n        return self._pdf.metadata\n\n    @property\n    def pages(self) -&gt; \"PageCollection\":\n        \"\"\"Access pages as a PageCollection object.\n\n        Provides access to individual pages of the PDF document through a\n        collection interface that supports indexing, slicing, and iteration.\n        Pages are lazy-loaded to minimize memory usage.\n\n        Returns:\n            PageCollection object that provides list-like access to PDF pages.\n\n        Raises:\n            AttributeError: If PDF pages are not yet initialized.\n\n        Example:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n\n            # Access individual pages\n            first_page = pdf.pages[0]\n            last_page = pdf.pages[-1]\n\n            # Slice pages\n            first_three = pdf.pages[0:3]\n\n            # Iterate over pages\n            for page in pdf.pages:\n                print(f\"Page {page.index} has {len(page.chars)} characters\")\n            ```\n        \"\"\"\n        from natural_pdf.core.page_collection import PageCollection\n\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n        return PageCollection(self._pages)\n\n    def clear_exclusions(self) -&gt; \"PDF\":\n        \"\"\"Clear all exclusion functions from the PDF.\n\n        Removes all previously added exclusion functions that were used to filter\n        out unwanted content (like headers, footers, or administrative text) from\n        text extraction and analysis operations.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            AttributeError: If PDF pages are not yet initialized.\n\n        Example:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n            pdf.add_exclusion(lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above())\n\n            # Later, remove all exclusions\n            pdf.clear_exclusions()\n            ```\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        self._exclusions = []\n\n        # Clear exclusions only from already-created (cached) pages to avoid forcing page creation\n        for i in range(len(self._pages)):\n            if self._pages._cache[i] is not None:  # Only clear from existing pages\n                try:\n                    self._pages._cache[i].clear_exclusions()\n                except Exception as e:\n                    logger.warning(f\"Failed to clear exclusions from existing page {i}: {e}\")\n        return self\n\n    def add_exclusion(self, exclusion_func, label: str = None) -&gt; \"PDF\":\n        \"\"\"Add an exclusion function to the PDF.\n\n        Exclusion functions define regions of each page that should be ignored during\n        text extraction and analysis operations. This is useful for filtering out headers,\n        footers, watermarks, or other administrative content that shouldn't be included\n        in the main document processing.\n\n        Args:\n            exclusion_func: A function that takes a Page object and returns a Region\n                to exclude from processing, or None if no exclusion should be applied\n                to that page. The function is called once per page.\n            label: Optional descriptive label for this exclusion rule, useful for\n                debugging and identification.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            AttributeError: If PDF pages are not yet initialized.\n\n        Example:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n\n            # Exclude headers (top 50 points of each page)\n            pdf.add_exclusion(\n                lambda page: page.region(0, 0, page.width, 50),\n                label=\"header_exclusion\"\n            )\n\n            # Exclude any text containing \"CONFIDENTIAL\"\n            pdf.add_exclusion(\n                lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above(include_source=True)\n                if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n                label=\"confidential_exclusion\"\n            )\n\n            # Chain multiple exclusions\n            pdf.add_exclusion(header_func).add_exclusion(footer_func)\n            ```\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        # ------------------------------------------------------------------\n        # Support selector strings and ElementCollection objects directly.\n        # Store exclusion and apply only to already-created pages.\n        # ------------------------------------------------------------------\n        from natural_pdf.elements.element_collection import ElementCollection  # local import\n\n        if isinstance(exclusion_func, str) or isinstance(exclusion_func, ElementCollection):\n            # Store for bookkeeping and lazy application\n            self._exclusions.append((exclusion_func, label))\n\n            # Apply only to already-created (cached) pages to avoid forcing page creation\n            for i in range(len(self._pages)):\n                if self._pages._cache[i] is not None:  # Only apply to existing pages\n                    try:\n                        self._pages._cache[i].add_exclusion(exclusion_func, label=label)\n                    except Exception as e:\n                        logger.warning(f\"Failed to apply exclusion to existing page {i}: {e}\")\n            return self\n\n        # Fallback to original callable / Region behaviour ------------------\n        exclusion_data = (exclusion_func, label)\n        self._exclusions.append(exclusion_data)\n\n        # Apply only to already-created (cached) pages to avoid forcing page creation\n        for i in range(len(self._pages)):\n            if self._pages._cache[i] is not None:  # Only apply to existing pages\n                try:\n                    self._pages._cache[i].add_exclusion(exclusion_func, label=label)\n                except Exception as e:\n                    logger.warning(f\"Failed to apply exclusion to existing page {i}: {e}\")\n\n        return self\n\n    def apply_ocr(\n        self,\n        engine: Optional[str] = None,\n        languages: Optional[List[str]] = None,\n        min_confidence: Optional[float] = None,\n        device: Optional[str] = None,\n        resolution: Optional[int] = None,\n        apply_exclusions: bool = True,\n        detect_only: bool = False,\n        replace: bool = True,\n        options: Optional[Any] = None,\n        pages: Optional[Union[Iterable[int], range, slice]] = None,\n    ) -&gt; \"PDF\":\n        \"\"\"Apply OCR to specified pages of the PDF using batch processing.\n\n        Performs optical character recognition on the specified pages, converting\n        image-based text into searchable and extractable text elements. This method\n        supports multiple OCR engines and provides batch processing for efficiency.\n\n        Args:\n            engine: Name of the OCR engine to use. Supported engines include\n                'easyocr' (default), 'surya', 'paddle', and 'doctr'. If None,\n                uses the global default from natural_pdf.options.ocr.engine.\n            languages: List of language codes for OCR recognition (e.g., ['en', 'es']).\n                If None, uses the global default from natural_pdf.options.ocr.languages.\n            min_confidence: Minimum confidence threshold (0.0-1.0) for accepting\n                OCR results. Text with lower confidence will be filtered out.\n                If None, uses the global default.\n            device: Device to run OCR on ('cpu', 'cuda', 'mps'). Engine-specific\n                availability varies. If None, uses engine defaults.\n            resolution: DPI resolution for rendering pages to images before OCR.\n                Higher values improve accuracy but increase processing time and memory.\n                Typical values: 150 (fast), 300 (balanced), 600 (high quality).\n            apply_exclusions: If True, mask excluded regions before OCR to prevent\n                processing of headers, footers, or other unwanted content.\n            detect_only: If True, only detect text bounding boxes without performing\n                character recognition. Useful for layout analysis workflows.\n            replace: If True, replace any existing OCR elements on the pages.\n                If False, append new OCR results to existing elements.\n            options: Engine-specific options object (e.g., EasyOCROptions, SuryaOptions).\n                Allows fine-tuning of engine behavior beyond common parameters.\n            pages: Page indices to process. Can be:\n                - None: Process all pages\n                - slice: Process a range of pages (e.g., slice(0, 10))\n                - Iterable[int]: Process specific page indices (e.g., [0, 2, 5])\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If invalid page index is provided.\n            TypeError: If pages parameter has invalid type.\n            RuntimeError: If OCR engine is not available or fails.\n\n        Example:\n            ```python\n            pdf = npdf.PDF(\"scanned_document.pdf\")\n\n            # Basic OCR on all pages\n            pdf.apply_ocr()\n\n            # High-quality OCR with specific settings\n            pdf.apply_ocr(\n                engine='easyocr',\n                languages=['en', 'es'],\n                resolution=300,\n                min_confidence=0.8\n            )\n\n            # OCR specific pages only\n            pdf.apply_ocr(pages=[0, 1, 2])  # First 3 pages\n            pdf.apply_ocr(pages=slice(5, 10))  # Pages 5-9\n\n            # Detection-only workflow for layout analysis\n            pdf.apply_ocr(detect_only=True, resolution=150)\n            ```\n\n        Note:\n            OCR processing can be time and memory intensive, especially at high\n            resolutions. Consider using exclusions to mask unwanted regions and\n            processing pages in batches for large documents.\n        \"\"\"\n        if not self._ocr_manager:\n            logger.error(\"OCRManager not available. Cannot apply OCR.\")\n            return self\n\n        # Apply global options as defaults, but allow explicit parameters to override\n        import natural_pdf\n\n        # Use global OCR options if parameters are not explicitly set\n        if engine is None:\n            engine = natural_pdf.options.ocr.engine\n        if languages is None:\n            languages = natural_pdf.options.ocr.languages\n        if min_confidence is None:\n            min_confidence = natural_pdf.options.ocr.min_confidence\n        if device is None:\n            pass  # No default device in options.ocr anymore\n\n        thread_id = threading.current_thread().name\n        logger.debug(f\"[{thread_id}] PDF.apply_ocr starting for {self.path}\")\n\n        target_pages = []\n\n        target_pages = []\n        if pages is None:\n            target_pages = self._pages\n        elif isinstance(pages, slice):\n            target_pages = self._pages[pages]\n        elif hasattr(pages, \"__iter__\"):\n            try:\n                target_pages = [self._pages[i] for i in pages]\n            except IndexError:\n                raise ValueError(\"Invalid page index provided in 'pages' iterable.\")\n            except TypeError:\n                raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n        else:\n            raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n\n        if not target_pages:\n            logger.warning(\"No pages selected for OCR processing.\")\n            return self\n\n        page_numbers = [p.number for p in target_pages]\n        logger.info(f\"Applying batch OCR to pages: {page_numbers}...\")\n\n        final_resolution = resolution or getattr(self, \"_config\", {}).get(\"resolution\", 150)\n        logger.debug(f\"Using OCR image resolution: {final_resolution} DPI\")\n\n        images_pil = []\n        page_image_map = []\n        logger.info(f\"[{thread_id}] Rendering {len(target_pages)} pages...\")\n        failed_page_num = \"unknown\"\n        render_start_time = time.monotonic()\n\n        try:\n            for i, page in enumerate(tqdm(target_pages, desc=\"Rendering pages\", leave=False)):\n                failed_page_num = page.number\n                logger.debug(f\"  Rendering page {page.number} (index {page.index})...\")\n                to_image_kwargs = {\n                    \"resolution\": final_resolution,\n                    \"include_highlights\": False,\n                    \"exclusions\": \"mask\" if apply_exclusions else None,\n                }\n                # Use render() for clean image without highlights\n                img = page.render(resolution=final_resolution)\n                if img is None:\n                    logger.error(f\"  Failed to render page {page.number} to image.\")\n                    continue\n                images_pil.append(img)\n                page_image_map.append((page, img))\n        except Exception as e:\n            logger.error(f\"Failed to render pages for batch OCR: {e}\")\n            logger.error(f\"Failed to render pages for batch OCR: {e}\")\n            raise RuntimeError(f\"Failed to render page {failed_page_num} for OCR.\") from e\n\n        render_end_time = time.monotonic()\n        logger.debug(\n            f\"[{thread_id}] Finished rendering {len(images_pil)} images (Duration: {render_end_time - render_start_time:.2f}s)\"\n        )\n        logger.debug(\n            f\"[{thread_id}] Finished rendering {len(images_pil)} images (Duration: {render_end_time - render_start_time:.2f}s)\"\n        )\n\n        if not images_pil or not page_image_map:\n            logger.error(\"No images were successfully rendered for batch OCR.\")\n            return self\n\n        manager_args = {\n            \"images\": images_pil,\n            \"engine\": engine,\n            \"languages\": languages,\n            \"min_confidence\": min_confidence,\n            \"min_confidence\": min_confidence,\n            \"device\": device,\n            \"options\": options,\n            \"detect_only\": detect_only,\n        }\n        manager_args = {k: v for k, v in manager_args.items() if v is not None}\n\n        ocr_call_args = {k: v for k, v in manager_args.items() if k != \"images\"}\n        logger.info(f\"[{thread_id}] Calling OCR Manager with args: {ocr_call_args}...\")\n        logger.info(f\"[{thread_id}] Calling OCR Manager with args: {ocr_call_args}...\")\n        ocr_start_time = time.monotonic()\n\n        batch_results = self._ocr_manager.apply_ocr(**manager_args)\n\n        if not isinstance(batch_results, list) or len(batch_results) != len(images_pil):\n            logger.error(f\"OCR Manager returned unexpected result format or length.\")\n            return self\n\n        logger.info(\"OCR Manager batch processing complete.\")\n\n        ocr_end_time = time.monotonic()\n        logger.debug(\n            f\"[{thread_id}] OCR processing finished (Duration: {ocr_end_time - ocr_start_time:.2f}s)\"\n        )\n\n        logger.info(\"Adding OCR results to respective pages...\")\n        total_elements_added = 0\n\n        for i, (page, img) in enumerate(page_image_map):\n            results_for_page = batch_results[i]\n            if not isinstance(results_for_page, list):\n                logger.warning(\n                    f\"Skipping results for page {page.number}: Expected list, got {type(results_for_page)}\"\n                )\n                continue\n\n            logger.debug(f\"  Processing {len(results_for_page)} results for page {page.number}...\")\n            try:\n                if manager_args.get(\"replace\", True) and hasattr(page, \"_element_mgr\"):\n                    page._element_mgr.remove_ocr_elements()\n\n                img_scale_x = page.width / img.width if img.width &gt; 0 else 1\n                img_scale_y = page.height / img.height if img.height &gt; 0 else 1\n                elements = page._element_mgr.create_text_elements_from_ocr(\n                    results_for_page, img_scale_x, img_scale_y\n                )\n\n                if elements:\n                    total_elements_added += len(elements)\n                    logger.debug(f\"  Added {len(elements)} OCR TextElements to page {page.number}.\")\n                else:\n                    logger.debug(f\"  No valid TextElements created for page {page.number}.\")\n            except Exception as e:\n                logger.error(f\"  Error adding OCR elements to page {page.number}: {e}\")\n\n        logger.info(f\"Finished adding OCR results. Total elements added: {total_elements_added}\")\n        return self\n\n    def add_region(\n        self, region_func: Callable[[\"Page\"], Optional[\"Region\"]], name: str = None\n    ) -&gt; \"PDF\":\n        \"\"\"\n        Add a region function to the PDF.\n\n        Args:\n            region_func: A function that takes a Page and returns a Region, or None\n            name: Optional name for the region\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        region_data = (region_func, name)\n        self._regions.append(region_data)\n\n        # Apply only to already-created (cached) pages to avoid forcing page creation\n        for i in range(len(self._pages)):\n            if self._pages._cache[i] is not None:  # Only apply to existing pages\n                page = self._pages._cache[i]\n                try:\n                    region_instance = region_func(page)\n                    if region_instance and isinstance(region_instance, Region):\n                        page.add_region(region_instance, name=name, source=\"named\")\n                    elif region_instance is not None:\n                        logger.warning(\n                            f\"Region function did not return a valid Region for page {page.number}\"\n                        )\n                except Exception as e:\n                    logger.error(f\"Error adding region for page {page.number}: {e}\")\n\n        return self\n\n    @overload\n    def find(\n        self,\n        *,\n        text: str,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[Any]: ...\n\n    @overload\n    def find(\n        self,\n        selector: str,\n        *,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[Any]: ...\n\n    def find(\n        self,\n        selector: Optional[str] = None,\n        *,\n        text: Optional[str] = None,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[Any]:\n        \"\"\"\n        Find the first element matching the selector OR text content across all pages.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional filter parameters.\n\n        Returns:\n            Element object or None if not found.\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        if selector is not None and text is not None:\n            raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n        if selector is None and text is None:\n            raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n        # Construct selector if 'text' is provided\n        effective_selector = \"\"\n        if text is not None:\n            escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n            effective_selector = f'text:contains(\"{escaped_text}\")'\n            logger.debug(\n                f\"Using text shortcut: find(text='{text}') -&gt; find('{effective_selector}')\"\n            )\n        elif selector is not None:\n            effective_selector = selector\n        else:\n            raise ValueError(\"Internal error: No selector or text provided.\")\n\n        selector_obj = parse_selector(effective_selector)\n\n        # Search page by page\n        for page in self.pages:\n            # Note: _apply_selector is on Page, so we call find directly here\n            # We pass the constructed/validated effective_selector\n            element = page.find(\n                selector=effective_selector,  # Use the processed selector\n                apply_exclusions=apply_exclusions,\n                regex=regex,  # Pass down flags\n                case=case,\n                **kwargs,\n            )\n            if element:\n                return element\n        return None  # Not found on any page\n\n    @overload\n    def find_all(\n        self,\n        *,\n        text: str,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    @overload\n    def find_all(\n        self,\n        selector: str,\n        *,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    def find_all(\n        self,\n        selector: Optional[str] = None,\n        *,\n        text: Optional[str] = None,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Find all elements matching the selector OR text content across all pages.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional filter parameters.\n\n        Returns:\n            ElementCollection with matching elements.\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        if selector is not None and text is not None:\n            raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n        if selector is None and text is None:\n            raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n        # Construct selector if 'text' is provided\n        effective_selector = \"\"\n        if text is not None:\n            escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n            effective_selector = f'text:contains(\"{escaped_text}\")'\n            logger.debug(\n                f\"Using text shortcut: find_all(text='{text}') -&gt; find_all('{effective_selector}')\"\n            )\n        elif selector is not None:\n            effective_selector = selector\n        else:\n            raise ValueError(\"Internal error: No selector or text provided.\")\n\n        # Instead of parsing here, let each page parse and apply\n        # This avoids parsing the same selector multiple times if not needed\n        # selector_obj = parse_selector(effective_selector)\n\n        # kwargs[\"regex\"] = regex # Removed: Already passed explicitly\n        # kwargs[\"case\"] = case   # Removed: Already passed explicitly\n\n        all_elements = []\n        for page in self.pages:\n            # Call page.find_all with the effective selector and flags\n            page_elements = page.find_all(\n                selector=effective_selector,\n                apply_exclusions=apply_exclusions,\n                regex=regex,\n                case=case,\n                **kwargs,\n            )\n            if page_elements:\n                all_elements.extend(page_elements.elements)\n\n        from natural_pdf.elements.element_collection import ElementCollection\n\n        return ElementCollection(all_elements)\n\n    def extract_text(\n        self,\n        selector: Optional[str] = None,\n        preserve_whitespace=True,\n        use_exclusions=True,\n        debug_exclusions=False,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"\n        Extract text from the entire document or matching elements.\n\n        Args:\n            selector: Optional selector to filter elements\n            preserve_whitespace: Whether to keep blank characters\n            use_exclusions: Whether to apply exclusion regions\n            debug_exclusions: Whether to output detailed debugging for exclusions\n            preserve_whitespace: Whether to keep blank characters\n            use_exclusions: Whether to apply exclusion regions\n            debug_exclusions: Whether to output detailed debugging for exclusions\n            **kwargs: Additional extraction parameters\n\n        Returns:\n            Extracted text as string\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        if selector:\n            elements = self.find_all(selector, apply_exclusions=use_exclusions, **kwargs)\n            return elements.extract_text(preserve_whitespace=preserve_whitespace, **kwargs)\n\n        if debug_exclusions:\n            print(f\"PDF: Extracting text with exclusions from {len(self.pages)} pages\")\n            print(f\"PDF: Found {len(self._exclusions)} document-level exclusions\")\n\n        texts = []\n        for page in self.pages:\n            texts.append(\n                page.extract_text(\n                    preserve_whitespace=preserve_whitespace,\n                    use_exclusions=use_exclusions,\n                    debug_exclusions=debug_exclusions,\n                    **kwargs,\n                )\n            )\n\n        if debug_exclusions:\n            print(f\"PDF: Combined {len(texts)} pages of text\")\n\n        return \"\\n\".join(texts)\n\n    def extract_tables(\n        self, selector: Optional[str] = None, merge_across_pages: bool = False, **kwargs\n    ) -&gt; List[Any]:\n        \"\"\"\n        Extract tables from the document or matching elements.\n\n        Args:\n            selector: Optional selector to filter tables\n            merge_across_pages: Whether to merge tables that span across pages\n            **kwargs: Additional extraction parameters\n\n        Returns:\n            List of extracted tables\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        logger.warning(\"PDF.extract_tables is not fully implemented yet.\")\n        all_tables = []\n\n        for page in self.pages:\n            if hasattr(page, \"extract_tables\"):\n                all_tables.extend(page.extract_tables(**kwargs))\n            else:\n                logger.debug(f\"Page {page.number} does not have extract_tables method.\")\n\n        if selector:\n            logger.warning(\"Filtering extracted tables by selector is not implemented.\")\n\n        if merge_across_pages:\n            logger.warning(\"Merging tables across pages is not implemented.\")\n\n        return all_tables\n\n    def get_sections(\n        self,\n        start_elements=None,\n        end_elements=None,\n        new_section_on_page_break=False,\n        include_boundaries=\"both\",\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Extract sections from the entire PDF based on start/end elements.\n\n        This method delegates to the PageCollection.get_sections() method,\n        providing a convenient way to extract document sections across all pages.\n\n        Args:\n            start_elements: Elements or selector string that mark the start of sections (optional)\n            end_elements: Elements or selector string that mark the end of sections (optional)\n            new_section_on_page_break: Whether to start a new section at page boundaries (default: False)\n            include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none' (default: 'both')\n\n        Returns:\n            ElementCollection of Region objects representing the extracted sections\n\n        Example:\n            Extract sections between headers:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n\n            # Get sections between headers\n            sections = pdf.get_sections(\n                start_elements='text[size&gt;14]:bold',\n                end_elements='text[size&gt;14]:bold'\n            )\n\n            # Get sections that break at page boundaries\n            sections = pdf.get_sections(\n                start_elements='text:contains(\"Chapter\")',\n                new_section_on_page_break=True\n            )\n            ```\n\n        Note:\n            You can provide only start_elements, only end_elements, or both.\n            - With only start_elements: sections go from each start to the next start (or end of document)\n            - With only end_elements: sections go from beginning of document to each end\n            - With both: sections go from each start to the corresponding end\n        \"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not yet initialized.\")\n\n        return self.pages.get_sections(\n            start_elements=start_elements,\n            end_elements=end_elements,\n            new_section_on_page_break=new_section_on_page_break,\n            include_boundaries=include_boundaries,\n        )\n\n    def save_searchable(self, output_path: Union[str, \"Path\"], dpi: int = 300, **kwargs):\n        \"\"\"\n        DEPRECATED: Use save_pdf(..., ocr=True) instead.\n        Saves the PDF with an OCR text layer, making content searchable.\n\n        Requires optional dependencies. Install with: pip install \\\"natural-pdf[ocr-export]\\\"\n\n        Args:\n            output_path: Path to save the searchable PDF\n            dpi: Resolution for rendering and OCR overlay\n            **kwargs: Additional keyword arguments passed to the exporter\n        \"\"\"\n        logger.warning(\n            \"PDF.save_searchable() is deprecated. Use PDF.save_pdf(..., ocr=True) instead.\"\n        )\n        if create_searchable_pdf is None:\n            raise ImportError(\n                \"Saving searchable PDF requires 'pikepdf'. \"\n                'Install with: pip install \"natural-pdf[ocr-export]\"'\n            )\n        output_path_str = str(output_path)\n        # Call the exporter directly, passing self (the PDF instance)\n        create_searchable_pdf(self, output_path_str, dpi=dpi, **kwargs)\n        # Logger info is handled within the exporter now\n        # logger.info(f\"Searchable PDF saved to: {output_path_str}\")\n\n    def save_pdf(\n        self,\n        output_path: Union[str, Path],\n        ocr: bool = False,\n        original: bool = False,\n        dpi: int = 300,\n    ):\n        \"\"\"\n        Saves the PDF object (all its pages) to a new file.\n\n        Choose one saving mode:\n        - `ocr=True`: Creates a new, image-based PDF using OCR results from all pages.\n          Text generated during the natural-pdf session becomes searchable,\n          but original vector content is lost. Requires 'ocr-export' extras.\n        - `original=True`: Saves a copy of the original PDF file this object represents.\n          Any OCR results or analyses from the natural-pdf session are NOT included.\n          If the PDF was opened from an in-memory buffer, this mode may not be suitable.\n          Requires 'ocr-export' extras.\n\n        Args:\n            output_path: Path to save the new PDF file.\n            ocr: If True, save as a searchable, image-based PDF using OCR data.\n            original: If True, save the original source PDF content.\n            dpi: Resolution (dots per inch) used only when ocr=True.\n\n        Raises:\n            ValueError: If the PDF has no pages, if neither or both 'ocr'\n                        and 'original' are True.\n            ImportError: If required libraries are not installed for the chosen mode.\n            RuntimeError: If an unexpected error occurs during saving.\n        \"\"\"\n        if not self.pages:\n            raise ValueError(\"Cannot save an empty PDF object.\")\n\n        if not (ocr ^ original):  # XOR: exactly one must be true\n            raise ValueError(\"Exactly one of 'ocr' or 'original' must be True.\")\n\n        output_path_obj = Path(output_path)\n        output_path_str = str(output_path_obj)\n\n        if ocr:\n            has_vector_elements = False\n            for page in self.pages:\n                if (\n                    hasattr(page, \"rects\")\n                    and page.rects\n                    or hasattr(page, \"lines\")\n                    and page.lines\n                    or hasattr(page, \"curves\")\n                    and page.curves\n                    or (\n                        hasattr(page, \"chars\")\n                        and any(getattr(el, \"source\", None) != \"ocr\" for el in page.chars)\n                    )\n                    or (\n                        hasattr(page, \"words\")\n                        and any(getattr(el, \"source\", None) != \"ocr\" for el in page.words)\n                    )\n                ):\n                    has_vector_elements = True\n                    break\n            if has_vector_elements:\n                logger.warning(\n                    \"Warning: Saving with ocr=True creates an image-based PDF. \"\n                    \"Original vector elements (rects, lines, non-OCR text/chars) \"\n                    \"will not be preserved in the output file.\"\n                )\n\n            logger.info(f\"Saving searchable PDF (OCR text layer) to: {output_path_str}\")\n            try:\n                # Delegate to the searchable PDF exporter, passing self (PDF instance)\n                create_searchable_pdf(self, output_path_str, dpi=dpi)\n            except Exception as e:\n                raise RuntimeError(f\"Failed to create searchable PDF: {e}\") from e\n\n        elif original:\n            if create_original_pdf is None:\n                raise ImportError(\n                    \"Saving with original=True requires 'pikepdf'. \"\n                    'Install with: pip install \"natural-pdf[ocr-export]\"'\n                )\n\n            # Optional: Add warning about losing OCR data similar to PageCollection\n            has_ocr_elements = False\n            for page in self.pages:\n                if hasattr(page, \"find_all\"):\n                    ocr_text_elements = page.find_all(\"text[source=ocr]\")\n                    if ocr_text_elements:\n                        has_ocr_elements = True\n                        break\n                elif hasattr(page, \"words\"):  # Fallback\n                    if any(getattr(el, \"source\", None) == \"ocr\" for el in page.words):\n                        has_ocr_elements = True\n                        break\n            if has_ocr_elements:\n                logger.warning(\n                    \"Warning: Saving with original=True preserves original page content. \"\n                    \"OCR text generated in this session will not be included in the saved file.\"\n                )\n\n            logger.info(f\"Saving original PDF content to: {output_path_str}\")\n            try:\n                # Delegate to the original PDF exporter, passing self (PDF instance)\n                create_original_pdf(self, output_path_str)\n            except Exception as e:\n                # Re-raise exception from exporter\n                raise e\n\n    def _get_render_specs(\n        self,\n        mode: Literal[\"show\", \"render\"] = \"show\",\n        color: Optional[Union[str, Tuple[int, int, int]]] = None,\n        highlights: Optional[List[Dict[str, Any]]] = None,\n        crop: Union[bool, Literal[\"content\"]] = False,\n        crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n        **kwargs,\n    ) -&gt; List[RenderSpec]:\n        \"\"\"Get render specifications for this PDF.\n\n        For PDF objects, this delegates to the pages collection to handle\n        multi-page rendering.\n\n        Args:\n            mode: Rendering mode - 'show' includes highlights, 'render' is clean\n            color: Color for highlighting pages in show mode\n            highlights: Additional highlight groups to show\n            crop: Whether to crop pages\n            crop_bbox: Explicit crop bounds\n            **kwargs: Additional parameters\n\n        Returns:\n            List of RenderSpec objects, one per page\n        \"\"\"\n        # Delegate to pages collection\n        return self.pages._get_render_specs(\n            mode=mode, color=color, highlights=highlights, crop=crop, crop_bbox=crop_bbox, **kwargs\n        )\n\n    def ask(\n        self,\n        question: str,\n        mode: str = \"extractive\",\n        pages: Union[int, List[int], range] = None,\n        min_confidence: float = 0.1,\n        model: str = None,\n        **kwargs,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Ask a single question about the document content.\n\n        Args:\n            question: Question string to ask about the document\n            mode: \"extractive\" to extract answer from document, \"generative\" to generate\n            pages: Specific pages to query (default: all pages)\n            min_confidence: Minimum confidence threshold for answers\n            model: Optional model name for question answering\n            **kwargs: Additional parameters passed to the QA engine\n\n        Returns:\n            Dict containing: answer, confidence, found, page_num, source_elements, etc.\n        \"\"\"\n        # Delegate to ask_batch and return the first result\n        results = self.ask_batch(\n            [question], mode=mode, pages=pages, min_confidence=min_confidence, model=model, **kwargs\n        )\n        return (\n            results[0]\n            if results\n            else {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": None,\n                \"source_elements\": [],\n            }\n        )\n\n    def ask_batch(\n        self,\n        questions: List[str],\n        mode: str = \"extractive\",\n        pages: Union[int, List[int], range] = None,\n        min_confidence: float = 0.1,\n        model: str = None,\n        **kwargs,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Ask multiple questions about the document content using batch processing.\n\n        This method processes multiple questions efficiently in a single batch,\n        avoiding the multiprocessing resource accumulation that can occur with\n        sequential individual question calls.\n\n        Args:\n            questions: List of question strings to ask about the document\n            mode: \"extractive\" to extract answer from document, \"generative\" to generate\n            pages: Specific pages to query (default: all pages)\n            min_confidence: Minimum confidence threshold for answers\n            model: Optional model name for question answering\n            **kwargs: Additional parameters passed to the QA engine\n\n        Returns:\n            List of Dicts, each containing: answer, confidence, found, page_num, source_elements, etc.\n        \"\"\"\n        from natural_pdf.qa import get_qa_engine\n\n        if not questions:\n            return []\n\n        if not isinstance(questions, list) or not all(isinstance(q, str) for q in questions):\n            raise TypeError(\"'questions' must be a list of strings\")\n\n        qa_engine = get_qa_engine() if model is None else get_qa_engine(model_name=model)\n\n        # Resolve target pages\n        if pages is None:\n            target_pages = self.pages\n        elif isinstance(pages, int):\n            if 0 &lt;= pages &lt; len(self.pages):\n                target_pages = [self.pages[pages]]\n            else:\n                raise IndexError(f\"Page index {pages} out of range (0-{len(self.pages)-1})\")\n        elif isinstance(pages, (list, range)):\n            target_pages = []\n            for page_idx in pages:\n                if 0 &lt;= page_idx &lt; len(self.pages):\n                    target_pages.append(self.pages[page_idx])\n                else:\n                    logger.warning(f\"Page index {page_idx} out of range, skipping\")\n        else:\n            raise ValueError(f\"Invalid pages parameter: {pages}\")\n\n        if not target_pages:\n            logger.warning(\"No valid pages found for QA processing.\")\n            return [\n                {\n                    \"answer\": None,\n                    \"confidence\": 0.0,\n                    \"found\": False,\n                    \"page_num\": None,\n                    \"source_elements\": [],\n                }\n                for _ in questions\n            ]\n\n        logger.info(\n            f\"Processing {len(questions)} question(s) across {len(target_pages)} page(s) using batch QA...\"\n        )\n\n        # Collect all page images and metadata for batch processing\n        page_images = []\n        page_word_boxes = []\n        page_metadata = []\n\n        for page in target_pages:\n            # Get page image\n            try:\n                # Use render() for clean image without highlights\n                page_image = page.render(resolution=150)\n                if page_image is None:\n                    logger.warning(f\"Failed to render image for page {page.number}, skipping\")\n                    continue\n\n                # Get text elements for word boxes\n                elements = page.find_all(\"text\")\n                if not elements:\n                    logger.warning(f\"No text elements found on page {page.number}\")\n                    word_boxes = []\n                else:\n                    word_boxes = qa_engine._get_word_boxes_from_elements(\n                        elements, offset_x=0, offset_y=0\n                    )\n\n                page_images.append(page_image)\n                page_word_boxes.append(word_boxes)\n                page_metadata.append({\"page_number\": page.number, \"page_object\": page})\n\n            except Exception as e:\n                logger.warning(f\"Error processing page {page.number}: {e}\")\n                continue\n\n        if not page_images:\n            logger.warning(\"No page images could be processed for QA.\")\n            return [\n                {\n                    \"answer\": None,\n                    \"confidence\": 0.0,\n                    \"found\": False,\n                    \"page_num\": None,\n                    \"source_elements\": [],\n                }\n                for _ in questions\n            ]\n\n        # Process all questions against all pages in batch\n        all_results = []\n\n        for question_text in questions:\n            question_results = []\n\n            # Ask this question against each page (but in batch per page)\n            for i, (page_image, word_boxes, page_meta) in enumerate(\n                zip(page_images, page_word_boxes, page_metadata)\n            ):\n                try:\n                    # Use the DocumentQA batch interface\n                    page_result = qa_engine.ask(\n                        image=page_image,\n                        question=question_text,\n                        word_boxes=word_boxes,\n                        min_confidence=min_confidence,\n                        **kwargs,\n                    )\n\n                    if page_result and page_result.found:\n                        # Add page metadata to result\n                        page_result_dict = {\n                            \"answer\": page_result.answer,\n                            \"confidence\": page_result.confidence,\n                            \"found\": page_result.found,\n                            \"page_num\": page_meta[\"page_number\"],\n                            \"source_elements\": getattr(page_result, \"source_elements\", []),\n                            \"start\": getattr(page_result, \"start\", -1),\n                            \"end\": getattr(page_result, \"end\", -1),\n                        }\n                        question_results.append(page_result_dict)\n\n                except Exception as e:\n                    logger.warning(\n                        f\"Error processing question '{question_text}' on page {page_meta['page_number']}: {e}\"\n                    )\n                    continue\n\n            # Sort results by confidence and take the best one for this question\n            question_results.sort(key=lambda x: x.get(\"confidence\", 0), reverse=True)\n\n            if question_results:\n                all_results.append(question_results[0])\n            else:\n                # No results found for this question\n                all_results.append(\n                    {\n                        \"answer\": None,\n                        \"confidence\": 0.0,\n                        \"found\": False,\n                        \"page_num\": None,\n                        \"source_elements\": [],\n                    }\n                )\n\n        return all_results\n\n    def search_within_index(\n        self,\n        query: Union[str, Path, Image.Image, \"Region\"],\n        search_service: \"SearchServiceProtocol\",\n        options: Optional[\"SearchOptions\"] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Finds relevant documents from this PDF within a search index.\n        Finds relevant documents from this PDF within a search index.\n\n        Args:\n            query: The search query (text, image path, PIL Image, Region)\n            search_service: A pre-configured SearchService instance\n            options: Optional SearchOptions to configure the query\n            query: The search query (text, image path, PIL Image, Region)\n            search_service: A pre-configured SearchService instance\n            options: Optional SearchOptions to configure the query\n\n        Returns:\n            A list of result dictionaries, sorted by relevance\n            A list of result dictionaries, sorted by relevance\n\n        Raises:\n            ImportError: If search dependencies are not installed\n            ValueError: If search_service is None\n            TypeError: If search_service does not conform to the protocol\n            FileNotFoundError: If the collection managed by the service does not exist\n            RuntimeError: For other search failures\n            ImportError: If search dependencies are not installed\n            ValueError: If search_service is None\n            TypeError: If search_service does not conform to the protocol\n            FileNotFoundError: If the collection managed by the service does not exist\n            RuntimeError: For other search failures\n        \"\"\"\n        if not search_service:\n            raise ValueError(\"A configured SearchServiceProtocol instance must be provided.\")\n\n        collection_name = getattr(search_service, \"collection_name\", \"&lt;Unknown Collection&gt;\")\n        logger.info(\n            f\"Searching within index '{collection_name}' for content from PDF '{self.path}'\"\n        )\n\n        service = search_service\n\n        query_input = query\n        effective_options = copy.deepcopy(options) if options is not None else TextSearchOptions()\n\n        if isinstance(query, Region):\n            logger.debug(\"Query is a Region object. Extracting text.\")\n            if not isinstance(effective_options, TextSearchOptions):\n                logger.warning(\n                    \"Querying with Region image requires MultiModalSearchOptions. Falling back to text extraction.\"\n                )\n            query_input = query.extract_text()\n            if not query_input or query_input.isspace():\n                logger.error(\"Region has no extractable text for query.\")\n                return []\n\n        # Add filter to scope search to THIS PDF\n        # Add filter to scope search to THIS PDF\n        pdf_scope_filter = {\n            \"field\": \"pdf_path\",\n            \"operator\": \"eq\",\n            \"value\": self.path,\n        }\n        logger.debug(f\"Applying filter to scope search to PDF: {pdf_scope_filter}\")\n\n        # Combine with existing filters in options (if any)\n        if effective_options.filters:\n            logger.debug(f\"Combining PDF scope filter with existing filters\")\n            if (\n                isinstance(effective_options.filters, dict)\n                and effective_options.filters.get(\"operator\") == \"AND\"\n            ):\n                effective_options.filters[\"conditions\"].append(pdf_scope_filter)\n            elif isinstance(effective_options.filters, list):\n                effective_options.filters = {\n                    \"operator\": \"AND\",\n                    \"conditions\": effective_options.filters + [pdf_scope_filter],\n                }\n            elif isinstance(effective_options.filters, dict):\n                effective_options.filters = {\n                    \"operator\": \"AND\",\n                    \"conditions\": [effective_options.filters, pdf_scope_filter],\n                }\n            else:\n                logger.warning(\n                    f\"Unsupported format for existing filters. Overwriting with PDF scope filter.\"\n                )\n                effective_options.filters = pdf_scope_filter\n        else:\n            effective_options.filters = pdf_scope_filter\n\n        logger.debug(f\"Final filters for service search: {effective_options.filters}\")\n\n        try:\n            results = service.search(\n                query=query_input,\n                options=effective_options,\n            )\n            logger.info(f\"SearchService returned {len(results)} results from PDF '{self.path}'\")\n            return results\n        except FileNotFoundError as fnf:\n            logger.error(f\"Search failed: Collection not found. Error: {fnf}\")\n            raise\n            logger.error(f\"Search failed: Collection not found. Error: {fnf}\")\n            raise\n        except Exception as e:\n            logger.error(f\"SearchService search failed: {e}\")\n            raise RuntimeError(f\"Search within index failed. See logs for details.\") from e\n            logger.error(f\"SearchService search failed: {e}\")\n            raise RuntimeError(f\"Search within index failed. See logs for details.\") from e\n\n    def export_ocr_correction_task(self, output_zip_path: str, **kwargs):\n        \"\"\"\n        Exports OCR results from this PDF into a correction task package.\n        Exports OCR results from this PDF into a correction task package.\n\n        Args:\n            output_zip_path: The path to save the output zip file\n            output_zip_path: The path to save the output zip file\n            **kwargs: Additional arguments passed to create_correction_task_package\n        \"\"\"\n        try:\n            from natural_pdf.utils.packaging import create_correction_task_package\n\n            create_correction_task_package(source=self, output_zip_path=output_zip_path, **kwargs)\n        except ImportError:\n            logger.error(\n                \"Failed to import 'create_correction_task_package'. Packaging utility might be missing.\"\n            )\n            logger.error(\n                \"Failed to import 'create_correction_task_package'. Packaging utility might be missing.\"\n            )\n        except Exception as e:\n            logger.error(f\"Failed to export correction task: {e}\")\n            raise\n            logger.error(f\"Failed to export correction task: {e}\")\n            raise\n\n    def update_text(\n        self,\n        transform: Callable[[Any], Optional[str]],\n        pages: Optional[Union[Iterable[int], range, slice]] = None,\n        selector: str = \"text\",\n        max_workers: Optional[int] = None,\n        progress_callback: Optional[Callable[[], None]] = None,\n    ) -&gt; \"PDF\":\n        \"\"\"\n        Applies corrections to text elements using a callback function.\n\n        Args:\n            correction_callback: Function that takes an element and returns corrected text or None\n            pages: Optional page indices/slice to limit the scope of correction\n            selector: Selector to apply corrections to (default: \"text\")\n            max_workers: Maximum number of threads to use for parallel execution\n            progress_callback: Optional callback function for progress updates\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        target_page_indices = []\n        if pages is None:\n            target_page_indices = list(range(len(self._pages)))\n        elif isinstance(pages, slice):\n            target_page_indices = list(range(*pages.indices(len(self._pages))))\n        elif hasattr(pages, \"__iter__\"):\n            try:\n                target_page_indices = [int(i) for i in pages]\n                for idx in target_page_indices:\n                    if not (0 &lt;= idx &lt; len(self._pages)):\n                        raise IndexError(f\"Page index {idx} out of range (0-{len(self._pages)-1}).\")\n            except (IndexError, TypeError, ValueError) as e:\n                raise ValueError(f\"Invalid page index in 'pages': {pages}. Error: {e}\") from e\n        else:\n            raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n\n        if not target_page_indices:\n            logger.warning(\"No pages selected for text update.\")\n            return self\n\n        logger.info(\n            f\"Starting text update for pages: {target_page_indices} with selector='{selector}'\"\n        )\n\n        for page_idx in target_page_indices:\n            page = self._pages[page_idx]\n            try:\n                page.update_text(\n                    transform=transform,\n                    selector=selector,\n                    max_workers=max_workers,\n                    progress_callback=progress_callback,\n                )\n            except Exception as e:\n                logger.error(f\"Error during text update on page {page_idx}: {e}\")\n                logger.error(f\"Error during text update on page {page_idx}: {e}\")\n\n        logger.info(\"Text update process finished.\")\n        return self\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of pages in the PDF.\"\"\"\n        if not hasattr(self, \"_pages\"):\n            return 0\n        return len(self._pages)\n\n    def __getitem__(self, key) -&gt; Union[\"Page\", \"PageCollection\"]:\n        \"\"\"Access pages by index or slice.\"\"\"\n        if not hasattr(self, \"_pages\"):\n            raise AttributeError(\"PDF pages not initialized yet.\")\n\n        if isinstance(key, slice):\n            from natural_pdf.core.page_collection import PageCollection\n\n            # Use the lazy page list's slicing which returns another _LazyPageList\n            lazy_slice = self._pages[key]\n            # Wrap in PageCollection for compatibility\n            return PageCollection(lazy_slice)\n        elif isinstance(key, int):\n            if 0 &lt;= key &lt; len(self._pages):\n                return self._pages[key]\n            else:\n                raise IndexError(f\"Page index {key} out of range (0-{len(self._pages)-1}).\")\n        else:\n            raise TypeError(f\"Page indices must be integers or slices, not {type(key)}.\")\n\n    def close(self):\n        \"\"\"Close the underlying PDF file and clean up any temporary files.\"\"\"\n        if hasattr(self, \"_pdf\") and self._pdf is not None:\n            try:\n                self._pdf.close()\n                logger.debug(f\"Closed pdfplumber PDF object for {self.source_path}\")\n            except Exception as e:\n                logger.warning(f\"Error closing pdfplumber object: {e}\")\n            finally:\n                self._pdf = None\n\n        if hasattr(self, \"_temp_file\") and self._temp_file is not None:\n            temp_file_path = None\n            try:\n                if hasattr(self._temp_file, \"name\") and self._temp_file.name:\n                    temp_file_path = self._temp_file.name\n                    # Only unlink if it exists and _is_stream is False (meaning WE created it)\n                    if not self._is_stream and os.path.exists(temp_file_path):\n                        os.unlink(temp_file_path)\n                        logger.debug(f\"Removed temporary PDF file: {temp_file_path}\")\n            except Exception as e:\n                logger.warning(f\"Failed to clean up temporary file '{temp_file_path}': {e}\")\n\n        # Cancels the weakref finalizer so we don't double-clean\n        if hasattr(self, \"_finalizer\") and self._finalizer.alive:\n            self._finalizer()\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the PDF object.\"\"\"\n        if not hasattr(self, \"_pages\"):\n            page_count_str = \"uninitialized\"\n        else:\n            page_count_str = str(len(self._pages))\n\n        source_info = getattr(self, \"source_path\", \"unknown source\")\n        return f\"&lt;PDF source='{source_info}' pages={page_count_str}&gt;\"\n\n    def get_id(self) -&gt; str:\n        \"\"\"Get unique identifier for this PDF.\"\"\"\n        \"\"\"Get unique identifier for this PDF.\"\"\"\n        return self.path\n\n    # --- Deskew Method --- #\n\n    def deskew(\n        self,\n        pages: Optional[Union[Iterable[int], range, slice]] = None,\n        resolution: int = 300,\n        angle: Optional[float] = None,\n        detection_resolution: int = 72,\n        force_overwrite: bool = False,\n        **deskew_kwargs,\n    ) -&gt; \"PDF\":\n        \"\"\"\n        Creates a new, in-memory PDF object containing deskewed versions of the\n        specified pages from the original PDF.\n\n        This method renders each selected page, detects and corrects skew using the 'deskew'\n        library, and then combines the resulting images into a new PDF using 'img2pdf'.\n        The new PDF object is returned directly.\n\n        Important: The returned PDF is image-based. Any existing text, OCR results,\n        annotations, or other elements from the original pages will *not* be carried over.\n\n        Args:\n            pages: Page indices/slice to include (0-based). If None, processes all pages.\n            resolution: DPI resolution for rendering the output deskewed pages.\n            angle: The specific angle (in degrees) to rotate by. If None, detects automatically.\n            detection_resolution: DPI resolution used for skew detection if angles are not\n                                  already cached on the page objects.\n            force_overwrite: If False (default), raises a ValueError if any target page\n                             already contains processed elements (text, OCR, regions) to\n                             prevent accidental data loss. Set to True to proceed anyway.\n            **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                             during automatic detection (e.g., `max_angle`, `num_peaks`).\n\n        Returns:\n            A new PDF object representing the deskewed document.\n\n        Raises:\n            ImportError: If 'deskew' or 'img2pdf' libraries are not installed.\n            ValueError: If `force_overwrite` is False and target pages contain elements.\n            FileNotFoundError: If the source PDF cannot be read (if file-based).\n            IOError: If creating the in-memory PDF fails.\n            RuntimeError: If rendering or deskewing individual pages fails.\n        \"\"\"\n        if not DESKEW_AVAILABLE:\n            raise ImportError(\n                \"Deskew/img2pdf libraries missing. Install with: pip install natural-pdf[deskew]\"\n            )\n\n        target_pages = self._get_target_pages(pages)  # Use helper to resolve pages\n\n        # --- Safety Check --- #\n        if not force_overwrite:\n            for page in target_pages:\n                # Check if the element manager has been initialized and contains any elements\n                if (\n                    hasattr(page, \"_element_mgr\")\n                    and page._element_mgr\n                    and page._element_mgr.has_elements()\n                ):\n                    raise ValueError(\n                        f\"Page {page.number} contains existing elements (text, OCR, etc.). \"\n                        f\"Deskewing creates an image-only PDF, discarding these elements. \"\n                        f\"Set force_overwrite=True to proceed.\"\n                    )\n\n        # --- Process Pages --- #\n        deskewed_images_bytes = []\n        logger.info(f\"Deskewing {len(target_pages)} pages (output resolution={resolution} DPI)...\")\n\n        for page in tqdm(target_pages, desc=\"Deskewing Pages\", leave=False):\n            try:\n                # Use page.deskew to get the corrected PIL image\n                # Pass down resolutions and kwargs\n                deskewed_img = page.deskew(\n                    resolution=resolution,\n                    angle=angle,  # Let page.deskew handle detection/caching\n                    detection_resolution=detection_resolution,\n                    **deskew_kwargs,\n                )\n\n                if not deskewed_img:\n                    logger.warning(\n                        f\"Page {page.number}: Failed to generate deskewed image, skipping.\"\n                    )\n                    continue\n\n                # Convert image to bytes for img2pdf (use PNG for lossless quality)\n                with io.BytesIO() as buf:\n                    deskewed_img.save(buf, format=\"PNG\")\n                    deskewed_images_bytes.append(buf.getvalue())\n\n            except Exception as e:\n                logger.error(\n                    f\"Page {page.number}: Failed during deskewing process: {e}\", exc_info=True\n                )\n                # Option: Raise a runtime error, or continue and skip the page?\n                # Raising makes the whole operation fail if one page fails.\n                raise RuntimeError(f\"Failed to process page {page.number} during deskewing.\") from e\n\n        # --- Create PDF --- #\n        if not deskewed_images_bytes:\n            raise RuntimeError(\"No pages were successfully processed to create the deskewed PDF.\")\n\n        logger.info(f\"Combining {len(deskewed_images_bytes)} deskewed images into in-memory PDF...\")\n        try:\n            # Use img2pdf to combine image bytes into PDF bytes\n            pdf_bytes = img2pdf.convert(deskewed_images_bytes)\n\n            # Wrap bytes in a stream\n            pdf_stream = io.BytesIO(pdf_bytes)\n\n            # Create a new PDF object from the stream using original config\n            logger.info(\"Creating new PDF object from deskewed stream...\")\n            new_pdf = PDF(\n                pdf_stream,\n                reading_order=self._reading_order,\n                font_attrs=self._font_attrs,\n                keep_spaces=self._config.get(\"keep_spaces\", True),\n                text_layer=self._text_layer,\n            )\n            return new_pdf\n        except Exception as e:\n            logger.error(f\"Failed to create in-memory PDF using img2pdf/PDF init: {e}\")\n            raise IOError(\"Failed to create deskewed PDF object from image stream.\") from e\n\n    # --- End Deskew Method --- #\n\n    # --- Classification Methods --- #\n\n    def classify_pages(\n        self,\n        labels: List[str],\n        model: Optional[str] = None,\n        pages: Optional[Union[Iterable[int], range, slice]] = None,\n        analysis_key: str = \"classification\",\n        using: Optional[str] = None,\n        **kwargs,\n    ) -&gt; \"PDF\":\n        \"\"\"\n        Classifies specified pages of the PDF.\n\n        Args:\n            labels: List of category names\n            model: Model identifier ('text', 'vision', or specific HF ID)\n            pages: Page indices, slice, or None for all pages\n            analysis_key: Key to store results in page's analyses dict\n            using: Processing mode ('text' or 'vision')\n            **kwargs: Additional arguments for the ClassificationManager\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        if not labels:\n            raise ValueError(\"Labels list cannot be empty.\")\n\n        try:\n            manager = self.get_manager(\"classification\")\n        except (ValueError, RuntimeError) as e:\n            raise ClassificationError(f\"Cannot get ClassificationManager: {e}\") from e\n\n        if not manager or not manager.is_available():\n            from natural_pdf.classification.manager import is_classification_available\n\n            if not is_classification_available():\n                raise ImportError(\n                    \"Classification dependencies missing. \"\n                    'Install with: pip install \"natural-pdf[ai]\"'\n                )\n            raise ClassificationError(\"ClassificationManager not available.\")\n\n        target_pages = []\n        if pages is None:\n            target_pages = self._pages\n        elif isinstance(pages, slice):\n            target_pages = self._pages[pages]\n        elif hasattr(pages, \"__iter__\"):\n            try:\n                target_pages = [self._pages[i] for i in pages]\n            except IndexError:\n                raise ValueError(\"Invalid page index provided.\")\n            except TypeError:\n                raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n        else:\n            raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n\n        if not target_pages:\n            logger.warning(\"No pages selected for classification.\")\n            return self\n\n        inferred_using = manager.infer_using(model if model else manager.DEFAULT_TEXT_MODEL, using)\n        logger.info(\n            f\"Classifying {len(target_pages)} pages using model '{model or '(default)'}' (mode: {inferred_using})\"\n        )\n\n        page_contents = []\n        pages_to_classify = []\n        logger.debug(f\"Gathering content for {len(target_pages)} pages...\")\n\n        for page in target_pages:\n            try:\n                content = page._get_classification_content(model_type=inferred_using, **kwargs)\n                page_contents.append(content)\n                pages_to_classify.append(page)\n            except ValueError as e:\n                logger.warning(f\"Skipping page {page.number}: Cannot get content - {e}\")\n            except Exception as e:\n                logger.warning(f\"Skipping page {page.number}: Error getting content - {e}\")\n\n        if not page_contents:\n            logger.warning(\"No content could be gathered for batch classification.\")\n            return self\n\n        logger.debug(f\"Gathered content for {len(pages_to_classify)} pages.\")\n\n        try:\n            batch_results = manager.classify_batch(\n                item_contents=page_contents,\n                labels=labels,\n                model_id=model,\n                using=inferred_using,\n                **kwargs,\n            )\n        except Exception as e:\n            logger.error(f\"Batch classification failed: {e}\")\n            raise ClassificationError(f\"Batch classification failed: {e}\") from e\n\n        if len(batch_results) != len(pages_to_classify):\n            logger.error(\n                f\"Mismatch between number of results ({len(batch_results)}) and pages ({len(pages_to_classify)})\"\n            )\n            return self\n\n        logger.debug(\n            f\"Distributing {len(batch_results)} results to pages under key '{analysis_key}'...\"\n        )\n        for page, result_obj in zip(pages_to_classify, batch_results):\n            try:\n                if not hasattr(page, \"analyses\") or page.analyses is None:\n                    page.analyses = {}\n                page.analyses[analysis_key] = result_obj\n            except Exception as e:\n                logger.warning(\n                    f\"Failed to store classification results for page {page.number}: {e}\"\n                )\n\n        logger.info(f\"Finished classifying PDF pages.\")\n        return self\n\n    # --- End Classification Methods --- #\n\n    # --- Extraction Support --- #\n    def _get_extraction_content(self, using: str = \"text\", **kwargs) -&gt; Any:\n        \"\"\"\n        Retrieves the content for the entire PDF.\n\n        Args:\n            using: 'text' or 'vision'\n            **kwargs: Additional arguments passed to extract_text or page.to_image\n\n        Returns:\n            str: Extracted text if using='text'\n            List[PIL.Image.Image]: List of page images if using='vision'\n            None: If content cannot be retrieved\n        \"\"\"\n        if using == \"text\":\n            try:\n                layout = kwargs.pop(\"layout\", True)\n                return self.extract_text(layout=layout, **kwargs)\n            except Exception as e:\n                logger.error(f\"Error extracting text from PDF: {e}\")\n                return None\n        elif using == \"vision\":\n            page_images = []\n            logger.info(f\"Rendering {len(self.pages)} pages to images...\")\n\n            resolution = kwargs.pop(\"resolution\", 72)\n            include_highlights = kwargs.pop(\"include_highlights\", False)\n            labels = kwargs.pop(\"labels\", False)\n\n            try:\n                for page in tqdm(self.pages, desc=\"Rendering Pages\"):\n                    # Use render() for clean images\n                    img = page.render(\n                        resolution=resolution,\n                        **kwargs,\n                    )\n                    if img:\n                        page_images.append(img)\n                    else:\n                        logger.warning(f\"Failed to render page {page.number}, skipping.\")\n                if not page_images:\n                    logger.error(\"Failed to render any pages.\")\n                    return None\n                return page_images\n            except Exception as e:\n                logger.error(f\"Error rendering pages: {e}\")\n                return None\n        else:\n            logger.error(f\"Unsupported value for 'using': {using}\")\n            return None\n\n    # --- End Extraction Support --- #\n\n    def _gather_analysis_data(\n        self,\n        analysis_keys: List[str],\n        include_content: bool,\n        include_images: bool,\n        image_dir: Optional[Path],\n        image_format: str,\n        image_resolution: int,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Gather analysis data from all pages in the PDF.\n\n        Args:\n            analysis_keys: Keys in the analyses dictionary to export\n            include_content: Whether to include extracted text\n            include_images: Whether to export images\n            image_dir: Directory to save images\n            image_format: Format to save images\n            image_resolution: Resolution for exported images\n\n        Returns:\n            List of dictionaries containing analysis data\n        \"\"\"\n        if not hasattr(self, \"_pages\") or not self._pages:\n            logger.warning(f\"No pages found in PDF {self.path}\")\n            return []\n\n        all_data = []\n\n        for page in tqdm(self._pages, desc=\"Gathering page data\", leave=False):\n            # Basic page information\n            page_data = {\n                \"pdf_path\": self.path,\n                \"page_number\": page.number,\n                \"page_index\": page.index,\n            }\n\n            # Include extracted text if requested\n            if include_content:\n                try:\n                    page_data[\"content\"] = page.extract_text(preserve_whitespace=True)\n                except Exception as e:\n                    logger.error(f\"Error extracting text from page {page.number}: {e}\")\n                    page_data[\"content\"] = \"\"\n\n            # Save image if requested\n            if include_images:\n                try:\n                    # Create image filename\n                    image_filename = f\"pdf_{Path(self.path).stem}_page_{page.number}.{image_format}\"\n                    image_path = image_dir / image_filename\n\n                    # Save image\n                    page.save_image(\n                        str(image_path), resolution=image_resolution, include_highlights=True\n                    )\n\n                    # Add relative path to data\n                    page_data[\"image_path\"] = str(Path(image_path).relative_to(image_dir.parent))\n                except Exception as e:\n                    logger.error(f\"Error saving image for page {page.number}: {e}\")\n                    page_data[\"image_path\"] = None\n\n            # Add analyses data\n            for key in analysis_keys:\n                if not hasattr(page, \"analyses\") or not page.analyses:\n                    raise ValueError(f\"Page {page.number} does not have analyses data\")\n\n                if key not in page.analyses:\n                    raise KeyError(f\"Analysis key '{key}' not found in page {page.number}\")\n\n                # Get the analysis result\n                analysis_result = page.analyses[key]\n\n                # If the result has a to_dict method, use it\n                if hasattr(analysis_result, \"to_dict\"):\n                    analysis_data = analysis_result.to_dict()\n                else:\n                    # Otherwise, use the result directly if it's dict-like\n                    try:\n                        analysis_data = dict(analysis_result)\n                    except (TypeError, ValueError):\n                        # Last resort: convert to string\n                        analysis_data = {\"raw_result\": str(analysis_result)}\n\n                # Add analysis data to page data with the key as prefix\n                for k, v in analysis_data.items():\n                    page_data[f\"{key}.{k}\"] = v\n\n            all_data.append(page_data)\n\n        return all_data\n\n    def _get_target_pages(\n        self, pages: Optional[Union[Iterable[int], range, slice]] = None\n    ) -&gt; List[\"Page\"]:\n        \"\"\"\n        Helper method to get a list of Page objects based on the input pages.\n\n        Args:\n            pages: Page indices, slice, or None for all pages\n\n        Returns:\n            List of Page objects\n        \"\"\"\n        if pages is None:\n            return self._pages\n        elif isinstance(pages, slice):\n            return self._pages[pages]\n        elif hasattr(pages, \"__iter__\"):\n            try:\n                return [self._pages[i] for i in pages]\n            except IndexError:\n                raise ValueError(\"Invalid page index provided in 'pages' iterable.\")\n            except TypeError:\n                raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n        else:\n            raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n\n    # --- Classification Mixin Implementation --- #\n\n    def _get_classification_manager(self) -&gt; \"ClassificationManager\":\n        \"\"\"Returns the ClassificationManager instance for this PDF.\"\"\"\n        try:\n            return self.get_manager(\"classification\")\n        except (KeyError, RuntimeError) as e:\n            raise AttributeError(f\"Could not retrieve ClassificationManager: {e}\") from e\n\n    def _get_classification_content(self, model_type: str, **kwargs) -&gt; Union[str, Image.Image]:\n        \"\"\"\n        Provides the content for classifying the entire PDF.\n\n        Args:\n            model_type: 'text' or 'vision'.\n            **kwargs: Additional arguments (e.g., for text extraction or image rendering).\n\n        Returns:\n            Extracted text (str) or the first page's image (PIL.Image).\n\n        Raises:\n            ValueError: If model_type is 'vision' and PDF has != 1 page,\n                      or if model_type is unsupported, or if content cannot be generated.\n        \"\"\"\n        if model_type == \"text\":\n            try:\n                # Extract text from the whole document\n                text = self.extract_text(**kwargs)  # Pass relevant kwargs\n                if not text or text.isspace():\n                    raise ValueError(\"PDF contains no extractable text for classification.\")\n                return text\n            except Exception as e:\n                logger.error(f\"Error extracting text for PDF classification: {e}\")\n                raise ValueError(\"Failed to extract text for classification.\") from e\n\n        elif model_type == \"vision\":\n            if len(self.pages) == 1:\n                # Use the single page's content method\n                try:\n                    return self.pages[0]._get_classification_content(model_type=\"vision\", **kwargs)\n                except Exception as e:\n                    logger.error(f\"Error getting image from single page for classification: {e}\")\n                    raise ValueError(\"Failed to get image from single page.\") from e\n            elif len(self.pages) == 0:\n                raise ValueError(\"Cannot classify empty PDF using vision model.\")\n            else:\n                raise ValueError(\n                    f\"Vision classification for a PDF object is only supported for single-page PDFs. \"\n                    f\"This PDF has {len(self.pages)} pages. Use pdf.pages[0].classify() or pdf.classify_pages().\"\n                )\n        else:\n            raise ValueError(f\"Unsupported model_type for PDF classification: {model_type}\")\n\n    # --- End Classification Mixin Implementation ---\n\n    # ------------------------------------------------------------------\n    # Unified analysis storage (maps to metadata[\"analysis\"])\n    # ------------------------------------------------------------------\n\n    @property\n    def analyses(self) -&gt; Dict[str, Any]:\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            # For PDF, metadata property returns self._pdf.metadata which may be None\n            self._pdf.metadata = self._pdf.metadata or {}\n        if self.metadata is None:\n            # Fallback safeguard\n            self._pdf.metadata = {}\n        return self.metadata.setdefault(\"analysis\", {})  # type: ignore[attr-defined]\n\n    @analyses.setter\n    def analyses(self, value: Dict[str, Any]):\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            self._pdf.metadata = self._pdf.metadata or {}\n        self.metadata[\"analysis\"] = value  # type: ignore[attr-defined]\n\n    # Static helper for weakref.finalize to avoid capturing 'self'\n    @staticmethod\n    def _finalize_cleanup(plumber_pdf, temp_file_obj, is_stream):\n        try:\n            if plumber_pdf is not None:\n                plumber_pdf.close()\n        except Exception:\n            pass\n\n        if temp_file_obj and not is_stream:\n            try:\n                path = temp_file_obj.name if hasattr(temp_file_obj, \"name\") else None\n                if path and os.path.exists(path):\n                    os.unlink(path)\n            except Exception as e:\n                logger.warning(f\"Failed to clean up temporary file '{path}': {e}\")\n\n    def analyze_layout(self, *args, **kwargs) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Analyzes the layout of all pages in the PDF.\n\n        This is a convenience method that calls analyze_layout on the PDF's\n        page collection.\n\n        Args:\n            *args: Positional arguments passed to pages.analyze_layout().\n            **kwargs: Keyword arguments passed to pages.analyze_layout().\n\n        Returns:\n            An ElementCollection of all detected Region objects.\n        \"\"\"\n        return self.pages.analyze_layout(*args, **kwargs)\n\n    def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n        \"\"\"\n        Create a highlight context for accumulating highlights.\n\n        This allows for clean syntax to show multiple highlight groups:\n\n        Example:\n            with pdf.highlights() as h:\n                h.add(pdf.find_all('table'), label='tables', color='blue')\n                h.add(pdf.find_all('text:bold'), label='bold text', color='red')\n                h.show()\n\n        Or with automatic display:\n            with pdf.highlights(show=True) as h:\n                h.add(pdf.find_all('table'), label='tables')\n                h.add(pdf.find_all('text:bold'), label='bold')\n                # Automatically shows when exiting the context\n\n        Args:\n            show: If True, automatically show highlights when exiting context\n\n        Returns:\n            HighlightContext for accumulating highlights\n        \"\"\"\n        from natural_pdf.core.highlighting_service import HighlightContext\n\n        return HighlightContext(self, show_on_exit=show)\n</code></pre>"},{"location":"api/#natural_pdf.PDF-attributes","title":"Attributes","text":"<code>natural_pdf.PDF.metadata</code> <code>property</code> <p>Access PDF metadata as a dictionary.</p> <p>Returns document metadata such as title, author, creation date, and other properties embedded in the PDF file. The exact keys available depend on what metadata was included when the PDF was created.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing PDF metadata. Common keys include 'Title',</p> <code>Dict[str, Any]</code> <p>'Author', 'Subject', 'Creator', 'Producer', 'CreationDate', and</p> <code>Dict[str, Any]</code> <p>'ModDate'. May be empty if no metadata is available.</p> Example <pre><code>pdf = npdf.PDF(\"document.pdf\")\nprint(pdf.metadata.get('Title', 'No title'))\nprint(f\"Created: {pdf.metadata.get('CreationDate')}\")\n</code></pre> <code>natural_pdf.PDF.pages</code> <code>property</code> <p>Access pages as a PageCollection object.</p> <p>Provides access to individual pages of the PDF document through a collection interface that supports indexing, slicing, and iteration. Pages are lazy-loaded to minimize memory usage.</p> <p>Returns:</p> Type Description <code>PageCollection</code> <p>PageCollection object that provides list-like access to PDF pages.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If PDF pages are not yet initialized.</p> Example <pre><code>pdf = npdf.PDF(\"document.pdf\")\n\n# Access individual pages\nfirst_page = pdf.pages[0]\nlast_page = pdf.pages[-1]\n\n# Slice pages\nfirst_three = pdf.pages[0:3]\n\n# Iterate over pages\nfor page in pdf.pages:\n    print(f\"Page {page.index} has {len(page.chars)} characters\")\n</code></pre>"},{"location":"api/#natural_pdf.PDF-functions","title":"Functions","text":"<code>natural_pdf.PDF.__enter__()</code> <p>Context manager entry.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre> <code>natural_pdf.PDF.__exit__(exc_type, exc_val, exc_tb)</code> <p>Context manager exit.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    self.close()\n</code></pre> <code>natural_pdf.PDF.__getitem__(key)</code> <p>Access pages by index or slice.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def __getitem__(self, key) -&gt; Union[\"Page\", \"PageCollection\"]:\n    \"\"\"Access pages by index or slice.\"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not initialized yet.\")\n\n    if isinstance(key, slice):\n        from natural_pdf.core.page_collection import PageCollection\n\n        # Use the lazy page list's slicing which returns another _LazyPageList\n        lazy_slice = self._pages[key]\n        # Wrap in PageCollection for compatibility\n        return PageCollection(lazy_slice)\n    elif isinstance(key, int):\n        if 0 &lt;= key &lt; len(self._pages):\n            return self._pages[key]\n        else:\n            raise IndexError(f\"Page index {key} out of range (0-{len(self._pages)-1}).\")\n    else:\n        raise TypeError(f\"Page indices must be integers or slices, not {type(key)}.\")\n</code></pre> <code>natural_pdf.PDF.__init__(path_or_url_or_stream, reading_order=True, font_attrs=None, keep_spaces=True, text_tolerance=None, auto_text_tolerance=True, text_layer=True)</code> <p>Initialize the enhanced PDF object.</p> <p>Parameters:</p> Name Type Description Default <code>path_or_url_or_stream</code> <p>Path to the PDF file (str/Path), a URL (str), or a file-like object (stream). URLs must start with 'http://' or 'https://'.</p> required <code>reading_order</code> <code>bool</code> <p>If True, use natural reading order for text extraction. Defaults to True.</p> <code>True</code> <code>font_attrs</code> <code>Optional[List[str]]</code> <p>List of font attributes for grouping characters into words. Common attributes include ['fontname', 'size']. Defaults to None.</p> <code>None</code> <code>keep_spaces</code> <code>bool</code> <p>If True, include spaces in word elements during text extraction. Defaults to True.</p> <code>True</code> <code>text_tolerance</code> <code>Optional[dict]</code> <p>PDFplumber-style tolerance settings for text grouping. Dictionary with keys like 'x_tolerance', 'y_tolerance'. Defaults to None.</p> <code>None</code> <code>auto_text_tolerance</code> <code>bool</code> <p>If True, automatically scale text tolerance based on font size and document characteristics. Defaults to True.</p> <code>True</code> <code>text_layer</code> <code>bool</code> <p>If True, preserve existing text layer from the PDF. If False, removes all existing text elements during initialization, useful for OCR-only workflows. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If path_or_url_or_stream is not a valid type.</p> <code>IOError</code> <p>If the PDF file cannot be opened or read.</p> <code>ValueError</code> <p>If URL download fails.</p> Example <pre><code># From file path\npdf = npdf.PDF(\"document.pdf\")\n\n# From URL\npdf = npdf.PDF(\"https://example.com/document.pdf\")\n\n# From stream\nwith open(\"document.pdf\", \"rb\") as f:\n    pdf = npdf.PDF(f)\n\n# With custom settings\npdf = npdf.PDF(\"document.pdf\",\n              reading_order=False,\n              text_layer=False,  # For OCR-only processing\n              font_attrs=['fontname', 'size', 'flags'])\n</code></pre> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def __init__(\n    self,\n    path_or_url_or_stream,\n    reading_order: bool = True,\n    font_attrs: Optional[List[str]] = None,\n    keep_spaces: bool = True,\n    text_tolerance: Optional[dict] = None,\n    auto_text_tolerance: bool = True,\n    text_layer: bool = True,\n):\n    \"\"\"Initialize the enhanced PDF object.\n\n    Args:\n        path_or_url_or_stream: Path to the PDF file (str/Path), a URL (str),\n            or a file-like object (stream). URLs must start with 'http://' or 'https://'.\n        reading_order: If True, use natural reading order for text extraction.\n            Defaults to True.\n        font_attrs: List of font attributes for grouping characters into words.\n            Common attributes include ['fontname', 'size']. Defaults to None.\n        keep_spaces: If True, include spaces in word elements during text extraction.\n            Defaults to True.\n        text_tolerance: PDFplumber-style tolerance settings for text grouping.\n            Dictionary with keys like 'x_tolerance', 'y_tolerance'. Defaults to None.\n        auto_text_tolerance: If True, automatically scale text tolerance based on\n            font size and document characteristics. Defaults to True.\n        text_layer: If True, preserve existing text layer from the PDF. If False,\n            removes all existing text elements during initialization, useful for\n            OCR-only workflows. Defaults to True.\n\n    Raises:\n        TypeError: If path_or_url_or_stream is not a valid type.\n        IOError: If the PDF file cannot be opened or read.\n        ValueError: If URL download fails.\n\n    Example:\n        ```python\n        # From file path\n        pdf = npdf.PDF(\"document.pdf\")\n\n        # From URL\n        pdf = npdf.PDF(\"https://example.com/document.pdf\")\n\n        # From stream\n        with open(\"document.pdf\", \"rb\") as f:\n            pdf = npdf.PDF(f)\n\n        # With custom settings\n        pdf = npdf.PDF(\"document.pdf\",\n                      reading_order=False,\n                      text_layer=False,  # For OCR-only processing\n                      font_attrs=['fontname', 'size', 'flags'])\n        ```\n    \"\"\"\n    self._original_path_or_stream = path_or_url_or_stream\n    self._temp_file = None\n    self._resolved_path = None\n    self._is_stream = False\n    self._text_layer = text_layer\n    stream_to_open = None\n\n    if hasattr(path_or_url_or_stream, \"read\"):  # Check if it's file-like\n        logger.info(\"Initializing PDF from in-memory stream.\")\n        self._is_stream = True\n        self._resolved_path = None  # No resolved file path for streams\n        self.source_path = \"&lt;stream&gt;\"  # Identifier for source\n        self.path = self.source_path  # Use source identifier as path for streams\n        stream_to_open = path_or_url_or_stream\n        try:\n            if hasattr(path_or_url_or_stream, \"read\"):\n                # If caller provided an in-memory binary stream, capture bytes for potential re-export\n                current_pos = path_or_url_or_stream.tell()\n                path_or_url_or_stream.seek(0)\n                self._original_bytes = path_or_url_or_stream.read()\n                path_or_url_or_stream.seek(current_pos)\n        except Exception:\n            pass\n    elif isinstance(path_or_url_or_stream, (str, Path)):\n        path_or_url = str(path_or_url_or_stream)\n        self.source_path = path_or_url  # Store original path/URL as source\n        is_url = path_or_url.startswith(\"http://\") or path_or_url.startswith(\"https://\")\n\n        if is_url:\n            logger.info(f\"Downloading PDF from URL: {path_or_url}\")\n            try:\n                with urllib.request.urlopen(path_or_url) as response:\n                    data = response.read()\n                # Load directly into an in-memory buffer \u2014 no temp file needed\n                buffer = io.BytesIO(data)\n                buffer.seek(0)\n                self._temp_file = None  # No on-disk temp file\n                self._resolved_path = path_or_url  # For repr / get_id purposes\n                stream_to_open = buffer  # pdfplumber accepts file-like objects\n            except Exception as e:\n                logger.error(f\"Failed to download PDF from URL: {e}\")\n                raise ValueError(f\"Failed to download PDF from URL: {e}\")\n        else:\n            self._resolved_path = str(Path(path_or_url).resolve())  # Resolve local paths\n            stream_to_open = self._resolved_path\n        self.path = self._resolved_path  # Use resolved path for file-based PDFs\n    else:\n        raise TypeError(\n            f\"Invalid input type: {type(path_or_url_or_stream)}. \"\n            f\"Expected path (str/Path), URL (str), or file-like object.\"\n        )\n\n    logger.info(f\"Opening PDF source: {self.source_path}\")\n    logger.debug(\n        f\"Parameters: reading_order={reading_order}, font_attrs={font_attrs}, keep_spaces={keep_spaces}\"\n    )\n\n    try:\n        self._pdf = pdfplumber.open(stream_to_open)\n    except Exception as e:\n        logger.error(f\"Failed to open PDF: {e}\", exc_info=True)\n        self.close()  # Attempt cleanup if opening fails\n        raise IOError(f\"Failed to open PDF source: {self.source_path}\") from e\n\n    # Store configuration used for initialization\n    self._reading_order = reading_order\n    self._config = {\"keep_spaces\": keep_spaces}\n    self._font_attrs = font_attrs\n\n    self._ocr_manager = OCRManager() if OCRManager else None\n    self._layout_manager = LayoutManager() if LayoutManager else None\n    self.highlighter = HighlightingService(self)\n    # self._classification_manager_instance = ClassificationManager() # Removed this line\n    self._manager_registry = {}\n\n    # Lazily instantiate pages only when accessed\n    self._pages = _LazyPageList(\n        self, self._pdf, font_attrs=font_attrs, load_text=self._text_layer\n    )\n\n    self._element_cache = {}\n    self._exclusions = []\n    self._regions = []\n\n    logger.info(f\"PDF '{self.source_path}' initialized with {len(self._pages)} pages.\")\n\n    self._initialize_managers()\n    self._initialize_highlighter()\n\n    # Remove text layer if requested\n    if not self._text_layer:\n        logger.info(\"Removing text layer as requested (text_layer=False)\")\n        # Text layer is not loaded when text_layer=False, so no need to remove\n        pass\n\n    # Analysis results accessed via self.analyses property (see below)\n\n    # --- Automatic cleanup when object is garbage-collected ---\n    self._finalizer = weakref.finalize(\n        self,\n        PDF._finalize_cleanup,\n        self._pdf,\n        getattr(self, \"_temp_file\", None),\n        getattr(self, \"_is_stream\", False),\n    )\n\n    # --- Text tolerance settings ------------------------------------\n    # Users can pass pdfplumber-style keys (x_tolerance, x_tolerance_ratio,\n    # y_tolerance, etc.) via *text_tolerance*.  We also keep a flag that\n    # enables automatic tolerance scaling when explicit values are not\n    # supplied.\n    self._config[\"auto_text_tolerance\"] = bool(auto_text_tolerance)\n    if text_tolerance:\n        # Only copy recognised primitives (numbers / None); ignore junk.\n        allowed = {\n            \"x_tolerance\",\n            \"x_tolerance_ratio\",\n            \"y_tolerance\",\n            \"keep_blank_chars\",  # passthrough convenience\n        }\n        for k, v in text_tolerance.items():\n            if k in allowed:\n                self._config[k] = v\n</code></pre> <code>natural_pdf.PDF.__len__()</code> <p>Return the number of pages in the PDF.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of pages in the PDF.\"\"\"\n    if not hasattr(self, \"_pages\"):\n        return 0\n    return len(self._pages)\n</code></pre> <code>natural_pdf.PDF.__repr__()</code> <p>Return a string representation of the PDF object.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the PDF object.\"\"\"\n    if not hasattr(self, \"_pages\"):\n        page_count_str = \"uninitialized\"\n    else:\n        page_count_str = str(len(self._pages))\n\n    source_info = getattr(self, \"source_path\", \"unknown source\")\n    return f\"&lt;PDF source='{source_info}' pages={page_count_str}&gt;\"\n</code></pre> <code>natural_pdf.PDF.add_exclusion(exclusion_func, label=None)</code> <p>Add an exclusion function to the PDF.</p> <p>Exclusion functions define regions of each page that should be ignored during text extraction and analysis operations. This is useful for filtering out headers, footers, watermarks, or other administrative content that shouldn't be included in the main document processing.</p> <p>Parameters:</p> Name Type Description Default <code>exclusion_func</code> <p>A function that takes a Page object and returns a Region to exclude from processing, or None if no exclusion should be applied to that page. The function is called once per page.</p> required <code>label</code> <code>str</code> <p>Optional descriptive label for this exclusion rule, useful for debugging and identification.</p> <code>None</code> <p>Returns:</p> Type Description <code>PDF</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If PDF pages are not yet initialized.</p> Example <pre><code>pdf = npdf.PDF(\"document.pdf\")\n\n# Exclude headers (top 50 points of each page)\npdf.add_exclusion(\n    lambda page: page.region(0, 0, page.width, 50),\n    label=\"header_exclusion\"\n)\n\n# Exclude any text containing \"CONFIDENTIAL\"\npdf.add_exclusion(\n    lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above(include_source=True)\n    if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n    label=\"confidential_exclusion\"\n)\n\n# Chain multiple exclusions\npdf.add_exclusion(header_func).add_exclusion(footer_func)\n</code></pre> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def add_exclusion(self, exclusion_func, label: str = None) -&gt; \"PDF\":\n    \"\"\"Add an exclusion function to the PDF.\n\n    Exclusion functions define regions of each page that should be ignored during\n    text extraction and analysis operations. This is useful for filtering out headers,\n    footers, watermarks, or other administrative content that shouldn't be included\n    in the main document processing.\n\n    Args:\n        exclusion_func: A function that takes a Page object and returns a Region\n            to exclude from processing, or None if no exclusion should be applied\n            to that page. The function is called once per page.\n        label: Optional descriptive label for this exclusion rule, useful for\n            debugging and identification.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        AttributeError: If PDF pages are not yet initialized.\n\n    Example:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n\n        # Exclude headers (top 50 points of each page)\n        pdf.add_exclusion(\n            lambda page: page.region(0, 0, page.width, 50),\n            label=\"header_exclusion\"\n        )\n\n        # Exclude any text containing \"CONFIDENTIAL\"\n        pdf.add_exclusion(\n            lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above(include_source=True)\n            if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n            label=\"confidential_exclusion\"\n        )\n\n        # Chain multiple exclusions\n        pdf.add_exclusion(header_func).add_exclusion(footer_func)\n        ```\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    # ------------------------------------------------------------------\n    # Support selector strings and ElementCollection objects directly.\n    # Store exclusion and apply only to already-created pages.\n    # ------------------------------------------------------------------\n    from natural_pdf.elements.element_collection import ElementCollection  # local import\n\n    if isinstance(exclusion_func, str) or isinstance(exclusion_func, ElementCollection):\n        # Store for bookkeeping and lazy application\n        self._exclusions.append((exclusion_func, label))\n\n        # Apply only to already-created (cached) pages to avoid forcing page creation\n        for i in range(len(self._pages)):\n            if self._pages._cache[i] is not None:  # Only apply to existing pages\n                try:\n                    self._pages._cache[i].add_exclusion(exclusion_func, label=label)\n                except Exception as e:\n                    logger.warning(f\"Failed to apply exclusion to existing page {i}: {e}\")\n        return self\n\n    # Fallback to original callable / Region behaviour ------------------\n    exclusion_data = (exclusion_func, label)\n    self._exclusions.append(exclusion_data)\n\n    # Apply only to already-created (cached) pages to avoid forcing page creation\n    for i in range(len(self._pages)):\n        if self._pages._cache[i] is not None:  # Only apply to existing pages\n            try:\n                self._pages._cache[i].add_exclusion(exclusion_func, label=label)\n            except Exception as e:\n                logger.warning(f\"Failed to apply exclusion to existing page {i}: {e}\")\n\n    return self\n</code></pre> <code>natural_pdf.PDF.add_region(region_func, name=None)</code> <p>Add a region function to the PDF.</p> <p>Parameters:</p> Name Type Description Default <code>region_func</code> <code>Callable[[Page], Optional[Region]]</code> <p>A function that takes a Page and returns a Region, or None</p> required <code>name</code> <code>str</code> <p>Optional name for the region</p> <code>None</code> <p>Returns:</p> Type Description <code>PDF</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def add_region(\n    self, region_func: Callable[[\"Page\"], Optional[\"Region\"]], name: str = None\n) -&gt; \"PDF\":\n    \"\"\"\n    Add a region function to the PDF.\n\n    Args:\n        region_func: A function that takes a Page and returns a Region, or None\n        name: Optional name for the region\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    region_data = (region_func, name)\n    self._regions.append(region_data)\n\n    # Apply only to already-created (cached) pages to avoid forcing page creation\n    for i in range(len(self._pages)):\n        if self._pages._cache[i] is not None:  # Only apply to existing pages\n            page = self._pages._cache[i]\n            try:\n                region_instance = region_func(page)\n                if region_instance and isinstance(region_instance, Region):\n                    page.add_region(region_instance, name=name, source=\"named\")\n                elif region_instance is not None:\n                    logger.warning(\n                        f\"Region function did not return a valid Region for page {page.number}\"\n                    )\n            except Exception as e:\n                logger.error(f\"Error adding region for page {page.number}: {e}\")\n\n    return self\n</code></pre> <code>natural_pdf.PDF.analyze_layout(*args, **kwargs)</code> <p>Analyzes the layout of all pages in the PDF.</p> <p>This is a convenience method that calls analyze_layout on the PDF's page collection.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments passed to pages.analyze_layout().</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments passed to pages.analyze_layout().</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection[Region]</code> <p>An ElementCollection of all detected Region objects.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def analyze_layout(self, *args, **kwargs) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Analyzes the layout of all pages in the PDF.\n\n    This is a convenience method that calls analyze_layout on the PDF's\n    page collection.\n\n    Args:\n        *args: Positional arguments passed to pages.analyze_layout().\n        **kwargs: Keyword arguments passed to pages.analyze_layout().\n\n    Returns:\n        An ElementCollection of all detected Region objects.\n    \"\"\"\n    return self.pages.analyze_layout(*args, **kwargs)\n</code></pre> <code>natural_pdf.PDF.apply_ocr(engine=None, languages=None, min_confidence=None, device=None, resolution=None, apply_exclusions=True, detect_only=False, replace=True, options=None, pages=None)</code> <p>Apply OCR to specified pages of the PDF using batch processing.</p> <p>Performs optical character recognition on the specified pages, converting image-based text into searchable and extractable text elements. This method supports multiple OCR engines and provides batch processing for efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Optional[str]</code> <p>Name of the OCR engine to use. Supported engines include 'easyocr' (default), 'surya', 'paddle', and 'doctr'. If None, uses the global default from natural_pdf.options.ocr.engine.</p> <code>None</code> <code>languages</code> <code>Optional[List[str]]</code> <p>List of language codes for OCR recognition (e.g., ['en', 'es']). If None, uses the global default from natural_pdf.options.ocr.languages.</p> <code>None</code> <code>min_confidence</code> <code>Optional[float]</code> <p>Minimum confidence threshold (0.0-1.0) for accepting OCR results. Text with lower confidence will be filtered out. If None, uses the global default.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to run OCR on ('cpu', 'cuda', 'mps'). Engine-specific availability varies. If None, uses engine defaults.</p> <code>None</code> <code>resolution</code> <code>Optional[int]</code> <p>DPI resolution for rendering pages to images before OCR. Higher values improve accuracy but increase processing time and memory. Typical values: 150 (fast), 300 (balanced), 600 (high quality).</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>If True, mask excluded regions before OCR to prevent processing of headers, footers, or other unwanted content.</p> <code>True</code> <code>detect_only</code> <code>bool</code> <p>If True, only detect text bounding boxes without performing character recognition. Useful for layout analysis workflows.</p> <code>False</code> <code>replace</code> <code>bool</code> <p>If True, replace any existing OCR elements on the pages. If False, append new OCR results to existing elements.</p> <code>True</code> <code>options</code> <code>Optional[Any]</code> <p>Engine-specific options object (e.g., EasyOCROptions, SuryaOptions). Allows fine-tuning of engine behavior beyond common parameters.</p> <code>None</code> <code>pages</code> <code>Optional[Union[Iterable[int], range, slice]]</code> <p>Page indices to process. Can be: - None: Process all pages - slice: Process a range of pages (e.g., slice(0, 10)) - Iterable[int]: Process specific page indices (e.g., [0, 2, 5])</p> <code>None</code> <p>Returns:</p> Type Description <code>PDF</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid page index is provided.</p> <code>TypeError</code> <p>If pages parameter has invalid type.</p> <code>RuntimeError</code> <p>If OCR engine is not available or fails.</p> Example <pre><code>pdf = npdf.PDF(\"scanned_document.pdf\")\n\n# Basic OCR on all pages\npdf.apply_ocr()\n\n# High-quality OCR with specific settings\npdf.apply_ocr(\n    engine='easyocr',\n    languages=['en', 'es'],\n    resolution=300,\n    min_confidence=0.8\n)\n\n# OCR specific pages only\npdf.apply_ocr(pages=[0, 1, 2])  # First 3 pages\npdf.apply_ocr(pages=slice(5, 10))  # Pages 5-9\n\n# Detection-only workflow for layout analysis\npdf.apply_ocr(detect_only=True, resolution=150)\n</code></pre> Note <p>OCR processing can be time and memory intensive, especially at high resolutions. Consider using exclusions to mask unwanted regions and processing pages in batches for large documents.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def apply_ocr(\n    self,\n    engine: Optional[str] = None,\n    languages: Optional[List[str]] = None,\n    min_confidence: Optional[float] = None,\n    device: Optional[str] = None,\n    resolution: Optional[int] = None,\n    apply_exclusions: bool = True,\n    detect_only: bool = False,\n    replace: bool = True,\n    options: Optional[Any] = None,\n    pages: Optional[Union[Iterable[int], range, slice]] = None,\n) -&gt; \"PDF\":\n    \"\"\"Apply OCR to specified pages of the PDF using batch processing.\n\n    Performs optical character recognition on the specified pages, converting\n    image-based text into searchable and extractable text elements. This method\n    supports multiple OCR engines and provides batch processing for efficiency.\n\n    Args:\n        engine: Name of the OCR engine to use. Supported engines include\n            'easyocr' (default), 'surya', 'paddle', and 'doctr'. If None,\n            uses the global default from natural_pdf.options.ocr.engine.\n        languages: List of language codes for OCR recognition (e.g., ['en', 'es']).\n            If None, uses the global default from natural_pdf.options.ocr.languages.\n        min_confidence: Minimum confidence threshold (0.0-1.0) for accepting\n            OCR results. Text with lower confidence will be filtered out.\n            If None, uses the global default.\n        device: Device to run OCR on ('cpu', 'cuda', 'mps'). Engine-specific\n            availability varies. If None, uses engine defaults.\n        resolution: DPI resolution for rendering pages to images before OCR.\n            Higher values improve accuracy but increase processing time and memory.\n            Typical values: 150 (fast), 300 (balanced), 600 (high quality).\n        apply_exclusions: If True, mask excluded regions before OCR to prevent\n            processing of headers, footers, or other unwanted content.\n        detect_only: If True, only detect text bounding boxes without performing\n            character recognition. Useful for layout analysis workflows.\n        replace: If True, replace any existing OCR elements on the pages.\n            If False, append new OCR results to existing elements.\n        options: Engine-specific options object (e.g., EasyOCROptions, SuryaOptions).\n            Allows fine-tuning of engine behavior beyond common parameters.\n        pages: Page indices to process. Can be:\n            - None: Process all pages\n            - slice: Process a range of pages (e.g., slice(0, 10))\n            - Iterable[int]: Process specific page indices (e.g., [0, 2, 5])\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If invalid page index is provided.\n        TypeError: If pages parameter has invalid type.\n        RuntimeError: If OCR engine is not available or fails.\n\n    Example:\n        ```python\n        pdf = npdf.PDF(\"scanned_document.pdf\")\n\n        # Basic OCR on all pages\n        pdf.apply_ocr()\n\n        # High-quality OCR with specific settings\n        pdf.apply_ocr(\n            engine='easyocr',\n            languages=['en', 'es'],\n            resolution=300,\n            min_confidence=0.8\n        )\n\n        # OCR specific pages only\n        pdf.apply_ocr(pages=[0, 1, 2])  # First 3 pages\n        pdf.apply_ocr(pages=slice(5, 10))  # Pages 5-9\n\n        # Detection-only workflow for layout analysis\n        pdf.apply_ocr(detect_only=True, resolution=150)\n        ```\n\n    Note:\n        OCR processing can be time and memory intensive, especially at high\n        resolutions. Consider using exclusions to mask unwanted regions and\n        processing pages in batches for large documents.\n    \"\"\"\n    if not self._ocr_manager:\n        logger.error(\"OCRManager not available. Cannot apply OCR.\")\n        return self\n\n    # Apply global options as defaults, but allow explicit parameters to override\n    import natural_pdf\n\n    # Use global OCR options if parameters are not explicitly set\n    if engine is None:\n        engine = natural_pdf.options.ocr.engine\n    if languages is None:\n        languages = natural_pdf.options.ocr.languages\n    if min_confidence is None:\n        min_confidence = natural_pdf.options.ocr.min_confidence\n    if device is None:\n        pass  # No default device in options.ocr anymore\n\n    thread_id = threading.current_thread().name\n    logger.debug(f\"[{thread_id}] PDF.apply_ocr starting for {self.path}\")\n\n    target_pages = []\n\n    target_pages = []\n    if pages is None:\n        target_pages = self._pages\n    elif isinstance(pages, slice):\n        target_pages = self._pages[pages]\n    elif hasattr(pages, \"__iter__\"):\n        try:\n            target_pages = [self._pages[i] for i in pages]\n        except IndexError:\n            raise ValueError(\"Invalid page index provided in 'pages' iterable.\")\n        except TypeError:\n            raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n    else:\n        raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n\n    if not target_pages:\n        logger.warning(\"No pages selected for OCR processing.\")\n        return self\n\n    page_numbers = [p.number for p in target_pages]\n    logger.info(f\"Applying batch OCR to pages: {page_numbers}...\")\n\n    final_resolution = resolution or getattr(self, \"_config\", {}).get(\"resolution\", 150)\n    logger.debug(f\"Using OCR image resolution: {final_resolution} DPI\")\n\n    images_pil = []\n    page_image_map = []\n    logger.info(f\"[{thread_id}] Rendering {len(target_pages)} pages...\")\n    failed_page_num = \"unknown\"\n    render_start_time = time.monotonic()\n\n    try:\n        for i, page in enumerate(tqdm(target_pages, desc=\"Rendering pages\", leave=False)):\n            failed_page_num = page.number\n            logger.debug(f\"  Rendering page {page.number} (index {page.index})...\")\n            to_image_kwargs = {\n                \"resolution\": final_resolution,\n                \"include_highlights\": False,\n                \"exclusions\": \"mask\" if apply_exclusions else None,\n            }\n            # Use render() for clean image without highlights\n            img = page.render(resolution=final_resolution)\n            if img is None:\n                logger.error(f\"  Failed to render page {page.number} to image.\")\n                continue\n            images_pil.append(img)\n            page_image_map.append((page, img))\n    except Exception as e:\n        logger.error(f\"Failed to render pages for batch OCR: {e}\")\n        logger.error(f\"Failed to render pages for batch OCR: {e}\")\n        raise RuntimeError(f\"Failed to render page {failed_page_num} for OCR.\") from e\n\n    render_end_time = time.monotonic()\n    logger.debug(\n        f\"[{thread_id}] Finished rendering {len(images_pil)} images (Duration: {render_end_time - render_start_time:.2f}s)\"\n    )\n    logger.debug(\n        f\"[{thread_id}] Finished rendering {len(images_pil)} images (Duration: {render_end_time - render_start_time:.2f}s)\"\n    )\n\n    if not images_pil or not page_image_map:\n        logger.error(\"No images were successfully rendered for batch OCR.\")\n        return self\n\n    manager_args = {\n        \"images\": images_pil,\n        \"engine\": engine,\n        \"languages\": languages,\n        \"min_confidence\": min_confidence,\n        \"min_confidence\": min_confidence,\n        \"device\": device,\n        \"options\": options,\n        \"detect_only\": detect_only,\n    }\n    manager_args = {k: v for k, v in manager_args.items() if v is not None}\n\n    ocr_call_args = {k: v for k, v in manager_args.items() if k != \"images\"}\n    logger.info(f\"[{thread_id}] Calling OCR Manager with args: {ocr_call_args}...\")\n    logger.info(f\"[{thread_id}] Calling OCR Manager with args: {ocr_call_args}...\")\n    ocr_start_time = time.monotonic()\n\n    batch_results = self._ocr_manager.apply_ocr(**manager_args)\n\n    if not isinstance(batch_results, list) or len(batch_results) != len(images_pil):\n        logger.error(f\"OCR Manager returned unexpected result format or length.\")\n        return self\n\n    logger.info(\"OCR Manager batch processing complete.\")\n\n    ocr_end_time = time.monotonic()\n    logger.debug(\n        f\"[{thread_id}] OCR processing finished (Duration: {ocr_end_time - ocr_start_time:.2f}s)\"\n    )\n\n    logger.info(\"Adding OCR results to respective pages...\")\n    total_elements_added = 0\n\n    for i, (page, img) in enumerate(page_image_map):\n        results_for_page = batch_results[i]\n        if not isinstance(results_for_page, list):\n            logger.warning(\n                f\"Skipping results for page {page.number}: Expected list, got {type(results_for_page)}\"\n            )\n            continue\n\n        logger.debug(f\"  Processing {len(results_for_page)} results for page {page.number}...\")\n        try:\n            if manager_args.get(\"replace\", True) and hasattr(page, \"_element_mgr\"):\n                page._element_mgr.remove_ocr_elements()\n\n            img_scale_x = page.width / img.width if img.width &gt; 0 else 1\n            img_scale_y = page.height / img.height if img.height &gt; 0 else 1\n            elements = page._element_mgr.create_text_elements_from_ocr(\n                results_for_page, img_scale_x, img_scale_y\n            )\n\n            if elements:\n                total_elements_added += len(elements)\n                logger.debug(f\"  Added {len(elements)} OCR TextElements to page {page.number}.\")\n            else:\n                logger.debug(f\"  No valid TextElements created for page {page.number}.\")\n        except Exception as e:\n            logger.error(f\"  Error adding OCR elements to page {page.number}: {e}\")\n\n    logger.info(f\"Finished adding OCR results. Total elements added: {total_elements_added}\")\n    return self\n</code></pre> <code>natural_pdf.PDF.ask(question, mode='extractive', pages=None, min_confidence=0.1, model=None, **kwargs)</code> <p>Ask a single question about the document content.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>str</code> <p>Question string to ask about the document</p> required <code>mode</code> <code>str</code> <p>\"extractive\" to extract answer from document, \"generative\" to generate</p> <code>'extractive'</code> <code>pages</code> <code>Union[int, List[int], range]</code> <p>Specific pages to query (default: all pages)</p> <code>None</code> <code>min_confidence</code> <code>float</code> <p>Minimum confidence threshold for answers</p> <code>0.1</code> <code>model</code> <code>str</code> <p>Optional model name for question answering</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters passed to the QA engine</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict containing: answer, confidence, found, page_num, source_elements, etc.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def ask(\n    self,\n    question: str,\n    mode: str = \"extractive\",\n    pages: Union[int, List[int], range] = None,\n    min_confidence: float = 0.1,\n    model: str = None,\n    **kwargs,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Ask a single question about the document content.\n\n    Args:\n        question: Question string to ask about the document\n        mode: \"extractive\" to extract answer from document, \"generative\" to generate\n        pages: Specific pages to query (default: all pages)\n        min_confidence: Minimum confidence threshold for answers\n        model: Optional model name for question answering\n        **kwargs: Additional parameters passed to the QA engine\n\n    Returns:\n        Dict containing: answer, confidence, found, page_num, source_elements, etc.\n    \"\"\"\n    # Delegate to ask_batch and return the first result\n    results = self.ask_batch(\n        [question], mode=mode, pages=pages, min_confidence=min_confidence, model=model, **kwargs\n    )\n    return (\n        results[0]\n        if results\n        else {\n            \"answer\": None,\n            \"confidence\": 0.0,\n            \"found\": False,\n            \"page_num\": None,\n            \"source_elements\": [],\n        }\n    )\n</code></pre> <code>natural_pdf.PDF.ask_batch(questions, mode='extractive', pages=None, min_confidence=0.1, model=None, **kwargs)</code> <p>Ask multiple questions about the document content using batch processing.</p> <p>This method processes multiple questions efficiently in a single batch, avoiding the multiprocessing resource accumulation that can occur with sequential individual question calls.</p> <p>Parameters:</p> Name Type Description Default <code>questions</code> <code>List[str]</code> <p>List of question strings to ask about the document</p> required <code>mode</code> <code>str</code> <p>\"extractive\" to extract answer from document, \"generative\" to generate</p> <code>'extractive'</code> <code>pages</code> <code>Union[int, List[int], range]</code> <p>Specific pages to query (default: all pages)</p> <code>None</code> <code>min_confidence</code> <code>float</code> <p>Minimum confidence threshold for answers</p> <code>0.1</code> <code>model</code> <code>str</code> <p>Optional model name for question answering</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters passed to the QA engine</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of Dicts, each containing: answer, confidence, found, page_num, source_elements, etc.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def ask_batch(\n    self,\n    questions: List[str],\n    mode: str = \"extractive\",\n    pages: Union[int, List[int], range] = None,\n    min_confidence: float = 0.1,\n    model: str = None,\n    **kwargs,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Ask multiple questions about the document content using batch processing.\n\n    This method processes multiple questions efficiently in a single batch,\n    avoiding the multiprocessing resource accumulation that can occur with\n    sequential individual question calls.\n\n    Args:\n        questions: List of question strings to ask about the document\n        mode: \"extractive\" to extract answer from document, \"generative\" to generate\n        pages: Specific pages to query (default: all pages)\n        min_confidence: Minimum confidence threshold for answers\n        model: Optional model name for question answering\n        **kwargs: Additional parameters passed to the QA engine\n\n    Returns:\n        List of Dicts, each containing: answer, confidence, found, page_num, source_elements, etc.\n    \"\"\"\n    from natural_pdf.qa import get_qa_engine\n\n    if not questions:\n        return []\n\n    if not isinstance(questions, list) or not all(isinstance(q, str) for q in questions):\n        raise TypeError(\"'questions' must be a list of strings\")\n\n    qa_engine = get_qa_engine() if model is None else get_qa_engine(model_name=model)\n\n    # Resolve target pages\n    if pages is None:\n        target_pages = self.pages\n    elif isinstance(pages, int):\n        if 0 &lt;= pages &lt; len(self.pages):\n            target_pages = [self.pages[pages]]\n        else:\n            raise IndexError(f\"Page index {pages} out of range (0-{len(self.pages)-1})\")\n    elif isinstance(pages, (list, range)):\n        target_pages = []\n        for page_idx in pages:\n            if 0 &lt;= page_idx &lt; len(self.pages):\n                target_pages.append(self.pages[page_idx])\n            else:\n                logger.warning(f\"Page index {page_idx} out of range, skipping\")\n    else:\n        raise ValueError(f\"Invalid pages parameter: {pages}\")\n\n    if not target_pages:\n        logger.warning(\"No valid pages found for QA processing.\")\n        return [\n            {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": None,\n                \"source_elements\": [],\n            }\n            for _ in questions\n        ]\n\n    logger.info(\n        f\"Processing {len(questions)} question(s) across {len(target_pages)} page(s) using batch QA...\"\n    )\n\n    # Collect all page images and metadata for batch processing\n    page_images = []\n    page_word_boxes = []\n    page_metadata = []\n\n    for page in target_pages:\n        # Get page image\n        try:\n            # Use render() for clean image without highlights\n            page_image = page.render(resolution=150)\n            if page_image is None:\n                logger.warning(f\"Failed to render image for page {page.number}, skipping\")\n                continue\n\n            # Get text elements for word boxes\n            elements = page.find_all(\"text\")\n            if not elements:\n                logger.warning(f\"No text elements found on page {page.number}\")\n                word_boxes = []\n            else:\n                word_boxes = qa_engine._get_word_boxes_from_elements(\n                    elements, offset_x=0, offset_y=0\n                )\n\n            page_images.append(page_image)\n            page_word_boxes.append(word_boxes)\n            page_metadata.append({\"page_number\": page.number, \"page_object\": page})\n\n        except Exception as e:\n            logger.warning(f\"Error processing page {page.number}: {e}\")\n            continue\n\n    if not page_images:\n        logger.warning(\"No page images could be processed for QA.\")\n        return [\n            {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": None,\n                \"source_elements\": [],\n            }\n            for _ in questions\n        ]\n\n    # Process all questions against all pages in batch\n    all_results = []\n\n    for question_text in questions:\n        question_results = []\n\n        # Ask this question against each page (but in batch per page)\n        for i, (page_image, word_boxes, page_meta) in enumerate(\n            zip(page_images, page_word_boxes, page_metadata)\n        ):\n            try:\n                # Use the DocumentQA batch interface\n                page_result = qa_engine.ask(\n                    image=page_image,\n                    question=question_text,\n                    word_boxes=word_boxes,\n                    min_confidence=min_confidence,\n                    **kwargs,\n                )\n\n                if page_result and page_result.found:\n                    # Add page metadata to result\n                    page_result_dict = {\n                        \"answer\": page_result.answer,\n                        \"confidence\": page_result.confidence,\n                        \"found\": page_result.found,\n                        \"page_num\": page_meta[\"page_number\"],\n                        \"source_elements\": getattr(page_result, \"source_elements\", []),\n                        \"start\": getattr(page_result, \"start\", -1),\n                        \"end\": getattr(page_result, \"end\", -1),\n                    }\n                    question_results.append(page_result_dict)\n\n            except Exception as e:\n                logger.warning(\n                    f\"Error processing question '{question_text}' on page {page_meta['page_number']}: {e}\"\n                )\n                continue\n\n        # Sort results by confidence and take the best one for this question\n        question_results.sort(key=lambda x: x.get(\"confidence\", 0), reverse=True)\n\n        if question_results:\n            all_results.append(question_results[0])\n        else:\n            # No results found for this question\n            all_results.append(\n                {\n                    \"answer\": None,\n                    \"confidence\": 0.0,\n                    \"found\": False,\n                    \"page_num\": None,\n                    \"source_elements\": [],\n                }\n            )\n\n    return all_results\n</code></pre> <code>natural_pdf.PDF.classify_pages(labels, model=None, pages=None, analysis_key='classification', using=None, **kwargs)</code> <p>Classifies specified pages of the PDF.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>List[str]</code> <p>List of category names</p> required <code>model</code> <code>Optional[str]</code> <p>Model identifier ('text', 'vision', or specific HF ID)</p> <code>None</code> <code>pages</code> <code>Optional[Union[Iterable[int], range, slice]]</code> <p>Page indices, slice, or None for all pages</p> <code>None</code> <code>analysis_key</code> <code>str</code> <p>Key to store results in page's analyses dict</p> <code>'classification'</code> <code>using</code> <code>Optional[str]</code> <p>Processing mode ('text' or 'vision')</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for the ClassificationManager</p> <code>{}</code> <p>Returns:</p> Type Description <code>PDF</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def classify_pages(\n    self,\n    labels: List[str],\n    model: Optional[str] = None,\n    pages: Optional[Union[Iterable[int], range, slice]] = None,\n    analysis_key: str = \"classification\",\n    using: Optional[str] = None,\n    **kwargs,\n) -&gt; \"PDF\":\n    \"\"\"\n    Classifies specified pages of the PDF.\n\n    Args:\n        labels: List of category names\n        model: Model identifier ('text', 'vision', or specific HF ID)\n        pages: Page indices, slice, or None for all pages\n        analysis_key: Key to store results in page's analyses dict\n        using: Processing mode ('text' or 'vision')\n        **kwargs: Additional arguments for the ClassificationManager\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    if not labels:\n        raise ValueError(\"Labels list cannot be empty.\")\n\n    try:\n        manager = self.get_manager(\"classification\")\n    except (ValueError, RuntimeError) as e:\n        raise ClassificationError(f\"Cannot get ClassificationManager: {e}\") from e\n\n    if not manager or not manager.is_available():\n        from natural_pdf.classification.manager import is_classification_available\n\n        if not is_classification_available():\n            raise ImportError(\n                \"Classification dependencies missing. \"\n                'Install with: pip install \"natural-pdf[ai]\"'\n            )\n        raise ClassificationError(\"ClassificationManager not available.\")\n\n    target_pages = []\n    if pages is None:\n        target_pages = self._pages\n    elif isinstance(pages, slice):\n        target_pages = self._pages[pages]\n    elif hasattr(pages, \"__iter__\"):\n        try:\n            target_pages = [self._pages[i] for i in pages]\n        except IndexError:\n            raise ValueError(\"Invalid page index provided.\")\n        except TypeError:\n            raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n    else:\n        raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n\n    if not target_pages:\n        logger.warning(\"No pages selected for classification.\")\n        return self\n\n    inferred_using = manager.infer_using(model if model else manager.DEFAULT_TEXT_MODEL, using)\n    logger.info(\n        f\"Classifying {len(target_pages)} pages using model '{model or '(default)'}' (mode: {inferred_using})\"\n    )\n\n    page_contents = []\n    pages_to_classify = []\n    logger.debug(f\"Gathering content for {len(target_pages)} pages...\")\n\n    for page in target_pages:\n        try:\n            content = page._get_classification_content(model_type=inferred_using, **kwargs)\n            page_contents.append(content)\n            pages_to_classify.append(page)\n        except ValueError as e:\n            logger.warning(f\"Skipping page {page.number}: Cannot get content - {e}\")\n        except Exception as e:\n            logger.warning(f\"Skipping page {page.number}: Error getting content - {e}\")\n\n    if not page_contents:\n        logger.warning(\"No content could be gathered for batch classification.\")\n        return self\n\n    logger.debug(f\"Gathered content for {len(pages_to_classify)} pages.\")\n\n    try:\n        batch_results = manager.classify_batch(\n            item_contents=page_contents,\n            labels=labels,\n            model_id=model,\n            using=inferred_using,\n            **kwargs,\n        )\n    except Exception as e:\n        logger.error(f\"Batch classification failed: {e}\")\n        raise ClassificationError(f\"Batch classification failed: {e}\") from e\n\n    if len(batch_results) != len(pages_to_classify):\n        logger.error(\n            f\"Mismatch between number of results ({len(batch_results)}) and pages ({len(pages_to_classify)})\"\n        )\n        return self\n\n    logger.debug(\n        f\"Distributing {len(batch_results)} results to pages under key '{analysis_key}'...\"\n    )\n    for page, result_obj in zip(pages_to_classify, batch_results):\n        try:\n            if not hasattr(page, \"analyses\") or page.analyses is None:\n                page.analyses = {}\n            page.analyses[analysis_key] = result_obj\n        except Exception as e:\n            logger.warning(\n                f\"Failed to store classification results for page {page.number}: {e}\"\n            )\n\n    logger.info(f\"Finished classifying PDF pages.\")\n    return self\n</code></pre> <code>natural_pdf.PDF.clear_exclusions()</code> <p>Clear all exclusion functions from the PDF.</p> <p>Removes all previously added exclusion functions that were used to filter out unwanted content (like headers, footers, or administrative text) from text extraction and analysis operations.</p> <p>Returns:</p> Type Description <code>PDF</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If PDF pages are not yet initialized.</p> Example <pre><code>pdf = npdf.PDF(\"document.pdf\")\npdf.add_exclusion(lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above())\n\n# Later, remove all exclusions\npdf.clear_exclusions()\n</code></pre> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def clear_exclusions(self) -&gt; \"PDF\":\n    \"\"\"Clear all exclusion functions from the PDF.\n\n    Removes all previously added exclusion functions that were used to filter\n    out unwanted content (like headers, footers, or administrative text) from\n    text extraction and analysis operations.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        AttributeError: If PDF pages are not yet initialized.\n\n    Example:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n        pdf.add_exclusion(lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above())\n\n        # Later, remove all exclusions\n        pdf.clear_exclusions()\n        ```\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    self._exclusions = []\n\n    # Clear exclusions only from already-created (cached) pages to avoid forcing page creation\n    for i in range(len(self._pages)):\n        if self._pages._cache[i] is not None:  # Only clear from existing pages\n            try:\n                self._pages._cache[i].clear_exclusions()\n            except Exception as e:\n                logger.warning(f\"Failed to clear exclusions from existing page {i}: {e}\")\n    return self\n</code></pre> <code>natural_pdf.PDF.close()</code> <p>Close the underlying PDF file and clean up any temporary files.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def close(self):\n    \"\"\"Close the underlying PDF file and clean up any temporary files.\"\"\"\n    if hasattr(self, \"_pdf\") and self._pdf is not None:\n        try:\n            self._pdf.close()\n            logger.debug(f\"Closed pdfplumber PDF object for {self.source_path}\")\n        except Exception as e:\n            logger.warning(f\"Error closing pdfplumber object: {e}\")\n        finally:\n            self._pdf = None\n\n    if hasattr(self, \"_temp_file\") and self._temp_file is not None:\n        temp_file_path = None\n        try:\n            if hasattr(self._temp_file, \"name\") and self._temp_file.name:\n                temp_file_path = self._temp_file.name\n                # Only unlink if it exists and _is_stream is False (meaning WE created it)\n                if not self._is_stream and os.path.exists(temp_file_path):\n                    os.unlink(temp_file_path)\n                    logger.debug(f\"Removed temporary PDF file: {temp_file_path}\")\n        except Exception as e:\n            logger.warning(f\"Failed to clean up temporary file '{temp_file_path}': {e}\")\n\n    # Cancels the weakref finalizer so we don't double-clean\n    if hasattr(self, \"_finalizer\") and self._finalizer.alive:\n        self._finalizer()\n</code></pre> <code>natural_pdf.PDF.deskew(pages=None, resolution=300, angle=None, detection_resolution=72, force_overwrite=False, **deskew_kwargs)</code> <p>Creates a new, in-memory PDF object containing deskewed versions of the specified pages from the original PDF.</p> <p>This method renders each selected page, detects and corrects skew using the 'deskew' library, and then combines the resulting images into a new PDF using 'img2pdf'. The new PDF object is returned directly.</p> <p>Important: The returned PDF is image-based. Any existing text, OCR results, annotations, or other elements from the original pages will not be carried over.</p> <p>Parameters:</p> Name Type Description Default <code>pages</code> <code>Optional[Union[Iterable[int], range, slice]]</code> <p>Page indices/slice to include (0-based). If None, processes all pages.</p> <code>None</code> <code>resolution</code> <code>int</code> <p>DPI resolution for rendering the output deskewed pages.</p> <code>300</code> <code>angle</code> <code>Optional[float]</code> <p>The specific angle (in degrees) to rotate by. If None, detects automatically.</p> <code>None</code> <code>detection_resolution</code> <code>int</code> <p>DPI resolution used for skew detection if angles are not                   already cached on the page objects.</p> <code>72</code> <code>force_overwrite</code> <code>bool</code> <p>If False (default), raises a ValueError if any target page              already contains processed elements (text, OCR, regions) to              prevent accidental data loss. Set to True to proceed anyway.</p> <code>False</code> <code>**deskew_kwargs</code> <p>Additional keyword arguments passed to <code>deskew.determine_skew</code>              during automatic detection (e.g., <code>max_angle</code>, <code>num_peaks</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>PDF</code> <p>A new PDF object representing the deskewed document.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If 'deskew' or 'img2pdf' libraries are not installed.</p> <code>ValueError</code> <p>If <code>force_overwrite</code> is False and target pages contain elements.</p> <code>FileNotFoundError</code> <p>If the source PDF cannot be read (if file-based).</p> <code>IOError</code> <p>If creating the in-memory PDF fails.</p> <code>RuntimeError</code> <p>If rendering or deskewing individual pages fails.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def deskew(\n    self,\n    pages: Optional[Union[Iterable[int], range, slice]] = None,\n    resolution: int = 300,\n    angle: Optional[float] = None,\n    detection_resolution: int = 72,\n    force_overwrite: bool = False,\n    **deskew_kwargs,\n) -&gt; \"PDF\":\n    \"\"\"\n    Creates a new, in-memory PDF object containing deskewed versions of the\n    specified pages from the original PDF.\n\n    This method renders each selected page, detects and corrects skew using the 'deskew'\n    library, and then combines the resulting images into a new PDF using 'img2pdf'.\n    The new PDF object is returned directly.\n\n    Important: The returned PDF is image-based. Any existing text, OCR results,\n    annotations, or other elements from the original pages will *not* be carried over.\n\n    Args:\n        pages: Page indices/slice to include (0-based). If None, processes all pages.\n        resolution: DPI resolution for rendering the output deskewed pages.\n        angle: The specific angle (in degrees) to rotate by. If None, detects automatically.\n        detection_resolution: DPI resolution used for skew detection if angles are not\n                              already cached on the page objects.\n        force_overwrite: If False (default), raises a ValueError if any target page\n                         already contains processed elements (text, OCR, regions) to\n                         prevent accidental data loss. Set to True to proceed anyway.\n        **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                         during automatic detection (e.g., `max_angle`, `num_peaks`).\n\n    Returns:\n        A new PDF object representing the deskewed document.\n\n    Raises:\n        ImportError: If 'deskew' or 'img2pdf' libraries are not installed.\n        ValueError: If `force_overwrite` is False and target pages contain elements.\n        FileNotFoundError: If the source PDF cannot be read (if file-based).\n        IOError: If creating the in-memory PDF fails.\n        RuntimeError: If rendering or deskewing individual pages fails.\n    \"\"\"\n    if not DESKEW_AVAILABLE:\n        raise ImportError(\n            \"Deskew/img2pdf libraries missing. Install with: pip install natural-pdf[deskew]\"\n        )\n\n    target_pages = self._get_target_pages(pages)  # Use helper to resolve pages\n\n    # --- Safety Check --- #\n    if not force_overwrite:\n        for page in target_pages:\n            # Check if the element manager has been initialized and contains any elements\n            if (\n                hasattr(page, \"_element_mgr\")\n                and page._element_mgr\n                and page._element_mgr.has_elements()\n            ):\n                raise ValueError(\n                    f\"Page {page.number} contains existing elements (text, OCR, etc.). \"\n                    f\"Deskewing creates an image-only PDF, discarding these elements. \"\n                    f\"Set force_overwrite=True to proceed.\"\n                )\n\n    # --- Process Pages --- #\n    deskewed_images_bytes = []\n    logger.info(f\"Deskewing {len(target_pages)} pages (output resolution={resolution} DPI)...\")\n\n    for page in tqdm(target_pages, desc=\"Deskewing Pages\", leave=False):\n        try:\n            # Use page.deskew to get the corrected PIL image\n            # Pass down resolutions and kwargs\n            deskewed_img = page.deskew(\n                resolution=resolution,\n                angle=angle,  # Let page.deskew handle detection/caching\n                detection_resolution=detection_resolution,\n                **deskew_kwargs,\n            )\n\n            if not deskewed_img:\n                logger.warning(\n                    f\"Page {page.number}: Failed to generate deskewed image, skipping.\"\n                )\n                continue\n\n            # Convert image to bytes for img2pdf (use PNG for lossless quality)\n            with io.BytesIO() as buf:\n                deskewed_img.save(buf, format=\"PNG\")\n                deskewed_images_bytes.append(buf.getvalue())\n\n        except Exception as e:\n            logger.error(\n                f\"Page {page.number}: Failed during deskewing process: {e}\", exc_info=True\n            )\n            # Option: Raise a runtime error, or continue and skip the page?\n            # Raising makes the whole operation fail if one page fails.\n            raise RuntimeError(f\"Failed to process page {page.number} during deskewing.\") from e\n\n    # --- Create PDF --- #\n    if not deskewed_images_bytes:\n        raise RuntimeError(\"No pages were successfully processed to create the deskewed PDF.\")\n\n    logger.info(f\"Combining {len(deskewed_images_bytes)} deskewed images into in-memory PDF...\")\n    try:\n        # Use img2pdf to combine image bytes into PDF bytes\n        pdf_bytes = img2pdf.convert(deskewed_images_bytes)\n\n        # Wrap bytes in a stream\n        pdf_stream = io.BytesIO(pdf_bytes)\n\n        # Create a new PDF object from the stream using original config\n        logger.info(\"Creating new PDF object from deskewed stream...\")\n        new_pdf = PDF(\n            pdf_stream,\n            reading_order=self._reading_order,\n            font_attrs=self._font_attrs,\n            keep_spaces=self._config.get(\"keep_spaces\", True),\n            text_layer=self._text_layer,\n        )\n        return new_pdf\n    except Exception as e:\n        logger.error(f\"Failed to create in-memory PDF using img2pdf/PDF init: {e}\")\n        raise IOError(\"Failed to create deskewed PDF object from image stream.\") from e\n</code></pre> <code>natural_pdf.PDF.export_ocr_correction_task(output_zip_path, **kwargs)</code> <p>Exports OCR results from this PDF into a correction task package. Exports OCR results from this PDF into a correction task package.</p> <p>Parameters:</p> Name Type Description Default <code>output_zip_path</code> <code>str</code> <p>The path to save the output zip file</p> required <code>output_zip_path</code> <code>str</code> <p>The path to save the output zip file</p> required <code>**kwargs</code> <p>Additional arguments passed to create_correction_task_package</p> <code>{}</code> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def export_ocr_correction_task(self, output_zip_path: str, **kwargs):\n    \"\"\"\n    Exports OCR results from this PDF into a correction task package.\n    Exports OCR results from this PDF into a correction task package.\n\n    Args:\n        output_zip_path: The path to save the output zip file\n        output_zip_path: The path to save the output zip file\n        **kwargs: Additional arguments passed to create_correction_task_package\n    \"\"\"\n    try:\n        from natural_pdf.utils.packaging import create_correction_task_package\n\n        create_correction_task_package(source=self, output_zip_path=output_zip_path, **kwargs)\n    except ImportError:\n        logger.error(\n            \"Failed to import 'create_correction_task_package'. Packaging utility might be missing.\"\n        )\n        logger.error(\n            \"Failed to import 'create_correction_task_package'. Packaging utility might be missing.\"\n        )\n    except Exception as e:\n        logger.error(f\"Failed to export correction task: {e}\")\n        raise\n        logger.error(f\"Failed to export correction task: {e}\")\n        raise\n</code></pre> <code>natural_pdf.PDF.extract_tables(selector=None, merge_across_pages=False, **kwargs)</code> <p>Extract tables from the document or matching elements.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>Optional selector to filter tables</p> <code>None</code> <code>merge_across_pages</code> <code>bool</code> <p>Whether to merge tables that span across pages</p> <code>False</code> <code>**kwargs</code> <p>Additional extraction parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Any]</code> <p>List of extracted tables</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def extract_tables(\n    self, selector: Optional[str] = None, merge_across_pages: bool = False, **kwargs\n) -&gt; List[Any]:\n    \"\"\"\n    Extract tables from the document or matching elements.\n\n    Args:\n        selector: Optional selector to filter tables\n        merge_across_pages: Whether to merge tables that span across pages\n        **kwargs: Additional extraction parameters\n\n    Returns:\n        List of extracted tables\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    logger.warning(\"PDF.extract_tables is not fully implemented yet.\")\n    all_tables = []\n\n    for page in self.pages:\n        if hasattr(page, \"extract_tables\"):\n            all_tables.extend(page.extract_tables(**kwargs))\n        else:\n            logger.debug(f\"Page {page.number} does not have extract_tables method.\")\n\n    if selector:\n        logger.warning(\"Filtering extracted tables by selector is not implemented.\")\n\n    if merge_across_pages:\n        logger.warning(\"Merging tables across pages is not implemented.\")\n\n    return all_tables\n</code></pre> <code>natural_pdf.PDF.extract_text(selector=None, preserve_whitespace=True, use_exclusions=True, debug_exclusions=False, **kwargs)</code> <p>Extract text from the entire document or matching elements.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>Optional selector to filter elements</p> <code>None</code> <code>preserve_whitespace</code> <p>Whether to keep blank characters</p> <code>True</code> <code>use_exclusions</code> <p>Whether to apply exclusion regions</p> <code>True</code> <code>debug_exclusions</code> <p>Whether to output detailed debugging for exclusions</p> <code>False</code> <code>preserve_whitespace</code> <p>Whether to keep blank characters</p> <code>True</code> <code>use_exclusions</code> <p>Whether to apply exclusion regions</p> <code>True</code> <code>debug_exclusions</code> <p>Whether to output detailed debugging for exclusions</p> <code>False</code> <code>**kwargs</code> <p>Additional extraction parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Extracted text as string</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def extract_text(\n    self,\n    selector: Optional[str] = None,\n    preserve_whitespace=True,\n    use_exclusions=True,\n    debug_exclusions=False,\n    **kwargs,\n) -&gt; str:\n    \"\"\"\n    Extract text from the entire document or matching elements.\n\n    Args:\n        selector: Optional selector to filter elements\n        preserve_whitespace: Whether to keep blank characters\n        use_exclusions: Whether to apply exclusion regions\n        debug_exclusions: Whether to output detailed debugging for exclusions\n        preserve_whitespace: Whether to keep blank characters\n        use_exclusions: Whether to apply exclusion regions\n        debug_exclusions: Whether to output detailed debugging for exclusions\n        **kwargs: Additional extraction parameters\n\n    Returns:\n        Extracted text as string\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    if selector:\n        elements = self.find_all(selector, apply_exclusions=use_exclusions, **kwargs)\n        return elements.extract_text(preserve_whitespace=preserve_whitespace, **kwargs)\n\n    if debug_exclusions:\n        print(f\"PDF: Extracting text with exclusions from {len(self.pages)} pages\")\n        print(f\"PDF: Found {len(self._exclusions)} document-level exclusions\")\n\n    texts = []\n    for page in self.pages:\n        texts.append(\n            page.extract_text(\n                preserve_whitespace=preserve_whitespace,\n                use_exclusions=use_exclusions,\n                debug_exclusions=debug_exclusions,\n                **kwargs,\n            )\n        )\n\n    if debug_exclusions:\n        print(f\"PDF: Combined {len(texts)} pages of text\")\n\n    return \"\\n\".join(texts)\n</code></pre> <code>natural_pdf.PDF.find(selector=None, *, text=None, apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find(*, text: str, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[Any]\n</code></pre><pre><code>find(selector: str, *, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[Any]\n</code></pre> <p>Find the first element matching the selector OR text content across all pages.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional filter parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Element object or None if not found.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def find(\n    self,\n    selector: Optional[str] = None,\n    *,\n    text: Optional[str] = None,\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; Optional[Any]:\n    \"\"\"\n    Find the first element matching the selector OR text content across all pages.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional filter parameters.\n\n    Returns:\n        Element object or None if not found.\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    if selector is not None and text is not None:\n        raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n    if selector is None and text is None:\n        raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n    # Construct selector if 'text' is provided\n    effective_selector = \"\"\n    if text is not None:\n        escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n        effective_selector = f'text:contains(\"{escaped_text}\")'\n        logger.debug(\n            f\"Using text shortcut: find(text='{text}') -&gt; find('{effective_selector}')\"\n        )\n    elif selector is not None:\n        effective_selector = selector\n    else:\n        raise ValueError(\"Internal error: No selector or text provided.\")\n\n    selector_obj = parse_selector(effective_selector)\n\n    # Search page by page\n    for page in self.pages:\n        # Note: _apply_selector is on Page, so we call find directly here\n        # We pass the constructed/validated effective_selector\n        element = page.find(\n            selector=effective_selector,  # Use the processed selector\n            apply_exclusions=apply_exclusions,\n            regex=regex,  # Pass down flags\n            case=case,\n            **kwargs,\n        )\n        if element:\n            return element\n    return None  # Not found on any page\n</code></pre> <code>natural_pdf.PDF.find_all(selector=None, *, text=None, apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find_all(*, text: str, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre><pre><code>find_all(selector: str, *, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre> <p>Find all elements matching the selector OR text content across all pages.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional filter parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection with matching elements.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def find_all(\n    self,\n    selector: Optional[str] = None,\n    *,\n    text: Optional[str] = None,\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Find all elements matching the selector OR text content across all pages.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional filter parameters.\n\n    Returns:\n        ElementCollection with matching elements.\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    if selector is not None and text is not None:\n        raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n    if selector is None and text is None:\n        raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n    # Construct selector if 'text' is provided\n    effective_selector = \"\"\n    if text is not None:\n        escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n        effective_selector = f'text:contains(\"{escaped_text}\")'\n        logger.debug(\n            f\"Using text shortcut: find_all(text='{text}') -&gt; find_all('{effective_selector}')\"\n        )\n    elif selector is not None:\n        effective_selector = selector\n    else:\n        raise ValueError(\"Internal error: No selector or text provided.\")\n\n    # Instead of parsing here, let each page parse and apply\n    # This avoids parsing the same selector multiple times if not needed\n    # selector_obj = parse_selector(effective_selector)\n\n    # kwargs[\"regex\"] = regex # Removed: Already passed explicitly\n    # kwargs[\"case\"] = case   # Removed: Already passed explicitly\n\n    all_elements = []\n    for page in self.pages:\n        # Call page.find_all with the effective selector and flags\n        page_elements = page.find_all(\n            selector=effective_selector,\n            apply_exclusions=apply_exclusions,\n            regex=regex,\n            case=case,\n            **kwargs,\n        )\n        if page_elements:\n            all_elements.extend(page_elements.elements)\n\n    from natural_pdf.elements.element_collection import ElementCollection\n\n    return ElementCollection(all_elements)\n</code></pre> <code>natural_pdf.PDF.get_id()</code> <p>Get unique identifier for this PDF.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"Get unique identifier for this PDF.\"\"\"\n    \"\"\"Get unique identifier for this PDF.\"\"\"\n    return self.path\n</code></pre> <code>natural_pdf.PDF.get_manager(key)</code> <p>Retrieve a manager instance by its key, instantiating it lazily if needed.</p> <p>Managers are specialized components that handle specific functionality like classification, structured data extraction, or OCR processing. They are instantiated on-demand to minimize memory usage and startup time.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The manager key to retrieve. Common keys include 'classification' and 'structured_data'.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The manager instance for the specified key.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no manager is registered for the given key.</p> <code>RuntimeError</code> <p>If the manager failed to initialize.</p> Example <pre><code>pdf = npdf.PDF(\"document.pdf\")\nclassification_mgr = pdf.get_manager('classification')\nstructured_data_mgr = pdf.get_manager('structured_data')\n</code></pre> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def get_manager(self, key: str) -&gt; Any:\n    \"\"\"Retrieve a manager instance by its key, instantiating it lazily if needed.\n\n    Managers are specialized components that handle specific functionality like\n    classification, structured data extraction, or OCR processing. They are\n    instantiated on-demand to minimize memory usage and startup time.\n\n    Args:\n        key: The manager key to retrieve. Common keys include 'classification'\n            and 'structured_data'.\n\n    Returns:\n        The manager instance for the specified key.\n\n    Raises:\n        KeyError: If no manager is registered for the given key.\n        RuntimeError: If the manager failed to initialize.\n\n    Example:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n        classification_mgr = pdf.get_manager('classification')\n        structured_data_mgr = pdf.get_manager('structured_data')\n        ```\n    \"\"\"\n    # Check if already instantiated\n    if key in self._managers:\n        manager_instance = self._managers[key]\n        if manager_instance is None:\n            raise RuntimeError(f\"Manager '{key}' failed to initialize previously.\")\n        return manager_instance\n\n    # Not instantiated yet: get factory/class\n    if not hasattr(self, \"_manager_factories\") or key not in self._manager_factories:\n        raise KeyError(\n            f\"No manager registered for key '{key}'. Available: {list(getattr(self, '_manager_factories', {}).keys())}\"\n        )\n    factory_or_class = self._manager_factories[key]\n    try:\n        resolved = factory_or_class\n        # If it's a callable that's not a class, call it to get the class/instance\n        if not isinstance(resolved, type) and callable(resolved):\n            resolved = resolved()\n        # If it's a class, instantiate it\n        if isinstance(resolved, type):\n            instance = resolved()\n        else:\n            instance = resolved  # Already an instance\n        self._managers[key] = instance\n        return instance\n    except Exception as e:\n        logger.error(f\"Failed to initialize manager for key '{key}': {e}\")\n        self._managers[key] = None\n        raise RuntimeError(f\"Manager '{key}' failed to initialize: {e}\") from e\n</code></pre> <code>natural_pdf.PDF.get_sections(start_elements=None, end_elements=None, new_section_on_page_break=False, include_boundaries='both')</code> <p>Extract sections from the entire PDF based on start/end elements.</p> <p>This method delegates to the PageCollection.get_sections() method, providing a convenient way to extract document sections across all pages.</p> <p>Parameters:</p> Name Type Description Default <code>start_elements</code> <p>Elements or selector string that mark the start of sections (optional)</p> <code>None</code> <code>end_elements</code> <p>Elements or selector string that mark the end of sections (optional)</p> <code>None</code> <code>new_section_on_page_break</code> <p>Whether to start a new section at page boundaries (default: False)</p> <code>False</code> <code>include_boundaries</code> <p>How to include boundary elements: 'start', 'end', 'both', or 'none' (default: 'both')</p> <code>'both'</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection of Region objects representing the extracted sections</p> Example <p>Extract sections between headers: <pre><code>pdf = npdf.PDF(\"document.pdf\")\n\n# Get sections between headers\nsections = pdf.get_sections(\n    start_elements='text[size&gt;14]:bold',\n    end_elements='text[size&gt;14]:bold'\n)\n\n# Get sections that break at page boundaries\nsections = pdf.get_sections(\n    start_elements='text:contains(\"Chapter\")',\n    new_section_on_page_break=True\n)\n</code></pre></p> Note <p>You can provide only start_elements, only end_elements, or both. - With only start_elements: sections go from each start to the next start (or end of document) - With only end_elements: sections go from beginning of document to each end - With both: sections go from each start to the corresponding end</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def get_sections(\n    self,\n    start_elements=None,\n    end_elements=None,\n    new_section_on_page_break=False,\n    include_boundaries=\"both\",\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Extract sections from the entire PDF based on start/end elements.\n\n    This method delegates to the PageCollection.get_sections() method,\n    providing a convenient way to extract document sections across all pages.\n\n    Args:\n        start_elements: Elements or selector string that mark the start of sections (optional)\n        end_elements: Elements or selector string that mark the end of sections (optional)\n        new_section_on_page_break: Whether to start a new section at page boundaries (default: False)\n        include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none' (default: 'both')\n\n    Returns:\n        ElementCollection of Region objects representing the extracted sections\n\n    Example:\n        Extract sections between headers:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n\n        # Get sections between headers\n        sections = pdf.get_sections(\n            start_elements='text[size&gt;14]:bold',\n            end_elements='text[size&gt;14]:bold'\n        )\n\n        # Get sections that break at page boundaries\n        sections = pdf.get_sections(\n            start_elements='text:contains(\"Chapter\")',\n            new_section_on_page_break=True\n        )\n        ```\n\n    Note:\n        You can provide only start_elements, only end_elements, or both.\n        - With only start_elements: sections go from each start to the next start (or end of document)\n        - With only end_elements: sections go from beginning of document to each end\n        - With both: sections go from each start to the corresponding end\n    \"\"\"\n    if not hasattr(self, \"_pages\"):\n        raise AttributeError(\"PDF pages not yet initialized.\")\n\n    return self.pages.get_sections(\n        start_elements=start_elements,\n        end_elements=end_elements,\n        new_section_on_page_break=new_section_on_page_break,\n        include_boundaries=include_boundaries,\n    )\n</code></pre> <code>natural_pdf.PDF.highlights(show=False)</code> <p>Create a highlight context for accumulating highlights.</p> <p>This allows for clean syntax to show multiple highlight groups:</p> Example <p>with pdf.highlights() as h:     h.add(pdf.find_all('table'), label='tables', color='blue')     h.add(pdf.find_all('text:bold'), label='bold text', color='red')     h.show()</p> Or with automatic display <p>with pdf.highlights(show=True) as h:     h.add(pdf.find_all('table'), label='tables')     h.add(pdf.find_all('text:bold'), label='bold')     # Automatically shows when exiting the context</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>If True, automatically show highlights when exiting context</p> <code>False</code> <p>Returns:</p> Type Description <code>HighlightContext</code> <p>HighlightContext for accumulating highlights</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n    \"\"\"\n    Create a highlight context for accumulating highlights.\n\n    This allows for clean syntax to show multiple highlight groups:\n\n    Example:\n        with pdf.highlights() as h:\n            h.add(pdf.find_all('table'), label='tables', color='blue')\n            h.add(pdf.find_all('text:bold'), label='bold text', color='red')\n            h.show()\n\n    Or with automatic display:\n        with pdf.highlights(show=True) as h:\n            h.add(pdf.find_all('table'), label='tables')\n            h.add(pdf.find_all('text:bold'), label='bold')\n            # Automatically shows when exiting the context\n\n    Args:\n        show: If True, automatically show highlights when exiting context\n\n    Returns:\n        HighlightContext for accumulating highlights\n    \"\"\"\n    from natural_pdf.core.highlighting_service import HighlightContext\n\n    return HighlightContext(self, show_on_exit=show)\n</code></pre> <code>natural_pdf.PDF.save_pdf(output_path, ocr=False, original=False, dpi=300)</code> <p>Saves the PDF object (all its pages) to a new file.</p> <p>Choose one saving mode: - <code>ocr=True</code>: Creates a new, image-based PDF using OCR results from all pages.   Text generated during the natural-pdf session becomes searchable,   but original vector content is lost. Requires 'ocr-export' extras. - <code>original=True</code>: Saves a copy of the original PDF file this object represents.   Any OCR results or analyses from the natural-pdf session are NOT included.   If the PDF was opened from an in-memory buffer, this mode may not be suitable.   Requires 'ocr-export' extras.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save the new PDF file.</p> required <code>ocr</code> <code>bool</code> <p>If True, save as a searchable, image-based PDF using OCR data.</p> <code>False</code> <code>original</code> <code>bool</code> <p>If True, save the original source PDF content.</p> <code>False</code> <code>dpi</code> <code>int</code> <p>Resolution (dots per inch) used only when ocr=True.</p> <code>300</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the PDF has no pages, if neither or both 'ocr'         and 'original' are True.</p> <code>ImportError</code> <p>If required libraries are not installed for the chosen mode.</p> <code>RuntimeError</code> <p>If an unexpected error occurs during saving.</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def save_pdf(\n    self,\n    output_path: Union[str, Path],\n    ocr: bool = False,\n    original: bool = False,\n    dpi: int = 300,\n):\n    \"\"\"\n    Saves the PDF object (all its pages) to a new file.\n\n    Choose one saving mode:\n    - `ocr=True`: Creates a new, image-based PDF using OCR results from all pages.\n      Text generated during the natural-pdf session becomes searchable,\n      but original vector content is lost. Requires 'ocr-export' extras.\n    - `original=True`: Saves a copy of the original PDF file this object represents.\n      Any OCR results or analyses from the natural-pdf session are NOT included.\n      If the PDF was opened from an in-memory buffer, this mode may not be suitable.\n      Requires 'ocr-export' extras.\n\n    Args:\n        output_path: Path to save the new PDF file.\n        ocr: If True, save as a searchable, image-based PDF using OCR data.\n        original: If True, save the original source PDF content.\n        dpi: Resolution (dots per inch) used only when ocr=True.\n\n    Raises:\n        ValueError: If the PDF has no pages, if neither or both 'ocr'\n                    and 'original' are True.\n        ImportError: If required libraries are not installed for the chosen mode.\n        RuntimeError: If an unexpected error occurs during saving.\n    \"\"\"\n    if not self.pages:\n        raise ValueError(\"Cannot save an empty PDF object.\")\n\n    if not (ocr ^ original):  # XOR: exactly one must be true\n        raise ValueError(\"Exactly one of 'ocr' or 'original' must be True.\")\n\n    output_path_obj = Path(output_path)\n    output_path_str = str(output_path_obj)\n\n    if ocr:\n        has_vector_elements = False\n        for page in self.pages:\n            if (\n                hasattr(page, \"rects\")\n                and page.rects\n                or hasattr(page, \"lines\")\n                and page.lines\n                or hasattr(page, \"curves\")\n                and page.curves\n                or (\n                    hasattr(page, \"chars\")\n                    and any(getattr(el, \"source\", None) != \"ocr\" for el in page.chars)\n                )\n                or (\n                    hasattr(page, \"words\")\n                    and any(getattr(el, \"source\", None) != \"ocr\" for el in page.words)\n                )\n            ):\n                has_vector_elements = True\n                break\n        if has_vector_elements:\n            logger.warning(\n                \"Warning: Saving with ocr=True creates an image-based PDF. \"\n                \"Original vector elements (rects, lines, non-OCR text/chars) \"\n                \"will not be preserved in the output file.\"\n            )\n\n        logger.info(f\"Saving searchable PDF (OCR text layer) to: {output_path_str}\")\n        try:\n            # Delegate to the searchable PDF exporter, passing self (PDF instance)\n            create_searchable_pdf(self, output_path_str, dpi=dpi)\n        except Exception as e:\n            raise RuntimeError(f\"Failed to create searchable PDF: {e}\") from e\n\n    elif original:\n        if create_original_pdf is None:\n            raise ImportError(\n                \"Saving with original=True requires 'pikepdf'. \"\n                'Install with: pip install \"natural-pdf[ocr-export]\"'\n            )\n\n        # Optional: Add warning about losing OCR data similar to PageCollection\n        has_ocr_elements = False\n        for page in self.pages:\n            if hasattr(page, \"find_all\"):\n                ocr_text_elements = page.find_all(\"text[source=ocr]\")\n                if ocr_text_elements:\n                    has_ocr_elements = True\n                    break\n            elif hasattr(page, \"words\"):  # Fallback\n                if any(getattr(el, \"source\", None) == \"ocr\" for el in page.words):\n                    has_ocr_elements = True\n                    break\n        if has_ocr_elements:\n            logger.warning(\n                \"Warning: Saving with original=True preserves original page content. \"\n                \"OCR text generated in this session will not be included in the saved file.\"\n            )\n\n        logger.info(f\"Saving original PDF content to: {output_path_str}\")\n        try:\n            # Delegate to the original PDF exporter, passing self (PDF instance)\n            create_original_pdf(self, output_path_str)\n        except Exception as e:\n            # Re-raise exception from exporter\n            raise e\n</code></pre> <code>natural_pdf.PDF.save_searchable(output_path, dpi=300, **kwargs)</code> <p>DEPRECATED: Use save_pdf(..., ocr=True) instead. Saves the PDF with an OCR text layer, making content searchable.</p> <p>Requires optional dependencies. Install with: pip install \"natural-pdf[ocr-export]\"</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save the searchable PDF</p> required <code>dpi</code> <code>int</code> <p>Resolution for rendering and OCR overlay</p> <code>300</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the exporter</p> <code>{}</code> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def save_searchable(self, output_path: Union[str, \"Path\"], dpi: int = 300, **kwargs):\n    \"\"\"\n    DEPRECATED: Use save_pdf(..., ocr=True) instead.\n    Saves the PDF with an OCR text layer, making content searchable.\n\n    Requires optional dependencies. Install with: pip install \\\"natural-pdf[ocr-export]\\\"\n\n    Args:\n        output_path: Path to save the searchable PDF\n        dpi: Resolution for rendering and OCR overlay\n        **kwargs: Additional keyword arguments passed to the exporter\n    \"\"\"\n    logger.warning(\n        \"PDF.save_searchable() is deprecated. Use PDF.save_pdf(..., ocr=True) instead.\"\n    )\n    if create_searchable_pdf is None:\n        raise ImportError(\n            \"Saving searchable PDF requires 'pikepdf'. \"\n            'Install with: pip install \"natural-pdf[ocr-export]\"'\n        )\n    output_path_str = str(output_path)\n    # Call the exporter directly, passing self (the PDF instance)\n    create_searchable_pdf(self, output_path_str, dpi=dpi, **kwargs)\n</code></pre> <code>natural_pdf.PDF.search_within_index(query, search_service, options=None)</code> <p>Finds relevant documents from this PDF within a search index. Finds relevant documents from this PDF within a search index.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Union[str, Path, Image, Region]</code> <p>The search query (text, image path, PIL Image, Region)</p> required <code>search_service</code> <code>SearchServiceProtocol</code> <p>A pre-configured SearchService instance</p> required <code>options</code> <code>Optional[SearchOptions]</code> <p>Optional SearchOptions to configure the query</p> <code>None</code> <code>query</code> <code>Union[str, Path, Image, Region]</code> <p>The search query (text, image path, PIL Image, Region)</p> required <code>search_service</code> <code>SearchServiceProtocol</code> <p>A pre-configured SearchService instance</p> required <code>options</code> <code>Optional[SearchOptions]</code> <p>Optional SearchOptions to configure the query</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>A list of result dictionaries, sorted by relevance</p> <code>List[Dict[str, Any]]</code> <p>A list of result dictionaries, sorted by relevance</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If search dependencies are not installed</p> <code>ValueError</code> <p>If search_service is None</p> <code>TypeError</code> <p>If search_service does not conform to the protocol</p> <code>FileNotFoundError</code> <p>If the collection managed by the service does not exist</p> <code>RuntimeError</code> <p>For other search failures</p> <code>ImportError</code> <p>If search dependencies are not installed</p> <code>ValueError</code> <p>If search_service is None</p> <code>TypeError</code> <p>If search_service does not conform to the protocol</p> <code>FileNotFoundError</code> <p>If the collection managed by the service does not exist</p> <code>RuntimeError</code> <p>For other search failures</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def search_within_index(\n    self,\n    query: Union[str, Path, Image.Image, \"Region\"],\n    search_service: \"SearchServiceProtocol\",\n    options: Optional[\"SearchOptions\"] = None,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Finds relevant documents from this PDF within a search index.\n    Finds relevant documents from this PDF within a search index.\n\n    Args:\n        query: The search query (text, image path, PIL Image, Region)\n        search_service: A pre-configured SearchService instance\n        options: Optional SearchOptions to configure the query\n        query: The search query (text, image path, PIL Image, Region)\n        search_service: A pre-configured SearchService instance\n        options: Optional SearchOptions to configure the query\n\n    Returns:\n        A list of result dictionaries, sorted by relevance\n        A list of result dictionaries, sorted by relevance\n\n    Raises:\n        ImportError: If search dependencies are not installed\n        ValueError: If search_service is None\n        TypeError: If search_service does not conform to the protocol\n        FileNotFoundError: If the collection managed by the service does not exist\n        RuntimeError: For other search failures\n        ImportError: If search dependencies are not installed\n        ValueError: If search_service is None\n        TypeError: If search_service does not conform to the protocol\n        FileNotFoundError: If the collection managed by the service does not exist\n        RuntimeError: For other search failures\n    \"\"\"\n    if not search_service:\n        raise ValueError(\"A configured SearchServiceProtocol instance must be provided.\")\n\n    collection_name = getattr(search_service, \"collection_name\", \"&lt;Unknown Collection&gt;\")\n    logger.info(\n        f\"Searching within index '{collection_name}' for content from PDF '{self.path}'\"\n    )\n\n    service = search_service\n\n    query_input = query\n    effective_options = copy.deepcopy(options) if options is not None else TextSearchOptions()\n\n    if isinstance(query, Region):\n        logger.debug(\"Query is a Region object. Extracting text.\")\n        if not isinstance(effective_options, TextSearchOptions):\n            logger.warning(\n                \"Querying with Region image requires MultiModalSearchOptions. Falling back to text extraction.\"\n            )\n        query_input = query.extract_text()\n        if not query_input or query_input.isspace():\n            logger.error(\"Region has no extractable text for query.\")\n            return []\n\n    # Add filter to scope search to THIS PDF\n    # Add filter to scope search to THIS PDF\n    pdf_scope_filter = {\n        \"field\": \"pdf_path\",\n        \"operator\": \"eq\",\n        \"value\": self.path,\n    }\n    logger.debug(f\"Applying filter to scope search to PDF: {pdf_scope_filter}\")\n\n    # Combine with existing filters in options (if any)\n    if effective_options.filters:\n        logger.debug(f\"Combining PDF scope filter with existing filters\")\n        if (\n            isinstance(effective_options.filters, dict)\n            and effective_options.filters.get(\"operator\") == \"AND\"\n        ):\n            effective_options.filters[\"conditions\"].append(pdf_scope_filter)\n        elif isinstance(effective_options.filters, list):\n            effective_options.filters = {\n                \"operator\": \"AND\",\n                \"conditions\": effective_options.filters + [pdf_scope_filter],\n            }\n        elif isinstance(effective_options.filters, dict):\n            effective_options.filters = {\n                \"operator\": \"AND\",\n                \"conditions\": [effective_options.filters, pdf_scope_filter],\n            }\n        else:\n            logger.warning(\n                f\"Unsupported format for existing filters. Overwriting with PDF scope filter.\"\n            )\n            effective_options.filters = pdf_scope_filter\n    else:\n        effective_options.filters = pdf_scope_filter\n\n    logger.debug(f\"Final filters for service search: {effective_options.filters}\")\n\n    try:\n        results = service.search(\n            query=query_input,\n            options=effective_options,\n        )\n        logger.info(f\"SearchService returned {len(results)} results from PDF '{self.path}'\")\n        return results\n    except FileNotFoundError as fnf:\n        logger.error(f\"Search failed: Collection not found. Error: {fnf}\")\n        raise\n        logger.error(f\"Search failed: Collection not found. Error: {fnf}\")\n        raise\n    except Exception as e:\n        logger.error(f\"SearchService search failed: {e}\")\n        raise RuntimeError(f\"Search within index failed. See logs for details.\") from e\n        logger.error(f\"SearchService search failed: {e}\")\n        raise RuntimeError(f\"Search within index failed. See logs for details.\") from e\n</code></pre> <code>natural_pdf.PDF.update_text(transform, pages=None, selector='text', max_workers=None, progress_callback=None)</code> <p>Applies corrections to text elements using a callback function.</p> <p>Parameters:</p> Name Type Description Default <code>correction_callback</code> <p>Function that takes an element and returns corrected text or None</p> required <code>pages</code> <code>Optional[Union[Iterable[int], range, slice]]</code> <p>Optional page indices/slice to limit the scope of correction</p> <code>None</code> <code>selector</code> <code>str</code> <p>Selector to apply corrections to (default: \"text\")</p> <code>'text'</code> <code>max_workers</code> <code>Optional[int]</code> <p>Maximum number of threads to use for parallel execution</p> <code>None</code> <code>progress_callback</code> <code>Optional[Callable[[], None]]</code> <p>Optional callback function for progress updates</p> <code>None</code> <p>Returns:</p> Type Description <code>PDF</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/pdf.py</code> <pre><code>def update_text(\n    self,\n    transform: Callable[[Any], Optional[str]],\n    pages: Optional[Union[Iterable[int], range, slice]] = None,\n    selector: str = \"text\",\n    max_workers: Optional[int] = None,\n    progress_callback: Optional[Callable[[], None]] = None,\n) -&gt; \"PDF\":\n    \"\"\"\n    Applies corrections to text elements using a callback function.\n\n    Args:\n        correction_callback: Function that takes an element and returns corrected text or None\n        pages: Optional page indices/slice to limit the scope of correction\n        selector: Selector to apply corrections to (default: \"text\")\n        max_workers: Maximum number of threads to use for parallel execution\n        progress_callback: Optional callback function for progress updates\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    target_page_indices = []\n    if pages is None:\n        target_page_indices = list(range(len(self._pages)))\n    elif isinstance(pages, slice):\n        target_page_indices = list(range(*pages.indices(len(self._pages))))\n    elif hasattr(pages, \"__iter__\"):\n        try:\n            target_page_indices = [int(i) for i in pages]\n            for idx in target_page_indices:\n                if not (0 &lt;= idx &lt; len(self._pages)):\n                    raise IndexError(f\"Page index {idx} out of range (0-{len(self._pages)-1}).\")\n        except (IndexError, TypeError, ValueError) as e:\n            raise ValueError(f\"Invalid page index in 'pages': {pages}. Error: {e}\") from e\n    else:\n        raise TypeError(\"'pages' must be None, a slice, or an iterable of page indices.\")\n\n    if not target_page_indices:\n        logger.warning(\"No pages selected for text update.\")\n        return self\n\n    logger.info(\n        f\"Starting text update for pages: {target_page_indices} with selector='{selector}'\"\n    )\n\n    for page_idx in target_page_indices:\n        page = self._pages[page_idx]\n        try:\n            page.update_text(\n                transform=transform,\n                selector=selector,\n                max_workers=max_workers,\n                progress_callback=progress_callback,\n            )\n        except Exception as e:\n            logger.error(f\"Error during text update on page {page_idx}: {e}\")\n            logger.error(f\"Error during text update on page {page_idx}: {e}\")\n\n    logger.info(\"Text update process finished.\")\n    return self\n</code></pre>"},{"location":"api/#natural_pdf.PDFCollection","title":"<code>natural_pdf.PDFCollection</code>","text":"<p>               Bases: <code>SearchableMixin</code>, <code>ApplyMixin</code>, <code>ExportMixin</code>, <code>ShapeDetectionMixin</code></p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>class PDFCollection(\n    SearchableMixin, ApplyMixin, ExportMixin, ShapeDetectionMixin\n):  # Add ExportMixin and ShapeDetectionMixin\n    def __init__(\n        self,\n        source: Union[str, Iterable[Union[str, \"PDF\"]]],\n        recursive: bool = True,\n        **pdf_options: Any,\n    ):\n        \"\"\"\n        Initializes a collection of PDF documents from various sources.\n\n        Args:\n            source: The source of PDF documents. Can be:\n                - An iterable (e.g., list) of existing PDF objects.\n                - An iterable (e.g., list) of file paths/URLs/globs (strings).\n                - A single file path/URL/directory/glob string.\n            recursive: If source involves directories or glob patterns,\n                       whether to search recursively (default: True).\n            **pdf_options: Keyword arguments passed to the PDF constructor.\n        \"\"\"\n        self._pdfs: List[\"PDF\"] = []\n        self._pdf_options = pdf_options  # Store options for potential slicing later\n        self._recursive = recursive  # Store setting for potential slicing\n\n        # Dynamically import PDF class within methods to avoid circular import at module load time\n        PDF = self._get_pdf_class()\n\n        if hasattr(source, \"__iter__\") and not isinstance(source, str):\n            source_list = list(source)\n            if not source_list:\n                return  # Empty list source\n            if isinstance(source_list[0], PDF):\n                if all(isinstance(item, PDF) for item in source_list):\n                    self._pdfs = source_list  # Direct assignment\n                    # Don't adopt search context anymore\n                    return\n                else:\n                    raise TypeError(\"Iterable source has mixed PDF/non-PDF objects.\")\n            # If it's an iterable but not PDFs, fall through to resolve sources\n\n        # Resolve string, iterable of strings, or single string source to paths/URLs\n        resolved_paths_or_urls = self._resolve_sources_to_paths(source)\n        self._initialize_pdfs(resolved_paths_or_urls, PDF)  # Pass PDF class\n\n        self._iter_index = 0\n\n        # Initialize internal search service reference\n        self._search_service: Optional[SearchServiceProtocol] = None\n\n    @staticmethod\n    def _get_pdf_class():\n        \"\"\"Helper method to dynamically import the PDF class.\"\"\"\n        from natural_pdf.core.pdf import PDF\n\n        return PDF\n\n    # --- Internal Helpers ---\n\n    def _is_url(self, s: str) -&gt; bool:\n        return s.startswith((\"http://\", \"https://\"))\n\n    def _has_glob_magic(self, s: str) -&gt; bool:\n        return py_glob.has_magic(s)\n\n    def _execute_glob(self, pattern: str) -&gt; Set[str]:\n        \"\"\"Glob for paths and return a set of valid PDF paths.\"\"\"\n        found_paths = set()\n        # Use iglob for potentially large directories/matches\n        paths_iter = py_glob.iglob(pattern, recursive=self._recursive)\n        for path_str in paths_iter:\n            # Use Path object for easier checking\n            p = Path(path_str)\n            if p.is_file() and p.suffix.lower() == \".pdf\":\n                found_paths.add(str(p.resolve()))  # Store resolved absolute path\n        return found_paths\n\n    def _resolve_sources_to_paths(self, source: Union[str, Iterable[str]]) -&gt; List[str]:\n        \"\"\"Resolves various source types into a list of unique PDF paths/URLs.\"\"\"\n        final_paths = set()\n        sources_to_process = []\n\n        if isinstance(source, str):\n            sources_to_process.append(source)\n        elif hasattr(source, \"__iter__\"):\n            sources_to_process.extend(list(source))\n        else:  # Should not happen based on __init__ checks, but safeguard\n            raise TypeError(f\"Unexpected source type in _resolve_sources_to_paths: {type(source)}\")\n\n        for item in sources_to_process:\n            if not isinstance(item, str):\n                logger.warning(f\"Skipping non-string item in source list: {type(item)}\")\n                continue\n\n            item_path = Path(item)\n\n            if self._is_url(item):\n                final_paths.add(item)  # Add URL directly\n            elif self._has_glob_magic(item):\n                glob_results = self._execute_glob(item)\n                final_paths.update(glob_results)\n            elif item_path.is_dir():\n                # Use glob to find PDFs in directory, respecting recursive flag\n                dir_pattern = (\n                    str(item_path / \"**\" / \"*.pdf\") if self._recursive else str(item_path / \"*.pdf\")\n                )\n                dir_glob_results = self._execute_glob(dir_pattern)\n                final_paths.update(dir_glob_results)\n            elif item_path.is_file() and item_path.suffix.lower() == \".pdf\":\n                final_paths.add(str(item_path.resolve()))  # Add resolved file path\n            else:\n                logger.warning(\n                    f\"Source item ignored (not a valid URL, directory, file, or glob): {item}\"\n                )\n\n        return sorted(list(final_paths))\n\n    def _initialize_pdfs(self, paths_or_urls: List[str], PDF_cls: Type):\n        \"\"\"Initializes PDF objects from a list of paths/URLs.\"\"\"\n        logger.info(f\"Initializing {len(paths_or_urls)} PDF objects...\")\n        failed_count = 0\n        for path_or_url in tqdm(paths_or_urls, desc=\"Loading PDFs\"):\n            try:\n                pdf_instance = PDF_cls(path_or_url, **self._pdf_options)\n                self._pdfs.append(pdf_instance)\n            except Exception as e:\n                logger.error(\n                    f\"Failed to load PDF: {path_or_url}. Error: {e}\", exc_info=False\n                )  # Keep log concise\n                failed_count += 1\n        logger.info(f\"Successfully initialized {len(self._pdfs)} PDFs. Failed: {failed_count}\")\n\n    # --- Public Factory Class Methods (Simplified) ---\n\n    @classmethod\n    def from_paths(cls, paths_or_urls: List[str], **pdf_options: Any) -&gt; \"PDFCollection\":\n        \"\"\"Creates a PDFCollection explicitly from a list of file paths or URLs.\"\"\"\n        # __init__ can handle List[str] directly now\n        return cls(paths_or_urls, **pdf_options)\n\n    @classmethod\n    def from_glob(cls, pattern: str, recursive: bool = True, **pdf_options: Any) -&gt; \"PDFCollection\":\n        \"\"\"Creates a PDFCollection explicitly from a single glob pattern.\"\"\"\n        # __init__ can handle single glob string directly\n        return cls(pattern, recursive=recursive, **pdf_options)\n\n    @classmethod\n    def from_globs(\n        cls, patterns: List[str], recursive: bool = True, **pdf_options: Any\n    ) -&gt; \"PDFCollection\":\n        \"\"\"Creates a PDFCollection explicitly from a list of glob patterns.\"\"\"\n        # __init__ can handle List[str] containing globs directly\n        return cls(patterns, recursive=recursive, **pdf_options)\n\n    @classmethod\n    def from_directory(\n        cls, directory_path: str, recursive: bool = True, **pdf_options: Any\n    ) -&gt; \"PDFCollection\":\n        \"\"\"Creates a PDFCollection explicitly from PDF files within a directory.\"\"\"\n        # __init__ can handle single directory string directly\n        return cls(directory_path, recursive=recursive, **pdf_options)\n\n    # --- Core Collection Methods ---\n    def __len__(self) -&gt; int:\n        return len(self._pdfs)\n\n    def __getitem__(self, key) -&gt; Union[\"PDF\", \"PDFCollection\"]:\n        # Use dynamic import here as well\n        PDF = self._get_pdf_class()\n        if isinstance(key, slice):\n            # Create a new collection with the sliced PDFs and original options\n            new_collection = PDFCollection.__new__(PDFCollection)  # Create blank instance\n            new_collection._pdfs = self._pdfs[key]\n            new_collection._pdf_options = self._pdf_options\n            new_collection._recursive = self._recursive\n            # Search context is not copied/inherited anymore\n            return new_collection\n        elif isinstance(key, int):\n            # Check bounds\n            if 0 &lt;= key &lt; len(self._pdfs):\n                return self._pdfs[key]\n            else:\n                raise IndexError(f\"PDF index {key} out of range (0-{len(self._pdfs)-1}).\")\n        else:\n            raise TypeError(f\"PDF indices must be integers or slices, not {type(key)}.\")\n\n    def __iter__(self):\n        return iter(self._pdfs)\n\n    def __repr__(self) -&gt; str:\n        # Removed search status\n        return f\"&lt;PDFCollection(count={len(self._pdfs)})&gt;\"\n        return f\"&lt;PDFCollection(count={len(self._pdfs)})&gt;\"\n\n    @property\n    def pdfs(self) -&gt; List[\"PDF\"]:\n        \"\"\"Returns the list of PDF objects held by the collection.\"\"\"\n        return self._pdfs\n\n    @overload\n    def find_all(\n        self,\n        *,\n        text: str,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    @overload\n    def find_all(\n        self,\n        selector: str,\n        *,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    def find_all(\n        self,\n        selector: Optional[str] = None,  # Now optional\n        *,\n        text: Optional[str] = None,  # New text parameter\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Find all elements matching the selector OR text across all PDFs in the collection.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        This creates an ElementCollection that can span multiple PDFs. Note that\n        some ElementCollection methods have limitations when spanning PDFs.\n\n        Args:\n            selector: CSS-like selector string to query elements.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional keyword arguments passed to the find_all method of each PDF.\n\n        Returns:\n            ElementCollection containing all matching elements across all PDFs.\n        \"\"\"\n        # Validation happens within pdf.find_all\n\n        # Collect elements from all PDFs\n        all_elements = []\n        for pdf in self._pdfs:\n            try:\n                # Pass the relevant arguments down to each PDF's find_all\n                elements = pdf.find_all(\n                    selector=selector,\n                    text=text,\n                    apply_exclusions=apply_exclusions,\n                    regex=regex,\n                    case=case,\n                    **kwargs,\n                )\n                all_elements.extend(elements.elements)\n            except Exception as e:\n                logger.error(f\"Error finding elements in {pdf.path}: {e}\", exc_info=True)\n\n        return ElementCollection(all_elements)\n\n    def apply_ocr(\n        self,\n        engine: Optional[str] = None,\n        languages: Optional[List[str]] = None,\n        min_confidence: Optional[float] = None,\n        device: Optional[str] = None,\n        resolution: Optional[int] = None,\n        apply_exclusions: bool = True,\n        detect_only: bool = False,\n        replace: bool = True,\n        options: Optional[Any] = None,\n        pages: Optional[Union[slice, List[int]]] = None,\n        max_workers: Optional[int] = None,\n    ) -&gt; \"PDFCollection\":\n        \"\"\"\n        Apply OCR to all PDFs in the collection, potentially in parallel.\n\n        Args:\n            engine: OCR engine to use (e.g., 'easyocr', 'paddleocr', 'surya')\n            languages: List of language codes for OCR\n            min_confidence: Minimum confidence threshold for text detection\n            device: Device to use for OCR (e.g., 'cpu', 'cuda')\n            resolution: DPI resolution for page rendering\n            apply_exclusions: Whether to apply exclusion regions\n            detect_only: If True, only detect text regions without extracting text\n            replace: If True, replace existing OCR elements\n            options: Engine-specific options\n            pages: Specific pages to process (None for all pages)\n            max_workers: Maximum number of threads to process PDFs concurrently.\n                         If None or 1, processing is sequential. (default: None)\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        PDF = self._get_pdf_class()\n        logger.info(\n            f\"Applying OCR to {len(self._pdfs)} PDFs in collection (max_workers={max_workers})...\"\n        )\n\n        # Worker function takes PDF object again\n        def _process_pdf(pdf: PDF):\n            \"\"\"Helper function to apply OCR to a single PDF, handling errors.\"\"\"\n            thread_id = threading.current_thread().name  # Get thread name for logging\n            pdf_path = pdf.path  # Get path for logging\n            logger.debug(f\"[{thread_id}] Starting OCR process for: {pdf_path}\")\n            start_time = time.monotonic()\n            pdf.apply_ocr(  # Call apply_ocr on the original PDF object\n                pages=pages,\n                engine=engine,\n                languages=languages,\n                min_confidence=min_confidence,\n                device=device,\n                resolution=resolution,\n                apply_exclusions=apply_exclusions,\n                detect_only=detect_only,\n                replace=replace,\n                options=options,\n                # Note: We might want a max_workers here too for page rendering?\n                # For now, PDF.apply_ocr doesn't have it.\n            )\n            end_time = time.monotonic()\n            logger.debug(\n                f\"[{thread_id}] Finished OCR process for: {pdf_path} (Duration: {end_time - start_time:.2f}s)\"\n            )\n            return pdf_path, None\n\n        # Use ThreadPoolExecutor for parallel processing if max_workers &gt; 1\n        if max_workers is not None and max_workers &gt; 1:\n            futures = []\n            with concurrent.futures.ThreadPoolExecutor(\n                max_workers=max_workers, thread_name_prefix=\"OCRWorker\"\n            ) as executor:\n                for pdf in self._pdfs:\n                    # Submit the PDF object to the worker function\n                    futures.append(executor.submit(_process_pdf, pdf))\n\n            # Use the selected tqdm class with as_completed for progress tracking\n            progress_bar = tqdm(\n                concurrent.futures.as_completed(futures),\n                total=len(self._pdfs),\n                desc=\"Applying OCR (Parallel)\",\n                unit=\"pdf\",\n            )\n\n            for future in progress_bar:\n                pdf_path, error = future.result()  # Get result (or exception)\n                if error:\n                    progress_bar.set_postfix_str(f\"Error: {pdf_path}\", refresh=True)\n                # Progress is updated automatically by tqdm\n\n        else:  # Sequential processing (max_workers is None or 1)\n            logger.info(\"Applying OCR sequentially...\")\n            # Use the selected tqdm class for sequential too for consistency\n            # Iterate over PDF objects directly for sequential\n            for pdf in tqdm(self._pdfs, desc=\"Applying OCR (Sequential)\", unit=\"pdf\"):\n                _process_pdf(pdf)  # Call helper directly with PDF object\n\n        logger.info(\"Finished applying OCR across the collection.\")\n        return self\n\n    def correct_ocr(\n        self,\n        correction_callback: Callable[[Any], Optional[str]],\n        max_workers: Optional[int] = None,\n        progress_callback: Optional[Callable[[], None]] = None,\n    ) -&gt; \"PDFCollection\":\n        \"\"\"\n        Apply OCR correction to all relevant elements across all pages and PDFs\n        in the collection using a single progress bar.\n\n        Args:\n            correction_callback: Function to apply to each OCR element.\n                                 It receives the element and should return\n                                 the corrected text (str) or None.\n            max_workers: Max threads to use for parallel execution within each page.\n            progress_callback: Optional callback function to call after processing each element.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        PDF = self._get_pdf_class()  # Ensure PDF class is available\n        if not callable(correction_callback):\n            raise TypeError(\"`correction_callback` must be a callable function.\")\n\n        logger.info(f\"Gathering OCR elements from {len(self._pdfs)} PDFs for correction...\")\n\n        # 1. Gather all target elements using the collection's find_all\n        #    Crucially, set apply_exclusions=False to include elements in headers/footers etc.\n        all_ocr_elements = self.find_all(\"text[source=ocr]\", apply_exclusions=False).elements\n\n        if not all_ocr_elements:\n            logger.info(\"No OCR elements found in the collection to correct.\")\n            return self\n\n        total_elements = len(all_ocr_elements)\n        logger.info(\n            f\"Found {total_elements} OCR elements across the collection. Starting correction process...\"\n        )\n\n        # 2. Initialize the progress bar\n        progress_bar = tqdm(total=total_elements, desc=\"Correcting OCR Elements\", unit=\"element\")\n\n        # 3. Iterate through PDFs and delegate to PDF.correct_ocr\n        #    PDF.correct_ocr handles page iteration and passing the progress callback down.\n        for pdf in self._pdfs:\n            if not pdf.pages:\n                continue\n            try:\n                pdf.correct_ocr(\n                    correction_callback=correction_callback,\n                    max_workers=max_workers,\n                    progress_callback=progress_bar.update,  # Pass the bar's update method\n                )\n            except Exception as e:\n                logger.error(\n                    f\"Error occurred during correction process for PDF {pdf.path}: {e}\",\n                    exc_info=True,\n                )\n                # Decide if we should stop or continue? For now, continue.\n\n        progress_bar.close()\n\n        return self\n\n    def categorize(self, labels: List[str], **kwargs):\n        \"\"\"Categorizes PDFs in the collection based on content or features.\"\"\"\n        # Implementation requires integrating with classification models or logic\n        raise NotImplementedError(\"categorize requires classification implementation.\")\n\n    def export_ocr_correction_task(self, output_zip_path: str, **kwargs):\n        \"\"\"\n        Exports OCR results from all PDFs in this collection into a single\n        correction task package (zip file).\n\n        Args:\n            output_zip_path: The path to save the output zip file.\n            **kwargs: Additional arguments passed to create_correction_task_package\n                      (e.g., image_render_scale, overwrite).\n        \"\"\"\n        from natural_pdf.utils.packaging import create_correction_task_package\n\n        # Pass the collection itself (self) as the source\n        create_correction_task_package(source=self, output_zip_path=output_zip_path, **kwargs)\n\n    # --- Mixin Required Implementation ---\n    def get_indexable_items(self) -&gt; Iterable[Indexable]:\n        \"\"\"Yields Page objects from the collection, conforming to Indexable.\"\"\"\n        if not self._pdfs:\n            return  # Return empty iterator if no PDFs\n\n        for pdf in self._pdfs:\n            if not pdf.pages:  # Handle case where a PDF might have 0 pages after loading\n                logger.warning(f\"PDF '{pdf.path}' has no pages. Skipping.\")\n                continue\n            for page in pdf.pages:\n                # Optional: Add filtering here if needed (e.g., skip empty pages)\n                # Assuming Page object conforms to Indexable\n                # We might still want the empty page check here for efficiency\n                # if not page.extract_text(use_exclusions=False).strip():\n                #     logger.debug(f\"Skipping empty page {page.page_number} from PDF '{pdf.path}'.\")\n                #     continue\n                yield page\n\n    # --- Classification Method --- #\n    def classify_all(\n        self,\n        labels: List[str],\n        using: Optional[str] = None,  # Default handled by PDF.classify -&gt; manager\n        model: Optional[str] = None,  # Optional model ID\n        analysis_key: str = \"classification\",  # Key for storing result in PDF.analyses\n        **kwargs,\n    ) -&gt; \"PDFCollection\":\n        \"\"\"\n        Classify each PDF document in the collection using batch processing.\n\n        This method gathers content from all PDFs and processes them in a single\n        batch to avoid multiprocessing resource accumulation that can occur with\n        sequential individual classifications.\n\n        Args:\n            labels: A list of string category names.\n            using: Processing mode ('text', 'vision'). If None, manager infers (defaulting to text).\n            model: Optional specific model identifier (e.g., HF ID). If None, manager uses default for 'using' mode.\n            analysis_key: Key under which to store the ClassificationResult in each PDF's `analyses` dict.\n            **kwargs: Additional arguments passed down to the ClassificationManager.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            ValueError: If labels list is empty, or if using='vision' on a multi-page PDF.\n            ClassificationError: If classification fails.\n            ImportError: If classification dependencies are missing.\n        \"\"\"\n        if not labels:\n            raise ValueError(\"Labels list cannot be empty.\")\n\n        if not self._pdfs:\n            logger.warning(\"PDFCollection is empty, skipping classification.\")\n            return self\n\n        mode_desc = f\"using='{using}'\" if using else f\"model='{model}'\" if model else \"default text\"\n        logger.info(\n            f\"Starting batch classification for {len(self._pdfs)} PDFs in collection ({mode_desc})...\"\n        )\n\n        # Get classification manager from first PDF\n        try:\n            first_pdf = self._pdfs[0]\n            if not hasattr(first_pdf, \"get_manager\"):\n                raise RuntimeError(\"PDFs do not support classification manager\")\n            manager = first_pdf.get_manager(\"classification\")\n            if not manager or not manager.is_available():\n                raise RuntimeError(\"ClassificationManager is not available\")\n        except Exception as e:\n            from natural_pdf.classification.manager import ClassificationError\n\n            raise ClassificationError(f\"Cannot access ClassificationManager: {e}\") from e\n\n        # Determine processing mode early\n        inferred_using = manager.infer_using(model if model else manager.DEFAULT_TEXT_MODEL, using)\n\n        # Gather content from all PDFs\n        pdf_contents = []\n        valid_pdfs = []\n\n        logger.info(f\"Gathering content from {len(self._pdfs)} PDFs for batch classification...\")\n\n        for pdf in self._pdfs:\n            try:\n                # Get the content for classification - use the same logic as individual PDF classify\n                if inferred_using == \"text\":\n                    # Extract text content from PDF\n                    content = pdf.extract_text()\n                    if not content or content.isspace():\n                        logger.warning(f\"Skipping PDF {pdf.path}: No text content found\")\n                        continue\n                elif inferred_using == \"vision\":\n                    # For vision, we need single-page PDFs only\n                    if len(pdf.pages) != 1:\n                        logger.warning(\n                            f\"Skipping PDF {pdf.path}: Vision classification requires single-page PDFs\"\n                        )\n                        continue\n                    # Get first page image\n                    content = pdf.pages[0].render()\n                else:\n                    raise ValueError(f\"Unsupported using mode: {inferred_using}\")\n\n                pdf_contents.append(content)\n                valid_pdfs.append(pdf)\n\n            except Exception as e:\n                logger.warning(f\"Skipping PDF {pdf.path}: Error getting content - {e}\")\n                continue\n\n        if not pdf_contents:\n            logger.warning(\"No valid content could be gathered from PDFs for classification.\")\n            return self\n\n        logger.info(\n            f\"Gathered content from {len(valid_pdfs)} PDFs. Running batch classification...\"\n        )\n\n        # Run batch classification\n        try:\n            batch_results = manager.classify_batch(\n                item_contents=pdf_contents,\n                labels=labels,\n                model_id=model,\n                using=inferred_using,\n                progress_bar=True,  # Let the manager handle progress display\n                **kwargs,\n            )\n        except Exception as e:\n            logger.error(f\"Batch classification failed: {e}\")\n            from natural_pdf.classification.manager import ClassificationError\n\n            raise ClassificationError(f\"Batch classification failed: {e}\") from e\n\n        # Assign results back to PDFs\n        if len(batch_results) != len(valid_pdfs):\n            logger.error(\n                f\"Batch classification result count ({len(batch_results)}) mismatch \"\n                f\"with PDFs processed ({len(valid_pdfs)}). Cannot assign results.\"\n            )\n            from natural_pdf.classification.manager import ClassificationError\n\n            raise ClassificationError(\"Batch result count mismatch with input PDFs\")\n\n        logger.info(f\"Assigning {len(batch_results)} results to PDFs under key '{analysis_key}'.\")\n\n        processed_count = 0\n        for pdf, result_obj in zip(valid_pdfs, batch_results):\n            try:\n                if not hasattr(pdf, \"analyses\") or pdf.analyses is None:\n                    pdf.analyses = {}\n                pdf.analyses[analysis_key] = result_obj\n                processed_count += 1\n            except Exception as e:\n                logger.warning(f\"Failed to store classification result for {pdf.path}: {e}\")\n\n        skipped_count = len(self._pdfs) - processed_count\n        final_message = f\"Finished batch classification. Processed: {processed_count}\"\n        if skipped_count &gt; 0:\n            final_message += f\", Skipped: {skipped_count}\"\n        logger.info(final_message + \".\")\n\n        return self\n\n    # --- End Classification Method --- #\n\n    def _gather_analysis_data(\n        self,\n        analysis_keys: List[str],\n        include_content: bool,\n        include_images: bool,\n        image_dir: Optional[Path],\n        image_format: str,\n        image_resolution: int,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Gather analysis data from all PDFs in the collection.\n\n        Args:\n            analysis_keys: Keys in the analyses dictionary to export\n            include_content: Whether to include extracted text\n            include_images: Whether to export images\n            image_dir: Directory to save images\n            image_format: Format to save images\n            image_resolution: Resolution for exported images\n\n        Returns:\n            List of dictionaries containing analysis data\n        \"\"\"\n        if not self._pdfs:\n            logger.warning(\"No PDFs found in collection\")\n            return []\n\n        all_data = []\n\n        for pdf in tqdm(self._pdfs, desc=\"Gathering PDF data\", leave=False):\n            # PDF level data\n            pdf_data = {\n                \"pdf_path\": pdf.path,\n                \"pdf_filename\": Path(pdf.path).name,\n                \"total_pages\": len(pdf.pages) if hasattr(pdf, \"pages\") else 0,\n            }\n\n            # Add metadata if available\n            if hasattr(pdf, \"metadata\") and pdf.metadata:\n                for k, v in pdf.metadata.items():\n                    if v:  # Only add non-empty metadata\n                        pdf_data[f\"metadata.{k}\"] = str(v)\n\n            all_data.append(pdf_data)\n\n        return all_data\n</code></pre>"},{"location":"api/#natural_pdf.PDFCollection-attributes","title":"Attributes","text":"<code>natural_pdf.PDFCollection.pdfs</code> <code>property</code> <p>Returns the list of PDF objects held by the collection.</p>"},{"location":"api/#natural_pdf.PDFCollection-functions","title":"Functions","text":"<code>natural_pdf.PDFCollection.__init__(source, recursive=True, **pdf_options)</code> <p>Initializes a collection of PDF documents from various sources.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Union[str, Iterable[Union[str, PDF]]]</code> <p>The source of PDF documents. Can be: - An iterable (e.g., list) of existing PDF objects. - An iterable (e.g., list) of file paths/URLs/globs (strings). - A single file path/URL/directory/glob string.</p> required <code>recursive</code> <code>bool</code> <p>If source involves directories or glob patterns,        whether to search recursively (default: True).</p> <code>True</code> <code>**pdf_options</code> <code>Any</code> <p>Keyword arguments passed to the PDF constructor.</p> <code>{}</code> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def __init__(\n    self,\n    source: Union[str, Iterable[Union[str, \"PDF\"]]],\n    recursive: bool = True,\n    **pdf_options: Any,\n):\n    \"\"\"\n    Initializes a collection of PDF documents from various sources.\n\n    Args:\n        source: The source of PDF documents. Can be:\n            - An iterable (e.g., list) of existing PDF objects.\n            - An iterable (e.g., list) of file paths/URLs/globs (strings).\n            - A single file path/URL/directory/glob string.\n        recursive: If source involves directories or glob patterns,\n                   whether to search recursively (default: True).\n        **pdf_options: Keyword arguments passed to the PDF constructor.\n    \"\"\"\n    self._pdfs: List[\"PDF\"] = []\n    self._pdf_options = pdf_options  # Store options for potential slicing later\n    self._recursive = recursive  # Store setting for potential slicing\n\n    # Dynamically import PDF class within methods to avoid circular import at module load time\n    PDF = self._get_pdf_class()\n\n    if hasattr(source, \"__iter__\") and not isinstance(source, str):\n        source_list = list(source)\n        if not source_list:\n            return  # Empty list source\n        if isinstance(source_list[0], PDF):\n            if all(isinstance(item, PDF) for item in source_list):\n                self._pdfs = source_list  # Direct assignment\n                # Don't adopt search context anymore\n                return\n            else:\n                raise TypeError(\"Iterable source has mixed PDF/non-PDF objects.\")\n        # If it's an iterable but not PDFs, fall through to resolve sources\n\n    # Resolve string, iterable of strings, or single string source to paths/URLs\n    resolved_paths_or_urls = self._resolve_sources_to_paths(source)\n    self._initialize_pdfs(resolved_paths_or_urls, PDF)  # Pass PDF class\n\n    self._iter_index = 0\n\n    # Initialize internal search service reference\n    self._search_service: Optional[SearchServiceProtocol] = None\n</code></pre> <code>natural_pdf.PDFCollection.apply_ocr(engine=None, languages=None, min_confidence=None, device=None, resolution=None, apply_exclusions=True, detect_only=False, replace=True, options=None, pages=None, max_workers=None)</code> <p>Apply OCR to all PDFs in the collection, potentially in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Optional[str]</code> <p>OCR engine to use (e.g., 'easyocr', 'paddleocr', 'surya')</p> <code>None</code> <code>languages</code> <code>Optional[List[str]]</code> <p>List of language codes for OCR</p> <code>None</code> <code>min_confidence</code> <code>Optional[float]</code> <p>Minimum confidence threshold for text detection</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to use for OCR (e.g., 'cpu', 'cuda')</p> <code>None</code> <code>resolution</code> <code>Optional[int]</code> <p>DPI resolution for page rendering</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to apply exclusion regions</p> <code>True</code> <code>detect_only</code> <code>bool</code> <p>If True, only detect text regions without extracting text</p> <code>False</code> <code>replace</code> <code>bool</code> <p>If True, replace existing OCR elements</p> <code>True</code> <code>options</code> <code>Optional[Any]</code> <p>Engine-specific options</p> <code>None</code> <code>pages</code> <code>Optional[Union[slice, List[int]]]</code> <p>Specific pages to process (None for all pages)</p> <code>None</code> <code>max_workers</code> <code>Optional[int]</code> <p>Maximum number of threads to process PDFs concurrently.          If None or 1, processing is sequential. (default: None)</p> <code>None</code> <p>Returns:</p> Type Description <code>PDFCollection</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def apply_ocr(\n    self,\n    engine: Optional[str] = None,\n    languages: Optional[List[str]] = None,\n    min_confidence: Optional[float] = None,\n    device: Optional[str] = None,\n    resolution: Optional[int] = None,\n    apply_exclusions: bool = True,\n    detect_only: bool = False,\n    replace: bool = True,\n    options: Optional[Any] = None,\n    pages: Optional[Union[slice, List[int]]] = None,\n    max_workers: Optional[int] = None,\n) -&gt; \"PDFCollection\":\n    \"\"\"\n    Apply OCR to all PDFs in the collection, potentially in parallel.\n\n    Args:\n        engine: OCR engine to use (e.g., 'easyocr', 'paddleocr', 'surya')\n        languages: List of language codes for OCR\n        min_confidence: Minimum confidence threshold for text detection\n        device: Device to use for OCR (e.g., 'cpu', 'cuda')\n        resolution: DPI resolution for page rendering\n        apply_exclusions: Whether to apply exclusion regions\n        detect_only: If True, only detect text regions without extracting text\n        replace: If True, replace existing OCR elements\n        options: Engine-specific options\n        pages: Specific pages to process (None for all pages)\n        max_workers: Maximum number of threads to process PDFs concurrently.\n                     If None or 1, processing is sequential. (default: None)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    PDF = self._get_pdf_class()\n    logger.info(\n        f\"Applying OCR to {len(self._pdfs)} PDFs in collection (max_workers={max_workers})...\"\n    )\n\n    # Worker function takes PDF object again\n    def _process_pdf(pdf: PDF):\n        \"\"\"Helper function to apply OCR to a single PDF, handling errors.\"\"\"\n        thread_id = threading.current_thread().name  # Get thread name for logging\n        pdf_path = pdf.path  # Get path for logging\n        logger.debug(f\"[{thread_id}] Starting OCR process for: {pdf_path}\")\n        start_time = time.monotonic()\n        pdf.apply_ocr(  # Call apply_ocr on the original PDF object\n            pages=pages,\n            engine=engine,\n            languages=languages,\n            min_confidence=min_confidence,\n            device=device,\n            resolution=resolution,\n            apply_exclusions=apply_exclusions,\n            detect_only=detect_only,\n            replace=replace,\n            options=options,\n            # Note: We might want a max_workers here too for page rendering?\n            # For now, PDF.apply_ocr doesn't have it.\n        )\n        end_time = time.monotonic()\n        logger.debug(\n            f\"[{thread_id}] Finished OCR process for: {pdf_path} (Duration: {end_time - start_time:.2f}s)\"\n        )\n        return pdf_path, None\n\n    # Use ThreadPoolExecutor for parallel processing if max_workers &gt; 1\n    if max_workers is not None and max_workers &gt; 1:\n        futures = []\n        with concurrent.futures.ThreadPoolExecutor(\n            max_workers=max_workers, thread_name_prefix=\"OCRWorker\"\n        ) as executor:\n            for pdf in self._pdfs:\n                # Submit the PDF object to the worker function\n                futures.append(executor.submit(_process_pdf, pdf))\n\n        # Use the selected tqdm class with as_completed for progress tracking\n        progress_bar = tqdm(\n            concurrent.futures.as_completed(futures),\n            total=len(self._pdfs),\n            desc=\"Applying OCR (Parallel)\",\n            unit=\"pdf\",\n        )\n\n        for future in progress_bar:\n            pdf_path, error = future.result()  # Get result (or exception)\n            if error:\n                progress_bar.set_postfix_str(f\"Error: {pdf_path}\", refresh=True)\n            # Progress is updated automatically by tqdm\n\n    else:  # Sequential processing (max_workers is None or 1)\n        logger.info(\"Applying OCR sequentially...\")\n        # Use the selected tqdm class for sequential too for consistency\n        # Iterate over PDF objects directly for sequential\n        for pdf in tqdm(self._pdfs, desc=\"Applying OCR (Sequential)\", unit=\"pdf\"):\n            _process_pdf(pdf)  # Call helper directly with PDF object\n\n    logger.info(\"Finished applying OCR across the collection.\")\n    return self\n</code></pre> <code>natural_pdf.PDFCollection.categorize(labels, **kwargs)</code> <p>Categorizes PDFs in the collection based on content or features.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def categorize(self, labels: List[str], **kwargs):\n    \"\"\"Categorizes PDFs in the collection based on content or features.\"\"\"\n    # Implementation requires integrating with classification models or logic\n    raise NotImplementedError(\"categorize requires classification implementation.\")\n</code></pre> <code>natural_pdf.PDFCollection.classify_all(labels, using=None, model=None, analysis_key='classification', **kwargs)</code> <p>Classify each PDF document in the collection using batch processing.</p> <p>This method gathers content from all PDFs and processes them in a single batch to avoid multiprocessing resource accumulation that can occur with sequential individual classifications.</p> <p>Parameters:</p> Name Type Description Default <code>labels</code> <code>List[str]</code> <p>A list of string category names.</p> required <code>using</code> <code>Optional[str]</code> <p>Processing mode ('text', 'vision'). If None, manager infers (defaulting to text).</p> <code>None</code> <code>model</code> <code>Optional[str]</code> <p>Optional specific model identifier (e.g., HF ID). If None, manager uses default for 'using' mode.</p> <code>None</code> <code>analysis_key</code> <code>str</code> <p>Key under which to store the ClassificationResult in each PDF's <code>analyses</code> dict.</p> <code>'classification'</code> <code>**kwargs</code> <p>Additional arguments passed down to the ClassificationManager.</p> <code>{}</code> <p>Returns:</p> Type Description <code>PDFCollection</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If labels list is empty, or if using='vision' on a multi-page PDF.</p> <code>ClassificationError</code> <p>If classification fails.</p> <code>ImportError</code> <p>If classification dependencies are missing.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def classify_all(\n    self,\n    labels: List[str],\n    using: Optional[str] = None,  # Default handled by PDF.classify -&gt; manager\n    model: Optional[str] = None,  # Optional model ID\n    analysis_key: str = \"classification\",  # Key for storing result in PDF.analyses\n    **kwargs,\n) -&gt; \"PDFCollection\":\n    \"\"\"\n    Classify each PDF document in the collection using batch processing.\n\n    This method gathers content from all PDFs and processes them in a single\n    batch to avoid multiprocessing resource accumulation that can occur with\n    sequential individual classifications.\n\n    Args:\n        labels: A list of string category names.\n        using: Processing mode ('text', 'vision'). If None, manager infers (defaulting to text).\n        model: Optional specific model identifier (e.g., HF ID). If None, manager uses default for 'using' mode.\n        analysis_key: Key under which to store the ClassificationResult in each PDF's `analyses` dict.\n        **kwargs: Additional arguments passed down to the ClassificationManager.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        ValueError: If labels list is empty, or if using='vision' on a multi-page PDF.\n        ClassificationError: If classification fails.\n        ImportError: If classification dependencies are missing.\n    \"\"\"\n    if not labels:\n        raise ValueError(\"Labels list cannot be empty.\")\n\n    if not self._pdfs:\n        logger.warning(\"PDFCollection is empty, skipping classification.\")\n        return self\n\n    mode_desc = f\"using='{using}'\" if using else f\"model='{model}'\" if model else \"default text\"\n    logger.info(\n        f\"Starting batch classification for {len(self._pdfs)} PDFs in collection ({mode_desc})...\"\n    )\n\n    # Get classification manager from first PDF\n    try:\n        first_pdf = self._pdfs[0]\n        if not hasattr(first_pdf, \"get_manager\"):\n            raise RuntimeError(\"PDFs do not support classification manager\")\n        manager = first_pdf.get_manager(\"classification\")\n        if not manager or not manager.is_available():\n            raise RuntimeError(\"ClassificationManager is not available\")\n    except Exception as e:\n        from natural_pdf.classification.manager import ClassificationError\n\n        raise ClassificationError(f\"Cannot access ClassificationManager: {e}\") from e\n\n    # Determine processing mode early\n    inferred_using = manager.infer_using(model if model else manager.DEFAULT_TEXT_MODEL, using)\n\n    # Gather content from all PDFs\n    pdf_contents = []\n    valid_pdfs = []\n\n    logger.info(f\"Gathering content from {len(self._pdfs)} PDFs for batch classification...\")\n\n    for pdf in self._pdfs:\n        try:\n            # Get the content for classification - use the same logic as individual PDF classify\n            if inferred_using == \"text\":\n                # Extract text content from PDF\n                content = pdf.extract_text()\n                if not content or content.isspace():\n                    logger.warning(f\"Skipping PDF {pdf.path}: No text content found\")\n                    continue\n            elif inferred_using == \"vision\":\n                # For vision, we need single-page PDFs only\n                if len(pdf.pages) != 1:\n                    logger.warning(\n                        f\"Skipping PDF {pdf.path}: Vision classification requires single-page PDFs\"\n                    )\n                    continue\n                # Get first page image\n                content = pdf.pages[0].render()\n            else:\n                raise ValueError(f\"Unsupported using mode: {inferred_using}\")\n\n            pdf_contents.append(content)\n            valid_pdfs.append(pdf)\n\n        except Exception as e:\n            logger.warning(f\"Skipping PDF {pdf.path}: Error getting content - {e}\")\n            continue\n\n    if not pdf_contents:\n        logger.warning(\"No valid content could be gathered from PDFs for classification.\")\n        return self\n\n    logger.info(\n        f\"Gathered content from {len(valid_pdfs)} PDFs. Running batch classification...\"\n    )\n\n    # Run batch classification\n    try:\n        batch_results = manager.classify_batch(\n            item_contents=pdf_contents,\n            labels=labels,\n            model_id=model,\n            using=inferred_using,\n            progress_bar=True,  # Let the manager handle progress display\n            **kwargs,\n        )\n    except Exception as e:\n        logger.error(f\"Batch classification failed: {e}\")\n        from natural_pdf.classification.manager import ClassificationError\n\n        raise ClassificationError(f\"Batch classification failed: {e}\") from e\n\n    # Assign results back to PDFs\n    if len(batch_results) != len(valid_pdfs):\n        logger.error(\n            f\"Batch classification result count ({len(batch_results)}) mismatch \"\n            f\"with PDFs processed ({len(valid_pdfs)}). Cannot assign results.\"\n        )\n        from natural_pdf.classification.manager import ClassificationError\n\n        raise ClassificationError(\"Batch result count mismatch with input PDFs\")\n\n    logger.info(f\"Assigning {len(batch_results)} results to PDFs under key '{analysis_key}'.\")\n\n    processed_count = 0\n    for pdf, result_obj in zip(valid_pdfs, batch_results):\n        try:\n            if not hasattr(pdf, \"analyses\") or pdf.analyses is None:\n                pdf.analyses = {}\n            pdf.analyses[analysis_key] = result_obj\n            processed_count += 1\n        except Exception as e:\n            logger.warning(f\"Failed to store classification result for {pdf.path}: {e}\")\n\n    skipped_count = len(self._pdfs) - processed_count\n    final_message = f\"Finished batch classification. Processed: {processed_count}\"\n    if skipped_count &gt; 0:\n        final_message += f\", Skipped: {skipped_count}\"\n    logger.info(final_message + \".\")\n\n    return self\n</code></pre> <code>natural_pdf.PDFCollection.correct_ocr(correction_callback, max_workers=None, progress_callback=None)</code> <p>Apply OCR correction to all relevant elements across all pages and PDFs in the collection using a single progress bar.</p> <p>Parameters:</p> Name Type Description Default <code>correction_callback</code> <code>Callable[[Any], Optional[str]]</code> <p>Function to apply to each OCR element.                  It receives the element and should return                  the corrected text (str) or None.</p> required <code>max_workers</code> <code>Optional[int]</code> <p>Max threads to use for parallel execution within each page.</p> <code>None</code> <code>progress_callback</code> <code>Optional[Callable[[], None]]</code> <p>Optional callback function to call after processing each element.</p> <code>None</code> <p>Returns:</p> Type Description <code>PDFCollection</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def correct_ocr(\n    self,\n    correction_callback: Callable[[Any], Optional[str]],\n    max_workers: Optional[int] = None,\n    progress_callback: Optional[Callable[[], None]] = None,\n) -&gt; \"PDFCollection\":\n    \"\"\"\n    Apply OCR correction to all relevant elements across all pages and PDFs\n    in the collection using a single progress bar.\n\n    Args:\n        correction_callback: Function to apply to each OCR element.\n                             It receives the element and should return\n                             the corrected text (str) or None.\n        max_workers: Max threads to use for parallel execution within each page.\n        progress_callback: Optional callback function to call after processing each element.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    PDF = self._get_pdf_class()  # Ensure PDF class is available\n    if not callable(correction_callback):\n        raise TypeError(\"`correction_callback` must be a callable function.\")\n\n    logger.info(f\"Gathering OCR elements from {len(self._pdfs)} PDFs for correction...\")\n\n    # 1. Gather all target elements using the collection's find_all\n    #    Crucially, set apply_exclusions=False to include elements in headers/footers etc.\n    all_ocr_elements = self.find_all(\"text[source=ocr]\", apply_exclusions=False).elements\n\n    if not all_ocr_elements:\n        logger.info(\"No OCR elements found in the collection to correct.\")\n        return self\n\n    total_elements = len(all_ocr_elements)\n    logger.info(\n        f\"Found {total_elements} OCR elements across the collection. Starting correction process...\"\n    )\n\n    # 2. Initialize the progress bar\n    progress_bar = tqdm(total=total_elements, desc=\"Correcting OCR Elements\", unit=\"element\")\n\n    # 3. Iterate through PDFs and delegate to PDF.correct_ocr\n    #    PDF.correct_ocr handles page iteration and passing the progress callback down.\n    for pdf in self._pdfs:\n        if not pdf.pages:\n            continue\n        try:\n            pdf.correct_ocr(\n                correction_callback=correction_callback,\n                max_workers=max_workers,\n                progress_callback=progress_bar.update,  # Pass the bar's update method\n            )\n        except Exception as e:\n            logger.error(\n                f\"Error occurred during correction process for PDF {pdf.path}: {e}\",\n                exc_info=True,\n            )\n            # Decide if we should stop or continue? For now, continue.\n\n    progress_bar.close()\n\n    return self\n</code></pre> <code>natural_pdf.PDFCollection.export_ocr_correction_task(output_zip_path, **kwargs)</code> <p>Exports OCR results from all PDFs in this collection into a single correction task package (zip file).</p> <p>Parameters:</p> Name Type Description Default <code>output_zip_path</code> <code>str</code> <p>The path to save the output zip file.</p> required <code>**kwargs</code> <p>Additional arguments passed to create_correction_task_package       (e.g., image_render_scale, overwrite).</p> <code>{}</code> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def export_ocr_correction_task(self, output_zip_path: str, **kwargs):\n    \"\"\"\n    Exports OCR results from all PDFs in this collection into a single\n    correction task package (zip file).\n\n    Args:\n        output_zip_path: The path to save the output zip file.\n        **kwargs: Additional arguments passed to create_correction_task_package\n                  (e.g., image_render_scale, overwrite).\n    \"\"\"\n    from natural_pdf.utils.packaging import create_correction_task_package\n\n    # Pass the collection itself (self) as the source\n    create_correction_task_package(source=self, output_zip_path=output_zip_path, **kwargs)\n</code></pre> <code>natural_pdf.PDFCollection.find_all(selector=None, *, text=None, apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find_all(*, text: str, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre><pre><code>find_all(selector: str, *, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre> <p>Find all elements matching the selector OR text across all PDFs in the collection.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>This creates an ElementCollection that can span multiple PDFs. Note that some ElementCollection methods have limitations when spanning PDFs.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string to query elements.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the find_all method of each PDF.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection containing all matching elements across all PDFs.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def find_all(\n    self,\n    selector: Optional[str] = None,  # Now optional\n    *,\n    text: Optional[str] = None,  # New text parameter\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Find all elements matching the selector OR text across all PDFs in the collection.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    This creates an ElementCollection that can span multiple PDFs. Note that\n    some ElementCollection methods have limitations when spanning PDFs.\n\n    Args:\n        selector: CSS-like selector string to query elements.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional keyword arguments passed to the find_all method of each PDF.\n\n    Returns:\n        ElementCollection containing all matching elements across all PDFs.\n    \"\"\"\n    # Validation happens within pdf.find_all\n\n    # Collect elements from all PDFs\n    all_elements = []\n    for pdf in self._pdfs:\n        try:\n            # Pass the relevant arguments down to each PDF's find_all\n            elements = pdf.find_all(\n                selector=selector,\n                text=text,\n                apply_exclusions=apply_exclusions,\n                regex=regex,\n                case=case,\n                **kwargs,\n            )\n            all_elements.extend(elements.elements)\n        except Exception as e:\n            logger.error(f\"Error finding elements in {pdf.path}: {e}\", exc_info=True)\n\n    return ElementCollection(all_elements)\n</code></pre> <code>natural_pdf.PDFCollection.from_directory(directory_path, recursive=True, **pdf_options)</code> <code>classmethod</code> <p>Creates a PDFCollection explicitly from PDF files within a directory.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>@classmethod\ndef from_directory(\n    cls, directory_path: str, recursive: bool = True, **pdf_options: Any\n) -&gt; \"PDFCollection\":\n    \"\"\"Creates a PDFCollection explicitly from PDF files within a directory.\"\"\"\n    # __init__ can handle single directory string directly\n    return cls(directory_path, recursive=recursive, **pdf_options)\n</code></pre> <code>natural_pdf.PDFCollection.from_glob(pattern, recursive=True, **pdf_options)</code> <code>classmethod</code> <p>Creates a PDFCollection explicitly from a single glob pattern.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>@classmethod\ndef from_glob(cls, pattern: str, recursive: bool = True, **pdf_options: Any) -&gt; \"PDFCollection\":\n    \"\"\"Creates a PDFCollection explicitly from a single glob pattern.\"\"\"\n    # __init__ can handle single glob string directly\n    return cls(pattern, recursive=recursive, **pdf_options)\n</code></pre> <code>natural_pdf.PDFCollection.from_globs(patterns, recursive=True, **pdf_options)</code> <code>classmethod</code> <p>Creates a PDFCollection explicitly from a list of glob patterns.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>@classmethod\ndef from_globs(\n    cls, patterns: List[str], recursive: bool = True, **pdf_options: Any\n) -&gt; \"PDFCollection\":\n    \"\"\"Creates a PDFCollection explicitly from a list of glob patterns.\"\"\"\n    # __init__ can handle List[str] containing globs directly\n    return cls(patterns, recursive=recursive, **pdf_options)\n</code></pre> <code>natural_pdf.PDFCollection.from_paths(paths_or_urls, **pdf_options)</code> <code>classmethod</code> <p>Creates a PDFCollection explicitly from a list of file paths or URLs.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>@classmethod\ndef from_paths(cls, paths_or_urls: List[str], **pdf_options: Any) -&gt; \"PDFCollection\":\n    \"\"\"Creates a PDFCollection explicitly from a list of file paths or URLs.\"\"\"\n    # __init__ can handle List[str] directly now\n    return cls(paths_or_urls, **pdf_options)\n</code></pre> <code>natural_pdf.PDFCollection.get_indexable_items()</code> <p>Yields Page objects from the collection, conforming to Indexable.</p> Source code in <code>natural_pdf/core/pdf_collection.py</code> <pre><code>def get_indexable_items(self) -&gt; Iterable[Indexable]:\n    \"\"\"Yields Page objects from the collection, conforming to Indexable.\"\"\"\n    if not self._pdfs:\n        return  # Return empty iterator if no PDFs\n\n    for pdf in self._pdfs:\n        if not pdf.pages:  # Handle case where a PDF might have 0 pages after loading\n            logger.warning(f\"PDF '{pdf.path}' has no pages. Skipping.\")\n            continue\n        for page in pdf.pages:\n            # Optional: Add filtering here if needed (e.g., skip empty pages)\n            # Assuming Page object conforms to Indexable\n            # We might still want the empty page check here for efficiency\n            # if not page.extract_text(use_exclusions=False).strip():\n            #     logger.debug(f\"Skipping empty page {page.page_number} from PDF '{pdf.path}'.\")\n            #     continue\n            yield page\n</code></pre>"},{"location":"api/#natural_pdf.Page","title":"<code>natural_pdf.Page</code>","text":"<p>               Bases: <code>TextMixin</code>, <code>ClassificationMixin</code>, <code>ExtractionMixin</code>, <code>ShapeDetectionMixin</code>, <code>DescribeMixin</code>, <code>Visualizable</code></p> <p>Enhanced Page wrapper built on top of pdfplumber.Page.</p> <p>This class provides a fluent interface for working with PDF pages, with improved selection, navigation, extraction, and question-answering capabilities. It integrates multiple analysis capabilities through mixins and provides spatial navigation with CSS-like selectors.</p> <p>The Page class serves as the primary interface for document analysis, offering: - Element selection and spatial navigation - OCR and layout analysis integration - Table detection and extraction - AI-powered classification and data extraction - Visual debugging with highlighting and cropping - Text style analysis and structure detection</p> <p>Attributes:</p> Name Type Description <code>index</code> <code>int</code> <p>Zero-based index of this page in the PDF.</p> <code>number</code> <code>int</code> <p>One-based page number (index + 1).</p> <code>width</code> <code>float</code> <p>Page width in points.</p> <code>height</code> <code>float</code> <p>Page height in points.</p> <code>bbox</code> <code>float</code> <p>Bounding box tuple (x0, top, x1, bottom) of the page.</p> <code>chars</code> <code>List[Any]</code> <p>Collection of character elements on the page.</p> <code>words</code> <code>List[Any]</code> <p>Collection of word elements on the page.</p> <code>lines</code> <code>List[Any]</code> <p>Collection of line elements on the page.</p> <code>rects</code> <code>List[Any]</code> <p>Collection of rectangle elements on the page.</p> <code>images</code> <code>List[Any]</code> <p>Collection of image elements on the page.</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Dictionary for storing analysis results and custom data.</p> Example <p>Basic usage: <pre><code>pdf = npdf.PDF(\"document.pdf\")\npage = pdf.pages[0]\n\n# Find elements with CSS-like selectors\nheaders = page.find_all('text[size&gt;12]:bold')\nsummaries = page.find('text:contains(\"Summary\")')\n\n# Spatial navigation\ncontent_below = summaries.below(until='text[size&gt;12]:bold')\n\n# Table extraction\ntables = page.extract_table()\n</code></pre></p> <p>Advanced usage: <pre><code># Apply OCR if needed\npage.apply_ocr(engine='easyocr', resolution=300)\n\n# Layout analysis\npage.analyze_layout(engine='yolo')\n\n# AI-powered extraction\ndata = page.extract_structured_data(MySchema)\n\n# Visual debugging\npage.find('text:contains(\"Important\")').show()\n</code></pre></p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>class Page(\n    TextMixin,\n    ClassificationMixin,\n    ExtractionMixin,\n    ShapeDetectionMixin,\n    DescribeMixin,\n    Visualizable,\n):\n    \"\"\"Enhanced Page wrapper built on top of pdfplumber.Page.\n\n    This class provides a fluent interface for working with PDF pages,\n    with improved selection, navigation, extraction, and question-answering capabilities.\n    It integrates multiple analysis capabilities through mixins and provides spatial\n    navigation with CSS-like selectors.\n\n    The Page class serves as the primary interface for document analysis, offering:\n    - Element selection and spatial navigation\n    - OCR and layout analysis integration\n    - Table detection and extraction\n    - AI-powered classification and data extraction\n    - Visual debugging with highlighting and cropping\n    - Text style analysis and structure detection\n\n    Attributes:\n        index: Zero-based index of this page in the PDF.\n        number: One-based page number (index + 1).\n        width: Page width in points.\n        height: Page height in points.\n        bbox: Bounding box tuple (x0, top, x1, bottom) of the page.\n        chars: Collection of character elements on the page.\n        words: Collection of word elements on the page.\n        lines: Collection of line elements on the page.\n        rects: Collection of rectangle elements on the page.\n        images: Collection of image elements on the page.\n        metadata: Dictionary for storing analysis results and custom data.\n\n    Example:\n        Basic usage:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n        page = pdf.pages[0]\n\n        # Find elements with CSS-like selectors\n        headers = page.find_all('text[size&gt;12]:bold')\n        summaries = page.find('text:contains(\"Summary\")')\n\n        # Spatial navigation\n        content_below = summaries.below(until='text[size&gt;12]:bold')\n\n        # Table extraction\n        tables = page.extract_table()\n        ```\n\n        Advanced usage:\n        ```python\n        # Apply OCR if needed\n        page.apply_ocr(engine='easyocr', resolution=300)\n\n        # Layout analysis\n        page.analyze_layout(engine='yolo')\n\n        # AI-powered extraction\n        data = page.extract_structured_data(MySchema)\n\n        # Visual debugging\n        page.find('text:contains(\"Important\")').show()\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        page: \"pdfplumber.page.Page\",\n        parent: \"PDF\",\n        index: int,\n        font_attrs=None,\n        load_text: bool = True,\n    ):\n        \"\"\"Initialize a page wrapper.\n\n        Creates an enhanced Page object that wraps a pdfplumber page with additional\n        functionality for spatial navigation, analysis, and AI-powered extraction.\n\n        Args:\n            page: The underlying pdfplumber page object that provides raw PDF data.\n            parent: Parent PDF object that contains this page and provides access\n                to managers and global settings.\n            index: Zero-based index of this page in the PDF document.\n            font_attrs: List of font attributes to consider when grouping characters\n                into words. Common attributes include ['fontname', 'size', 'flags'].\n                If None, uses default character-to-word grouping rules.\n            load_text: If True, load and process text elements from the PDF's text layer.\n                If False, skip text layer processing (useful for OCR-only workflows).\n\n        Note:\n            This constructor is typically called automatically when accessing pages\n            through the PDF.pages collection. Direct instantiation is rarely needed.\n\n        Example:\n            ```python\n            # Pages are usually accessed through the PDF object\n            pdf = npdf.PDF(\"document.pdf\")\n            page = pdf.pages[0]  # Page object created automatically\n\n            # Direct construction (advanced usage)\n            import pdfplumber\n            with pdfplumber.open(\"document.pdf\") as plumber_pdf:\n                plumber_page = plumber_pdf.pages[0]\n                page = Page(plumber_page, pdf, 0, load_text=True)\n            ```\n        \"\"\"\n        self._page = page\n        self._parent = parent\n        self._index = index\n        self._load_text = load_text\n        self._text_styles = None  # Lazy-loaded text style analyzer results\n        self._exclusions = []  # List to store exclusion functions/regions\n        self._skew_angle: Optional[float] = None  # Stores detected skew angle\n\n        # --- ADDED --- Metadata store for mixins\n        self.metadata: Dict[str, Any] = {}\n        # --- END ADDED ---\n\n        # Region management\n        self._regions = {\n            \"detected\": [],  # Layout detection results\n            \"named\": {},  # Named regions (name -&gt; region)\n        }\n\n        # -------------------------------------------------------------\n        # Page-scoped configuration begins as a shallow copy of the parent\n        # PDF-level configuration so that auto-computed tolerances or other\n        # page-specific values do not overwrite siblings.\n        # -------------------------------------------------------------\n        self._config = dict(getattr(self._parent, \"_config\", {}))\n\n        # Initialize ElementManager, passing font_attrs\n        self._element_mgr = ElementManager(self, font_attrs=font_attrs, load_text=self._load_text)\n        # self._highlighter = HighlightingService(self) # REMOVED - Use property accessor\n        # --- NEW --- Central registry for analysis results\n        self.analyses: Dict[str, Any] = {}\n\n        # --- Get OCR Manager Instance ---\n        if (\n            OCRManager\n            and hasattr(parent, \"_ocr_manager\")\n            and isinstance(parent._ocr_manager, OCRManager)\n        ):\n            self._ocr_manager = parent._ocr_manager\n            logger.debug(f\"Page {self.number}: Using OCRManager instance from parent PDF.\")\n        else:\n            self._ocr_manager = None\n            if OCRManager:\n                logger.warning(\n                    f\"Page {self.number}: OCRManager instance not found on parent PDF object.\"\n                )\n\n        # --- Get Layout Manager Instance ---\n        if (\n            LayoutManager\n            and hasattr(parent, \"_layout_manager\")\n            and isinstance(parent._layout_manager, LayoutManager)\n        ):\n            self._layout_manager = parent._layout_manager\n            logger.debug(f\"Page {self.number}: Using LayoutManager instance from parent PDF.\")\n        else:\n            self._layout_manager = None\n            if LayoutManager:\n                logger.warning(\n                    f\"Page {self.number}: LayoutManager instance not found on parent PDF object. Layout analysis will fail.\"\n                )\n\n        # Initialize the internal variable with a single underscore\n        self._layout_analyzer = None\n\n        self._load_elements()\n        self._to_image_cache: Dict[tuple, Optional[\"Image.Image\"]] = {}\n\n    def _get_render_specs(\n        self,\n        mode: Literal[\"show\", \"render\"] = \"show\",\n        color: Optional[Union[str, Tuple[int, int, int]]] = None,\n        highlights: Optional[List[Dict[str, Any]]] = None,\n        crop: Union[bool, Literal[\"content\"]] = False,\n        crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n        **kwargs,\n    ) -&gt; List[RenderSpec]:\n        \"\"\"Get render specifications for this page.\n\n        Args:\n            mode: Rendering mode - 'show' includes page highlights, 'render' is clean\n            color: Default color for highlights in show mode\n            highlights: Additional highlight groups to show\n            crop: Whether to crop the page\n            crop_bbox: Explicit crop bounds\n            **kwargs: Additional parameters\n\n        Returns:\n            List containing a single RenderSpec for this page\n        \"\"\"\n        spec = RenderSpec(page=self)\n\n        # Handle cropping\n        if crop_bbox:\n            spec.crop_bbox = crop_bbox\n        elif crop == \"content\":\n            # Calculate content bounds from all elements\n            elements = self.get_elements(apply_exclusions=False)\n            if elements:\n                # Get bounding box of all elements\n                x_coords = []\n                y_coords = []\n                for elem in elements:\n                    if hasattr(elem, \"bbox\") and elem.bbox:\n                        x0, y0, x1, y1 = elem.bbox\n                        x_coords.extend([x0, x1])\n                        y_coords.extend([y0, y1])\n\n                if x_coords and y_coords:\n                    spec.crop_bbox = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))\n        elif crop is True:\n            # Crop to full page (no-op, but included for consistency)\n            spec.crop_bbox = (0, 0, self.width, self.height)\n\n        # Add highlights in show mode\n        if mode == \"show\":\n            # Add page's persistent highlights if any\n            page_highlights = self._highlighter.get_highlights_for_page(self.index)\n            for highlight in page_highlights:\n                spec.add_highlight(\n                    bbox=highlight.bbox,\n                    polygon=highlight.polygon,\n                    color=highlight.color,\n                    label=highlight.label,\n                    element=None,  # Persistent highlights don't have element refs\n                )\n\n            # Add additional highlight groups if provided\n            if highlights:\n                for group in highlights:\n                    elements = group.get(\"elements\", [])\n                    group_color = group.get(\"color\", color)\n                    group_label = group.get(\"label\")\n\n                    for elem in elements:\n                        spec.add_highlight(element=elem, color=group_color, label=group_label)\n\n            # Handle exclusions visualization\n            exclusions_param = kwargs.get(\"exclusions\")\n            if exclusions_param:\n                # Get exclusion regions\n                exclusion_regions = self._get_exclusion_regions(include_callable=True)\n\n                if exclusion_regions:\n                    # Determine color for exclusions\n                    exclusion_color = (\n                        exclusions_param if isinstance(exclusions_param, str) else \"red\"\n                    )\n\n                    # Add exclusion regions as highlights\n                    for region in exclusion_regions:\n                        spec.add_highlight(\n                            element=region,\n                            color=exclusion_color,\n                            label=f\"Exclusion: {region.label or 'unnamed'}\",\n                        )\n\n        return [spec]\n\n    @property\n    def pdf(self) -&gt; \"PDF\":\n        \"\"\"Provides public access to the parent PDF object.\"\"\"\n        return self._parent\n\n    @property\n    def number(self) -&gt; int:\n        \"\"\"Get page number (1-based).\"\"\"\n        return self._page.page_number\n\n    @property\n    def page_number(self) -&gt; int:\n        \"\"\"Get page number (1-based).\"\"\"\n        return self._page.page_number\n\n    @property\n    def index(self) -&gt; int:\n        \"\"\"Get page index (0-based).\"\"\"\n        return self._index\n\n    @property\n    def width(self) -&gt; float:\n        \"\"\"Get page width.\"\"\"\n        return self._page.width\n\n    @property\n    def height(self) -&gt; float:\n        \"\"\"Get page height.\"\"\"\n        return self._page.height\n\n    # --- Highlighting Service Accessor ---\n    @property\n    def _highlighter(self) -&gt; \"HighlightingService\":\n        \"\"\"Provides access to the parent PDF's HighlightingService.\"\"\"\n        if not hasattr(self._parent, \"highlighter\"):\n            # This should ideally not happen if PDF.__init__ works correctly\n            raise AttributeError(\"Parent PDF object does not have a 'highlighter' attribute.\")\n        return self._parent.highlighter\n\n    def clear_exclusions(self) -&gt; \"Page\":\n        \"\"\"\n        Clear all exclusions from the page.\n        \"\"\"\n        self._exclusions = []\n        return self\n\n    def add_exclusion(\n        self,\n        exclusion_func_or_region: Union[\n            Callable[[\"Page\"], \"Region\"], \"Region\", List[Any], Tuple[Any, ...], Any\n        ],\n        label: Optional[str] = None,\n        method: str = \"region\",\n    ) -&gt; \"Page\":\n        \"\"\"\n        Add an exclusion to the page. Text from these regions will be excluded from extraction.\n        Ensures non-callable items are stored as Region objects if possible.\n\n        Args:\n            exclusion_func_or_region: Either a callable function returning a Region,\n                                      a Region object, a list/tuple of regions or elements,\n                                      or another object with a valid .bbox attribute.\n            label: Optional label for this exclusion (e.g., 'header', 'footer').\n            method: Exclusion method - 'region' (exclude all elements in bounding box) or\n                    'element' (exclude only the specific elements). Default: 'region'.\n\n        Returns:\n            Self for method chaining\n\n        Raises:\n            TypeError: If a non-callable, non-Region object without a valid bbox is provided.\n            ValueError: If method is not 'region' or 'element'.\n        \"\"\"\n        # Validate method parameter\n        if method not in (\"region\", \"element\"):\n            raise ValueError(f\"Invalid exclusion method '{method}'. Must be 'region' or 'element'.\")\n\n        # ------------------------------------------------------------------\n        # NEW: Handle selector strings and ElementCollection instances\n        # ------------------------------------------------------------------\n        # If a user supplies a selector string (e.g. \"text:bold\") we resolve it\n        # immediately *on this page* to the matching elements and turn each into\n        # a Region object which is added to the internal exclusions list.\n        #\n        # Likewise, if an ElementCollection is passed we iterate over its\n        # elements and create Regions for each one.\n        # ------------------------------------------------------------------\n        # Import ElementCollection from the new module path (old path removed)\n        from natural_pdf.elements.element_collection import ElementCollection\n\n        # Selector string ---------------------------------------------------\n        if isinstance(exclusion_func_or_region, str):\n            selector_str = exclusion_func_or_region\n            matching_elements = self.find_all(selector_str, apply_exclusions=False)\n\n            if not matching_elements:\n                logger.warning(\n                    f\"Page {self.index}: Selector '{selector_str}' returned no elements \u2013 no exclusions added.\"\n                )\n            else:\n                if method == \"element\":\n                    # Store the actual elements for element-based exclusion\n                    for el in matching_elements:\n                        self._exclusions.append((el, label, method))\n                        logger.debug(\n                            f\"Page {self.index}: Added element exclusion from selector '{selector_str}' -&gt; {el}\"\n                        )\n                else:  # method == \"region\"\n                    for el in matching_elements:\n                        try:\n                            bbox_coords = (\n                                float(el.x0),\n                                float(el.top),\n                                float(el.x1),\n                                float(el.bottom),\n                            )\n                            region = Region(self, bbox_coords, label=label)\n                            # Store directly as a Region tuple so we don't recurse endlessly\n                            self._exclusions.append((region, label, method))\n                            logger.debug(\n                                f\"Page {self.index}: Added exclusion region from selector '{selector_str}' -&gt; {bbox_coords}\"\n                            )\n                        except Exception as e:\n                            # Re-raise so calling code/test sees the failure immediately\n                            logger.error(\n                                f\"Page {self.index}: Failed to create exclusion region from element {el}: {e}\",\n                                exc_info=False,\n                            )\n                            raise\n            return self  # Completed processing for selector input\n\n        # ElementCollection -----------------------------------------------\n        if isinstance(exclusion_func_or_region, ElementCollection):\n            if method == \"element\":\n                # Store the actual elements for element-based exclusion\n                for el in exclusion_func_or_region:\n                    self._exclusions.append((el, label, method))\n                    logger.debug(\n                        f\"Page {self.index}: Added element exclusion from ElementCollection -&gt; {el}\"\n                    )\n            else:  # method == \"region\"\n                # Convert each element to a Region and add\n                for el in exclusion_func_or_region:\n                    try:\n                        if not (hasattr(el, \"bbox\") and len(el.bbox) == 4):\n                            logger.warning(\n                                f\"Page {self.index}: Skipping element without bbox in ElementCollection exclusion: {el}\"\n                            )\n                            continue\n                        bbox_coords = tuple(float(v) for v in el.bbox)\n                        region = Region(self, bbox_coords, label=label)\n                        self._exclusions.append((region, label, method))\n                        logger.debug(\n                            f\"Page {self.index}: Added exclusion region from ElementCollection element {bbox_coords}\"\n                        )\n                    except Exception as e:\n                        logger.error(\n                            f\"Page {self.index}: Failed to convert ElementCollection element to Region: {e}\",\n                            exc_info=False,\n                        )\n                        raise\n            return self  # Completed processing for ElementCollection input\n\n        # ------------------------------------------------------------------\n        # Existing logic (callable, Region, bbox-bearing objects)\n        # ------------------------------------------------------------------\n        exclusion_data = None  # Initialize exclusion data\n\n        if callable(exclusion_func_or_region):\n            # Store callable functions along with their label and method\n            exclusion_data = (exclusion_func_or_region, label, method)\n            logger.debug(\n                f\"Page {self.index}: Added callable exclusion '{label}' with method '{method}': {exclusion_func_or_region}\"\n            )\n        elif isinstance(exclusion_func_or_region, Region):\n            # Store Region objects directly, assigning the label\n            exclusion_func_or_region.label = label  # Assign label\n            exclusion_data = (\n                exclusion_func_or_region,\n                label,\n                method,\n            )  # Store as tuple for consistency\n            logger.debug(\n                f\"Page {self.index}: Added Region exclusion '{label}' with method '{method}': {exclusion_func_or_region}\"\n            )\n        elif (\n            hasattr(exclusion_func_or_region, \"bbox\")\n            and isinstance(getattr(exclusion_func_or_region, \"bbox\", None), (tuple, list))\n            and len(exclusion_func_or_region.bbox) == 4\n        ):\n            if method == \"element\":\n                # For element method, store the element directly\n                exclusion_data = (exclusion_func_or_region, label, method)\n                logger.debug(\n                    f\"Page {self.index}: Added element exclusion '{label}': {exclusion_func_or_region}\"\n                )\n            else:  # method == \"region\"\n                # Convert objects with a valid bbox to a Region before storing\n                try:\n                    bbox_coords = tuple(float(v) for v in exclusion_func_or_region.bbox)\n                    # Pass the label to the Region constructor\n                    region_to_add = Region(self, bbox_coords, label=label)\n                    exclusion_data = (region_to_add, label, method)  # Store as tuple\n                    logger.debug(\n                        f\"Page {self.index}: Added exclusion '{label}' with method '{method}' converted to Region from {type(exclusion_func_or_region)}: {region_to_add}\"\n                    )\n                except (ValueError, TypeError, Exception) as e:\n                    # Raise an error if conversion fails\n                    raise TypeError(\n                        f\"Failed to convert exclusion object {exclusion_func_or_region} with bbox {getattr(exclusion_func_or_region, 'bbox', 'N/A')} to Region: {e}\"\n                    ) from e\n        elif isinstance(exclusion_func_or_region, (list, tuple)):\n            # Handle lists/tuples of regions or elements\n            if not exclusion_func_or_region:\n                logger.warning(f\"Page {self.index}: Empty list provided for exclusion, ignoring.\")\n                return self\n\n            if method == \"element\":\n                # Store each element directly\n                for item in exclusion_func_or_region:\n                    if hasattr(item, \"bbox\") and len(getattr(item, \"bbox\", [])) == 4:\n                        self._exclusions.append((item, label, method))\n                        logger.debug(\n                            f\"Page {self.index}: Added element exclusion from list -&gt; {item}\"\n                        )\n                    else:\n                        logger.warning(\n                            f\"Page {self.index}: Skipping item without valid bbox in list: {item}\"\n                        )\n            else:  # method == \"region\"\n                # Convert each item to a Region and add\n                for item in exclusion_func_or_region:\n                    try:\n                        if isinstance(item, Region):\n                            item.label = label\n                            self._exclusions.append((item, label, method))\n                            logger.debug(f\"Page {self.index}: Added Region from list: {item}\")\n                        elif hasattr(item, \"bbox\") and len(getattr(item, \"bbox\", [])) == 4:\n                            bbox_coords = tuple(float(v) for v in item.bbox)\n                            region = Region(self, bbox_coords, label=label)\n                            self._exclusions.append((region, label, method))\n                            logger.debug(\n                                f\"Page {self.index}: Added exclusion region from list item {bbox_coords}\"\n                            )\n                        else:\n                            logger.warning(\n                                f\"Page {self.index}: Skipping item without valid bbox in list: {item}\"\n                            )\n                    except Exception as e:\n                        logger.error(\n                            f\"Page {self.index}: Failed to convert list item to Region: {e}\"\n                        )\n                        continue\n            return self\n        else:\n            # Reject invalid types\n            raise TypeError(\n                f\"Invalid exclusion type: {type(exclusion_func_or_region)}. Must be callable, Region, list/tuple of regions/elements, or have a valid .bbox attribute.\"\n            )\n\n        # Append the stored data (tuple of object/callable, label, and method)\n        if exclusion_data:\n            self._exclusions.append(exclusion_data)\n\n        return self\n\n    def add_region(self, region: \"Region\", name: Optional[str] = None) -&gt; \"Page\":\n        \"\"\"\n        Add a region to the page.\n\n        Args:\n            region: Region object to add\n            name: Optional name for the region\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Check if it's actually a Region object\n        if not isinstance(region, Region):\n            raise TypeError(\"region must be a Region object\")\n\n        # Set the source and name\n        region.source = \"named\"\n\n        if name:\n            region.name = name\n            # Add to named regions dictionary (overwriting if name already exists)\n            self._regions[\"named\"][name] = region\n        else:\n            # Add to detected regions list (unnamed but registered)\n            self._regions[\"detected\"].append(region)\n\n        # Add to element manager for selector queries\n        self._element_mgr.add_region(region)\n\n        return self\n\n    def add_regions(self, regions: List[\"Region\"], prefix: Optional[str] = None) -&gt; \"Page\":\n        \"\"\"\n        Add multiple regions to the page.\n\n        Args:\n            regions: List of Region objects to add\n            prefix: Optional prefix for automatic naming (regions will be named prefix_1, prefix_2, etc.)\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        if prefix:\n            # Add with automatic sequential naming\n            for i, region in enumerate(regions):\n                self.add_region(region, name=f\"{prefix}_{i+1}\")\n        else:\n            # Add without names\n            for region in regions:\n                self.add_region(region)\n\n        return self\n\n    def _get_exclusion_regions(self, include_callable=True, debug=False) -&gt; List[\"Region\"]:\n        \"\"\"\n        Get all exclusion regions for this page.\n        Now handles both region-based and element-based exclusions.\n        Assumes self._exclusions contains tuples of (callable/Region/Element, label, method).\n\n        Args:\n            include_callable: Whether to evaluate callable exclusion functions\n            debug: Enable verbose debug logging for exclusion evaluation\n\n        Returns:\n            List of Region objects to exclude, with labels assigned.\n        \"\"\"\n        regions = []\n\n        if debug:\n            print(f\"\\nPage {self.index}: Evaluating {len(self._exclusions)} exclusions\")\n\n        for i, exclusion_data in enumerate(self._exclusions):\n            # Handle both old format (2-tuple) and new format (3-tuple) for backward compatibility\n            if len(exclusion_data) == 2:\n                # Old format: (exclusion_item, label)\n                exclusion_item, label = exclusion_data\n                method = \"region\"  # Default to region for old format\n            else:\n                # New format: (exclusion_item, label, method)\n                exclusion_item, label, method = exclusion_data\n\n            exclusion_label = label if label else f\"exclusion {i}\"\n\n            # Process callable exclusion functions\n            if callable(exclusion_item) and include_callable:\n                try:\n                    if debug:\n                        print(f\"  - Evaluating callable '{exclusion_label}'...\")\n\n                    # Temporarily clear exclusions (consider if really needed)\n                    temp_original_exclusions = self._exclusions\n                    self._exclusions = []\n\n                    # Call the function - Expects it to return a Region or None\n                    region_result = exclusion_item(self)\n\n                    # Restore exclusions\n                    self._exclusions = temp_original_exclusions\n\n                    if isinstance(region_result, Region):\n                        # Assign the label to the returned region\n                        region_result.label = label\n                        regions.append(region_result)\n                        if debug:\n                            print(f\"    \u2713 Added region from callable '{label}': {region_result}\")\n                    elif hasattr(region_result, \"__iter__\") and hasattr(region_result, \"__len__\"):\n                        # Handle ElementCollection or other iterables\n                        from natural_pdf.elements.element_collection import ElementCollection\n\n                        if isinstance(region_result, ElementCollection) or (\n                            hasattr(region_result, \"__iter__\") and region_result\n                        ):\n                            if debug:\n                                print(\n                                    f\"    Converting {type(region_result)} with {len(region_result)} elements to regions...\"\n                                )\n\n                            # Convert each element to a region\n                            for elem in region_result:\n                                try:\n                                    if hasattr(elem, \"bbox\") and len(elem.bbox) == 4:\n                                        bbox_coords = tuple(float(v) for v in elem.bbox)\n                                        region = Region(self, bbox_coords, label=label)\n                                        regions.append(region)\n                                        if debug:\n                                            print(\n                                                f\"      \u2713 Added region from element: {bbox_coords}\"\n                                            )\n                                    else:\n                                        if debug:\n                                            print(\n                                                f\"      \u2717 Skipping element without valid bbox: {elem}\"\n                                            )\n                                except Exception as e:\n                                    if debug:\n                                        print(f\"      \u2717 Failed to convert element to region: {e}\")\n                                    continue\n\n                            if debug and len(region_result) &gt; 0:\n                                print(\n                                    f\"    \u2713 Converted {len(region_result)} elements from callable '{label}'\"\n                                )\n                        else:\n                            if debug:\n                                print(f\"    \u2717 Empty iterable returned from callable '{label}'\")\n                    elif region_result:\n                        logger.warning(\n                            f\"Callable exclusion '{exclusion_label}' returned non-Region object: {type(region_result)}. Skipping.\"\n                        )\n                        if debug:\n                            print(f\"    \u2717 Callable returned non-Region/None: {type(region_result)}\")\n                    else:\n                        if debug:\n                            print(\n                                f\"    \u2717 Callable '{exclusion_label}' returned None, no region added\"\n                            )\n\n                except Exception as e:\n                    error_msg = f\"Error evaluating callable exclusion '{exclusion_label}' for page {self.index}: {e}\"\n                    print(error_msg)\n                    import traceback\n\n                    print(f\"    Traceback: {traceback.format_exc().splitlines()[-3:]}\")\n\n            # Process direct Region objects (label was assigned in add_exclusion)\n            elif isinstance(exclusion_item, Region):\n                regions.append(exclusion_item)  # Label is already on the Region object\n                if debug:\n                    print(f\"  - Added direct region '{label}': {exclusion_item}\")\n            # Element-based exclusions are not converted to regions here\n            # They will be handled separately in _filter_elements_by_exclusions\n\n        if debug:\n            print(f\"Page {self.index}: Found {len(regions)} valid exclusion regions to apply\")\n\n        return regions\n\n    def _filter_elements_by_exclusions(\n        self, elements: List[\"Element\"], debug_exclusions: bool = False\n    ) -&gt; List[\"Element\"]:\n        \"\"\"\n        Filters a list of elements, removing those based on exclusion rules.\n        Handles both region-based exclusions (exclude all in area) and\n        element-based exclusions (exclude only specific elements).\n\n        Args:\n            elements: The list of elements to filter.\n            debug_exclusions: Whether to output detailed exclusion debugging info (default: False).\n\n        Returns:\n            A new list containing only the elements not excluded.\n        \"\"\"\n        if not self._exclusions:\n            if debug_exclusions:\n                print(\n                    f\"Page {self.index}: No exclusions defined, returning all {len(elements)} elements.\"\n                )\n            return elements\n\n        # Get all exclusion regions, including evaluating callable functions\n        exclusion_regions = self._get_exclusion_regions(\n            include_callable=True, debug=debug_exclusions\n        )\n\n        # Collect element-based exclusions\n        excluded_elements = set()  # Use set for O(1) lookup\n\n        for exclusion_data in self._exclusions:\n            # Handle both old format (2-tuple) and new format (3-tuple)\n            if len(exclusion_data) == 2:\n                exclusion_item, label = exclusion_data\n                method = \"region\"\n            else:\n                exclusion_item, label, method = exclusion_data\n\n            # Skip callables (already handled in _get_exclusion_regions)\n            if callable(exclusion_item):\n                continue\n\n            # Skip regions (already in exclusion_regions)\n            if isinstance(exclusion_item, Region):\n                continue\n\n            # Handle element-based exclusions\n            if method == \"element\" and hasattr(exclusion_item, \"bbox\"):\n                excluded_elements.add(id(exclusion_item))\n                if debug_exclusions:\n                    print(f\"  - Added element exclusion: {exclusion_item}\")\n\n        if debug_exclusions:\n            print(\n                f\"Page {self.index}: Applying {len(exclusion_regions)} region exclusions \"\n                f\"and {len(excluded_elements)} element exclusions to {len(elements)} elements.\"\n            )\n\n        filtered_elements = []\n        region_excluded_count = 0\n        element_excluded_count = 0\n\n        for element in elements:\n            exclude = False\n\n            # Check element-based exclusions first (faster)\n            if id(element) in excluded_elements:\n                exclude = True\n                element_excluded_count += 1\n                if debug_exclusions:\n                    print(f\"    Element {element} excluded by element-based rule\")\n            else:\n                # Check region-based exclusions\n                for region in exclusion_regions:\n                    # Use the region's method to check if the element is inside\n                    if region._is_element_in_region(element):\n                        exclude = True\n                        region_excluded_count += 1\n                        if debug_exclusions:\n                            print(f\"    Element {element} excluded by region {region}\")\n                        break  # No need to check other regions for this element\n\n            if not exclude:\n                filtered_elements.append(element)\n\n        if debug_exclusions:\n            print(\n                f\"Page {self.index}: Excluded {region_excluded_count} by regions, \"\n                f\"{element_excluded_count} by elements, keeping {len(filtered_elements)}.\"\n            )\n\n        return filtered_elements\n\n    @overload\n    def find(\n        self,\n        *,\n        text: str,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[Any]: ...\n\n    @overload\n    def find(\n        self,\n        selector: str,\n        *,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[Any]: ...\n\n    def find(\n        self,\n        selector: Optional[str] = None,  # Now optional\n        *,  # Force subsequent args to be keyword-only\n        text: Optional[str] = None,  # New text parameter\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[Any]:\n        \"\"\"\n        Find first element on this page matching selector OR text content.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional filter parameters.\n\n        Returns:\n            Element object or None if not found.\n        \"\"\"\n        if selector is not None and text is not None:\n            raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n        if selector is None and text is None:\n            raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n        # Construct selector if 'text' is provided\n        effective_selector = \"\"\n        if text is not None:\n            # Escape quotes within the text for the selector string\n            escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n            # Default to 'text:contains(...)'\n            effective_selector = f'text:contains(\"{escaped_text}\")'\n            # Note: regex/case handled by kwargs passed down\n            logger.debug(\n                f\"Using text shortcut: find(text='{text}') -&gt; find('{effective_selector}')\"\n            )\n        elif selector is not None:\n            effective_selector = selector\n        else:\n            # Should be unreachable due to checks above\n            raise ValueError(\"Internal error: No selector or text provided.\")\n\n        selector_obj = parse_selector(effective_selector)\n\n        # Pass regex and case flags to selector function via kwargs\n        kwargs[\"regex\"] = regex\n        kwargs[\"case\"] = case\n\n        # First get all matching elements without applying exclusions initially within _apply_selector\n        results_collection = self._apply_selector(\n            selector_obj, **kwargs\n        )  # _apply_selector doesn't filter\n\n        # Filter the results based on exclusions if requested\n        if apply_exclusions and self._exclusions and results_collection:\n            filtered_elements = self._filter_elements_by_exclusions(results_collection.elements)\n            # Return the first element from the filtered list\n            return filtered_elements[0] if filtered_elements else None\n        elif results_collection:\n            # Return the first element from the unfiltered results\n            return results_collection.first\n        else:\n            return None\n\n    @overload\n    def find_all(\n        self,\n        *,\n        text: str,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    @overload\n    def find_all(\n        self,\n        selector: str,\n        *,\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    def find_all(\n        self,\n        selector: Optional[str] = None,  # Now optional\n        *,  # Force subsequent args to be keyword-only\n        text: Optional[str] = None,  # New text parameter\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Find all elements on this page matching selector OR text content.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional filter parameters.\n\n        Returns:\n            ElementCollection with matching elements.\n        \"\"\"\n        from natural_pdf.elements.element_collection import (  # Import here for type hint\n            ElementCollection,\n        )\n\n        if selector is not None and text is not None:\n            raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n        if selector is None and text is None:\n            raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n        # Construct selector if 'text' is provided\n        effective_selector = \"\"\n        if text is not None:\n            # Escape quotes within the text for the selector string\n            escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n            # Default to 'text:contains(...)'\n            effective_selector = f'text:contains(\"{escaped_text}\")'\n            logger.debug(\n                f\"Using text shortcut: find_all(text='{text}') -&gt; find_all('{effective_selector}')\"\n            )\n        elif selector is not None:\n            effective_selector = selector\n        else:\n            # Should be unreachable due to checks above\n            raise ValueError(\"Internal error: No selector or text provided.\")\n\n        selector_obj = parse_selector(effective_selector)\n\n        # Pass regex and case flags to selector function via kwargs\n        kwargs[\"regex\"] = regex\n        kwargs[\"case\"] = case\n\n        # First get all matching elements without applying exclusions initially within _apply_selector\n        results_collection = self._apply_selector(\n            selector_obj, **kwargs\n        )  # _apply_selector doesn't filter\n\n        # Filter the results based on exclusions if requested\n        if apply_exclusions and self._exclusions and results_collection:\n            filtered_elements = self._filter_elements_by_exclusions(results_collection.elements)\n            return ElementCollection(filtered_elements)\n        else:\n            # Return the unfiltered collection\n            return results_collection\n\n    def _apply_selector(\n        self, selector_obj: Dict, **kwargs\n    ) -&gt; \"ElementCollection\":  # Removed apply_exclusions arg\n        \"\"\"\n        Apply selector to page elements.\n        Exclusions are now handled by the calling methods (find, find_all) if requested.\n\n        Args:\n            selector_obj: Parsed selector dictionary (single or compound OR selector)\n            **kwargs: Additional filter parameters including 'regex' and 'case'\n\n        Returns:\n            ElementCollection of matching elements (unfiltered by exclusions)\n        \"\"\"\n        from natural_pdf.selectors.parser import selector_to_filter_func\n\n        # Handle compound OR selectors\n        if selector_obj.get(\"type\") == \"or\":\n            # For OR selectors, search all elements and let the filter function decide\n            elements_to_search = self._element_mgr.get_all_elements()\n\n            # Create filter function from compound selector\n            filter_func = selector_to_filter_func(selector_obj, **kwargs)\n\n            # Apply the filter to all elements\n            matching_elements = [element for element in elements_to_search if filter_func(element)]\n\n            # Sort elements in reading order if requested\n            if kwargs.get(\"reading_order\", True):\n                if all(hasattr(el, \"top\") and hasattr(el, \"x0\") for el in matching_elements):\n                    matching_elements.sort(key=lambda el: (el.top, el.x0))\n                else:\n                    logger.warning(\n                        \"Cannot sort elements in reading order: Missing required attributes (top, x0).\"\n                    )\n\n            # Handle collection-level pseudo-classes (:first, :last) for OR selectors\n            # Note: We only apply :first/:last if they appear in any of the sub-selectors\n            has_first = False\n            has_last = False\n            for sub_selector in selector_obj.get(\"selectors\", []):\n                for pseudo in sub_selector.get(\"pseudo_classes\", []):\n                    if pseudo.get(\"name\") == \"first\":\n                        has_first = True\n                    elif pseudo.get(\"name\") == \"last\":\n                        has_last = True\n\n            if has_first:\n                matching_elements = matching_elements[:1] if matching_elements else []\n            elif has_last:\n                matching_elements = matching_elements[-1:] if matching_elements else []\n\n            # Return result collection\n            return ElementCollection(matching_elements)\n\n        # Handle single selectors (existing logic)\n        # Get element type to filter\n        element_type = selector_obj.get(\"type\", \"any\").lower()\n\n        # Determine which elements to search based on element type\n        elements_to_search = []\n        if element_type == \"any\":\n            elements_to_search = self._element_mgr.get_all_elements()\n        elif element_type == \"text\":\n            elements_to_search = self._element_mgr.words\n        elif element_type == \"char\":\n            elements_to_search = self._element_mgr.chars\n        elif element_type == \"word\":\n            elements_to_search = self._element_mgr.words\n        elif element_type == \"rect\" or element_type == \"rectangle\":\n            elements_to_search = self._element_mgr.rects\n        elif element_type == \"line\":\n            elements_to_search = self._element_mgr.lines\n        elif element_type == \"region\":\n            elements_to_search = self._element_mgr.regions\n        else:\n            elements_to_search = self._element_mgr.get_all_elements()\n\n        # Create filter function from selector, passing any additional parameters\n        filter_func = selector_to_filter_func(selector_obj, **kwargs)\n\n        # Apply the filter to matching elements\n        matching_elements = [element for element in elements_to_search if filter_func(element)]\n\n        # Handle spatial pseudo-classes that require relationship checking\n        for pseudo in selector_obj.get(\"pseudo_classes\", []):\n            name = pseudo.get(\"name\")\n            args = pseudo.get(\"args\", \"\")\n\n            if name in (\"above\", \"below\", \"near\", \"left-of\", \"right-of\"):\n                # Find the reference element first\n                from natural_pdf.selectors.parser import parse_selector\n\n                ref_selector = parse_selector(args) if isinstance(args, str) else args\n                # Recursively call _apply_selector for reference element (exclusions handled later)\n                ref_elements = self._apply_selector(ref_selector, **kwargs)\n\n                if not ref_elements:\n                    return ElementCollection([])\n\n                ref_element = ref_elements.first\n                if not ref_element:\n                    continue\n\n                # Filter elements based on spatial relationship\n                if name == \"above\":\n                    matching_elements = [\n                        el\n                        for el in matching_elements\n                        if hasattr(el, \"bottom\")\n                        and hasattr(ref_element, \"top\")\n                        and el.bottom &lt;= ref_element.top\n                    ]\n                elif name == \"below\":\n                    matching_elements = [\n                        el\n                        for el in matching_elements\n                        if hasattr(el, \"top\")\n                        and hasattr(ref_element, \"bottom\")\n                        and el.top &gt;= ref_element.bottom\n                    ]\n                elif name == \"left-of\":\n                    matching_elements = [\n                        el\n                        for el in matching_elements\n                        if hasattr(el, \"x1\")\n                        and hasattr(ref_element, \"x0\")\n                        and el.x1 &lt;= ref_element.x0\n                    ]\n                elif name == \"right-of\":\n                    matching_elements = [\n                        el\n                        for el in matching_elements\n                        if hasattr(el, \"x0\")\n                        and hasattr(ref_element, \"x1\")\n                        and el.x0 &gt;= ref_element.x1\n                    ]\n                elif name == \"near\":\n\n                    def distance(el1, el2):\n                        if not (\n                            hasattr(el1, \"x0\")\n                            and hasattr(el1, \"x1\")\n                            and hasattr(el1, \"top\")\n                            and hasattr(el1, \"bottom\")\n                            and hasattr(el2, \"x0\")\n                            and hasattr(el2, \"x1\")\n                            and hasattr(el2, \"top\")\n                            and hasattr(el2, \"bottom\")\n                        ):\n                            return float(\"inf\")  # Cannot calculate distance\n                        el1_center_x = (el1.x0 + el1.x1) / 2\n                        el1_center_y = (el1.top + el1.bottom) / 2\n                        el2_center_x = (el2.x0 + el2.x1) / 2\n                        el2_center_y = (el2.top + el2.bottom) / 2\n                        return (\n                            (el1_center_x - el2_center_x) ** 2 + (el1_center_y - el2_center_y) ** 2\n                        ) ** 0.5\n\n                    threshold = kwargs.get(\"near_threshold\", 50)\n                    matching_elements = [\n                        el for el in matching_elements if distance(el, ref_element) &lt;= threshold\n                    ]\n\n        # Sort elements in reading order if requested\n        if kwargs.get(\"reading_order\", True):\n            if all(hasattr(el, \"top\") and hasattr(el, \"x0\") for el in matching_elements):\n                matching_elements.sort(key=lambda el: (el.top, el.x0))\n            else:\n                logger.warning(\n                    \"Cannot sort elements in reading order: Missing required attributes (top, x0).\"\n                )\n\n        # Handle collection-level pseudo-classes (:first, :last)\n        for pseudo in selector_obj.get(\"pseudo_classes\", []):\n            name = pseudo.get(\"name\")\n\n            if name == \"first\":\n                matching_elements = matching_elements[:1] if matching_elements else []\n            elif name == \"last\":\n                matching_elements = matching_elements[-1:] if matching_elements else []\n\n        # Create result collection - exclusions are handled by the calling methods (find, find_all)\n        result = ElementCollection(matching_elements)\n\n        return result\n\n    def create_region(self, x0: float, top: float, x1: float, bottom: float) -&gt; Any:\n        \"\"\"\n        Create a region on this page with the specified coordinates.\n\n        Args:\n            x0: Left x-coordinate\n            top: Top y-coordinate\n            x1: Right x-coordinate\n            bottom: Bottom y-coordinate\n\n        Returns:\n            Region object for the specified coordinates\n        \"\"\"\n        from natural_pdf.elements.region import Region\n\n        return Region(self, (x0, top, x1, bottom))\n\n    def region(\n        self,\n        left: float = None,\n        top: float = None,\n        right: float = None,\n        bottom: float = None,\n        width: Union[str, float, None] = None,\n        height: Optional[float] = None,\n    ) -&gt; Any:\n        \"\"\"\n        Create a region on this page with more intuitive named parameters,\n        allowing definition by coordinates or by coordinate + dimension.\n\n        Args:\n            left: Left x-coordinate (default: 0 if width not used).\n            top: Top y-coordinate (default: 0 if height not used).\n            right: Right x-coordinate (default: page width if width not used).\n            bottom: Bottom y-coordinate (default: page height if height not used).\n            width: Width definition. Can be:\n                   - Numeric: The width of the region in points. Cannot be used with both left and right.\n                   - String 'full': Sets region width to full page width (overrides left/right).\n                   - String 'element' or None (default): Uses provided/calculated left/right,\n                     defaulting to page width if neither are specified.\n            height: Numeric height of the region. Cannot be used with both top and bottom.\n\n        Returns:\n            Region object for the specified coordinates\n\n        Raises:\n            ValueError: If conflicting arguments are provided (e.g., top, bottom, and height)\n                      or if width is an invalid string.\n\n        Examples:\n            &gt;&gt;&gt; page.region(top=100, height=50)  # Region from y=100 to y=150, default width\n            &gt;&gt;&gt; page.region(left=50, width=100)   # Region from x=50 to x=150, default height\n            &gt;&gt;&gt; page.region(bottom=500, height=50) # Region from y=450 to y=500\n            &gt;&gt;&gt; page.region(right=200, width=50)  # Region from x=150 to x=200\n            &gt;&gt;&gt; page.region(top=100, bottom=200, width=\"full\") # Explicit full width\n        \"\"\"\n        # ------------------------------------------------------------------\n        # Percentage support \u2013 convert strings like \"30%\" to absolute values\n        # based on page dimensions.  X-axis params (left, right, width) use\n        # page.width; Y-axis params (top, bottom, height) use page.height.\n        # ------------------------------------------------------------------\n\n        def _pct_to_abs(val, axis: str):\n            if isinstance(val, str) and val.strip().endswith(\"%\"):\n                try:\n                    pct = float(val.strip()[:-1]) / 100.0\n                except ValueError:\n                    return val  # leave unchanged if not a number\n                return pct * (self.width if axis == \"x\" else self.height)\n            return val\n\n        left = _pct_to_abs(left, \"x\")\n        right = _pct_to_abs(right, \"x\")\n        width = _pct_to_abs(width, \"x\")\n        top = _pct_to_abs(top, \"y\")\n        bottom = _pct_to_abs(bottom, \"y\")\n        height = _pct_to_abs(height, \"y\")\n\n        # --- Type checking and basic validation ---\n        is_width_numeric = isinstance(width, (int, float))\n        is_width_string = isinstance(width, str)\n        width_mode = \"element\"  # Default mode\n\n        if height is not None and top is not None and bottom is not None:\n            raise ValueError(\"Cannot specify top, bottom, and height simultaneously.\")\n        if is_width_numeric and left is not None and right is not None:\n            raise ValueError(\"Cannot specify left, right, and a numeric width simultaneously.\")\n        if is_width_string:\n            width_lower = width.lower()\n            if width_lower not in [\"full\", \"element\"]:\n                raise ValueError(\"String width argument must be 'full' or 'element'.\")\n            width_mode = width_lower\n\n        # --- Calculate Coordinates ---\n        final_top = top\n        final_bottom = bottom\n        final_left = left\n        final_right = right\n\n        # Height calculations\n        if height is not None:\n            if top is not None:\n                final_bottom = top + height\n            elif bottom is not None:\n                final_top = bottom - height\n            else:  # Neither top nor bottom provided, default top to 0\n                final_top = 0\n                final_bottom = height\n\n        # Width calculations (numeric only)\n        if is_width_numeric:\n            if left is not None:\n                final_right = left + width\n            elif right is not None:\n                final_left = right - width\n            else:  # Neither left nor right provided, default left to 0\n                final_left = 0\n                final_right = width\n\n        # --- Apply Defaults for Unset Coordinates ---\n        # Only default coordinates if they weren't set by dimension calculation\n        if final_top is None:\n            final_top = 0\n        if final_bottom is None:\n            # Check if bottom should have been set by height calc\n            if height is None or top is None:\n                final_bottom = self.height\n\n        if final_left is None:\n            final_left = 0\n        if final_right is None:\n            # Check if right should have been set by width calc\n            if not is_width_numeric or left is None:\n                final_right = self.width\n\n        # --- Handle width_mode == 'full' ---\n        if width_mode == \"full\":\n            # Override left/right if mode is full\n            final_left = 0\n            final_right = self.width\n\n        # --- Final Validation &amp; Creation ---\n        # Ensure coordinates are within page bounds (clamp)\n        final_left = max(0, final_left)\n        final_top = max(0, final_top)\n        final_right = min(self.width, final_right)\n        final_bottom = min(self.height, final_bottom)\n\n        # Ensure valid box (x0&lt;=x1, top&lt;=bottom)\n        if final_left &gt; final_right:\n            logger.warning(f\"Calculated left ({final_left}) &gt; right ({final_right}). Swapping.\")\n            final_left, final_right = final_right, final_left\n        if final_top &gt; final_bottom:\n            logger.warning(f\"Calculated top ({final_top}) &gt; bottom ({final_bottom}). Swapping.\")\n            final_top, final_bottom = final_bottom, final_top\n\n        from natural_pdf.elements.region import Region\n\n        region = Region(self, (final_left, final_top, final_right, final_bottom))\n        return region\n\n    def get_elements(\n        self, apply_exclusions=True, debug_exclusions: bool = False\n    ) -&gt; List[\"Element\"]:\n        \"\"\"\n        Get all elements on this page.\n\n        Args:\n            apply_exclusions: Whether to apply exclusion regions (default: True).\n            debug_exclusions: Whether to output detailed exclusion debugging info (default: False).\n\n        Returns:\n            List of all elements on the page, potentially filtered by exclusions.\n        \"\"\"\n        # Get all elements from the element manager\n        all_elements = self._element_mgr.get_all_elements()\n\n        # Apply exclusions if requested\n        if apply_exclusions and self._exclusions:\n            return self._filter_elements_by_exclusions(\n                all_elements, debug_exclusions=debug_exclusions\n            )\n        else:\n            if debug_exclusions:\n                print(\n                    f\"Page {self.index}: get_elements returning all {len(all_elements)} elements (exclusions not applied).\"\n                )\n            return all_elements\n\n    def filter_elements(\n        self, elements: List[\"Element\"], selector: str, **kwargs\n    ) -&gt; List[\"Element\"]:\n        \"\"\"\n        Filter a list of elements based on a selector.\n\n        Args:\n            elements: List of elements to filter\n            selector: CSS-like selector string\n            **kwargs: Additional filter parameters\n\n        Returns:\n            List of elements that match the selector\n        \"\"\"\n        from natural_pdf.selectors.parser import parse_selector, selector_to_filter_func\n\n        # Parse the selector\n        selector_obj = parse_selector(selector)\n\n        # Create filter function from selector\n        filter_func = selector_to_filter_func(selector_obj, **kwargs)\n\n        # Apply the filter to the elements\n        matching_elements = [element for element in elements if filter_func(element)]\n\n        # Sort elements in reading order if requested\n        if kwargs.get(\"reading_order\", True):\n            if all(hasattr(el, \"top\") and hasattr(el, \"x0\") for el in matching_elements):\n                matching_elements.sort(key=lambda el: (el.top, el.x0))\n            else:\n                logger.warning(\n                    \"Cannot sort elements in reading order: Missing required attributes (top, x0).\"\n                )\n\n        return matching_elements\n\n    def until(self, selector: str, include_endpoint: bool = True, **kwargs) -&gt; Any:\n        \"\"\"\n        Select content from the top of the page until matching selector.\n\n        Args:\n            selector: CSS-like selector string\n            include_endpoint: Whether to include the endpoint element in the region\n            **kwargs: Additional selection parameters\n\n        Returns:\n            Region object representing the selected content\n\n        Examples:\n            &gt;&gt;&gt; page.until('text:contains(\"Conclusion\")')  # Select from top to conclusion\n            &gt;&gt;&gt; page.until('line[width&gt;=2]', include_endpoint=False)  # Select up to thick line\n        \"\"\"\n        # Find the target element\n        target = self.find(selector, **kwargs)\n        if not target:\n            # If target not found, return a default region (full page)\n            from natural_pdf.elements.region import Region\n\n            return Region(self, (0, 0, self.width, self.height))\n\n        # Create a region from the top of the page to the target\n        from natural_pdf.elements.region import Region\n\n        # Ensure target has positional attributes before using them\n        target_top = getattr(target, \"top\", 0)\n        target_bottom = getattr(target, \"bottom\", self.height)\n\n        if include_endpoint:\n            # Include the target element\n            region = Region(self, (0, 0, self.width, target_bottom))\n        else:\n            # Up to the target element\n            region = Region(self, (0, 0, self.width, target_top))\n\n        region.end_element = target\n        return region\n\n    def crop(self, bbox=None, **kwargs) -&gt; Any:\n        \"\"\"\n        Crop the page to the specified bounding box.\n\n        This is a direct wrapper around pdfplumber's crop method.\n\n        Args:\n            bbox: Bounding box (x0, top, x1, bottom) or None\n            **kwargs: Additional parameters (top, bottom, left, right)\n\n        Returns:\n            Cropped page object (pdfplumber.Page)\n        \"\"\"\n        # Returns the pdfplumber page object, not a natural-pdf Page\n        return self._page.crop(bbox, **kwargs)\n\n    def extract_text(\n        self,\n        preserve_whitespace=True,\n        use_exclusions=True,\n        debug_exclusions=False,\n        content_filter=None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"\n        Extract text from this page, respecting exclusions and using pdfplumber's\n        layout engine (chars_to_textmap) if layout arguments are provided or default.\n\n        Args:\n            use_exclusions: Whether to apply exclusion regions (default: True).\n                          Note: Filtering logic is now always applied if exclusions exist.\n            debug_exclusions: Whether to output detailed exclusion debugging info (default: False).\n            content_filter: Optional content filter to exclude specific text patterns. Can be:\n                - A regex pattern string (characters matching the pattern are EXCLUDED)\n                - A callable that takes text and returns True to KEEP the character\n                - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n            **kwargs: Additional layout parameters passed directly to pdfplumber's\n                      `chars_to_textmap` function. Common parameters include:\n                      - layout (bool): If True (default), inserts spaces/newlines.\n                      - x_density (float): Pixels per character horizontally.\n                      - y_density (float): Pixels per line vertically.\n                      - x_tolerance (float): Tolerance for horizontal character grouping.\n                      - y_tolerance (float): Tolerance for vertical character grouping.\n                      - line_dir (str): 'ttb', 'btt', 'ltr', 'rtl'\n                      - char_dir (str): 'ttb', 'btt', 'ltr', 'rtl'\n                      See pdfplumber documentation for more.\n\n        Returns:\n            Extracted text as string, potentially with layout-based spacing.\n        \"\"\"\n        logger.debug(f\"Page {self.number}: extract_text called with kwargs: {kwargs}\")\n        debug = kwargs.get(\"debug\", debug_exclusions)  # Allow 'debug' kwarg\n\n        # 1. Get Word Elements (triggers load_elements if needed)\n        word_elements = self.words\n        if not word_elements:\n            logger.debug(f\"Page {self.number}: No word elements found.\")\n            return \"\"\n\n        # 2. Apply element-based exclusions if enabled\n        if use_exclusions and self._exclusions:\n            # Filter word elements through _filter_elements_by_exclusions\n            # This handles both element-based and region-based exclusions\n            word_elements = self._filter_elements_by_exclusions(\n                word_elements, debug_exclusions=debug\n            )\n            if debug:\n                logger.debug(\n                    f\"Page {self.number}: {len(word_elements)} words remaining after exclusion filtering.\"\n                )\n\n        # 3. Get region-based exclusions for spatial filtering\n        apply_exclusions_flag = kwargs.get(\"use_exclusions\", use_exclusions)\n        exclusion_regions = []\n        if apply_exclusions_flag and self._exclusions:\n            exclusion_regions = self._get_exclusion_regions(include_callable=True, debug=debug)\n            if debug:\n                logger.debug(\n                    f\"Page {self.number}: Found {len(exclusion_regions)} region exclusions for spatial filtering.\"\n                )\n        elif debug:\n            logger.debug(f\"Page {self.number}: Not applying exclusions.\")\n\n        # 4. Collect All Character Dictionaries from remaining Word Elements\n        all_char_dicts = []\n        for word in word_elements:\n            all_char_dicts.extend(getattr(word, \"_char_dicts\", []))\n\n        # 5. Spatially Filter Characters (only by regions, elements already filtered above)\n        filtered_chars = filter_chars_spatially(\n            char_dicts=all_char_dicts,\n            exclusion_regions=exclusion_regions,\n            target_region=None,  # No target region for full page extraction\n            debug=debug,\n        )\n\n        # 5. Generate Text Layout using Utility\n        # Pass page bbox as layout context\n        page_bbox = (0, 0, self.width, self.height)\n        # Merge PDF-level default tolerances if caller did not override\n        merged_kwargs = dict(kwargs)\n        tol_keys = [\"x_tolerance\", \"x_tolerance_ratio\", \"y_tolerance\"]\n        for k in tol_keys:\n            if k not in merged_kwargs:\n                if k in self._config:\n                    merged_kwargs[k] = self._config[k]\n                elif k in getattr(self._parent, \"_config\", {}):\n                    merged_kwargs[k] = self._parent._config[k]\n\n        # Add content_filter to kwargs if provided\n        if content_filter is not None:\n            merged_kwargs[\"content_filter\"] = content_filter\n\n        result = generate_text_layout(\n            char_dicts=filtered_chars,\n            layout_context_bbox=page_bbox,\n            user_kwargs=merged_kwargs,\n        )\n\n        # --- Optional: apply Unicode BiDi algorithm for mixed RTL/LTR correctness ---\n        apply_bidi = kwargs.get(\"bidi\", True)\n        if apply_bidi and result:\n            # Quick check for any RTL character\n            import unicodedata\n\n            def _contains_rtl(s):\n                return any(unicodedata.bidirectional(ch) in (\"R\", \"AL\", \"AN\") for ch in s)\n\n            if _contains_rtl(result):\n                try:\n                    from bidi.algorithm import get_display  # type: ignore\n\n                    from natural_pdf.utils.bidi_mirror import mirror_brackets\n\n                    result = \"\\n\".join(\n                        mirror_brackets(\n                            get_display(\n                                line,\n                                base_dir=(\n                                    \"R\"\n                                    if any(\n                                        unicodedata.bidirectional(ch) in (\"R\", \"AL\", \"AN\")\n                                        for ch in line\n                                    )\n                                    else \"L\"\n                                ),\n                            )\n                        )\n                        for line in result.split(\"\\n\")\n                    )\n                except ModuleNotFoundError:\n                    pass  # silently skip if python-bidi not available\n\n        logger.debug(f\"Page {self.number}: extract_text finished, result length: {len(result)}.\")\n        return result\n\n    def extract_table(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        use_ocr: bool = False,\n        ocr_config: Optional[dict] = None,\n        text_options: Optional[Dict] = None,\n        cell_extraction_func: Optional[Callable[[\"Region\"], Optional[str]]] = None,\n        show_progress: bool = False,\n        content_filter=None,\n    ) -&gt; List[List[Optional[str]]]:\n        \"\"\"\n        Extract the largest table from this page using enhanced region-based extraction.\n\n        Args:\n            method: Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).\n            table_settings: Settings for pdfplumber table extraction.\n            use_ocr: Whether to use OCR for text extraction (currently only applicable with 'tatr' method).\n            ocr_config: OCR configuration parameters.\n            text_options: Dictionary of options for the 'text' method.\n            cell_extraction_func: Optional callable function that takes a cell Region object\n                                  and returns its string content. For 'text' method only.\n            show_progress: If True, display a progress bar during cell text extraction for the 'text' method.\n            content_filter: Optional content filter to apply during cell text extraction. Can be:\n                - A regex pattern string (characters matching the pattern are EXCLUDED)\n                - A callable that takes text and returns True to KEEP the character\n                - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n\n        Returns:\n            Table data as a list of rows, where each row is a list of cell values (str or None).\n        \"\"\"\n        # Create a full-page region and delegate to its enhanced extract_table method\n        page_region = self.create_region(0, 0, self.width, self.height)\n        return page_region.extract_table(\n            method=method,\n            table_settings=table_settings,\n            use_ocr=use_ocr,\n            ocr_config=ocr_config,\n            text_options=text_options,\n            cell_extraction_func=cell_extraction_func,\n            show_progress=show_progress,\n            content_filter=content_filter,\n        )\n\n    def extract_tables(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n        check_tatr: bool = True,\n    ) -&gt; List[List[List[str]]]:\n        \"\"\"\n        Extract all tables from this page with enhanced method support.\n\n        Args:\n            method: Method to use: 'pdfplumber', 'stream', 'lattice', or None (auto-detect).\n                    'stream' uses text-based strategies, 'lattice' uses line-based strategies.\n                    Note: 'tatr' and 'text' methods are not supported for extract_tables.\n            table_settings: Settings for pdfplumber table extraction.\n            check_tatr: If True (default), first check for TATR-detected table regions\n                        and extract from those before falling back to pdfplumber methods.\n\n        Returns:\n            List of tables, where each table is a list of rows, and each row is a list of cell values.\n        \"\"\"\n        if table_settings is None:\n            table_settings = {}\n\n        # Check for TATR-detected table regions first if enabled\n        if check_tatr:\n            try:\n                tatr_tables = self.find_all(\"region[type=table][model=tatr]\")\n                if tatr_tables:\n                    logger.debug(\n                        f\"Page {self.number}: Found {len(tatr_tables)} TATR table regions, extracting from those...\"\n                    )\n                    extracted_tables = []\n                    for table_region in tatr_tables:\n                        try:\n                            table_data = table_region.extract_table(method=\"tatr\")\n                            if table_data:  # Only add non-empty tables\n                                extracted_tables.append(table_data)\n                        except Exception as e:\n                            logger.warning(\n                                f\"Failed to extract table from TATR region {table_region.bbox}: {e}\"\n                            )\n\n                    if extracted_tables:\n                        logger.debug(\n                            f\"Page {self.number}: Successfully extracted {len(extracted_tables)} tables from TATR regions\"\n                        )\n                        return extracted_tables\n                    else:\n                        logger.debug(\n                            f\"Page {self.number}: TATR regions found but no tables extracted, falling back to pdfplumber\"\n                        )\n                else:\n                    logger.debug(\n                        f\"Page {self.number}: No TATR table regions found, using pdfplumber methods\"\n                    )\n            except Exception as e:\n                logger.debug(\n                    f\"Page {self.number}: Error checking TATR regions: {e}, falling back to pdfplumber\"\n                )\n\n        # Auto-detect method if not specified (try lattice first, then stream)\n        if method is None:\n            logger.debug(f\"Page {self.number}: Auto-detecting tables extraction method...\")\n\n            # Try lattice first\n            try:\n                lattice_settings = table_settings.copy()\n                lattice_settings.setdefault(\"vertical_strategy\", \"lines\")\n                lattice_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n                logger.debug(f\"Page {self.number}: Trying 'lattice' method first for tables...\")\n                lattice_result = self._page.extract_tables(lattice_settings)\n\n                # Check if lattice found meaningful tables\n                if (\n                    lattice_result\n                    and len(lattice_result) &gt; 0\n                    and any(\n                        any(\n                            any(cell and cell.strip() for cell in row if cell)\n                            for row in table\n                            if table\n                        )\n                        for table in lattice_result\n                    )\n                ):\n                    logger.debug(\n                        f\"Page {self.number}: 'lattice' method found {len(lattice_result)} tables\"\n                    )\n                    return lattice_result\n                else:\n                    logger.debug(f\"Page {self.number}: 'lattice' method found no meaningful tables\")\n\n            except Exception as e:\n                logger.debug(f\"Page {self.number}: 'lattice' method failed: {e}\")\n\n            # Fall back to stream\n            logger.debug(f\"Page {self.number}: Falling back to 'stream' method for tables...\")\n            stream_settings = table_settings.copy()\n            stream_settings.setdefault(\"vertical_strategy\", \"text\")\n            stream_settings.setdefault(\"horizontal_strategy\", \"text\")\n\n            return self._page.extract_tables(stream_settings)\n\n        effective_method = method\n\n        # Handle method aliases\n        if effective_method == \"stream\":\n            logger.debug(\"Using 'stream' method alias for 'pdfplumber' with text-based strategies.\")\n            effective_method = \"pdfplumber\"\n            table_settings.setdefault(\"vertical_strategy\", \"text\")\n            table_settings.setdefault(\"horizontal_strategy\", \"text\")\n        elif effective_method == \"lattice\":\n            logger.debug(\n                \"Using 'lattice' method alias for 'pdfplumber' with line-based strategies.\"\n            )\n            effective_method = \"pdfplumber\"\n            table_settings.setdefault(\"vertical_strategy\", \"lines\")\n            table_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n        # Use the selected method\n        if effective_method == \"pdfplumber\":\n            # ---------------------------------------------------------\n            # Inject auto-computed or user-specified text tolerances so\n            # pdfplumber uses the same numbers we used for word grouping\n            # whenever the table algorithm relies on word positions.\n            # ---------------------------------------------------------\n            if \"text\" in (\n                table_settings.get(\"vertical_strategy\"),\n                table_settings.get(\"horizontal_strategy\"),\n            ):\n                print(\"SETTING IT UP\")\n                pdf_cfg = getattr(self, \"_config\", getattr(self._parent, \"_config\", {}))\n                if \"text_x_tolerance\" not in table_settings and \"x_tolerance\" not in table_settings:\n                    x_tol = pdf_cfg.get(\"x_tolerance\")\n                    if x_tol is not None:\n                        table_settings.setdefault(\"text_x_tolerance\", x_tol)\n                if \"text_y_tolerance\" not in table_settings and \"y_tolerance\" not in table_settings:\n                    y_tol = pdf_cfg.get(\"y_tolerance\")\n                    if y_tol is not None:\n                        table_settings.setdefault(\"text_y_tolerance\", y_tol)\n\n                # pdfplumber's text strategy benefits from a tight snap tolerance.\n                if (\n                    \"snap_tolerance\" not in table_settings\n                    and \"snap_x_tolerance\" not in table_settings\n                ):\n                    # Derive from y_tol if available, else default 1\n                    snap = max(1, round((pdf_cfg.get(\"y_tolerance\", 1)) * 0.9))\n                    table_settings.setdefault(\"snap_tolerance\", snap)\n                if (\n                    \"join_tolerance\" not in table_settings\n                    and \"join_x_tolerance\" not in table_settings\n                ):\n                    join = table_settings.get(\"snap_tolerance\", 1)\n                    table_settings.setdefault(\"join_tolerance\", join)\n                    table_settings.setdefault(\"join_x_tolerance\", join)\n                    table_settings.setdefault(\"join_y_tolerance\", join)\n\n            raw_tables = self._page.extract_tables(table_settings)\n\n            # Apply RTL text processing to all extracted tables\n            if raw_tables:\n                processed_tables = []\n                for table in raw_tables:\n                    processed_table = []\n                    for row in table:\n                        processed_row = []\n                        for cell in row:\n                            if cell is not None:\n                                # Apply RTL text processing to each cell\n                                rtl_processed_cell = self._apply_rtl_processing_to_text(cell)\n                                processed_row.append(rtl_processed_cell)\n                            else:\n                                processed_row.append(cell)\n                        processed_table.append(processed_row)\n                    processed_tables.append(processed_table)\n                return processed_tables\n\n            return raw_tables\n        else:\n            raise ValueError(\n                f\"Unknown tables extraction method: '{method}'. Choose from 'pdfplumber', 'stream', 'lattice'.\"\n            )\n\n    def _load_elements(self):\n        \"\"\"Load all elements from the page via ElementManager.\"\"\"\n        self._element_mgr.load_elements()\n\n    def _create_char_elements(self):\n        \"\"\"DEPRECATED: Use self._element_mgr.chars\"\"\"\n        logger.warning(\"_create_char_elements is deprecated. Access via self._element_mgr.chars.\")\n        return self._element_mgr.chars  # Delegate\n\n    def _process_font_information(self, char_dict):\n        \"\"\"DEPRECATED: Handled by ElementManager\"\"\"\n        logger.warning(\"_process_font_information is deprecated. Handled by ElementManager.\")\n        # ElementManager handles this internally\n        pass\n\n    def _group_chars_into_words(self, keep_spaces=True, font_attrs=None):\n        \"\"\"DEPRECATED: Use self._element_mgr.words\"\"\"\n        logger.warning(\"_group_chars_into_words is deprecated. Access via self._element_mgr.words.\")\n        return self._element_mgr.words  # Delegate\n\n    def _process_line_into_words(self, line_chars, keep_spaces, font_attrs):\n        \"\"\"DEPRECATED: Handled by ElementManager\"\"\"\n        logger.warning(\"_process_line_into_words is deprecated. Handled by ElementManager.\")\n        pass\n\n    def _check_font_attributes_match(self, char, prev_char, font_attrs):\n        \"\"\"DEPRECATED: Handled by ElementManager\"\"\"\n        logger.warning(\"_check_font_attributes_match is deprecated. Handled by ElementManager.\")\n        pass\n\n    def _create_word_element(self, chars, font_attrs):\n        \"\"\"DEPRECATED: Handled by ElementManager\"\"\"\n        logger.warning(\"_create_word_element is deprecated. Handled by ElementManager.\")\n        pass\n\n    @property\n    def chars(self) -&gt; List[Any]:\n        \"\"\"Get all character elements on this page.\"\"\"\n        return self._element_mgr.chars\n\n    @property\n    def words(self) -&gt; List[Any]:\n        \"\"\"Get all word elements on this page.\"\"\"\n        return self._element_mgr.words\n\n    @property\n    def rects(self) -&gt; List[Any]:\n        \"\"\"Get all rectangle elements on this page.\"\"\"\n        return self._element_mgr.rects\n\n    @property\n    def lines(self) -&gt; List[Any]:\n        \"\"\"Get all line elements on this page.\"\"\"\n        return self._element_mgr.lines\n\n    def highlight(\n        self,\n        bbox: Optional[Tuple[float, float, float, float]] = None,\n        color: Optional[Union[Tuple, str]] = None,\n        label: Optional[str] = None,\n        use_color_cycling: bool = False,\n        element: Optional[Any] = None,\n        annotate: Optional[List[str]] = None,\n        existing: str = \"append\",\n    ) -&gt; \"Page\":\n        \"\"\"\n        Highlight a bounding box or the entire page.\n        Delegates to the central HighlightingService.\n\n        Args:\n            bbox: Bounding box (x0, top, x1, bottom). If None, highlight entire page.\n            color: RGBA color tuple/string for the highlight.\n            label: Optional label for the highlight.\n            use_color_cycling: If True and no label/color, use next cycle color.\n            element: Optional original element being highlighted (for attribute extraction).\n            annotate: List of attribute names from 'element' to display.\n            existing: How to handle existing highlights ('append' or 'replace').\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        target_bbox = bbox if bbox is not None else (0, 0, self.width, self.height)\n        self._highlighter.add(\n            page_index=self.index,\n            bbox=target_bbox,\n            color=color,\n            label=label,\n            use_color_cycling=use_color_cycling,\n            element=element,\n            annotate=annotate,\n            existing=existing,\n        )\n        return self\n\n    def highlight_polygon(\n        self,\n        polygon: List[Tuple[float, float]],\n        color: Optional[Union[Tuple, str]] = None,\n        label: Optional[str] = None,\n        use_color_cycling: bool = False,\n        element: Optional[Any] = None,\n        annotate: Optional[List[str]] = None,\n        existing: str = \"append\",\n    ) -&gt; \"Page\":\n        \"\"\"\n        Highlight a polygon shape on the page.\n        Delegates to the central HighlightingService.\n\n        Args:\n            polygon: List of (x, y) points defining the polygon.\n            color: RGBA color tuple/string for the highlight.\n            label: Optional label for the highlight.\n            use_color_cycling: If True and no label/color, use next cycle color.\n            element: Optional original element being highlighted (for attribute extraction).\n            annotate: List of attribute names from 'element' to display.\n            existing: How to handle existing highlights ('append' or 'replace').\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        self._highlighter.add_polygon(\n            page_index=self.index,\n            polygon=polygon,\n            color=color,\n            label=label,\n            use_color_cycling=use_color_cycling,\n            element=element,\n            annotate=annotate,\n            existing=existing,\n        )\n        return self\n\n    def save_image(\n        self,\n        filename: str,\n        width: Optional[int] = None,\n        labels: bool = True,\n        legend_position: str = \"right\",\n        render_ocr: bool = False,\n        include_highlights: bool = True,  # Allow saving without highlights\n        resolution: float = 144,\n        **kwargs,\n    ) -&gt; \"Page\":\n        \"\"\"\n        Save the page image to a file, rendering highlights via HighlightingService.\n\n        Args:\n            filename: Path to save the image to.\n            width: Optional width for the output image.\n            labels: Whether to include a legend.\n            legend_position: Position of the legend.\n            render_ocr: Whether to render OCR text.\n            include_highlights: Whether to render highlights.\n            resolution: Resolution in DPI for base image rendering (default: 144 DPI, equivalent to previous scale=2.0).\n            **kwargs: Additional args for pdfplumber's internal to_image.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        # Use export() to save the image\n        if include_highlights:\n            self.export(\n                path=filename,\n                resolution=resolution,\n                width=width,\n                labels=labels,\n                legend_position=legend_position,\n                render_ocr=render_ocr,\n                **kwargs,\n            )\n        else:\n            # For saving without highlights, use render() and save manually\n            img = self.render(resolution=resolution, **kwargs)\n            if img:\n                # Resize if width is specified\n                if width is not None and width &gt; 0 and img.width &gt; 0:\n                    aspect_ratio = img.height / img.width\n                    height = int(width * aspect_ratio)\n                    try:\n                        img = img.resize((width, height), Image.Resampling.LANCZOS)\n                    except Exception as e:\n                        logger.warning(f\"Could not resize image: {e}\")\n\n                # Save the image\n                try:\n                    if os.path.dirname(filename):\n                        os.makedirs(os.path.dirname(filename), exist_ok=True)\n                    img.save(filename)\n                except Exception as e:\n                    logger.error(f\"Failed to save image to {filename}: {e}\")\n\n        return self\n\n    def clear_highlights(self) -&gt; \"Page\":\n        \"\"\"\n        Clear all highlights *from this specific page* via HighlightingService.\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        self._highlighter.clear_page(self.index)\n        return self\n\n    def analyze_text_styles(\n        self, options: Optional[TextStyleOptions] = None\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Analyze text elements by style, adding attributes directly to elements.\n\n        This method uses TextStyleAnalyzer to process text elements (typically words)\n        on the page. It adds the following attributes to each processed element:\n        - style_label: A descriptive or numeric label for the style group.\n        - style_key: A hashable tuple representing the style properties used for grouping.\n        - style_properties: A dictionary containing the extracted style properties.\n\n        Args:\n            options: Optional TextStyleOptions to configure the analysis.\n                     If None, the analyzer's default options are used.\n\n        Returns:\n            ElementCollection containing all processed text elements with added style attributes.\n        \"\"\"\n        # Create analyzer (optionally pass default options from PDF config here)\n        # For now, it uses its own defaults if options=None\n        analyzer = TextStyleAnalyzer()\n\n        # Analyze the page. The analyzer now modifies elements directly\n        # and returns the collection of processed elements.\n        processed_elements_collection = analyzer.analyze(self, options=options)\n\n        # Return the collection of elements which now have style attributes\n        return processed_elements_collection\n\n    def _create_text_elements_from_ocr(\n        self, ocr_results: List[Dict[str, Any]], image_width=None, image_height=None\n    ) -&gt; List[\"TextElement\"]:\n        \"\"\"DEPRECATED: Use self._element_mgr.create_text_elements_from_ocr\"\"\"\n        logger.warning(\n            \"_create_text_elements_from_ocr is deprecated. Use self._element_mgr version.\"\n        )\n        return self._element_mgr.create_text_elements_from_ocr(\n            ocr_results, image_width, image_height\n        )\n\n    def apply_ocr(\n        self,\n        engine: Optional[str] = None,\n        options: Optional[\"OCROptions\"] = None,\n        languages: Optional[List[str]] = None,\n        min_confidence: Optional[float] = None,\n        device: Optional[str] = None,\n        resolution: Optional[int] = None,\n        detect_only: bool = False,\n        apply_exclusions: bool = True,\n        replace: bool = True,\n    ) -&gt; \"Page\":\n        \"\"\"\n        Apply OCR to THIS page and add results to page elements via PDF.apply_ocr.\n\n        Args:\n            engine: Name of the OCR engine.\n            options: Engine-specific options object or dict.\n            languages: List of engine-specific language codes.\n            min_confidence: Minimum confidence threshold.\n            device: Device to run OCR on.\n            resolution: DPI resolution for rendering page image before OCR.\n            apply_exclusions: If True (default), render page image for OCR\n                              with excluded areas masked (whited out).\n            detect_only: If True, only detect text bounding boxes, don't perform OCR.\n            replace: If True (default), remove any existing OCR elements before\n                    adding new ones. If False, add new OCR elements to existing ones.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        if not hasattr(self._parent, \"apply_ocr\"):\n            logger.error(f\"Page {self.number}: Parent PDF missing 'apply_ocr'. Cannot apply OCR.\")\n            return self  # Return self for chaining\n\n        # Remove existing OCR elements if replace is True\n        if replace and hasattr(self, \"_element_mgr\"):\n            logger.info(\n                f\"Page {self.number}: Removing existing OCR elements before applying new OCR.\"\n            )\n            self._element_mgr.remove_ocr_elements()\n\n        logger.info(f\"Page {self.number}: Delegating apply_ocr to PDF.apply_ocr.\")\n        # Delegate to parent PDF, targeting only this page's index\n        # Pass all relevant parameters through, including apply_exclusions\n        self._parent.apply_ocr(\n            pages=[self.index],\n            engine=engine,\n            options=options,\n            languages=languages,\n            min_confidence=min_confidence,\n            device=device,\n            resolution=resolution,\n            detect_only=detect_only,\n            apply_exclusions=apply_exclusions,\n            replace=replace,  # Pass the replace parameter to PDF.apply_ocr\n        )\n\n        # Return self for chaining\n        return self\n\n    def extract_ocr_elements(\n        self,\n        engine: Optional[str] = None,\n        options: Optional[\"OCROptions\"] = None,\n        languages: Optional[List[str]] = None,\n        min_confidence: Optional[float] = None,\n        device: Optional[str] = None,\n        resolution: Optional[int] = None,\n    ) -&gt; List[\"TextElement\"]:\n        \"\"\"\n        Extract text elements using OCR *without* adding them to the page's elements.\n        Uses the shared OCRManager instance.\n\n        Args:\n            engine: Name of the OCR engine.\n            options: Engine-specific options object or dict.\n            languages: List of engine-specific language codes.\n            min_confidence: Minimum confidence threshold.\n            device: Device to run OCR on.\n            resolution: DPI resolution for rendering page image before OCR.\n\n        Returns:\n            List of created TextElement objects derived from OCR results for this page.\n        \"\"\"\n        if not self._ocr_manager:\n            logger.error(\n                f\"Page {self.number}: OCRManager not available. Cannot extract OCR elements.\"\n            )\n            return []\n\n        logger.info(f\"Page {self.number}: Extracting OCR elements (extract only)...\")\n\n        # Determine rendering resolution\n        final_resolution = resolution if resolution is not None else 150  # Default to 150 DPI\n        logger.debug(f\"  Using rendering resolution: {final_resolution} DPI\")\n\n        try:\n            # Get base image without highlights using the determined resolution\n            # Use the global PDF rendering lock\n            with pdf_render_lock:\n                # Use render() for clean image without highlights\n                image = self.render(resolution=final_resolution)\n                if not image:\n                    logger.error(\n                        f\"  Failed to render page {self.number} to image for OCR extraction.\"\n                    )\n                    return []\n                logger.debug(f\"  Rendered image size: {image.width}x{image.height}\")\n        except Exception as e:\n            logger.error(f\"  Failed to render page {self.number} to image: {e}\", exc_info=True)\n            return []\n\n        # Prepare arguments for the OCR Manager call\n        manager_args = {\n            \"images\": image,\n            \"engine\": engine,\n            \"languages\": languages,\n            \"min_confidence\": min_confidence,\n            \"device\": device,\n            \"options\": options,\n        }\n        manager_args = {k: v for k, v in manager_args.items() if v is not None}\n\n        logger.debug(\n            f\"  Calling OCR Manager (extract only) with args: { {k:v for k,v in manager_args.items() if k != 'images'} }\"\n        )\n        try:\n            # apply_ocr now returns List[List[Dict]] or List[Dict]\n            results_list = self._ocr_manager.apply_ocr(**manager_args)\n            # If it returned a list of lists (batch mode), take the first list\n            results = (\n                results_list[0]\n                if isinstance(results_list, list)\n                and results_list\n                and isinstance(results_list[0], list)\n                else results_list\n            )\n            if not isinstance(results, list):\n                logger.error(f\"  OCR Manager returned unexpected type: {type(results)}\")\n                results = []\n            logger.info(f\"  OCR Manager returned {len(results)} results for extraction.\")\n        except Exception as e:\n            logger.error(f\"  OCR processing failed during extraction: {e}\", exc_info=True)\n            return []\n\n        # Convert results but DO NOT add to ElementManager\n        logger.debug(f\"  Converting OCR results to TextElements (extract only)...\")\n        temp_elements = []\n        scale_x = self.width / image.width if image.width else 1\n        scale_y = self.height / image.height if image.height else 1\n        for result in results:\n            try:  # Added try-except around result processing\n                x0, top, x1, bottom = [float(c) for c in result[\"bbox\"]]\n                elem_data = {\n                    \"text\": result[\"text\"],\n                    \"confidence\": result[\"confidence\"],\n                    \"x0\": x0 * scale_x,\n                    \"top\": top * scale_y,\n                    \"x1\": x1 * scale_x,\n                    \"bottom\": bottom * scale_y,\n                    \"width\": (x1 - x0) * scale_x,\n                    \"height\": (bottom - top) * scale_y,\n                    \"object_type\": \"text\",  # Using text for temporary elements\n                    \"source\": \"ocr\",\n                    \"fontname\": \"OCR-extract\",  # Different name for clarity\n                    \"size\": 10.0,\n                    \"page_number\": self.number,\n                }\n                temp_elements.append(TextElement(elem_data, self))\n            except (KeyError, ValueError, TypeError) as convert_err:\n                logger.warning(\n                    f\"  Skipping invalid OCR result during conversion: {result}. Error: {convert_err}\"\n                )\n\n        logger.info(f\"  Created {len(temp_elements)} TextElements from OCR (extract only).\")\n        return temp_elements\n\n    @property\n    def size(self) -&gt; Tuple[float, float]:\n        \"\"\"Get the size of the page in points.\"\"\"\n        return (self._page.width, self._page.height)\n\n    @property\n    def layout_analyzer(self) -&gt; \"LayoutAnalyzer\":\n        \"\"\"Get or create the layout analyzer for this page.\"\"\"\n        if self._layout_analyzer is None:\n            if not self._layout_manager:\n                logger.warning(\"LayoutManager not available, cannot create LayoutAnalyzer.\")\n                return None\n            self._layout_analyzer = LayoutAnalyzer(self)\n        return self._layout_analyzer\n\n    def analyze_layout(\n        self,\n        engine: Optional[str] = None,\n        options: Optional[\"LayoutOptions\"] = None,\n        confidence: Optional[float] = None,\n        classes: Optional[List[str]] = None,\n        exclude_classes: Optional[List[str]] = None,\n        device: Optional[str] = None,\n        existing: str = \"replace\",\n        model_name: Optional[str] = None,\n        client: Optional[Any] = None,  # Add client parameter\n    ) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Analyze the page layout using the configured LayoutManager.\n        Adds detected Region objects to the page's element manager.\n\n        Returns:\n            ElementCollection containing the detected Region objects.\n        \"\"\"\n        analyzer = self.layout_analyzer\n        if not analyzer:\n            logger.error(\n                \"Layout analysis failed: LayoutAnalyzer not initialized (is LayoutManager available?).\"\n            )\n            return ElementCollection([])  # Return empty collection\n\n        # Clear existing detected regions if 'replace' is specified\n        if existing == \"replace\":\n            self.clear_detected_layout_regions()\n\n        # The analyzer's analyze_layout method already adds regions to the page\n        # and its element manager. We just need to retrieve them.\n        analyzer.analyze_layout(\n            engine=engine,\n            options=options,\n            confidence=confidence,\n            classes=classes,\n            exclude_classes=exclude_classes,\n            device=device,\n            existing=existing,\n            model_name=model_name,\n            client=client,  # Pass client down\n        )\n\n        # Retrieve the detected regions from the element manager\n        # Filter regions based on source='detected' and potentially the model used if available\n        detected_regions = [\n            r\n            for r in self._element_mgr.regions\n            if r.source == \"detected\" and (not engine or getattr(r, \"model\", None) == engine)\n        ]\n\n        return ElementCollection(detected_regions)\n\n    def clear_detected_layout_regions(self) -&gt; \"Page\":\n        \"\"\"\n        Removes all regions from this page that were added by layout analysis\n        (i.e., regions where `source` attribute is 'detected').\n\n        This clears the regions both from the page's internal `_regions['detected']` list\n        and from the ElementManager's internal list of regions.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        if (\n            not hasattr(self._element_mgr, \"regions\")\n            or not hasattr(self._element_mgr, \"_elements\")\n            or \"regions\" not in self._element_mgr._elements\n        ):\n            logger.debug(\n                f\"Page {self.index}: No regions found in ElementManager, nothing to clear.\"\n            )\n            self._regions[\"detected\"] = []  # Ensure page's list is also clear\n            return self\n\n        # Filter ElementManager's list to keep only non-detected regions\n        original_count = len(self._element_mgr.regions)\n        self._element_mgr._elements[\"regions\"] = [\n            r for r in self._element_mgr.regions if getattr(r, \"source\", None) != \"detected\"\n        ]\n        new_count = len(self._element_mgr.regions)\n        removed_count = original_count - new_count\n\n        # Clear the page's specific list of detected regions\n        self._regions[\"detected\"] = []\n\n        logger.info(f\"Page {self.index}: Cleared {removed_count} detected layout regions.\")\n        return self\n\n    def get_section_between(\n        self, start_element=None, end_element=None, include_boundaries=\"both\"\n    ) -&gt; Optional[\"Region\"]:  # Return Optional\n        \"\"\"\n        Get a section between two elements on this page.\n        \"\"\"\n        # Create a full-page region to operate within\n        page_region = self.create_region(0, 0, self.width, self.height)\n\n        # Delegate to the region's method\n        try:\n            return page_region.get_section_between(\n                start_element=start_element,\n                end_element=end_element,\n                include_boundaries=include_boundaries,\n            )\n        except Exception as e:\n            logger.error(\n                f\"Error getting section between elements on page {self.index}: {e}\", exc_info=True\n            )\n            return None\n\n    def split(self, divider, **kwargs) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Divides the page into sections based on the provided divider elements.\n        \"\"\"\n        sections = self.get_sections(start_elements=divider, **kwargs)\n        top = self.region(0, 0, self.width, sections[0].top)\n        sections.append(top)\n\n        return sections\n\n    def get_sections(\n        self,\n        start_elements=None,\n        end_elements=None,\n        include_boundaries=\"start\",\n        y_threshold=5.0,\n        bounding_box=None,\n    ) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Get sections of a page defined by start/end elements.\n        Uses the page-level implementation.\n\n        Returns:\n            An ElementCollection containing the found Region objects.\n        \"\"\"\n\n        # Helper function to get bounds from bounding_box parameter\n        def get_bounds():\n            if bounding_box:\n                x0, top, x1, bottom = bounding_box\n                # Clamp to page boundaries\n                return max(0, x0), max(0, top), min(self.width, x1), min(self.height, bottom)\n            else:\n                return 0, 0, self.width, self.height\n\n        regions = []\n\n        # Handle cases where elements are provided as strings (selectors)\n        if isinstance(start_elements, str):\n            start_elements = self.find_all(start_elements).elements  # Get list of elements\n        elif hasattr(start_elements, \"elements\"):  # Handle ElementCollection input\n            start_elements = start_elements.elements\n\n        if isinstance(end_elements, str):\n            end_elements = self.find_all(end_elements).elements\n        elif hasattr(end_elements, \"elements\"):\n            end_elements = end_elements.elements\n\n        # Ensure start_elements is a list\n        if start_elements is None:\n            start_elements = []\n        if end_elements is None:\n            end_elements = []\n\n        valid_inclusions = [\"start\", \"end\", \"both\", \"none\"]\n        if include_boundaries not in valid_inclusions:\n            raise ValueError(f\"include_boundaries must be one of {valid_inclusions}\")\n\n        if not start_elements:\n            # Return an empty ElementCollection if no start elements\n            return ElementCollection([])\n\n        # Combine start and end elements with their type\n        all_boundaries = []\n        for el in start_elements:\n            all_boundaries.append((el, \"start\"))\n        for el in end_elements:\n            all_boundaries.append((el, \"end\"))\n\n        # Sort all boundary elements primarily by top, then x0\n        try:\n            all_boundaries.sort(key=lambda x: (x[0].top, x[0].x0))\n        except AttributeError as e:\n            logger.error(f\"Error sorting boundaries: Element missing top/x0 attribute? {e}\")\n            return ElementCollection([])  # Cannot proceed if elements lack position\n\n        # Process sorted boundaries to find sections\n        current_start_element = None\n        active_section_started = False\n\n        for element, element_type in all_boundaries:\n            if element_type == \"start\":\n                # If we have an active section, this start implicitly ends it\n                if active_section_started:\n                    end_boundary_el = element  # Use this start as the end boundary\n                    # Determine region boundaries\n                    sec_top = (\n                        current_start_element.top\n                        if include_boundaries in [\"start\", \"both\"]\n                        else current_start_element.bottom\n                    )\n                    sec_bottom = (\n                        end_boundary_el.top\n                        if include_boundaries not in [\"end\", \"both\"]\n                        else end_boundary_el.bottom\n                    )\n\n                    if sec_top &lt; sec_bottom:  # Ensure valid region\n                        x0, _, x1, _ = get_bounds()\n                        region = self.create_region(x0, sec_top, x1, sec_bottom)\n                        region.start_element = current_start_element\n                        region.end_element = end_boundary_el  # Mark the element that ended it\n                        region.is_end_next_start = True  # Mark how it ended\n                        regions.append(region)\n                    active_section_started = False  # Reset for the new start\n\n                # Set this as the potential start of the next section\n                current_start_element = element\n                active_section_started = True\n\n            elif element_type == \"end\" and active_section_started:\n                # We found an explicit end for the current section\n                end_boundary_el = element\n                sec_top = (\n                    current_start_element.top\n                    if include_boundaries in [\"start\", \"both\"]\n                    else current_start_element.bottom\n                )\n                sec_bottom = (\n                    end_boundary_el.bottom\n                    if include_boundaries in [\"end\", \"both\"]\n                    else end_boundary_el.top\n                )\n\n                if sec_top &lt; sec_bottom:  # Ensure valid region\n                    x0, _, x1, _ = get_bounds()\n                    region = self.create_region(x0, sec_top, x1, sec_bottom)\n                    region.start_element = current_start_element\n                    region.end_element = end_boundary_el\n                    region.is_end_next_start = False\n                    regions.append(region)\n\n                # Reset: section ended explicitly\n                current_start_element = None\n                active_section_started = False\n\n        # Handle the last section if it was started but never explicitly ended\n        if active_section_started:\n            sec_top = (\n                current_start_element.top\n                if include_boundaries in [\"start\", \"both\"]\n                else current_start_element.bottom\n            )\n            x0, _, x1, page_bottom = get_bounds()\n            if sec_top &lt; page_bottom:\n                region = self.create_region(x0, sec_top, x1, page_bottom)\n                region.start_element = current_start_element\n                region.end_element = None  # Ended by page end\n                region.is_end_next_start = False\n                regions.append(region)\n\n        return ElementCollection(regions)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the page.\"\"\"\n        return f\"&lt;Page number={self.number} index={self.index}&gt;\"\n\n    def ask(\n        self,\n        question: Union[str, List[str], Tuple[str, ...]],\n        min_confidence: float = 0.1,\n        model: str = None,\n        debug: bool = False,\n        **kwargs,\n    ) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"\n        Ask a question about the page content using document QA.\n        \"\"\"\n        try:\n            from natural_pdf.qa.document_qa import get_qa_engine\n\n            # Get or initialize QA engine with specified model\n            qa_engine = get_qa_engine(model_name=model) if model else get_qa_engine()\n            # Ask the question using the QA engine\n            return qa_engine.ask_pdf_page(\n                self, question, min_confidence=min_confidence, debug=debug, **kwargs\n            )\n        except ImportError:\n            logger.error(\n                \"Question answering requires the 'natural_pdf.qa' module. Please install necessary dependencies.\"\n            )\n            return {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": self.number,\n                \"source_elements\": [],\n            }\n        except Exception as e:\n            logger.error(f\"Error during page.ask: {e}\", exc_info=True)\n            return {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": self.number,\n                \"source_elements\": [],\n            }\n\n    def show_preview(\n        self,\n        temporary_highlights: List[Dict],\n        resolution: float = 144,\n        width: Optional[int] = None,\n        labels: bool = True,\n        legend_position: str = \"right\",\n        render_ocr: bool = False,\n    ) -&gt; Optional[Image.Image]:\n        \"\"\"\n        Generates and returns a non-stateful preview image containing only\n        the provided temporary highlights.\n\n        Args:\n            temporary_highlights: List of highlight data dictionaries (as prepared by\n                                  ElementCollection._prepare_highlight_data).\n            resolution: Resolution in DPI for rendering (default: 144 DPI, equivalent to previous scale=2.0).\n            width: Optional width for the output image.\n            labels: Whether to include a legend.\n            legend_position: Position of the legend.\n            render_ocr: Whether to render OCR text.\n\n        Returns:\n            PIL Image object of the preview, or None if rendering fails.\n        \"\"\"\n        try:\n            # Delegate rendering to the highlighter service's preview method\n            img = self._highlighter.render_preview(\n                page_index=self.index,\n                temporary_highlights=temporary_highlights,\n                resolution=resolution,\n                labels=labels,\n                legend_position=legend_position,\n                render_ocr=render_ocr,\n            )\n        except AttributeError:\n            logger.error(f\"HighlightingService does not have the required 'render_preview' method.\")\n            return None\n        except Exception as e:\n            logger.error(\n                f\"Error calling highlighter.render_preview for page {self.index}: {e}\",\n                exc_info=True,\n            )\n            return None\n\n        # Return the rendered image directly\n        return img\n\n    @property\n    def text_style_labels(self) -&gt; List[str]:\n        \"\"\"\n        Get a sorted list of unique text style labels found on the page.\n\n        Runs text style analysis with default options if it hasn't been run yet.\n        To use custom options, call `analyze_text_styles(options=...)` explicitly first.\n\n        Returns:\n            A sorted list of unique style label strings.\n        \"\"\"\n        # Check if the summary attribute exists from a previous run\n        if not hasattr(self, \"_text_styles_summary\") or not self._text_styles_summary:\n            # If not, run the analysis with default options\n            logger.debug(f\"Page {self.number}: Running default text style analysis to get labels.\")\n            self.analyze_text_styles()  # Use default options\n\n        # Extract labels from the summary dictionary\n        if hasattr(self, \"_text_styles_summary\") and self._text_styles_summary:\n            # The summary maps style_key -&gt; {'label': ..., 'properties': ...}\n            labels = {style_info[\"label\"] for style_info in self._text_styles_summary.values()}\n            return sorted(list(labels))\n        else:\n            # Fallback if summary wasn't created for some reason (e.g., no text elements)\n            logger.warning(f\"Page {self.number}: Text style summary not found after analysis.\")\n            return []\n\n    def viewer(\n        self,\n        # elements_to_render: Optional[List['Element']] = None, # No longer needed, from_page handles it\n        # include_source_types: List[str] = ['word', 'line', 'rect', 'region'] # No longer needed\n    ) -&gt; Optional[\"InteractiveViewerWidget\"]:  # Return type hint updated\n        \"\"\"\n        Creates and returns an interactive ipywidget for exploring elements on this page.\n\n        Uses InteractiveViewerWidget.from_page() to create the viewer.\n\n        Returns:\n            A InteractiveViewerWidget instance ready for display in Jupyter,\n            or None if ipywidgets is not installed or widget creation fails.\n\n        Raises:\n            # Optional: Could raise ImportError instead of returning None\n            # ImportError: If required dependencies (ipywidgets) are missing.\n            ValueError: If image rendering or data preparation fails within from_page.\n        \"\"\"\n        # Check for availability using the imported flag and class variable\n        if not _IPYWIDGETS_AVAILABLE or InteractiveViewerWidget is None:\n            logger.error(\n                \"Interactive viewer requires 'ipywidgets'. \"\n                'Please install with: pip install \"ipywidgets&gt;=7.0.0,&lt;10.0.0\"'\n            )\n            # raise ImportError(\"ipywidgets not found.\") # Option 1: Raise error\n            return None  # Option 2: Return None gracefully\n\n        # If we reach here, InteractiveViewerWidget should be the actual class\n        try:\n            # Pass self (the Page object) to the factory method\n            return InteractiveViewerWidget.from_page(self)\n        except Exception as e:\n            # Catch potential errors during widget creation (e.g., image rendering)\n            logger.error(\n                f\"Error creating viewer widget from page {self.number}: {e}\", exc_info=True\n            )\n            # raise # Option 1: Re-raise error (might include ValueError from from_page)\n            return None  # Option 2: Return None on creation error\n\n    # --- Indexable Protocol Methods ---\n    def get_id(self) -&gt; str:\n        \"\"\"Returns a unique identifier for the page (required by Indexable protocol).\"\"\"\n        # Ensure path is safe for use in IDs (replace problematic chars)\n        safe_path = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", str(self.pdf.path))\n        return f\"pdf_{safe_path}_page_{self.page_number}\"\n\n    def get_metadata(self) -&gt; Dict[str, Any]:\n        \"\"\"Returns metadata associated with the page (required by Indexable protocol).\"\"\"\n        # Add content hash here for sync\n        metadata = {\n            \"pdf_path\": str(self.pdf.path),\n            \"page_number\": self.page_number,\n            \"width\": self.width,\n            \"height\": self.height,\n            \"content_hash\": self.get_content_hash(),  # Include the hash\n        }\n        return metadata\n\n    def get_content(self) -&gt; \"Page\":\n        \"\"\"\n        Returns the primary content object (self) for indexing (required by Indexable protocol).\n        SearchService implementations decide how to process this (e.g., call extract_text).\n        \"\"\"\n        return self  # Return the Page object itself\n\n    def get_content_hash(self) -&gt; str:\n        \"\"\"Returns a SHA256 hash of the extracted text content (required by Indexable for sync).\"\"\"\n        # Hash the extracted text (without exclusions for consistency)\n        # Consider if exclusions should be part of the hash? For now, hash raw text.\n        # Using extract_text directly might be slow if called repeatedly. Cache? TODO: Optimization\n        text_content = self.extract_text(\n            use_exclusions=False, preserve_whitespace=False\n        )  # Normalize whitespace?\n        return hashlib.sha256(text_content.encode(\"utf-8\")).hexdigest()\n\n    # --- New Method: save_searchable ---\n    def save_searchable(self, output_path: Union[str, \"Path\"], dpi: int = 300, **kwargs):\n        \"\"\"\n        Saves the PDF page with an OCR text layer, making content searchable.\n\n        Requires optional dependencies. Install with: pip install \"natural-pdf[ocr-save]\"\n\n        Note: OCR must have been applied to the pages beforehand\n              (e.g., pdf.apply_ocr()).\n\n        Args:\n            output_path: Path to save the searchable PDF.\n            dpi: Resolution for rendering and OCR overlay (default 300).\n            **kwargs: Additional keyword arguments passed to the exporter.\n        \"\"\"\n        # Import moved here, assuming it's always available now\n        from natural_pdf.exporters.searchable_pdf import create_searchable_pdf\n\n        # Convert pathlib.Path to string if necessary\n        output_path_str = str(output_path)\n\n        create_searchable_pdf(self, output_path_str, dpi=dpi, **kwargs)\n        logger.info(f\"Searchable PDF saved to: {output_path_str}\")\n\n    # --- Added correct_ocr method ---\n    def update_text(\n        self,\n        transform: Callable[[Any], Optional[str]],\n        selector: str = \"text\",\n        max_workers: Optional[int] = None,\n        progress_callback: Optional[Callable[[], None]] = None,  # Added progress callback\n    ) -&gt; \"Page\":  # Return self for chaining\n        \"\"\"\n        Applies corrections to text elements on this page\n        using a user-provided callback function, potentially in parallel.\n\n        Finds text elements on this page matching the *selector* argument and\n        calls the ``transform`` for each, passing the element itself.\n        Updates the element's text if the callback returns a new string.\n\n        Args:\n            transform: A function accepting an element and returning\n                       `Optional[str]` (new text or None).\n            selector: CSS-like selector string to match text elements.\n            max_workers: The maximum number of threads to use for parallel execution.\n                         If None or 0 or 1, runs sequentially.\n            progress_callback: Optional callback function to call after processing each element.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        logger.info(\n            f\"Page {self.number}: Starting text update with callback '{transform.__name__}' (max_workers={max_workers}) and selector='{selector}'\"\n        )\n\n        target_elements_collection = self.find_all(selector=selector, apply_exclusions=False)\n        target_elements = target_elements_collection.elements  # Get the list\n\n        if not target_elements:\n            logger.info(f\"Page {self.number}: No text elements found to update.\")\n            return self\n\n        element_pbar = None\n        try:\n            element_pbar = tqdm(\n                total=len(target_elements),\n                desc=f\"Updating text Page {self.number}\",\n                unit=\"element\",\n                leave=False,\n            )\n\n            processed_count = 0\n            updated_count = 0\n            error_count = 0\n\n            # Define the task to be run by the worker thread or sequentially\n            def _process_element_task(element):\n                try:\n                    current_text = getattr(element, \"text\", None)\n                    # Call the user-provided callback\n                    corrected_text = transform(element)\n\n                    # Validate result type\n                    if corrected_text is not None and not isinstance(corrected_text, str):\n                        logger.warning(\n                            f\"Page {self.number}: Correction callback for element '{getattr(element, 'text', '')[:20]}...' returned non-string, non-None type: {type(corrected_text)}. Skipping update.\"\n                        )\n                        return element, None, None  # Treat as no correction\n\n                    return element, corrected_text, None  # Return element, result, no error\n                except Exception as e:\n                    logger.error(\n                        f\"Page {self.number}: Error applying correction callback to element '{getattr(element, 'text', '')[:30]}...' ({element.bbox}): {e}\",\n                        exc_info=False,  # Keep log concise\n                    )\n                    return element, None, e  # Return element, no result, error\n                finally:\n                    # --- Update internal tqdm progress bar ---\n                    if element_pbar:\n                        element_pbar.update(1)\n                    # --- Call user's progress callback --- #\n                    if progress_callback:\n                        try:\n                            progress_callback()\n                        except Exception as cb_e:\n                            # Log error in callback itself, but don't stop processing\n                            logger.error(\n                                f\"Page {self.number}: Error executing progress_callback: {cb_e}\",\n                                exc_info=False,\n                            )\n\n            # Choose execution strategy based on max_workers\n            if max_workers is not None and max_workers &gt; 1:\n                # --- Parallel execution --- #\n                logger.info(\n                    f\"Page {self.number}: Running text update in parallel with {max_workers} workers.\"\n                )\n                futures = []\n                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n                    # Submit all tasks\n                    future_to_element = {\n                        executor.submit(_process_element_task, element): element\n                        for element in target_elements\n                    }\n\n                    # Process results as they complete (progress_callback called by worker)\n                    for future in concurrent.futures.as_completed(future_to_element):\n                        processed_count += 1\n                        try:\n                            element, corrected_text, error = future.result()\n                            if error:\n                                error_count += 1\n                                # Error already logged in worker\n                            elif corrected_text is not None:\n                                # Apply correction if text changed\n                                current_text = getattr(element, \"text\", None)\n                                if corrected_text != current_text:\n                                    element.text = corrected_text\n                                    updated_count += 1\n                        except Exception as exc:\n                            # Catch errors from future.result() itself\n                            element = future_to_element[future]  # Find original element\n                            logger.error(\n                                f\"Page {self.number}: Internal error retrieving correction result for element {element.bbox}: {exc}\",\n                                exc_info=True,\n                            )\n                            error_count += 1\n                            # Note: progress_callback was already called in the worker's finally block\n\n            else:\n                # --- Sequential execution --- #\n                logger.info(f\"Page {self.number}: Running text update sequentially.\")\n                for element in target_elements:\n                    # Call the task function directly (it handles progress_callback)\n                    processed_count += 1\n                    _element, corrected_text, error = _process_element_task(element)\n                    if error:\n                        error_count += 1\n                    elif corrected_text is not None:\n                        # Apply correction if text changed\n                        current_text = getattr(_element, \"text\", None)\n                        if corrected_text != current_text:\n                            _element.text = corrected_text\n                            updated_count += 1\n\n            logger.info(\n                f\"Page {self.number}: Text update finished. Processed: {processed_count}/{len(target_elements)}, Updated: {updated_count}, Errors: {error_count}.\"\n            )\n\n            return self  # Return self for chaining\n        finally:\n            if element_pbar:\n                element_pbar.close()\n\n    # --- Classification Mixin Implementation --- #\n    def _get_classification_manager(self) -&gt; \"ClassificationManager\":\n        if not hasattr(self, \"pdf\") or not hasattr(self.pdf, \"get_manager\"):\n            raise AttributeError(\n                \"ClassificationManager cannot be accessed: Parent PDF or get_manager method missing.\"\n            )\n        try:\n            # Use the PDF's manager registry accessor\n            return self.pdf.get_manager(\"classification\")\n        except (ValueError, RuntimeError, AttributeError) as e:\n            # Wrap potential errors from get_manager for clarity\n            raise AttributeError(f\"Failed to get ClassificationManager from PDF: {e}\") from e\n\n    def _get_classification_content(\n        self, model_type: str, **kwargs\n    ) -&gt; Union[str, \"Image\"]:  # Use \"Image\" for lazy import\n        if model_type == \"text\":\n            text_content = self.extract_text(\n                layout=False, use_exclusions=False\n            )  # Simple join, ignore exclusions for classification\n            if not text_content or text_content.isspace():\n                raise ValueError(\"Cannot classify page with 'text' model: No text content found.\")\n            return text_content\n        elif model_type == \"vision\":\n            # Get resolution from manager/kwargs if possible, else default\n            manager = self._get_classification_manager()\n            default_resolution = 150\n            # Access kwargs passed to classify method if needed\n            resolution = (\n                kwargs.get(\"resolution\", default_resolution)\n                if \"kwargs\" in locals()\n                else default_resolution\n            )\n\n            # Use render() for clean image without highlights\n            img = self.render(resolution=resolution)\n            if img is None:\n                raise ValueError(\n                    \"Cannot classify page with 'vision' model: Failed to render image.\"\n                )\n            return img\n        else:\n            raise ValueError(f\"Unsupported model_type for classification: {model_type}\")\n\n    def _get_metadata_storage(self) -&gt; Dict[str, Any]:\n        # Ensure metadata exists\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            self.metadata = {}\n        return self.metadata\n\n    # --- Content Extraction ---\n\n    # --- Skew Detection and Correction --- #\n\n    @property\n    def skew_angle(self) -&gt; Optional[float]:\n        \"\"\"Get the detected skew angle for this page (if calculated).\"\"\"\n        return self._skew_angle\n\n    def detect_skew_angle(\n        self,\n        resolution: int = 72,\n        grayscale: bool = True,\n        force_recalculate: bool = False,\n        **deskew_kwargs,\n    ) -&gt; Optional[float]:\n        \"\"\"\n        Detects the skew angle of the page image and stores it.\n\n        Args:\n            resolution: DPI resolution for rendering the page image for detection.\n            grayscale: Whether to convert the image to grayscale before detection.\n            force_recalculate: If True, recalculate even if an angle exists.\n            **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                             (e.g., `max_angle`, `num_peaks`).\n\n        Returns:\n            The detected skew angle in degrees, or None if detection failed.\n\n        Raises:\n            ImportError: If the 'deskew' library is not installed.\n        \"\"\"\n        if not DESKEW_AVAILABLE:\n            raise ImportError(\n                \"Deskew library not found. Install with: pip install natural-pdf[deskew]\"\n            )\n\n        if self._skew_angle is not None and not force_recalculate:\n            logger.debug(f\"Page {self.number}: Returning cached skew angle: {self._skew_angle:.2f}\")\n            return self._skew_angle\n\n        logger.debug(f\"Page {self.number}: Detecting skew angle (resolution={resolution} DPI)...\")\n        try:\n            # Render the page at the specified detection resolution\n            # Use render() for clean image without highlights\n            img = self.render(resolution=resolution)\n            if not img:\n                logger.warning(f\"Page {self.number}: Failed to render image for skew detection.\")\n                self._skew_angle = None\n                return None\n\n            # Convert to numpy array\n            img_np = np.array(img)\n\n            # Convert to grayscale if needed\n            if grayscale:\n                if len(img_np.shape) == 3 and img_np.shape[2] &gt;= 3:\n                    gray_np = np.mean(img_np[:, :, :3], axis=2).astype(np.uint8)\n                elif len(img_np.shape) == 2:\n                    gray_np = img_np  # Already grayscale\n                else:\n                    logger.warning(\n                        f\"Page {self.number}: Unexpected image shape {img_np.shape} for grayscale conversion.\"\n                    )\n                    gray_np = img_np  # Try using it anyway\n            else:\n                gray_np = img_np  # Use original if grayscale=False\n\n            # Determine skew angle using the deskew library\n            angle = determine_skew(gray_np, **deskew_kwargs)\n            self._skew_angle = angle\n            logger.debug(f\"Page {self.number}: Detected skew angle = {angle}\")\n            return angle\n\n        except Exception as e:\n            logger.warning(f\"Page {self.number}: Failed during skew detection: {e}\", exc_info=True)\n            self._skew_angle = None\n            return None\n\n    def deskew(\n        self,\n        resolution: int = 300,\n        angle: Optional[float] = None,\n        detection_resolution: int = 72,\n        **deskew_kwargs,\n    ) -&gt; Optional[Image.Image]:\n        \"\"\"\n        Creates and returns a deskewed PIL image of the page.\n\n        If `angle` is not provided, it will first try to detect the skew angle\n        using `detect_skew_angle` (or use the cached angle if available).\n\n        Args:\n            resolution: DPI resolution for the output deskewed image.\n            angle: The specific angle (in degrees) to rotate by. If None, detects automatically.\n            detection_resolution: DPI resolution used for detection if `angle` is None.\n            **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                             if automatic detection is performed.\n\n        Returns:\n            A deskewed PIL.Image.Image object, or None if rendering/rotation fails.\n\n        Raises:\n            ImportError: If the 'deskew' library is not installed.\n        \"\"\"\n        if not DESKEW_AVAILABLE:\n            raise ImportError(\n                \"Deskew library not found. Install with: pip install natural-pdf[deskew]\"\n            )\n\n        # Determine the angle to use\n        rotation_angle = angle\n        if rotation_angle is None:\n            # Detect angle (or use cached) if not explicitly provided\n            rotation_angle = self.detect_skew_angle(\n                resolution=detection_resolution, **deskew_kwargs\n            )\n\n        logger.debug(\n            f\"Page {self.number}: Preparing to deskew (output resolution={resolution} DPI). Using angle: {rotation_angle}\"\n        )\n\n        try:\n            # Render the original page at the desired output resolution\n            # Use render() for clean image without highlights\n            img = self.render(resolution=resolution)\n            if not img:\n                logger.error(f\"Page {self.number}: Failed to render image for deskewing.\")\n                return None\n\n            # Rotate if a significant angle was found/provided\n            if rotation_angle is not None and abs(rotation_angle) &gt; 0.05:\n                logger.debug(f\"Page {self.number}: Rotating by {rotation_angle:.2f} degrees.\")\n                # Determine fill color based on image mode\n                fill = (255, 255, 255) if img.mode == \"RGB\" else 255  # White background\n                # Rotate the image using PIL\n                rotated_img = img.rotate(\n                    rotation_angle,  # deskew provides angle, PIL rotates counter-clockwise\n                    resample=Image.Resampling.BILINEAR,\n                    expand=True,  # Expand image to fit rotated content\n                    fillcolor=fill,\n                )\n                return rotated_img\n            else:\n                logger.debug(\n                    f\"Page {self.number}: No significant rotation needed (angle={rotation_angle}). Returning original render.\"\n                )\n                return img  # Return the original rendered image if no rotation needed\n\n        except Exception as e:\n            logger.error(\n                f\"Page {self.number}: Error during deskewing image generation: {e}\", exc_info=True\n            )\n            return None\n\n    # --- End Skew Detection and Correction --- #\n\n    # ------------------------------------------------------------------\n    # Unified analysis storage (maps to metadata[\"analysis\"])\n    # ------------------------------------------------------------------\n\n    @property\n    def analyses(self) -&gt; Dict[str, Any]:\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            self.metadata = {}\n        return self.metadata.setdefault(\"analysis\", {})\n\n    @analyses.setter\n    def analyses(self, value: Dict[str, Any]):\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            self.metadata = {}\n        self.metadata[\"analysis\"] = value\n\n    def inspect(self, limit: int = 30) -&gt; \"InspectionSummary\":\n        \"\"\"\n        Inspect all elements on this page with detailed tabular view.\n        Equivalent to page.find_all('*').inspect().\n\n        Args:\n            limit: Maximum elements per type to show (default: 30)\n\n        Returns:\n            InspectionSummary with element tables showing coordinates,\n            properties, and other details for each element\n        \"\"\"\n        return self.find_all(\"*\").inspect(limit=limit)\n\n    def remove_text_layer(self) -&gt; \"Page\":\n        \"\"\"\n        Remove all text elements from this page.\n\n        This removes all text elements (words and characters) from the page,\n        effectively clearing the text layer.\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        logger.info(f\"Page {self.number}: Removing all text elements...\")\n\n        # Remove all words and chars from the element manager\n        removed_words = len(self._element_mgr.words)\n        removed_chars = len(self._element_mgr.chars)\n\n        # Clear the lists\n        self._element_mgr._elements[\"words\"] = []\n        self._element_mgr._elements[\"chars\"] = []\n\n        logger.info(\n            f\"Page {self.number}: Removed {removed_words} words and {removed_chars} characters\"\n        )\n        return self\n\n    def _apply_rtl_processing_to_text(self, text: str) -&gt; str:\n        \"\"\"\n        Apply RTL (Right-to-Left) text processing to a string.\n\n        This converts visual order text (as stored in PDFs) to logical order\n        for proper display of Arabic, Hebrew, and other RTL scripts.\n\n        Args:\n            text: Input text string in visual order\n\n        Returns:\n            Text string in logical order\n        \"\"\"\n        if not text or not text.strip():\n            return text\n\n        # Quick check for RTL characters - if none found, return as-is\n        import unicodedata\n\n        def _contains_rtl(s):\n            return any(unicodedata.bidirectional(ch) in (\"R\", \"AL\", \"AN\") for ch in s)\n\n        if not _contains_rtl(text):\n            return text\n\n        try:\n            from bidi.algorithm import get_display  # type: ignore\n\n            from natural_pdf.utils.bidi_mirror import mirror_brackets\n\n            # Apply BiDi algorithm to convert from visual to logical order\n            # Process line by line to handle mixed content properly\n            processed_lines = []\n            for line in text.split(\"\\n\"):\n                if line.strip():\n                    # Determine base direction for this line\n                    base_dir = \"R\" if _contains_rtl(line) else \"L\"\n                    logical_line = get_display(line, base_dir=base_dir)\n                    # Apply bracket mirroring for correct logical order\n                    processed_lines.append(mirror_brackets(logical_line))\n                else:\n                    processed_lines.append(line)\n\n            return \"\\n\".join(processed_lines)\n\n        except (ImportError, Exception):\n            # If bidi library is not available or fails, return original text\n            return text\n\n    @property\n    def lines(self) -&gt; List[Any]:\n        \"\"\"Get all line elements on this page.\"\"\"\n        return self._element_mgr.lines\n\n    # ------------------------------------------------------------------\n    # Image elements\n    # ------------------------------------------------------------------\n\n    @property\n    def images(self) -&gt; List[Any]:\n        \"\"\"Get all embedded raster images on this page.\"\"\"\n        return self._element_mgr.images\n\n    def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n        \"\"\"\n        Create a highlight context for accumulating highlights.\n\n        This allows for clean syntax to show multiple highlight groups:\n\n        Example:\n            with page.highlights() as h:\n                h.add(page.find_all('table'), label='tables', color='blue')\n                h.add(page.find_all('text:bold'), label='bold text', color='red')\n                h.show()\n\n        Or with automatic display:\n            with page.highlights(show=True) as h:\n                h.add(page.find_all('table'), label='tables')\n                h.add(page.find_all('text:bold'), label='bold')\n                # Automatically shows when exiting the context\n\n        Args:\n            show: If True, automatically show highlights when exiting context\n\n        Returns:\n            HighlightContext for accumulating highlights\n        \"\"\"\n        from natural_pdf.core.highlighting_service import HighlightContext\n\n        return HighlightContext(self, show_on_exit=show)\n</code></pre>"},{"location":"api/#natural_pdf.Page-attributes","title":"Attributes","text":"<code>natural_pdf.Page.chars</code> <code>property</code> <p>Get all character elements on this page.</p> <code>natural_pdf.Page.height</code> <code>property</code> <p>Get page height.</p> <code>natural_pdf.Page.images</code> <code>property</code> <p>Get all embedded raster images on this page.</p> <code>natural_pdf.Page.index</code> <code>property</code> <p>Get page index (0-based).</p> <code>natural_pdf.Page.layout_analyzer</code> <code>property</code> <p>Get or create the layout analyzer for this page.</p> <code>natural_pdf.Page.lines</code> <code>property</code> <p>Get all line elements on this page.</p> <code>natural_pdf.Page.number</code> <code>property</code> <p>Get page number (1-based).</p> <code>natural_pdf.Page.page_number</code> <code>property</code> <p>Get page number (1-based).</p> <code>natural_pdf.Page.pdf</code> <code>property</code> <p>Provides public access to the parent PDF object.</p> <code>natural_pdf.Page.rects</code> <code>property</code> <p>Get all rectangle elements on this page.</p> <code>natural_pdf.Page.size</code> <code>property</code> <p>Get the size of the page in points.</p> <code>natural_pdf.Page.skew_angle</code> <code>property</code> <p>Get the detected skew angle for this page (if calculated).</p> <code>natural_pdf.Page.text_style_labels</code> <code>property</code> <p>Get a sorted list of unique text style labels found on the page.</p> <p>Runs text style analysis with default options if it hasn't been run yet. To use custom options, call <code>analyze_text_styles(options=...)</code> explicitly first.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>A sorted list of unique style label strings.</p> <code>natural_pdf.Page.width</code> <code>property</code> <p>Get page width.</p> <code>natural_pdf.Page.words</code> <code>property</code> <p>Get all word elements on this page.</p>"},{"location":"api/#natural_pdf.Page-functions","title":"Functions","text":"<code>natural_pdf.Page.__init__(page, parent, index, font_attrs=None, load_text=True)</code> <p>Initialize a page wrapper.</p> <p>Creates an enhanced Page object that wraps a pdfplumber page with additional functionality for spatial navigation, analysis, and AI-powered extraction.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>The underlying pdfplumber page object that provides raw PDF data.</p> required <code>parent</code> <code>PDF</code> <p>Parent PDF object that contains this page and provides access to managers and global settings.</p> required <code>index</code> <code>int</code> <p>Zero-based index of this page in the PDF document.</p> required <code>font_attrs</code> <p>List of font attributes to consider when grouping characters into words. Common attributes include ['fontname', 'size', 'flags']. If None, uses default character-to-word grouping rules.</p> <code>None</code> <code>load_text</code> <code>bool</code> <p>If True, load and process text elements from the PDF's text layer. If False, skip text layer processing (useful for OCR-only workflows).</p> <code>True</code> Note <p>This constructor is typically called automatically when accessing pages through the PDF.pages collection. Direct instantiation is rarely needed.</p> Example <pre><code># Pages are usually accessed through the PDF object\npdf = npdf.PDF(\"document.pdf\")\npage = pdf.pages[0]  # Page object created automatically\n\n# Direct construction (advanced usage)\nimport pdfplumber\nwith pdfplumber.open(\"document.pdf\") as plumber_pdf:\n    plumber_page = plumber_pdf.pages[0]\n    page = Page(plumber_page, pdf, 0, load_text=True)\n</code></pre> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def __init__(\n    self,\n    page: \"pdfplumber.page.Page\",\n    parent: \"PDF\",\n    index: int,\n    font_attrs=None,\n    load_text: bool = True,\n):\n    \"\"\"Initialize a page wrapper.\n\n    Creates an enhanced Page object that wraps a pdfplumber page with additional\n    functionality for spatial navigation, analysis, and AI-powered extraction.\n\n    Args:\n        page: The underlying pdfplumber page object that provides raw PDF data.\n        parent: Parent PDF object that contains this page and provides access\n            to managers and global settings.\n        index: Zero-based index of this page in the PDF document.\n        font_attrs: List of font attributes to consider when grouping characters\n            into words. Common attributes include ['fontname', 'size', 'flags'].\n            If None, uses default character-to-word grouping rules.\n        load_text: If True, load and process text elements from the PDF's text layer.\n            If False, skip text layer processing (useful for OCR-only workflows).\n\n    Note:\n        This constructor is typically called automatically when accessing pages\n        through the PDF.pages collection. Direct instantiation is rarely needed.\n\n    Example:\n        ```python\n        # Pages are usually accessed through the PDF object\n        pdf = npdf.PDF(\"document.pdf\")\n        page = pdf.pages[0]  # Page object created automatically\n\n        # Direct construction (advanced usage)\n        import pdfplumber\n        with pdfplumber.open(\"document.pdf\") as plumber_pdf:\n            plumber_page = plumber_pdf.pages[0]\n            page = Page(plumber_page, pdf, 0, load_text=True)\n        ```\n    \"\"\"\n    self._page = page\n    self._parent = parent\n    self._index = index\n    self._load_text = load_text\n    self._text_styles = None  # Lazy-loaded text style analyzer results\n    self._exclusions = []  # List to store exclusion functions/regions\n    self._skew_angle: Optional[float] = None  # Stores detected skew angle\n\n    # --- ADDED --- Metadata store for mixins\n    self.metadata: Dict[str, Any] = {}\n    # --- END ADDED ---\n\n    # Region management\n    self._regions = {\n        \"detected\": [],  # Layout detection results\n        \"named\": {},  # Named regions (name -&gt; region)\n    }\n\n    # -------------------------------------------------------------\n    # Page-scoped configuration begins as a shallow copy of the parent\n    # PDF-level configuration so that auto-computed tolerances or other\n    # page-specific values do not overwrite siblings.\n    # -------------------------------------------------------------\n    self._config = dict(getattr(self._parent, \"_config\", {}))\n\n    # Initialize ElementManager, passing font_attrs\n    self._element_mgr = ElementManager(self, font_attrs=font_attrs, load_text=self._load_text)\n    # self._highlighter = HighlightingService(self) # REMOVED - Use property accessor\n    # --- NEW --- Central registry for analysis results\n    self.analyses: Dict[str, Any] = {}\n\n    # --- Get OCR Manager Instance ---\n    if (\n        OCRManager\n        and hasattr(parent, \"_ocr_manager\")\n        and isinstance(parent._ocr_manager, OCRManager)\n    ):\n        self._ocr_manager = parent._ocr_manager\n        logger.debug(f\"Page {self.number}: Using OCRManager instance from parent PDF.\")\n    else:\n        self._ocr_manager = None\n        if OCRManager:\n            logger.warning(\n                f\"Page {self.number}: OCRManager instance not found on parent PDF object.\"\n            )\n\n    # --- Get Layout Manager Instance ---\n    if (\n        LayoutManager\n        and hasattr(parent, \"_layout_manager\")\n        and isinstance(parent._layout_manager, LayoutManager)\n    ):\n        self._layout_manager = parent._layout_manager\n        logger.debug(f\"Page {self.number}: Using LayoutManager instance from parent PDF.\")\n    else:\n        self._layout_manager = None\n        if LayoutManager:\n            logger.warning(\n                f\"Page {self.number}: LayoutManager instance not found on parent PDF object. Layout analysis will fail.\"\n            )\n\n    # Initialize the internal variable with a single underscore\n    self._layout_analyzer = None\n\n    self._load_elements()\n    self._to_image_cache: Dict[tuple, Optional[\"Image.Image\"]] = {}\n</code></pre> <code>natural_pdf.Page.__repr__()</code> <p>String representation of the page.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the page.\"\"\"\n    return f\"&lt;Page number={self.number} index={self.index}&gt;\"\n</code></pre> <code>natural_pdf.Page.add_exclusion(exclusion_func_or_region, label=None, method='region')</code> <p>Add an exclusion to the page. Text from these regions will be excluded from extraction. Ensures non-callable items are stored as Region objects if possible.</p> <p>Parameters:</p> Name Type Description Default <code>exclusion_func_or_region</code> <code>Union[Callable[[Page], Region], Region, List[Any], Tuple[Any, ...], Any]</code> <p>Either a callable function returning a Region,                       a Region object, a list/tuple of regions or elements,                       or another object with a valid .bbox attribute.</p> required <code>label</code> <code>Optional[str]</code> <p>Optional label for this exclusion (e.g., 'header', 'footer').</p> <code>None</code> <code>method</code> <code>str</code> <p>Exclusion method - 'region' (exclude all elements in bounding box) or     'element' (exclude only the specific elements). Default: 'region'.</p> <code>'region'</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If a non-callable, non-Region object without a valid bbox is provided.</p> <code>ValueError</code> <p>If method is not 'region' or 'element'.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def add_exclusion(\n    self,\n    exclusion_func_or_region: Union[\n        Callable[[\"Page\"], \"Region\"], \"Region\", List[Any], Tuple[Any, ...], Any\n    ],\n    label: Optional[str] = None,\n    method: str = \"region\",\n) -&gt; \"Page\":\n    \"\"\"\n    Add an exclusion to the page. Text from these regions will be excluded from extraction.\n    Ensures non-callable items are stored as Region objects if possible.\n\n    Args:\n        exclusion_func_or_region: Either a callable function returning a Region,\n                                  a Region object, a list/tuple of regions or elements,\n                                  or another object with a valid .bbox attribute.\n        label: Optional label for this exclusion (e.g., 'header', 'footer').\n        method: Exclusion method - 'region' (exclude all elements in bounding box) or\n                'element' (exclude only the specific elements). Default: 'region'.\n\n    Returns:\n        Self for method chaining\n\n    Raises:\n        TypeError: If a non-callable, non-Region object without a valid bbox is provided.\n        ValueError: If method is not 'region' or 'element'.\n    \"\"\"\n    # Validate method parameter\n    if method not in (\"region\", \"element\"):\n        raise ValueError(f\"Invalid exclusion method '{method}'. Must be 'region' or 'element'.\")\n\n    # ------------------------------------------------------------------\n    # NEW: Handle selector strings and ElementCollection instances\n    # ------------------------------------------------------------------\n    # If a user supplies a selector string (e.g. \"text:bold\") we resolve it\n    # immediately *on this page* to the matching elements and turn each into\n    # a Region object which is added to the internal exclusions list.\n    #\n    # Likewise, if an ElementCollection is passed we iterate over its\n    # elements and create Regions for each one.\n    # ------------------------------------------------------------------\n    # Import ElementCollection from the new module path (old path removed)\n    from natural_pdf.elements.element_collection import ElementCollection\n\n    # Selector string ---------------------------------------------------\n    if isinstance(exclusion_func_or_region, str):\n        selector_str = exclusion_func_or_region\n        matching_elements = self.find_all(selector_str, apply_exclusions=False)\n\n        if not matching_elements:\n            logger.warning(\n                f\"Page {self.index}: Selector '{selector_str}' returned no elements \u2013 no exclusions added.\"\n            )\n        else:\n            if method == \"element\":\n                # Store the actual elements for element-based exclusion\n                for el in matching_elements:\n                    self._exclusions.append((el, label, method))\n                    logger.debug(\n                        f\"Page {self.index}: Added element exclusion from selector '{selector_str}' -&gt; {el}\"\n                    )\n            else:  # method == \"region\"\n                for el in matching_elements:\n                    try:\n                        bbox_coords = (\n                            float(el.x0),\n                            float(el.top),\n                            float(el.x1),\n                            float(el.bottom),\n                        )\n                        region = Region(self, bbox_coords, label=label)\n                        # Store directly as a Region tuple so we don't recurse endlessly\n                        self._exclusions.append((region, label, method))\n                        logger.debug(\n                            f\"Page {self.index}: Added exclusion region from selector '{selector_str}' -&gt; {bbox_coords}\"\n                        )\n                    except Exception as e:\n                        # Re-raise so calling code/test sees the failure immediately\n                        logger.error(\n                            f\"Page {self.index}: Failed to create exclusion region from element {el}: {e}\",\n                            exc_info=False,\n                        )\n                        raise\n        return self  # Completed processing for selector input\n\n    # ElementCollection -----------------------------------------------\n    if isinstance(exclusion_func_or_region, ElementCollection):\n        if method == \"element\":\n            # Store the actual elements for element-based exclusion\n            for el in exclusion_func_or_region:\n                self._exclusions.append((el, label, method))\n                logger.debug(\n                    f\"Page {self.index}: Added element exclusion from ElementCollection -&gt; {el}\"\n                )\n        else:  # method == \"region\"\n            # Convert each element to a Region and add\n            for el in exclusion_func_or_region:\n                try:\n                    if not (hasattr(el, \"bbox\") and len(el.bbox) == 4):\n                        logger.warning(\n                            f\"Page {self.index}: Skipping element without bbox in ElementCollection exclusion: {el}\"\n                        )\n                        continue\n                    bbox_coords = tuple(float(v) for v in el.bbox)\n                    region = Region(self, bbox_coords, label=label)\n                    self._exclusions.append((region, label, method))\n                    logger.debug(\n                        f\"Page {self.index}: Added exclusion region from ElementCollection element {bbox_coords}\"\n                    )\n                except Exception as e:\n                    logger.error(\n                        f\"Page {self.index}: Failed to convert ElementCollection element to Region: {e}\",\n                        exc_info=False,\n                    )\n                    raise\n        return self  # Completed processing for ElementCollection input\n\n    # ------------------------------------------------------------------\n    # Existing logic (callable, Region, bbox-bearing objects)\n    # ------------------------------------------------------------------\n    exclusion_data = None  # Initialize exclusion data\n\n    if callable(exclusion_func_or_region):\n        # Store callable functions along with their label and method\n        exclusion_data = (exclusion_func_or_region, label, method)\n        logger.debug(\n            f\"Page {self.index}: Added callable exclusion '{label}' with method '{method}': {exclusion_func_or_region}\"\n        )\n    elif isinstance(exclusion_func_or_region, Region):\n        # Store Region objects directly, assigning the label\n        exclusion_func_or_region.label = label  # Assign label\n        exclusion_data = (\n            exclusion_func_or_region,\n            label,\n            method,\n        )  # Store as tuple for consistency\n        logger.debug(\n            f\"Page {self.index}: Added Region exclusion '{label}' with method '{method}': {exclusion_func_or_region}\"\n        )\n    elif (\n        hasattr(exclusion_func_or_region, \"bbox\")\n        and isinstance(getattr(exclusion_func_or_region, \"bbox\", None), (tuple, list))\n        and len(exclusion_func_or_region.bbox) == 4\n    ):\n        if method == \"element\":\n            # For element method, store the element directly\n            exclusion_data = (exclusion_func_or_region, label, method)\n            logger.debug(\n                f\"Page {self.index}: Added element exclusion '{label}': {exclusion_func_or_region}\"\n            )\n        else:  # method == \"region\"\n            # Convert objects with a valid bbox to a Region before storing\n            try:\n                bbox_coords = tuple(float(v) for v in exclusion_func_or_region.bbox)\n                # Pass the label to the Region constructor\n                region_to_add = Region(self, bbox_coords, label=label)\n                exclusion_data = (region_to_add, label, method)  # Store as tuple\n                logger.debug(\n                    f\"Page {self.index}: Added exclusion '{label}' with method '{method}' converted to Region from {type(exclusion_func_or_region)}: {region_to_add}\"\n                )\n            except (ValueError, TypeError, Exception) as e:\n                # Raise an error if conversion fails\n                raise TypeError(\n                    f\"Failed to convert exclusion object {exclusion_func_or_region} with bbox {getattr(exclusion_func_or_region, 'bbox', 'N/A')} to Region: {e}\"\n                ) from e\n    elif isinstance(exclusion_func_or_region, (list, tuple)):\n        # Handle lists/tuples of regions or elements\n        if not exclusion_func_or_region:\n            logger.warning(f\"Page {self.index}: Empty list provided for exclusion, ignoring.\")\n            return self\n\n        if method == \"element\":\n            # Store each element directly\n            for item in exclusion_func_or_region:\n                if hasattr(item, \"bbox\") and len(getattr(item, \"bbox\", [])) == 4:\n                    self._exclusions.append((item, label, method))\n                    logger.debug(\n                        f\"Page {self.index}: Added element exclusion from list -&gt; {item}\"\n                    )\n                else:\n                    logger.warning(\n                        f\"Page {self.index}: Skipping item without valid bbox in list: {item}\"\n                    )\n        else:  # method == \"region\"\n            # Convert each item to a Region and add\n            for item in exclusion_func_or_region:\n                try:\n                    if isinstance(item, Region):\n                        item.label = label\n                        self._exclusions.append((item, label, method))\n                        logger.debug(f\"Page {self.index}: Added Region from list: {item}\")\n                    elif hasattr(item, \"bbox\") and len(getattr(item, \"bbox\", [])) == 4:\n                        bbox_coords = tuple(float(v) for v in item.bbox)\n                        region = Region(self, bbox_coords, label=label)\n                        self._exclusions.append((region, label, method))\n                        logger.debug(\n                            f\"Page {self.index}: Added exclusion region from list item {bbox_coords}\"\n                        )\n                    else:\n                        logger.warning(\n                            f\"Page {self.index}: Skipping item without valid bbox in list: {item}\"\n                        )\n                except Exception as e:\n                    logger.error(\n                        f\"Page {self.index}: Failed to convert list item to Region: {e}\"\n                    )\n                    continue\n        return self\n    else:\n        # Reject invalid types\n        raise TypeError(\n            f\"Invalid exclusion type: {type(exclusion_func_or_region)}. Must be callable, Region, list/tuple of regions/elements, or have a valid .bbox attribute.\"\n        )\n\n    # Append the stored data (tuple of object/callable, label, and method)\n    if exclusion_data:\n        self._exclusions.append(exclusion_data)\n\n    return self\n</code></pre> <code>natural_pdf.Page.add_region(region, name=None)</code> <p>Add a region to the page.</p> <p>Parameters:</p> Name Type Description Default <code>region</code> <code>Region</code> <p>Region object to add</p> required <code>name</code> <code>Optional[str]</code> <p>Optional name for the region</p> <code>None</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def add_region(self, region: \"Region\", name: Optional[str] = None) -&gt; \"Page\":\n    \"\"\"\n    Add a region to the page.\n\n    Args:\n        region: Region object to add\n        name: Optional name for the region\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Check if it's actually a Region object\n    if not isinstance(region, Region):\n        raise TypeError(\"region must be a Region object\")\n\n    # Set the source and name\n    region.source = \"named\"\n\n    if name:\n        region.name = name\n        # Add to named regions dictionary (overwriting if name already exists)\n        self._regions[\"named\"][name] = region\n    else:\n        # Add to detected regions list (unnamed but registered)\n        self._regions[\"detected\"].append(region)\n\n    # Add to element manager for selector queries\n    self._element_mgr.add_region(region)\n\n    return self\n</code></pre> <code>natural_pdf.Page.add_regions(regions, prefix=None)</code> <p>Add multiple regions to the page.</p> <p>Parameters:</p> Name Type Description Default <code>regions</code> <code>List[Region]</code> <p>List of Region objects to add</p> required <code>prefix</code> <code>Optional[str]</code> <p>Optional prefix for automatic naming (regions will be named prefix_1, prefix_2, etc.)</p> <code>None</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def add_regions(self, regions: List[\"Region\"], prefix: Optional[str] = None) -&gt; \"Page\":\n    \"\"\"\n    Add multiple regions to the page.\n\n    Args:\n        regions: List of Region objects to add\n        prefix: Optional prefix for automatic naming (regions will be named prefix_1, prefix_2, etc.)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    if prefix:\n        # Add with automatic sequential naming\n        for i, region in enumerate(regions):\n            self.add_region(region, name=f\"{prefix}_{i+1}\")\n    else:\n        # Add without names\n        for region in regions:\n            self.add_region(region)\n\n    return self\n</code></pre> <code>natural_pdf.Page.analyze_layout(engine=None, options=None, confidence=None, classes=None, exclude_classes=None, device=None, existing='replace', model_name=None, client=None)</code> <p>Analyze the page layout using the configured LayoutManager. Adds detected Region objects to the page's element manager.</p> <p>Returns:</p> Type Description <code>ElementCollection[Region]</code> <p>ElementCollection containing the detected Region objects.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def analyze_layout(\n    self,\n    engine: Optional[str] = None,\n    options: Optional[\"LayoutOptions\"] = None,\n    confidence: Optional[float] = None,\n    classes: Optional[List[str]] = None,\n    exclude_classes: Optional[List[str]] = None,\n    device: Optional[str] = None,\n    existing: str = \"replace\",\n    model_name: Optional[str] = None,\n    client: Optional[Any] = None,  # Add client parameter\n) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Analyze the page layout using the configured LayoutManager.\n    Adds detected Region objects to the page's element manager.\n\n    Returns:\n        ElementCollection containing the detected Region objects.\n    \"\"\"\n    analyzer = self.layout_analyzer\n    if not analyzer:\n        logger.error(\n            \"Layout analysis failed: LayoutAnalyzer not initialized (is LayoutManager available?).\"\n        )\n        return ElementCollection([])  # Return empty collection\n\n    # Clear existing detected regions if 'replace' is specified\n    if existing == \"replace\":\n        self.clear_detected_layout_regions()\n\n    # The analyzer's analyze_layout method already adds regions to the page\n    # and its element manager. We just need to retrieve them.\n    analyzer.analyze_layout(\n        engine=engine,\n        options=options,\n        confidence=confidence,\n        classes=classes,\n        exclude_classes=exclude_classes,\n        device=device,\n        existing=existing,\n        model_name=model_name,\n        client=client,  # Pass client down\n    )\n\n    # Retrieve the detected regions from the element manager\n    # Filter regions based on source='detected' and potentially the model used if available\n    detected_regions = [\n        r\n        for r in self._element_mgr.regions\n        if r.source == \"detected\" and (not engine or getattr(r, \"model\", None) == engine)\n    ]\n\n    return ElementCollection(detected_regions)\n</code></pre> <code>natural_pdf.Page.analyze_text_styles(options=None)</code> <p>Analyze text elements by style, adding attributes directly to elements.</p> <p>This method uses TextStyleAnalyzer to process text elements (typically words) on the page. It adds the following attributes to each processed element: - style_label: A descriptive or numeric label for the style group. - style_key: A hashable tuple representing the style properties used for grouping. - style_properties: A dictionary containing the extracted style properties.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>Optional[TextStyleOptions]</code> <p>Optional TextStyleOptions to configure the analysis.      If None, the analyzer's default options are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection containing all processed text elements with added style attributes.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def analyze_text_styles(\n    self, options: Optional[TextStyleOptions] = None\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Analyze text elements by style, adding attributes directly to elements.\n\n    This method uses TextStyleAnalyzer to process text elements (typically words)\n    on the page. It adds the following attributes to each processed element:\n    - style_label: A descriptive or numeric label for the style group.\n    - style_key: A hashable tuple representing the style properties used for grouping.\n    - style_properties: A dictionary containing the extracted style properties.\n\n    Args:\n        options: Optional TextStyleOptions to configure the analysis.\n                 If None, the analyzer's default options are used.\n\n    Returns:\n        ElementCollection containing all processed text elements with added style attributes.\n    \"\"\"\n    # Create analyzer (optionally pass default options from PDF config here)\n    # For now, it uses its own defaults if options=None\n    analyzer = TextStyleAnalyzer()\n\n    # Analyze the page. The analyzer now modifies elements directly\n    # and returns the collection of processed elements.\n    processed_elements_collection = analyzer.analyze(self, options=options)\n\n    # Return the collection of elements which now have style attributes\n    return processed_elements_collection\n</code></pre> <code>natural_pdf.Page.apply_ocr(engine=None, options=None, languages=None, min_confidence=None, device=None, resolution=None, detect_only=False, apply_exclusions=True, replace=True)</code> <p>Apply OCR to THIS page and add results to page elements via PDF.apply_ocr.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Optional[str]</code> <p>Name of the OCR engine.</p> <code>None</code> <code>options</code> <code>Optional[OCROptions]</code> <p>Engine-specific options object or dict.</p> <code>None</code> <code>languages</code> <code>Optional[List[str]]</code> <p>List of engine-specific language codes.</p> <code>None</code> <code>min_confidence</code> <code>Optional[float]</code> <p>Minimum confidence threshold.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to run OCR on.</p> <code>None</code> <code>resolution</code> <code>Optional[int]</code> <p>DPI resolution for rendering page image before OCR.</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>If True (default), render page image for OCR               with excluded areas masked (whited out).</p> <code>True</code> <code>detect_only</code> <code>bool</code> <p>If True, only detect text bounding boxes, don't perform OCR.</p> <code>False</code> <code>replace</code> <code>bool</code> <p>If True (default), remove any existing OCR elements before     adding new ones. If False, add new OCR elements to existing ones.</p> <code>True</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def apply_ocr(\n    self,\n    engine: Optional[str] = None,\n    options: Optional[\"OCROptions\"] = None,\n    languages: Optional[List[str]] = None,\n    min_confidence: Optional[float] = None,\n    device: Optional[str] = None,\n    resolution: Optional[int] = None,\n    detect_only: bool = False,\n    apply_exclusions: bool = True,\n    replace: bool = True,\n) -&gt; \"Page\":\n    \"\"\"\n    Apply OCR to THIS page and add results to page elements via PDF.apply_ocr.\n\n    Args:\n        engine: Name of the OCR engine.\n        options: Engine-specific options object or dict.\n        languages: List of engine-specific language codes.\n        min_confidence: Minimum confidence threshold.\n        device: Device to run OCR on.\n        resolution: DPI resolution for rendering page image before OCR.\n        apply_exclusions: If True (default), render page image for OCR\n                          with excluded areas masked (whited out).\n        detect_only: If True, only detect text bounding boxes, don't perform OCR.\n        replace: If True (default), remove any existing OCR elements before\n                adding new ones. If False, add new OCR elements to existing ones.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    if not hasattr(self._parent, \"apply_ocr\"):\n        logger.error(f\"Page {self.number}: Parent PDF missing 'apply_ocr'. Cannot apply OCR.\")\n        return self  # Return self for chaining\n\n    # Remove existing OCR elements if replace is True\n    if replace and hasattr(self, \"_element_mgr\"):\n        logger.info(\n            f\"Page {self.number}: Removing existing OCR elements before applying new OCR.\"\n        )\n        self._element_mgr.remove_ocr_elements()\n\n    logger.info(f\"Page {self.number}: Delegating apply_ocr to PDF.apply_ocr.\")\n    # Delegate to parent PDF, targeting only this page's index\n    # Pass all relevant parameters through, including apply_exclusions\n    self._parent.apply_ocr(\n        pages=[self.index],\n        engine=engine,\n        options=options,\n        languages=languages,\n        min_confidence=min_confidence,\n        device=device,\n        resolution=resolution,\n        detect_only=detect_only,\n        apply_exclusions=apply_exclusions,\n        replace=replace,  # Pass the replace parameter to PDF.apply_ocr\n    )\n\n    # Return self for chaining\n    return self\n</code></pre> <code>natural_pdf.Page.ask(question, min_confidence=0.1, model=None, debug=False, **kwargs)</code> <p>Ask a question about the page content using document QA.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def ask(\n    self,\n    question: Union[str, List[str], Tuple[str, ...]],\n    min_confidence: float = 0.1,\n    model: str = None,\n    debug: bool = False,\n    **kwargs,\n) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Ask a question about the page content using document QA.\n    \"\"\"\n    try:\n        from natural_pdf.qa.document_qa import get_qa_engine\n\n        # Get or initialize QA engine with specified model\n        qa_engine = get_qa_engine(model_name=model) if model else get_qa_engine()\n        # Ask the question using the QA engine\n        return qa_engine.ask_pdf_page(\n            self, question, min_confidence=min_confidence, debug=debug, **kwargs\n        )\n    except ImportError:\n        logger.error(\n            \"Question answering requires the 'natural_pdf.qa' module. Please install necessary dependencies.\"\n        )\n        return {\n            \"answer\": None,\n            \"confidence\": 0.0,\n            \"found\": False,\n            \"page_num\": self.number,\n            \"source_elements\": [],\n        }\n    except Exception as e:\n        logger.error(f\"Error during page.ask: {e}\", exc_info=True)\n        return {\n            \"answer\": None,\n            \"confidence\": 0.0,\n            \"found\": False,\n            \"page_num\": self.number,\n            \"source_elements\": [],\n        }\n</code></pre> <code>natural_pdf.Page.clear_detected_layout_regions()</code> <p>Removes all regions from this page that were added by layout analysis (i.e., regions where <code>source</code> attribute is 'detected').</p> <p>This clears the regions both from the page's internal <code>_regions['detected']</code> list and from the ElementManager's internal list of regions.</p> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def clear_detected_layout_regions(self) -&gt; \"Page\":\n    \"\"\"\n    Removes all regions from this page that were added by layout analysis\n    (i.e., regions where `source` attribute is 'detected').\n\n    This clears the regions both from the page's internal `_regions['detected']` list\n    and from the ElementManager's internal list of regions.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    if (\n        not hasattr(self._element_mgr, \"regions\")\n        or not hasattr(self._element_mgr, \"_elements\")\n        or \"regions\" not in self._element_mgr._elements\n    ):\n        logger.debug(\n            f\"Page {self.index}: No regions found in ElementManager, nothing to clear.\"\n        )\n        self._regions[\"detected\"] = []  # Ensure page's list is also clear\n        return self\n\n    # Filter ElementManager's list to keep only non-detected regions\n    original_count = len(self._element_mgr.regions)\n    self._element_mgr._elements[\"regions\"] = [\n        r for r in self._element_mgr.regions if getattr(r, \"source\", None) != \"detected\"\n    ]\n    new_count = len(self._element_mgr.regions)\n    removed_count = original_count - new_count\n\n    # Clear the page's specific list of detected regions\n    self._regions[\"detected\"] = []\n\n    logger.info(f\"Page {self.index}: Cleared {removed_count} detected layout regions.\")\n    return self\n</code></pre> <code>natural_pdf.Page.clear_exclusions()</code> <p>Clear all exclusions from the page.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def clear_exclusions(self) -&gt; \"Page\":\n    \"\"\"\n    Clear all exclusions from the page.\n    \"\"\"\n    self._exclusions = []\n    return self\n</code></pre> <code>natural_pdf.Page.clear_highlights()</code> <p>Clear all highlights from this specific page via HighlightingService.</p> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def clear_highlights(self) -&gt; \"Page\":\n    \"\"\"\n    Clear all highlights *from this specific page* via HighlightingService.\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self._highlighter.clear_page(self.index)\n    return self\n</code></pre> <code>natural_pdf.Page.create_region(x0, top, x1, bottom)</code> <p>Create a region on this page with the specified coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x0</code> <code>float</code> <p>Left x-coordinate</p> required <code>top</code> <code>float</code> <p>Top y-coordinate</p> required <code>x1</code> <code>float</code> <p>Right x-coordinate</p> required <code>bottom</code> <code>float</code> <p>Bottom y-coordinate</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Region object for the specified coordinates</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def create_region(self, x0: float, top: float, x1: float, bottom: float) -&gt; Any:\n    \"\"\"\n    Create a region on this page with the specified coordinates.\n\n    Args:\n        x0: Left x-coordinate\n        top: Top y-coordinate\n        x1: Right x-coordinate\n        bottom: Bottom y-coordinate\n\n    Returns:\n        Region object for the specified coordinates\n    \"\"\"\n    from natural_pdf.elements.region import Region\n\n    return Region(self, (x0, top, x1, bottom))\n</code></pre> <code>natural_pdf.Page.crop(bbox=None, **kwargs)</code> <p>Crop the page to the specified bounding box.</p> <p>This is a direct wrapper around pdfplumber's crop method.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <p>Bounding box (x0, top, x1, bottom) or None</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters (top, bottom, left, right)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Cropped page object (pdfplumber.Page)</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def crop(self, bbox=None, **kwargs) -&gt; Any:\n    \"\"\"\n    Crop the page to the specified bounding box.\n\n    This is a direct wrapper around pdfplumber's crop method.\n\n    Args:\n        bbox: Bounding box (x0, top, x1, bottom) or None\n        **kwargs: Additional parameters (top, bottom, left, right)\n\n    Returns:\n        Cropped page object (pdfplumber.Page)\n    \"\"\"\n    # Returns the pdfplumber page object, not a natural-pdf Page\n    return self._page.crop(bbox, **kwargs)\n</code></pre> <code>natural_pdf.Page.deskew(resolution=300, angle=None, detection_resolution=72, **deskew_kwargs)</code> <p>Creates and returns a deskewed PIL image of the page.</p> <p>If <code>angle</code> is not provided, it will first try to detect the skew angle using <code>detect_skew_angle</code> (or use the cached angle if available).</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>int</code> <p>DPI resolution for the output deskewed image.</p> <code>300</code> <code>angle</code> <code>Optional[float]</code> <p>The specific angle (in degrees) to rotate by. If None, detects automatically.</p> <code>None</code> <code>detection_resolution</code> <code>int</code> <p>DPI resolution used for detection if <code>angle</code> is None.</p> <code>72</code> <code>**deskew_kwargs</code> <p>Additional keyword arguments passed to <code>deskew.determine_skew</code>              if automatic detection is performed.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Image]</code> <p>A deskewed PIL.Image.Image object, or None if rendering/rotation fails.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the 'deskew' library is not installed.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def deskew(\n    self,\n    resolution: int = 300,\n    angle: Optional[float] = None,\n    detection_resolution: int = 72,\n    **deskew_kwargs,\n) -&gt; Optional[Image.Image]:\n    \"\"\"\n    Creates and returns a deskewed PIL image of the page.\n\n    If `angle` is not provided, it will first try to detect the skew angle\n    using `detect_skew_angle` (or use the cached angle if available).\n\n    Args:\n        resolution: DPI resolution for the output deskewed image.\n        angle: The specific angle (in degrees) to rotate by. If None, detects automatically.\n        detection_resolution: DPI resolution used for detection if `angle` is None.\n        **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                         if automatic detection is performed.\n\n    Returns:\n        A deskewed PIL.Image.Image object, or None if rendering/rotation fails.\n\n    Raises:\n        ImportError: If the 'deskew' library is not installed.\n    \"\"\"\n    if not DESKEW_AVAILABLE:\n        raise ImportError(\n            \"Deskew library not found. Install with: pip install natural-pdf[deskew]\"\n        )\n\n    # Determine the angle to use\n    rotation_angle = angle\n    if rotation_angle is None:\n        # Detect angle (or use cached) if not explicitly provided\n        rotation_angle = self.detect_skew_angle(\n            resolution=detection_resolution, **deskew_kwargs\n        )\n\n    logger.debug(\n        f\"Page {self.number}: Preparing to deskew (output resolution={resolution} DPI). Using angle: {rotation_angle}\"\n    )\n\n    try:\n        # Render the original page at the desired output resolution\n        # Use render() for clean image without highlights\n        img = self.render(resolution=resolution)\n        if not img:\n            logger.error(f\"Page {self.number}: Failed to render image for deskewing.\")\n            return None\n\n        # Rotate if a significant angle was found/provided\n        if rotation_angle is not None and abs(rotation_angle) &gt; 0.05:\n            logger.debug(f\"Page {self.number}: Rotating by {rotation_angle:.2f} degrees.\")\n            # Determine fill color based on image mode\n            fill = (255, 255, 255) if img.mode == \"RGB\" else 255  # White background\n            # Rotate the image using PIL\n            rotated_img = img.rotate(\n                rotation_angle,  # deskew provides angle, PIL rotates counter-clockwise\n                resample=Image.Resampling.BILINEAR,\n                expand=True,  # Expand image to fit rotated content\n                fillcolor=fill,\n            )\n            return rotated_img\n        else:\n            logger.debug(\n                f\"Page {self.number}: No significant rotation needed (angle={rotation_angle}). Returning original render.\"\n            )\n            return img  # Return the original rendered image if no rotation needed\n\n    except Exception as e:\n        logger.error(\n            f\"Page {self.number}: Error during deskewing image generation: {e}\", exc_info=True\n        )\n        return None\n</code></pre> <code>natural_pdf.Page.detect_skew_angle(resolution=72, grayscale=True, force_recalculate=False, **deskew_kwargs)</code> <p>Detects the skew angle of the page image and stores it.</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>int</code> <p>DPI resolution for rendering the page image for detection.</p> <code>72</code> <code>grayscale</code> <code>bool</code> <p>Whether to convert the image to grayscale before detection.</p> <code>True</code> <code>force_recalculate</code> <code>bool</code> <p>If True, recalculate even if an angle exists.</p> <code>False</code> <code>**deskew_kwargs</code> <p>Additional keyword arguments passed to <code>deskew.determine_skew</code>              (e.g., <code>max_angle</code>, <code>num_peaks</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>The detected skew angle in degrees, or None if detection failed.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If the 'deskew' library is not installed.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def detect_skew_angle(\n    self,\n    resolution: int = 72,\n    grayscale: bool = True,\n    force_recalculate: bool = False,\n    **deskew_kwargs,\n) -&gt; Optional[float]:\n    \"\"\"\n    Detects the skew angle of the page image and stores it.\n\n    Args:\n        resolution: DPI resolution for rendering the page image for detection.\n        grayscale: Whether to convert the image to grayscale before detection.\n        force_recalculate: If True, recalculate even if an angle exists.\n        **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                         (e.g., `max_angle`, `num_peaks`).\n\n    Returns:\n        The detected skew angle in degrees, or None if detection failed.\n\n    Raises:\n        ImportError: If the 'deskew' library is not installed.\n    \"\"\"\n    if not DESKEW_AVAILABLE:\n        raise ImportError(\n            \"Deskew library not found. Install with: pip install natural-pdf[deskew]\"\n        )\n\n    if self._skew_angle is not None and not force_recalculate:\n        logger.debug(f\"Page {self.number}: Returning cached skew angle: {self._skew_angle:.2f}\")\n        return self._skew_angle\n\n    logger.debug(f\"Page {self.number}: Detecting skew angle (resolution={resolution} DPI)...\")\n    try:\n        # Render the page at the specified detection resolution\n        # Use render() for clean image without highlights\n        img = self.render(resolution=resolution)\n        if not img:\n            logger.warning(f\"Page {self.number}: Failed to render image for skew detection.\")\n            self._skew_angle = None\n            return None\n\n        # Convert to numpy array\n        img_np = np.array(img)\n\n        # Convert to grayscale if needed\n        if grayscale:\n            if len(img_np.shape) == 3 and img_np.shape[2] &gt;= 3:\n                gray_np = np.mean(img_np[:, :, :3], axis=2).astype(np.uint8)\n            elif len(img_np.shape) == 2:\n                gray_np = img_np  # Already grayscale\n            else:\n                logger.warning(\n                    f\"Page {self.number}: Unexpected image shape {img_np.shape} for grayscale conversion.\"\n                )\n                gray_np = img_np  # Try using it anyway\n        else:\n            gray_np = img_np  # Use original if grayscale=False\n\n        # Determine skew angle using the deskew library\n        angle = determine_skew(gray_np, **deskew_kwargs)\n        self._skew_angle = angle\n        logger.debug(f\"Page {self.number}: Detected skew angle = {angle}\")\n        return angle\n\n    except Exception as e:\n        logger.warning(f\"Page {self.number}: Failed during skew detection: {e}\", exc_info=True)\n        self._skew_angle = None\n        return None\n</code></pre> <code>natural_pdf.Page.extract_ocr_elements(engine=None, options=None, languages=None, min_confidence=None, device=None, resolution=None)</code> <p>Extract text elements using OCR without adding them to the page's elements. Uses the shared OCRManager instance.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Optional[str]</code> <p>Name of the OCR engine.</p> <code>None</code> <code>options</code> <code>Optional[OCROptions]</code> <p>Engine-specific options object or dict.</p> <code>None</code> <code>languages</code> <code>Optional[List[str]]</code> <p>List of engine-specific language codes.</p> <code>None</code> <code>min_confidence</code> <code>Optional[float]</code> <p>Minimum confidence threshold.</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to run OCR on.</p> <code>None</code> <code>resolution</code> <code>Optional[int]</code> <p>DPI resolution for rendering page image before OCR.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[TextElement]</code> <p>List of created TextElement objects derived from OCR results for this page.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def extract_ocr_elements(\n    self,\n    engine: Optional[str] = None,\n    options: Optional[\"OCROptions\"] = None,\n    languages: Optional[List[str]] = None,\n    min_confidence: Optional[float] = None,\n    device: Optional[str] = None,\n    resolution: Optional[int] = None,\n) -&gt; List[\"TextElement\"]:\n    \"\"\"\n    Extract text elements using OCR *without* adding them to the page's elements.\n    Uses the shared OCRManager instance.\n\n    Args:\n        engine: Name of the OCR engine.\n        options: Engine-specific options object or dict.\n        languages: List of engine-specific language codes.\n        min_confidence: Minimum confidence threshold.\n        device: Device to run OCR on.\n        resolution: DPI resolution for rendering page image before OCR.\n\n    Returns:\n        List of created TextElement objects derived from OCR results for this page.\n    \"\"\"\n    if not self._ocr_manager:\n        logger.error(\n            f\"Page {self.number}: OCRManager not available. Cannot extract OCR elements.\"\n        )\n        return []\n\n    logger.info(f\"Page {self.number}: Extracting OCR elements (extract only)...\")\n\n    # Determine rendering resolution\n    final_resolution = resolution if resolution is not None else 150  # Default to 150 DPI\n    logger.debug(f\"  Using rendering resolution: {final_resolution} DPI\")\n\n    try:\n        # Get base image without highlights using the determined resolution\n        # Use the global PDF rendering lock\n        with pdf_render_lock:\n            # Use render() for clean image without highlights\n            image = self.render(resolution=final_resolution)\n            if not image:\n                logger.error(\n                    f\"  Failed to render page {self.number} to image for OCR extraction.\"\n                )\n                return []\n            logger.debug(f\"  Rendered image size: {image.width}x{image.height}\")\n    except Exception as e:\n        logger.error(f\"  Failed to render page {self.number} to image: {e}\", exc_info=True)\n        return []\n\n    # Prepare arguments for the OCR Manager call\n    manager_args = {\n        \"images\": image,\n        \"engine\": engine,\n        \"languages\": languages,\n        \"min_confidence\": min_confidence,\n        \"device\": device,\n        \"options\": options,\n    }\n    manager_args = {k: v for k, v in manager_args.items() if v is not None}\n\n    logger.debug(\n        f\"  Calling OCR Manager (extract only) with args: { {k:v for k,v in manager_args.items() if k != 'images'} }\"\n    )\n    try:\n        # apply_ocr now returns List[List[Dict]] or List[Dict]\n        results_list = self._ocr_manager.apply_ocr(**manager_args)\n        # If it returned a list of lists (batch mode), take the first list\n        results = (\n            results_list[0]\n            if isinstance(results_list, list)\n            and results_list\n            and isinstance(results_list[0], list)\n            else results_list\n        )\n        if not isinstance(results, list):\n            logger.error(f\"  OCR Manager returned unexpected type: {type(results)}\")\n            results = []\n        logger.info(f\"  OCR Manager returned {len(results)} results for extraction.\")\n    except Exception as e:\n        logger.error(f\"  OCR processing failed during extraction: {e}\", exc_info=True)\n        return []\n\n    # Convert results but DO NOT add to ElementManager\n    logger.debug(f\"  Converting OCR results to TextElements (extract only)...\")\n    temp_elements = []\n    scale_x = self.width / image.width if image.width else 1\n    scale_y = self.height / image.height if image.height else 1\n    for result in results:\n        try:  # Added try-except around result processing\n            x0, top, x1, bottom = [float(c) for c in result[\"bbox\"]]\n            elem_data = {\n                \"text\": result[\"text\"],\n                \"confidence\": result[\"confidence\"],\n                \"x0\": x0 * scale_x,\n                \"top\": top * scale_y,\n                \"x1\": x1 * scale_x,\n                \"bottom\": bottom * scale_y,\n                \"width\": (x1 - x0) * scale_x,\n                \"height\": (bottom - top) * scale_y,\n                \"object_type\": \"text\",  # Using text for temporary elements\n                \"source\": \"ocr\",\n                \"fontname\": \"OCR-extract\",  # Different name for clarity\n                \"size\": 10.0,\n                \"page_number\": self.number,\n            }\n            temp_elements.append(TextElement(elem_data, self))\n        except (KeyError, ValueError, TypeError) as convert_err:\n            logger.warning(\n                f\"  Skipping invalid OCR result during conversion: {result}. Error: {convert_err}\"\n            )\n\n    logger.info(f\"  Created {len(temp_elements)} TextElements from OCR (extract only).\")\n    return temp_elements\n</code></pre> <code>natural_pdf.Page.extract_table(method=None, table_settings=None, use_ocr=False, ocr_config=None, text_options=None, cell_extraction_func=None, show_progress=False, content_filter=None)</code> <p>Extract the largest table from this page using enhanced region-based extraction.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Optional[str]</code> <p>Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).</p> <code>None</code> <code>table_settings</code> <code>Optional[dict]</code> <p>Settings for pdfplumber table extraction.</p> <code>None</code> <code>use_ocr</code> <code>bool</code> <p>Whether to use OCR for text extraction (currently only applicable with 'tatr' method).</p> <code>False</code> <code>ocr_config</code> <code>Optional[dict]</code> <p>OCR configuration parameters.</p> <code>None</code> <code>text_options</code> <code>Optional[Dict]</code> <p>Dictionary of options for the 'text' method.</p> <code>None</code> <code>cell_extraction_func</code> <code>Optional[Callable[[Region], Optional[str]]]</code> <p>Optional callable function that takes a cell Region object                   and returns its string content. For 'text' method only.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>If True, display a progress bar during cell text extraction for the 'text' method.</p> <code>False</code> <code>content_filter</code> <p>Optional content filter to apply during cell text extraction. Can be: - A regex pattern string (characters matching the pattern are EXCLUDED) - A callable that takes text and returns True to KEEP the character - A list of regex patterns (characters matching ANY pattern are EXCLUDED)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[Optional[str]]]</code> <p>Table data as a list of rows, where each row is a list of cell values (str or None).</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def extract_table(\n    self,\n    method: Optional[str] = None,\n    table_settings: Optional[dict] = None,\n    use_ocr: bool = False,\n    ocr_config: Optional[dict] = None,\n    text_options: Optional[Dict] = None,\n    cell_extraction_func: Optional[Callable[[\"Region\"], Optional[str]]] = None,\n    show_progress: bool = False,\n    content_filter=None,\n) -&gt; List[List[Optional[str]]]:\n    \"\"\"\n    Extract the largest table from this page using enhanced region-based extraction.\n\n    Args:\n        method: Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).\n        table_settings: Settings for pdfplumber table extraction.\n        use_ocr: Whether to use OCR for text extraction (currently only applicable with 'tatr' method).\n        ocr_config: OCR configuration parameters.\n        text_options: Dictionary of options for the 'text' method.\n        cell_extraction_func: Optional callable function that takes a cell Region object\n                              and returns its string content. For 'text' method only.\n        show_progress: If True, display a progress bar during cell text extraction for the 'text' method.\n        content_filter: Optional content filter to apply during cell text extraction. Can be:\n            - A regex pattern string (characters matching the pattern are EXCLUDED)\n            - A callable that takes text and returns True to KEEP the character\n            - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n\n    Returns:\n        Table data as a list of rows, where each row is a list of cell values (str or None).\n    \"\"\"\n    # Create a full-page region and delegate to its enhanced extract_table method\n    page_region = self.create_region(0, 0, self.width, self.height)\n    return page_region.extract_table(\n        method=method,\n        table_settings=table_settings,\n        use_ocr=use_ocr,\n        ocr_config=ocr_config,\n        text_options=text_options,\n        cell_extraction_func=cell_extraction_func,\n        show_progress=show_progress,\n        content_filter=content_filter,\n    )\n</code></pre> <code>natural_pdf.Page.extract_tables(method=None, table_settings=None, check_tatr=True)</code> <p>Extract all tables from this page with enhanced method support.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Optional[str]</code> <p>Method to use: 'pdfplumber', 'stream', 'lattice', or None (auto-detect).     'stream' uses text-based strategies, 'lattice' uses line-based strategies.     Note: 'tatr' and 'text' methods are not supported for extract_tables.</p> <code>None</code> <code>table_settings</code> <code>Optional[dict]</code> <p>Settings for pdfplumber table extraction.</p> <code>None</code> <code>check_tatr</code> <code>bool</code> <p>If True (default), first check for TATR-detected table regions         and extract from those before falling back to pdfplumber methods.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[List[List[str]]]</code> <p>List of tables, where each table is a list of rows, and each row is a list of cell values.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def extract_tables(\n    self,\n    method: Optional[str] = None,\n    table_settings: Optional[dict] = None,\n    check_tatr: bool = True,\n) -&gt; List[List[List[str]]]:\n    \"\"\"\n    Extract all tables from this page with enhanced method support.\n\n    Args:\n        method: Method to use: 'pdfplumber', 'stream', 'lattice', or None (auto-detect).\n                'stream' uses text-based strategies, 'lattice' uses line-based strategies.\n                Note: 'tatr' and 'text' methods are not supported for extract_tables.\n        table_settings: Settings for pdfplumber table extraction.\n        check_tatr: If True (default), first check for TATR-detected table regions\n                    and extract from those before falling back to pdfplumber methods.\n\n    Returns:\n        List of tables, where each table is a list of rows, and each row is a list of cell values.\n    \"\"\"\n    if table_settings is None:\n        table_settings = {}\n\n    # Check for TATR-detected table regions first if enabled\n    if check_tatr:\n        try:\n            tatr_tables = self.find_all(\"region[type=table][model=tatr]\")\n            if tatr_tables:\n                logger.debug(\n                    f\"Page {self.number}: Found {len(tatr_tables)} TATR table regions, extracting from those...\"\n                )\n                extracted_tables = []\n                for table_region in tatr_tables:\n                    try:\n                        table_data = table_region.extract_table(method=\"tatr\")\n                        if table_data:  # Only add non-empty tables\n                            extracted_tables.append(table_data)\n                    except Exception as e:\n                        logger.warning(\n                            f\"Failed to extract table from TATR region {table_region.bbox}: {e}\"\n                        )\n\n                if extracted_tables:\n                    logger.debug(\n                        f\"Page {self.number}: Successfully extracted {len(extracted_tables)} tables from TATR regions\"\n                    )\n                    return extracted_tables\n                else:\n                    logger.debug(\n                        f\"Page {self.number}: TATR regions found but no tables extracted, falling back to pdfplumber\"\n                    )\n            else:\n                logger.debug(\n                    f\"Page {self.number}: No TATR table regions found, using pdfplumber methods\"\n                )\n        except Exception as e:\n            logger.debug(\n                f\"Page {self.number}: Error checking TATR regions: {e}, falling back to pdfplumber\"\n            )\n\n    # Auto-detect method if not specified (try lattice first, then stream)\n    if method is None:\n        logger.debug(f\"Page {self.number}: Auto-detecting tables extraction method...\")\n\n        # Try lattice first\n        try:\n            lattice_settings = table_settings.copy()\n            lattice_settings.setdefault(\"vertical_strategy\", \"lines\")\n            lattice_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n            logger.debug(f\"Page {self.number}: Trying 'lattice' method first for tables...\")\n            lattice_result = self._page.extract_tables(lattice_settings)\n\n            # Check if lattice found meaningful tables\n            if (\n                lattice_result\n                and len(lattice_result) &gt; 0\n                and any(\n                    any(\n                        any(cell and cell.strip() for cell in row if cell)\n                        for row in table\n                        if table\n                    )\n                    for table in lattice_result\n                )\n            ):\n                logger.debug(\n                    f\"Page {self.number}: 'lattice' method found {len(lattice_result)} tables\"\n                )\n                return lattice_result\n            else:\n                logger.debug(f\"Page {self.number}: 'lattice' method found no meaningful tables\")\n\n        except Exception as e:\n            logger.debug(f\"Page {self.number}: 'lattice' method failed: {e}\")\n\n        # Fall back to stream\n        logger.debug(f\"Page {self.number}: Falling back to 'stream' method for tables...\")\n        stream_settings = table_settings.copy()\n        stream_settings.setdefault(\"vertical_strategy\", \"text\")\n        stream_settings.setdefault(\"horizontal_strategy\", \"text\")\n\n        return self._page.extract_tables(stream_settings)\n\n    effective_method = method\n\n    # Handle method aliases\n    if effective_method == \"stream\":\n        logger.debug(\"Using 'stream' method alias for 'pdfplumber' with text-based strategies.\")\n        effective_method = \"pdfplumber\"\n        table_settings.setdefault(\"vertical_strategy\", \"text\")\n        table_settings.setdefault(\"horizontal_strategy\", \"text\")\n    elif effective_method == \"lattice\":\n        logger.debug(\n            \"Using 'lattice' method alias for 'pdfplumber' with line-based strategies.\"\n        )\n        effective_method = \"pdfplumber\"\n        table_settings.setdefault(\"vertical_strategy\", \"lines\")\n        table_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n    # Use the selected method\n    if effective_method == \"pdfplumber\":\n        # ---------------------------------------------------------\n        # Inject auto-computed or user-specified text tolerances so\n        # pdfplumber uses the same numbers we used for word grouping\n        # whenever the table algorithm relies on word positions.\n        # ---------------------------------------------------------\n        if \"text\" in (\n            table_settings.get(\"vertical_strategy\"),\n            table_settings.get(\"horizontal_strategy\"),\n        ):\n            print(\"SETTING IT UP\")\n            pdf_cfg = getattr(self, \"_config\", getattr(self._parent, \"_config\", {}))\n            if \"text_x_tolerance\" not in table_settings and \"x_tolerance\" not in table_settings:\n                x_tol = pdf_cfg.get(\"x_tolerance\")\n                if x_tol is not None:\n                    table_settings.setdefault(\"text_x_tolerance\", x_tol)\n            if \"text_y_tolerance\" not in table_settings and \"y_tolerance\" not in table_settings:\n                y_tol = pdf_cfg.get(\"y_tolerance\")\n                if y_tol is not None:\n                    table_settings.setdefault(\"text_y_tolerance\", y_tol)\n\n            # pdfplumber's text strategy benefits from a tight snap tolerance.\n            if (\n                \"snap_tolerance\" not in table_settings\n                and \"snap_x_tolerance\" not in table_settings\n            ):\n                # Derive from y_tol if available, else default 1\n                snap = max(1, round((pdf_cfg.get(\"y_tolerance\", 1)) * 0.9))\n                table_settings.setdefault(\"snap_tolerance\", snap)\n            if (\n                \"join_tolerance\" not in table_settings\n                and \"join_x_tolerance\" not in table_settings\n            ):\n                join = table_settings.get(\"snap_tolerance\", 1)\n                table_settings.setdefault(\"join_tolerance\", join)\n                table_settings.setdefault(\"join_x_tolerance\", join)\n                table_settings.setdefault(\"join_y_tolerance\", join)\n\n        raw_tables = self._page.extract_tables(table_settings)\n\n        # Apply RTL text processing to all extracted tables\n        if raw_tables:\n            processed_tables = []\n            for table in raw_tables:\n                processed_table = []\n                for row in table:\n                    processed_row = []\n                    for cell in row:\n                        if cell is not None:\n                            # Apply RTL text processing to each cell\n                            rtl_processed_cell = self._apply_rtl_processing_to_text(cell)\n                            processed_row.append(rtl_processed_cell)\n                        else:\n                            processed_row.append(cell)\n                    processed_table.append(processed_row)\n                processed_tables.append(processed_table)\n            return processed_tables\n\n        return raw_tables\n    else:\n        raise ValueError(\n            f\"Unknown tables extraction method: '{method}'. Choose from 'pdfplumber', 'stream', 'lattice'.\"\n        )\n</code></pre> <code>natural_pdf.Page.extract_text(preserve_whitespace=True, use_exclusions=True, debug_exclusions=False, content_filter=None, **kwargs)</code> <p>Extract text from this page, respecting exclusions and using pdfplumber's layout engine (chars_to_textmap) if layout arguments are provided or default.</p> <p>Parameters:</p> Name Type Description Default <code>use_exclusions</code> <p>Whether to apply exclusion regions (default: True).           Note: Filtering logic is now always applied if exclusions exist.</p> <code>True</code> <code>debug_exclusions</code> <p>Whether to output detailed exclusion debugging info (default: False).</p> <code>False</code> <code>content_filter</code> <p>Optional content filter to exclude specific text patterns. Can be: - A regex pattern string (characters matching the pattern are EXCLUDED) - A callable that takes text and returns True to KEEP the character - A list of regex patterns (characters matching ANY pattern are EXCLUDED)</p> <code>None</code> <code>**kwargs</code> <p>Additional layout parameters passed directly to pdfplumber's       <code>chars_to_textmap</code> function. Common parameters include:       - layout (bool): If True (default), inserts spaces/newlines.       - x_density (float): Pixels per character horizontally.       - y_density (float): Pixels per line vertically.       - x_tolerance (float): Tolerance for horizontal character grouping.       - y_tolerance (float): Tolerance for vertical character grouping.       - line_dir (str): 'ttb', 'btt', 'ltr', 'rtl'       - char_dir (str): 'ttb', 'btt', 'ltr', 'rtl'       See pdfplumber documentation for more.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Extracted text as string, potentially with layout-based spacing.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def extract_text(\n    self,\n    preserve_whitespace=True,\n    use_exclusions=True,\n    debug_exclusions=False,\n    content_filter=None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"\n    Extract text from this page, respecting exclusions and using pdfplumber's\n    layout engine (chars_to_textmap) if layout arguments are provided or default.\n\n    Args:\n        use_exclusions: Whether to apply exclusion regions (default: True).\n                      Note: Filtering logic is now always applied if exclusions exist.\n        debug_exclusions: Whether to output detailed exclusion debugging info (default: False).\n        content_filter: Optional content filter to exclude specific text patterns. Can be:\n            - A regex pattern string (characters matching the pattern are EXCLUDED)\n            - A callable that takes text and returns True to KEEP the character\n            - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n        **kwargs: Additional layout parameters passed directly to pdfplumber's\n                  `chars_to_textmap` function. Common parameters include:\n                  - layout (bool): If True (default), inserts spaces/newlines.\n                  - x_density (float): Pixels per character horizontally.\n                  - y_density (float): Pixels per line vertically.\n                  - x_tolerance (float): Tolerance for horizontal character grouping.\n                  - y_tolerance (float): Tolerance for vertical character grouping.\n                  - line_dir (str): 'ttb', 'btt', 'ltr', 'rtl'\n                  - char_dir (str): 'ttb', 'btt', 'ltr', 'rtl'\n                  See pdfplumber documentation for more.\n\n    Returns:\n        Extracted text as string, potentially with layout-based spacing.\n    \"\"\"\n    logger.debug(f\"Page {self.number}: extract_text called with kwargs: {kwargs}\")\n    debug = kwargs.get(\"debug\", debug_exclusions)  # Allow 'debug' kwarg\n\n    # 1. Get Word Elements (triggers load_elements if needed)\n    word_elements = self.words\n    if not word_elements:\n        logger.debug(f\"Page {self.number}: No word elements found.\")\n        return \"\"\n\n    # 2. Apply element-based exclusions if enabled\n    if use_exclusions and self._exclusions:\n        # Filter word elements through _filter_elements_by_exclusions\n        # This handles both element-based and region-based exclusions\n        word_elements = self._filter_elements_by_exclusions(\n            word_elements, debug_exclusions=debug\n        )\n        if debug:\n            logger.debug(\n                f\"Page {self.number}: {len(word_elements)} words remaining after exclusion filtering.\"\n            )\n\n    # 3. Get region-based exclusions for spatial filtering\n    apply_exclusions_flag = kwargs.get(\"use_exclusions\", use_exclusions)\n    exclusion_regions = []\n    if apply_exclusions_flag and self._exclusions:\n        exclusion_regions = self._get_exclusion_regions(include_callable=True, debug=debug)\n        if debug:\n            logger.debug(\n                f\"Page {self.number}: Found {len(exclusion_regions)} region exclusions for spatial filtering.\"\n            )\n    elif debug:\n        logger.debug(f\"Page {self.number}: Not applying exclusions.\")\n\n    # 4. Collect All Character Dictionaries from remaining Word Elements\n    all_char_dicts = []\n    for word in word_elements:\n        all_char_dicts.extend(getattr(word, \"_char_dicts\", []))\n\n    # 5. Spatially Filter Characters (only by regions, elements already filtered above)\n    filtered_chars = filter_chars_spatially(\n        char_dicts=all_char_dicts,\n        exclusion_regions=exclusion_regions,\n        target_region=None,  # No target region for full page extraction\n        debug=debug,\n    )\n\n    # 5. Generate Text Layout using Utility\n    # Pass page bbox as layout context\n    page_bbox = (0, 0, self.width, self.height)\n    # Merge PDF-level default tolerances if caller did not override\n    merged_kwargs = dict(kwargs)\n    tol_keys = [\"x_tolerance\", \"x_tolerance_ratio\", \"y_tolerance\"]\n    for k in tol_keys:\n        if k not in merged_kwargs:\n            if k in self._config:\n                merged_kwargs[k] = self._config[k]\n            elif k in getattr(self._parent, \"_config\", {}):\n                merged_kwargs[k] = self._parent._config[k]\n\n    # Add content_filter to kwargs if provided\n    if content_filter is not None:\n        merged_kwargs[\"content_filter\"] = content_filter\n\n    result = generate_text_layout(\n        char_dicts=filtered_chars,\n        layout_context_bbox=page_bbox,\n        user_kwargs=merged_kwargs,\n    )\n\n    # --- Optional: apply Unicode BiDi algorithm for mixed RTL/LTR correctness ---\n    apply_bidi = kwargs.get(\"bidi\", True)\n    if apply_bidi and result:\n        # Quick check for any RTL character\n        import unicodedata\n\n        def _contains_rtl(s):\n            return any(unicodedata.bidirectional(ch) in (\"R\", \"AL\", \"AN\") for ch in s)\n\n        if _contains_rtl(result):\n            try:\n                from bidi.algorithm import get_display  # type: ignore\n\n                from natural_pdf.utils.bidi_mirror import mirror_brackets\n\n                result = \"\\n\".join(\n                    mirror_brackets(\n                        get_display(\n                            line,\n                            base_dir=(\n                                \"R\"\n                                if any(\n                                    unicodedata.bidirectional(ch) in (\"R\", \"AL\", \"AN\")\n                                    for ch in line\n                                )\n                                else \"L\"\n                            ),\n                        )\n                    )\n                    for line in result.split(\"\\n\")\n                )\n            except ModuleNotFoundError:\n                pass  # silently skip if python-bidi not available\n\n    logger.debug(f\"Page {self.number}: extract_text finished, result length: {len(result)}.\")\n    return result\n</code></pre> <code>natural_pdf.Page.filter_elements(elements, selector, **kwargs)</code> <p>Filter a list of elements based on a selector.</p> <p>Parameters:</p> Name Type Description Default <code>elements</code> <code>List[Element]</code> <p>List of elements to filter</p> required <code>selector</code> <code>str</code> <p>CSS-like selector string</p> required <code>**kwargs</code> <p>Additional filter parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Element]</code> <p>List of elements that match the selector</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def filter_elements(\n    self, elements: List[\"Element\"], selector: str, **kwargs\n) -&gt; List[\"Element\"]:\n    \"\"\"\n    Filter a list of elements based on a selector.\n\n    Args:\n        elements: List of elements to filter\n        selector: CSS-like selector string\n        **kwargs: Additional filter parameters\n\n    Returns:\n        List of elements that match the selector\n    \"\"\"\n    from natural_pdf.selectors.parser import parse_selector, selector_to_filter_func\n\n    # Parse the selector\n    selector_obj = parse_selector(selector)\n\n    # Create filter function from selector\n    filter_func = selector_to_filter_func(selector_obj, **kwargs)\n\n    # Apply the filter to the elements\n    matching_elements = [element for element in elements if filter_func(element)]\n\n    # Sort elements in reading order if requested\n    if kwargs.get(\"reading_order\", True):\n        if all(hasattr(el, \"top\") and hasattr(el, \"x0\") for el in matching_elements):\n            matching_elements.sort(key=lambda el: (el.top, el.x0))\n        else:\n            logger.warning(\n                \"Cannot sort elements in reading order: Missing required attributes (top, x0).\"\n            )\n\n    return matching_elements\n</code></pre> <code>natural_pdf.Page.find(selector=None, *, text=None, apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find(*, text: str, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[Any]\n</code></pre><pre><code>find(selector: str, *, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[Any]\n</code></pre> <p>Find first element on this page matching selector OR text content.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional filter parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Element object or None if not found.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def find(\n    self,\n    selector: Optional[str] = None,  # Now optional\n    *,  # Force subsequent args to be keyword-only\n    text: Optional[str] = None,  # New text parameter\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; Optional[Any]:\n    \"\"\"\n    Find first element on this page matching selector OR text content.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional filter parameters.\n\n    Returns:\n        Element object or None if not found.\n    \"\"\"\n    if selector is not None and text is not None:\n        raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n    if selector is None and text is None:\n        raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n    # Construct selector if 'text' is provided\n    effective_selector = \"\"\n    if text is not None:\n        # Escape quotes within the text for the selector string\n        escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n        # Default to 'text:contains(...)'\n        effective_selector = f'text:contains(\"{escaped_text}\")'\n        # Note: regex/case handled by kwargs passed down\n        logger.debug(\n            f\"Using text shortcut: find(text='{text}') -&gt; find('{effective_selector}')\"\n        )\n    elif selector is not None:\n        effective_selector = selector\n    else:\n        # Should be unreachable due to checks above\n        raise ValueError(\"Internal error: No selector or text provided.\")\n\n    selector_obj = parse_selector(effective_selector)\n\n    # Pass regex and case flags to selector function via kwargs\n    kwargs[\"regex\"] = regex\n    kwargs[\"case\"] = case\n\n    # First get all matching elements without applying exclusions initially within _apply_selector\n    results_collection = self._apply_selector(\n        selector_obj, **kwargs\n    )  # _apply_selector doesn't filter\n\n    # Filter the results based on exclusions if requested\n    if apply_exclusions and self._exclusions and results_collection:\n        filtered_elements = self._filter_elements_by_exclusions(results_collection.elements)\n        # Return the first element from the filtered list\n        return filtered_elements[0] if filtered_elements else None\n    elif results_collection:\n        # Return the first element from the unfiltered results\n        return results_collection.first\n    else:\n        return None\n</code></pre> <code>natural_pdf.Page.find_all(selector=None, *, text=None, apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find_all(*, text: str, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre><pre><code>find_all(selector: str, *, apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre> <p>Find all elements on this page matching selector OR text content.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional filter parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection with matching elements.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def find_all(\n    self,\n    selector: Optional[str] = None,  # Now optional\n    *,  # Force subsequent args to be keyword-only\n    text: Optional[str] = None,  # New text parameter\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Find all elements on this page matching selector OR text content.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional filter parameters.\n\n    Returns:\n        ElementCollection with matching elements.\n    \"\"\"\n    from natural_pdf.elements.element_collection import (  # Import here for type hint\n        ElementCollection,\n    )\n\n    if selector is not None and text is not None:\n        raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n    if selector is None and text is None:\n        raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n    # Construct selector if 'text' is provided\n    effective_selector = \"\"\n    if text is not None:\n        # Escape quotes within the text for the selector string\n        escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n        # Default to 'text:contains(...)'\n        effective_selector = f'text:contains(\"{escaped_text}\")'\n        logger.debug(\n            f\"Using text shortcut: find_all(text='{text}') -&gt; find_all('{effective_selector}')\"\n        )\n    elif selector is not None:\n        effective_selector = selector\n    else:\n        # Should be unreachable due to checks above\n        raise ValueError(\"Internal error: No selector or text provided.\")\n\n    selector_obj = parse_selector(effective_selector)\n\n    # Pass regex and case flags to selector function via kwargs\n    kwargs[\"regex\"] = regex\n    kwargs[\"case\"] = case\n\n    # First get all matching elements without applying exclusions initially within _apply_selector\n    results_collection = self._apply_selector(\n        selector_obj, **kwargs\n    )  # _apply_selector doesn't filter\n\n    # Filter the results based on exclusions if requested\n    if apply_exclusions and self._exclusions and results_collection:\n        filtered_elements = self._filter_elements_by_exclusions(results_collection.elements)\n        return ElementCollection(filtered_elements)\n    else:\n        # Return the unfiltered collection\n        return results_collection\n</code></pre> <code>natural_pdf.Page.get_content()</code> <p>Returns the primary content object (self) for indexing (required by Indexable protocol). SearchService implementations decide how to process this (e.g., call extract_text).</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def get_content(self) -&gt; \"Page\":\n    \"\"\"\n    Returns the primary content object (self) for indexing (required by Indexable protocol).\n    SearchService implementations decide how to process this (e.g., call extract_text).\n    \"\"\"\n    return self  # Return the Page object itself\n</code></pre> <code>natural_pdf.Page.get_content_hash()</code> <p>Returns a SHA256 hash of the extracted text content (required by Indexable for sync).</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def get_content_hash(self) -&gt; str:\n    \"\"\"Returns a SHA256 hash of the extracted text content (required by Indexable for sync).\"\"\"\n    # Hash the extracted text (without exclusions for consistency)\n    # Consider if exclusions should be part of the hash? For now, hash raw text.\n    # Using extract_text directly might be slow if called repeatedly. Cache? TODO: Optimization\n    text_content = self.extract_text(\n        use_exclusions=False, preserve_whitespace=False\n    )  # Normalize whitespace?\n    return hashlib.sha256(text_content.encode(\"utf-8\")).hexdigest()\n</code></pre> <code>natural_pdf.Page.get_elements(apply_exclusions=True, debug_exclusions=False)</code> <p>Get all elements on this page.</p> <p>Parameters:</p> Name Type Description Default <code>apply_exclusions</code> <p>Whether to apply exclusion regions (default: True).</p> <code>True</code> <code>debug_exclusions</code> <code>bool</code> <p>Whether to output detailed exclusion debugging info (default: False).</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Element]</code> <p>List of all elements on the page, potentially filtered by exclusions.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def get_elements(\n    self, apply_exclusions=True, debug_exclusions: bool = False\n) -&gt; List[\"Element\"]:\n    \"\"\"\n    Get all elements on this page.\n\n    Args:\n        apply_exclusions: Whether to apply exclusion regions (default: True).\n        debug_exclusions: Whether to output detailed exclusion debugging info (default: False).\n\n    Returns:\n        List of all elements on the page, potentially filtered by exclusions.\n    \"\"\"\n    # Get all elements from the element manager\n    all_elements = self._element_mgr.get_all_elements()\n\n    # Apply exclusions if requested\n    if apply_exclusions and self._exclusions:\n        return self._filter_elements_by_exclusions(\n            all_elements, debug_exclusions=debug_exclusions\n        )\n    else:\n        if debug_exclusions:\n            print(\n                f\"Page {self.index}: get_elements returning all {len(all_elements)} elements (exclusions not applied).\"\n            )\n        return all_elements\n</code></pre> <code>natural_pdf.Page.get_id()</code> <p>Returns a unique identifier for the page (required by Indexable protocol).</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def get_id(self) -&gt; str:\n    \"\"\"Returns a unique identifier for the page (required by Indexable protocol).\"\"\"\n    # Ensure path is safe for use in IDs (replace problematic chars)\n    safe_path = re.sub(r\"[^a-zA-Z0-9_-]\", \"_\", str(self.pdf.path))\n    return f\"pdf_{safe_path}_page_{self.page_number}\"\n</code></pre> <code>natural_pdf.Page.get_metadata()</code> <p>Returns metadata associated with the page (required by Indexable protocol).</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def get_metadata(self) -&gt; Dict[str, Any]:\n    \"\"\"Returns metadata associated with the page (required by Indexable protocol).\"\"\"\n    # Add content hash here for sync\n    metadata = {\n        \"pdf_path\": str(self.pdf.path),\n        \"page_number\": self.page_number,\n        \"width\": self.width,\n        \"height\": self.height,\n        \"content_hash\": self.get_content_hash(),  # Include the hash\n    }\n    return metadata\n</code></pre> <code>natural_pdf.Page.get_section_between(start_element=None, end_element=None, include_boundaries='both')</code> <p>Get a section between two elements on this page.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def get_section_between(\n    self, start_element=None, end_element=None, include_boundaries=\"both\"\n) -&gt; Optional[\"Region\"]:  # Return Optional\n    \"\"\"\n    Get a section between two elements on this page.\n    \"\"\"\n    # Create a full-page region to operate within\n    page_region = self.create_region(0, 0, self.width, self.height)\n\n    # Delegate to the region's method\n    try:\n        return page_region.get_section_between(\n            start_element=start_element,\n            end_element=end_element,\n            include_boundaries=include_boundaries,\n        )\n    except Exception as e:\n        logger.error(\n            f\"Error getting section between elements on page {self.index}: {e}\", exc_info=True\n        )\n        return None\n</code></pre> <code>natural_pdf.Page.get_sections(start_elements=None, end_elements=None, include_boundaries='start', y_threshold=5.0, bounding_box=None)</code> <p>Get sections of a page defined by start/end elements. Uses the page-level implementation.</p> <p>Returns:</p> Type Description <code>ElementCollection[Region]</code> <p>An ElementCollection containing the found Region objects.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def get_sections(\n    self,\n    start_elements=None,\n    end_elements=None,\n    include_boundaries=\"start\",\n    y_threshold=5.0,\n    bounding_box=None,\n) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Get sections of a page defined by start/end elements.\n    Uses the page-level implementation.\n\n    Returns:\n        An ElementCollection containing the found Region objects.\n    \"\"\"\n\n    # Helper function to get bounds from bounding_box parameter\n    def get_bounds():\n        if bounding_box:\n            x0, top, x1, bottom = bounding_box\n            # Clamp to page boundaries\n            return max(0, x0), max(0, top), min(self.width, x1), min(self.height, bottom)\n        else:\n            return 0, 0, self.width, self.height\n\n    regions = []\n\n    # Handle cases where elements are provided as strings (selectors)\n    if isinstance(start_elements, str):\n        start_elements = self.find_all(start_elements).elements  # Get list of elements\n    elif hasattr(start_elements, \"elements\"):  # Handle ElementCollection input\n        start_elements = start_elements.elements\n\n    if isinstance(end_elements, str):\n        end_elements = self.find_all(end_elements).elements\n    elif hasattr(end_elements, \"elements\"):\n        end_elements = end_elements.elements\n\n    # Ensure start_elements is a list\n    if start_elements is None:\n        start_elements = []\n    if end_elements is None:\n        end_elements = []\n\n    valid_inclusions = [\"start\", \"end\", \"both\", \"none\"]\n    if include_boundaries not in valid_inclusions:\n        raise ValueError(f\"include_boundaries must be one of {valid_inclusions}\")\n\n    if not start_elements:\n        # Return an empty ElementCollection if no start elements\n        return ElementCollection([])\n\n    # Combine start and end elements with their type\n    all_boundaries = []\n    for el in start_elements:\n        all_boundaries.append((el, \"start\"))\n    for el in end_elements:\n        all_boundaries.append((el, \"end\"))\n\n    # Sort all boundary elements primarily by top, then x0\n    try:\n        all_boundaries.sort(key=lambda x: (x[0].top, x[0].x0))\n    except AttributeError as e:\n        logger.error(f\"Error sorting boundaries: Element missing top/x0 attribute? {e}\")\n        return ElementCollection([])  # Cannot proceed if elements lack position\n\n    # Process sorted boundaries to find sections\n    current_start_element = None\n    active_section_started = False\n\n    for element, element_type in all_boundaries:\n        if element_type == \"start\":\n            # If we have an active section, this start implicitly ends it\n            if active_section_started:\n                end_boundary_el = element  # Use this start as the end boundary\n                # Determine region boundaries\n                sec_top = (\n                    current_start_element.top\n                    if include_boundaries in [\"start\", \"both\"]\n                    else current_start_element.bottom\n                )\n                sec_bottom = (\n                    end_boundary_el.top\n                    if include_boundaries not in [\"end\", \"both\"]\n                    else end_boundary_el.bottom\n                )\n\n                if sec_top &lt; sec_bottom:  # Ensure valid region\n                    x0, _, x1, _ = get_bounds()\n                    region = self.create_region(x0, sec_top, x1, sec_bottom)\n                    region.start_element = current_start_element\n                    region.end_element = end_boundary_el  # Mark the element that ended it\n                    region.is_end_next_start = True  # Mark how it ended\n                    regions.append(region)\n                active_section_started = False  # Reset for the new start\n\n            # Set this as the potential start of the next section\n            current_start_element = element\n            active_section_started = True\n\n        elif element_type == \"end\" and active_section_started:\n            # We found an explicit end for the current section\n            end_boundary_el = element\n            sec_top = (\n                current_start_element.top\n                if include_boundaries in [\"start\", \"both\"]\n                else current_start_element.bottom\n            )\n            sec_bottom = (\n                end_boundary_el.bottom\n                if include_boundaries in [\"end\", \"both\"]\n                else end_boundary_el.top\n            )\n\n            if sec_top &lt; sec_bottom:  # Ensure valid region\n                x0, _, x1, _ = get_bounds()\n                region = self.create_region(x0, sec_top, x1, sec_bottom)\n                region.start_element = current_start_element\n                region.end_element = end_boundary_el\n                region.is_end_next_start = False\n                regions.append(region)\n\n            # Reset: section ended explicitly\n            current_start_element = None\n            active_section_started = False\n\n    # Handle the last section if it was started but never explicitly ended\n    if active_section_started:\n        sec_top = (\n            current_start_element.top\n            if include_boundaries in [\"start\", \"both\"]\n            else current_start_element.bottom\n        )\n        x0, _, x1, page_bottom = get_bounds()\n        if sec_top &lt; page_bottom:\n            region = self.create_region(x0, sec_top, x1, page_bottom)\n            region.start_element = current_start_element\n            region.end_element = None  # Ended by page end\n            region.is_end_next_start = False\n            regions.append(region)\n\n    return ElementCollection(regions)\n</code></pre> <code>natural_pdf.Page.highlight(bbox=None, color=None, label=None, use_color_cycling=False, element=None, annotate=None, existing='append')</code> <p>Highlight a bounding box or the entire page. Delegates to the central HighlightingService.</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>Optional[Tuple[float, float, float, float]]</code> <p>Bounding box (x0, top, x1, bottom). If None, highlight entire page.</p> <code>None</code> <code>color</code> <code>Optional[Union[Tuple, str]]</code> <p>RGBA color tuple/string for the highlight.</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>Optional label for the highlight.</p> <code>None</code> <code>use_color_cycling</code> <code>bool</code> <p>If True and no label/color, use next cycle color.</p> <code>False</code> <code>element</code> <code>Optional[Any]</code> <p>Optional original element being highlighted (for attribute extraction).</p> <code>None</code> <code>annotate</code> <code>Optional[List[str]]</code> <p>List of attribute names from 'element' to display.</p> <code>None</code> <code>existing</code> <code>str</code> <p>How to handle existing highlights ('append' or 'replace').</p> <code>'append'</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def highlight(\n    self,\n    bbox: Optional[Tuple[float, float, float, float]] = None,\n    color: Optional[Union[Tuple, str]] = None,\n    label: Optional[str] = None,\n    use_color_cycling: bool = False,\n    element: Optional[Any] = None,\n    annotate: Optional[List[str]] = None,\n    existing: str = \"append\",\n) -&gt; \"Page\":\n    \"\"\"\n    Highlight a bounding box or the entire page.\n    Delegates to the central HighlightingService.\n\n    Args:\n        bbox: Bounding box (x0, top, x1, bottom). If None, highlight entire page.\n        color: RGBA color tuple/string for the highlight.\n        label: Optional label for the highlight.\n        use_color_cycling: If True and no label/color, use next cycle color.\n        element: Optional original element being highlighted (for attribute extraction).\n        annotate: List of attribute names from 'element' to display.\n        existing: How to handle existing highlights ('append' or 'replace').\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    target_bbox = bbox if bbox is not None else (0, 0, self.width, self.height)\n    self._highlighter.add(\n        page_index=self.index,\n        bbox=target_bbox,\n        color=color,\n        label=label,\n        use_color_cycling=use_color_cycling,\n        element=element,\n        annotate=annotate,\n        existing=existing,\n    )\n    return self\n</code></pre> <code>natural_pdf.Page.highlight_polygon(polygon, color=None, label=None, use_color_cycling=False, element=None, annotate=None, existing='append')</code> <p>Highlight a polygon shape on the page. Delegates to the central HighlightingService.</p> <p>Parameters:</p> Name Type Description Default <code>polygon</code> <code>List[Tuple[float, float]]</code> <p>List of (x, y) points defining the polygon.</p> required <code>color</code> <code>Optional[Union[Tuple, str]]</code> <p>RGBA color tuple/string for the highlight.</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>Optional label for the highlight.</p> <code>None</code> <code>use_color_cycling</code> <code>bool</code> <p>If True and no label/color, use next cycle color.</p> <code>False</code> <code>element</code> <code>Optional[Any]</code> <p>Optional original element being highlighted (for attribute extraction).</p> <code>None</code> <code>annotate</code> <code>Optional[List[str]]</code> <p>List of attribute names from 'element' to display.</p> <code>None</code> <code>existing</code> <code>str</code> <p>How to handle existing highlights ('append' or 'replace').</p> <code>'append'</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def highlight_polygon(\n    self,\n    polygon: List[Tuple[float, float]],\n    color: Optional[Union[Tuple, str]] = None,\n    label: Optional[str] = None,\n    use_color_cycling: bool = False,\n    element: Optional[Any] = None,\n    annotate: Optional[List[str]] = None,\n    existing: str = \"append\",\n) -&gt; \"Page\":\n    \"\"\"\n    Highlight a polygon shape on the page.\n    Delegates to the central HighlightingService.\n\n    Args:\n        polygon: List of (x, y) points defining the polygon.\n        color: RGBA color tuple/string for the highlight.\n        label: Optional label for the highlight.\n        use_color_cycling: If True and no label/color, use next cycle color.\n        element: Optional original element being highlighted (for attribute extraction).\n        annotate: List of attribute names from 'element' to display.\n        existing: How to handle existing highlights ('append' or 'replace').\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    self._highlighter.add_polygon(\n        page_index=self.index,\n        polygon=polygon,\n        color=color,\n        label=label,\n        use_color_cycling=use_color_cycling,\n        element=element,\n        annotate=annotate,\n        existing=existing,\n    )\n    return self\n</code></pre> <code>natural_pdf.Page.highlights(show=False)</code> <p>Create a highlight context for accumulating highlights.</p> <p>This allows for clean syntax to show multiple highlight groups:</p> Example <p>with page.highlights() as h:     h.add(page.find_all('table'), label='tables', color='blue')     h.add(page.find_all('text:bold'), label='bold text', color='red')     h.show()</p> Or with automatic display <p>with page.highlights(show=True) as h:     h.add(page.find_all('table'), label='tables')     h.add(page.find_all('text:bold'), label='bold')     # Automatically shows when exiting the context</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>If True, automatically show highlights when exiting context</p> <code>False</code> <p>Returns:</p> Type Description <code>HighlightContext</code> <p>HighlightContext for accumulating highlights</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n    \"\"\"\n    Create a highlight context for accumulating highlights.\n\n    This allows for clean syntax to show multiple highlight groups:\n\n    Example:\n        with page.highlights() as h:\n            h.add(page.find_all('table'), label='tables', color='blue')\n            h.add(page.find_all('text:bold'), label='bold text', color='red')\n            h.show()\n\n    Or with automatic display:\n        with page.highlights(show=True) as h:\n            h.add(page.find_all('table'), label='tables')\n            h.add(page.find_all('text:bold'), label='bold')\n            # Automatically shows when exiting the context\n\n    Args:\n        show: If True, automatically show highlights when exiting context\n\n    Returns:\n        HighlightContext for accumulating highlights\n    \"\"\"\n    from natural_pdf.core.highlighting_service import HighlightContext\n\n    return HighlightContext(self, show_on_exit=show)\n</code></pre> <code>natural_pdf.Page.inspect(limit=30)</code> <p>Inspect all elements on this page with detailed tabular view. Equivalent to page.find_all('*').inspect().</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum elements per type to show (default: 30)</p> <code>30</code> <p>Returns:</p> Type Description <code>InspectionSummary</code> <p>InspectionSummary with element tables showing coordinates,</p> <code>InspectionSummary</code> <p>properties, and other details for each element</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def inspect(self, limit: int = 30) -&gt; \"InspectionSummary\":\n    \"\"\"\n    Inspect all elements on this page with detailed tabular view.\n    Equivalent to page.find_all('*').inspect().\n\n    Args:\n        limit: Maximum elements per type to show (default: 30)\n\n    Returns:\n        InspectionSummary with element tables showing coordinates,\n        properties, and other details for each element\n    \"\"\"\n    return self.find_all(\"*\").inspect(limit=limit)\n</code></pre> <code>natural_pdf.Page.region(left=None, top=None, right=None, bottom=None, width=None, height=None)</code> <p>Create a region on this page with more intuitive named parameters, allowing definition by coordinates or by coordinate + dimension.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>float</code> <p>Left x-coordinate (default: 0 if width not used).</p> <code>None</code> <code>top</code> <code>float</code> <p>Top y-coordinate (default: 0 if height not used).</p> <code>None</code> <code>right</code> <code>float</code> <p>Right x-coordinate (default: page width if width not used).</p> <code>None</code> <code>bottom</code> <code>float</code> <p>Bottom y-coordinate (default: page height if height not used).</p> <code>None</code> <code>width</code> <code>Union[str, float, None]</code> <p>Width definition. Can be:    - Numeric: The width of the region in points. Cannot be used with both left and right.    - String 'full': Sets region width to full page width (overrides left/right).    - String 'element' or None (default): Uses provided/calculated left/right,      defaulting to page width if neither are specified.</p> <code>None</code> <code>height</code> <code>Optional[float]</code> <p>Numeric height of the region. Cannot be used with both top and bottom.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>Region object for the specified coordinates</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If conflicting arguments are provided (e.g., top, bottom, and height)       or if width is an invalid string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; page.region(top=100, height=50)  # Region from y=100 to y=150, default width\n&gt;&gt;&gt; page.region(left=50, width=100)   # Region from x=50 to x=150, default height\n&gt;&gt;&gt; page.region(bottom=500, height=50) # Region from y=450 to y=500\n&gt;&gt;&gt; page.region(right=200, width=50)  # Region from x=150 to x=200\n&gt;&gt;&gt; page.region(top=100, bottom=200, width=\"full\") # Explicit full width\n</code></pre> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def region(\n    self,\n    left: float = None,\n    top: float = None,\n    right: float = None,\n    bottom: float = None,\n    width: Union[str, float, None] = None,\n    height: Optional[float] = None,\n) -&gt; Any:\n    \"\"\"\n    Create a region on this page with more intuitive named parameters,\n    allowing definition by coordinates or by coordinate + dimension.\n\n    Args:\n        left: Left x-coordinate (default: 0 if width not used).\n        top: Top y-coordinate (default: 0 if height not used).\n        right: Right x-coordinate (default: page width if width not used).\n        bottom: Bottom y-coordinate (default: page height if height not used).\n        width: Width definition. Can be:\n               - Numeric: The width of the region in points. Cannot be used with both left and right.\n               - String 'full': Sets region width to full page width (overrides left/right).\n               - String 'element' or None (default): Uses provided/calculated left/right,\n                 defaulting to page width if neither are specified.\n        height: Numeric height of the region. Cannot be used with both top and bottom.\n\n    Returns:\n        Region object for the specified coordinates\n\n    Raises:\n        ValueError: If conflicting arguments are provided (e.g., top, bottom, and height)\n                  or if width is an invalid string.\n\n    Examples:\n        &gt;&gt;&gt; page.region(top=100, height=50)  # Region from y=100 to y=150, default width\n        &gt;&gt;&gt; page.region(left=50, width=100)   # Region from x=50 to x=150, default height\n        &gt;&gt;&gt; page.region(bottom=500, height=50) # Region from y=450 to y=500\n        &gt;&gt;&gt; page.region(right=200, width=50)  # Region from x=150 to x=200\n        &gt;&gt;&gt; page.region(top=100, bottom=200, width=\"full\") # Explicit full width\n    \"\"\"\n    # ------------------------------------------------------------------\n    # Percentage support \u2013 convert strings like \"30%\" to absolute values\n    # based on page dimensions.  X-axis params (left, right, width) use\n    # page.width; Y-axis params (top, bottom, height) use page.height.\n    # ------------------------------------------------------------------\n\n    def _pct_to_abs(val, axis: str):\n        if isinstance(val, str) and val.strip().endswith(\"%\"):\n            try:\n                pct = float(val.strip()[:-1]) / 100.0\n            except ValueError:\n                return val  # leave unchanged if not a number\n            return pct * (self.width if axis == \"x\" else self.height)\n        return val\n\n    left = _pct_to_abs(left, \"x\")\n    right = _pct_to_abs(right, \"x\")\n    width = _pct_to_abs(width, \"x\")\n    top = _pct_to_abs(top, \"y\")\n    bottom = _pct_to_abs(bottom, \"y\")\n    height = _pct_to_abs(height, \"y\")\n\n    # --- Type checking and basic validation ---\n    is_width_numeric = isinstance(width, (int, float))\n    is_width_string = isinstance(width, str)\n    width_mode = \"element\"  # Default mode\n\n    if height is not None and top is not None and bottom is not None:\n        raise ValueError(\"Cannot specify top, bottom, and height simultaneously.\")\n    if is_width_numeric and left is not None and right is not None:\n        raise ValueError(\"Cannot specify left, right, and a numeric width simultaneously.\")\n    if is_width_string:\n        width_lower = width.lower()\n        if width_lower not in [\"full\", \"element\"]:\n            raise ValueError(\"String width argument must be 'full' or 'element'.\")\n        width_mode = width_lower\n\n    # --- Calculate Coordinates ---\n    final_top = top\n    final_bottom = bottom\n    final_left = left\n    final_right = right\n\n    # Height calculations\n    if height is not None:\n        if top is not None:\n            final_bottom = top + height\n        elif bottom is not None:\n            final_top = bottom - height\n        else:  # Neither top nor bottom provided, default top to 0\n            final_top = 0\n            final_bottom = height\n\n    # Width calculations (numeric only)\n    if is_width_numeric:\n        if left is not None:\n            final_right = left + width\n        elif right is not None:\n            final_left = right - width\n        else:  # Neither left nor right provided, default left to 0\n            final_left = 0\n            final_right = width\n\n    # --- Apply Defaults for Unset Coordinates ---\n    # Only default coordinates if they weren't set by dimension calculation\n    if final_top is None:\n        final_top = 0\n    if final_bottom is None:\n        # Check if bottom should have been set by height calc\n        if height is None or top is None:\n            final_bottom = self.height\n\n    if final_left is None:\n        final_left = 0\n    if final_right is None:\n        # Check if right should have been set by width calc\n        if not is_width_numeric or left is None:\n            final_right = self.width\n\n    # --- Handle width_mode == 'full' ---\n    if width_mode == \"full\":\n        # Override left/right if mode is full\n        final_left = 0\n        final_right = self.width\n\n    # --- Final Validation &amp; Creation ---\n    # Ensure coordinates are within page bounds (clamp)\n    final_left = max(0, final_left)\n    final_top = max(0, final_top)\n    final_right = min(self.width, final_right)\n    final_bottom = min(self.height, final_bottom)\n\n    # Ensure valid box (x0&lt;=x1, top&lt;=bottom)\n    if final_left &gt; final_right:\n        logger.warning(f\"Calculated left ({final_left}) &gt; right ({final_right}). Swapping.\")\n        final_left, final_right = final_right, final_left\n    if final_top &gt; final_bottom:\n        logger.warning(f\"Calculated top ({final_top}) &gt; bottom ({final_bottom}). Swapping.\")\n        final_top, final_bottom = final_bottom, final_top\n\n    from natural_pdf.elements.region import Region\n\n    region = Region(self, (final_left, final_top, final_right, final_bottom))\n    return region\n</code></pre> <code>natural_pdf.Page.remove_text_layer()</code> <p>Remove all text elements from this page.</p> <p>This removes all text elements (words and characters) from the page, effectively clearing the text layer.</p> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def remove_text_layer(self) -&gt; \"Page\":\n    \"\"\"\n    Remove all text elements from this page.\n\n    This removes all text elements (words and characters) from the page,\n    effectively clearing the text layer.\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    logger.info(f\"Page {self.number}: Removing all text elements...\")\n\n    # Remove all words and chars from the element manager\n    removed_words = len(self._element_mgr.words)\n    removed_chars = len(self._element_mgr.chars)\n\n    # Clear the lists\n    self._element_mgr._elements[\"words\"] = []\n    self._element_mgr._elements[\"chars\"] = []\n\n    logger.info(\n        f\"Page {self.number}: Removed {removed_words} words and {removed_chars} characters\"\n    )\n    return self\n</code></pre> <code>natural_pdf.Page.save_image(filename, width=None, labels=True, legend_position='right', render_ocr=False, include_highlights=True, resolution=144, **kwargs)</code> <p>Save the page image to a file, rendering highlights via HighlightingService.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to save the image to.</p> required <code>width</code> <code>Optional[int]</code> <p>Optional width for the output image.</p> <code>None</code> <code>labels</code> <code>bool</code> <p>Whether to include a legend.</p> <code>True</code> <code>legend_position</code> <code>str</code> <p>Position of the legend.</p> <code>'right'</code> <code>render_ocr</code> <code>bool</code> <p>Whether to render OCR text.</p> <code>False</code> <code>include_highlights</code> <code>bool</code> <p>Whether to render highlights.</p> <code>True</code> <code>resolution</code> <code>float</code> <p>Resolution in DPI for base image rendering (default: 144 DPI, equivalent to previous scale=2.0).</p> <code>144</code> <code>**kwargs</code> <p>Additional args for pdfplumber's internal to_image.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def save_image(\n    self,\n    filename: str,\n    width: Optional[int] = None,\n    labels: bool = True,\n    legend_position: str = \"right\",\n    render_ocr: bool = False,\n    include_highlights: bool = True,  # Allow saving without highlights\n    resolution: float = 144,\n    **kwargs,\n) -&gt; \"Page\":\n    \"\"\"\n    Save the page image to a file, rendering highlights via HighlightingService.\n\n    Args:\n        filename: Path to save the image to.\n        width: Optional width for the output image.\n        labels: Whether to include a legend.\n        legend_position: Position of the legend.\n        render_ocr: Whether to render OCR text.\n        include_highlights: Whether to render highlights.\n        resolution: Resolution in DPI for base image rendering (default: 144 DPI, equivalent to previous scale=2.0).\n        **kwargs: Additional args for pdfplumber's internal to_image.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    # Use export() to save the image\n    if include_highlights:\n        self.export(\n            path=filename,\n            resolution=resolution,\n            width=width,\n            labels=labels,\n            legend_position=legend_position,\n            render_ocr=render_ocr,\n            **kwargs,\n        )\n    else:\n        # For saving without highlights, use render() and save manually\n        img = self.render(resolution=resolution, **kwargs)\n        if img:\n            # Resize if width is specified\n            if width is not None and width &gt; 0 and img.width &gt; 0:\n                aspect_ratio = img.height / img.width\n                height = int(width * aspect_ratio)\n                try:\n                    img = img.resize((width, height), Image.Resampling.LANCZOS)\n                except Exception as e:\n                    logger.warning(f\"Could not resize image: {e}\")\n\n            # Save the image\n            try:\n                if os.path.dirname(filename):\n                    os.makedirs(os.path.dirname(filename), exist_ok=True)\n                img.save(filename)\n            except Exception as e:\n                logger.error(f\"Failed to save image to {filename}: {e}\")\n\n    return self\n</code></pre> <code>natural_pdf.Page.save_searchable(output_path, dpi=300, **kwargs)</code> <p>Saves the PDF page with an OCR text layer, making content searchable.</p> <p>Requires optional dependencies. Install with: pip install \"natural-pdf[ocr-save]\"</p> OCR must have been applied to the pages beforehand <p>(e.g., pdf.apply_ocr()).</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save the searchable PDF.</p> required <code>dpi</code> <code>int</code> <p>Resolution for rendering and OCR overlay (default 300).</p> <code>300</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the exporter.</p> <code>{}</code> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def save_searchable(self, output_path: Union[str, \"Path\"], dpi: int = 300, **kwargs):\n    \"\"\"\n    Saves the PDF page with an OCR text layer, making content searchable.\n\n    Requires optional dependencies. Install with: pip install \"natural-pdf[ocr-save]\"\n\n    Note: OCR must have been applied to the pages beforehand\n          (e.g., pdf.apply_ocr()).\n\n    Args:\n        output_path: Path to save the searchable PDF.\n        dpi: Resolution for rendering and OCR overlay (default 300).\n        **kwargs: Additional keyword arguments passed to the exporter.\n    \"\"\"\n    # Import moved here, assuming it's always available now\n    from natural_pdf.exporters.searchable_pdf import create_searchable_pdf\n\n    # Convert pathlib.Path to string if necessary\n    output_path_str = str(output_path)\n\n    create_searchable_pdf(self, output_path_str, dpi=dpi, **kwargs)\n    logger.info(f\"Searchable PDF saved to: {output_path_str}\")\n</code></pre> <code>natural_pdf.Page.show_preview(temporary_highlights, resolution=144, width=None, labels=True, legend_position='right', render_ocr=False)</code> <p>Generates and returns a non-stateful preview image containing only the provided temporary highlights.</p> <p>Parameters:</p> Name Type Description Default <code>temporary_highlights</code> <code>List[Dict]</code> <p>List of highlight data dictionaries (as prepared by                   ElementCollection._prepare_highlight_data).</p> required <code>resolution</code> <code>float</code> <p>Resolution in DPI for rendering (default: 144 DPI, equivalent to previous scale=2.0).</p> <code>144</code> <code>width</code> <code>Optional[int]</code> <p>Optional width for the output image.</p> <code>None</code> <code>labels</code> <code>bool</code> <p>Whether to include a legend.</p> <code>True</code> <code>legend_position</code> <code>str</code> <p>Position of the legend.</p> <code>'right'</code> <code>render_ocr</code> <code>bool</code> <p>Whether to render OCR text.</p> <code>False</code> <p>Returns:</p> Type Description <code>Optional[Image]</code> <p>PIL Image object of the preview, or None if rendering fails.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def show_preview(\n    self,\n    temporary_highlights: List[Dict],\n    resolution: float = 144,\n    width: Optional[int] = None,\n    labels: bool = True,\n    legend_position: str = \"right\",\n    render_ocr: bool = False,\n) -&gt; Optional[Image.Image]:\n    \"\"\"\n    Generates and returns a non-stateful preview image containing only\n    the provided temporary highlights.\n\n    Args:\n        temporary_highlights: List of highlight data dictionaries (as prepared by\n                              ElementCollection._prepare_highlight_data).\n        resolution: Resolution in DPI for rendering (default: 144 DPI, equivalent to previous scale=2.0).\n        width: Optional width for the output image.\n        labels: Whether to include a legend.\n        legend_position: Position of the legend.\n        render_ocr: Whether to render OCR text.\n\n    Returns:\n        PIL Image object of the preview, or None if rendering fails.\n    \"\"\"\n    try:\n        # Delegate rendering to the highlighter service's preview method\n        img = self._highlighter.render_preview(\n            page_index=self.index,\n            temporary_highlights=temporary_highlights,\n            resolution=resolution,\n            labels=labels,\n            legend_position=legend_position,\n            render_ocr=render_ocr,\n        )\n    except AttributeError:\n        logger.error(f\"HighlightingService does not have the required 'render_preview' method.\")\n        return None\n    except Exception as e:\n        logger.error(\n            f\"Error calling highlighter.render_preview for page {self.index}: {e}\",\n            exc_info=True,\n        )\n        return None\n\n    # Return the rendered image directly\n    return img\n</code></pre> <code>natural_pdf.Page.split(divider, **kwargs)</code> <p>Divides the page into sections based on the provided divider elements.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def split(self, divider, **kwargs) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Divides the page into sections based on the provided divider elements.\n    \"\"\"\n    sections = self.get_sections(start_elements=divider, **kwargs)\n    top = self.region(0, 0, self.width, sections[0].top)\n    sections.append(top)\n\n    return sections\n</code></pre> <code>natural_pdf.Page.until(selector, include_endpoint=True, **kwargs)</code> <p>Select content from the top of the page until matching selector.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>str</code> <p>CSS-like selector string</p> required <code>include_endpoint</code> <code>bool</code> <p>Whether to include the endpoint element in the region</p> <code>True</code> <code>**kwargs</code> <p>Additional selection parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>Region object representing the selected content</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; page.until('text:contains(\"Conclusion\")')  # Select from top to conclusion\n&gt;&gt;&gt; page.until('line[width&gt;=2]', include_endpoint=False)  # Select up to thick line\n</code></pre> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def until(self, selector: str, include_endpoint: bool = True, **kwargs) -&gt; Any:\n    \"\"\"\n    Select content from the top of the page until matching selector.\n\n    Args:\n        selector: CSS-like selector string\n        include_endpoint: Whether to include the endpoint element in the region\n        **kwargs: Additional selection parameters\n\n    Returns:\n        Region object representing the selected content\n\n    Examples:\n        &gt;&gt;&gt; page.until('text:contains(\"Conclusion\")')  # Select from top to conclusion\n        &gt;&gt;&gt; page.until('line[width&gt;=2]', include_endpoint=False)  # Select up to thick line\n    \"\"\"\n    # Find the target element\n    target = self.find(selector, **kwargs)\n    if not target:\n        # If target not found, return a default region (full page)\n        from natural_pdf.elements.region import Region\n\n        return Region(self, (0, 0, self.width, self.height))\n\n    # Create a region from the top of the page to the target\n    from natural_pdf.elements.region import Region\n\n    # Ensure target has positional attributes before using them\n    target_top = getattr(target, \"top\", 0)\n    target_bottom = getattr(target, \"bottom\", self.height)\n\n    if include_endpoint:\n        # Include the target element\n        region = Region(self, (0, 0, self.width, target_bottom))\n    else:\n        # Up to the target element\n        region = Region(self, (0, 0, self.width, target_top))\n\n    region.end_element = target\n    return region\n</code></pre> <code>natural_pdf.Page.update_text(transform, selector='text', max_workers=None, progress_callback=None)</code> <p>Applies corrections to text elements on this page using a user-provided callback function, potentially in parallel.</p> <p>Finds text elements on this page matching the selector argument and calls the <code>transform</code> for each, passing the element itself. Updates the element's text if the callback returns a new string.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Any], Optional[str]]</code> <p>A function accepting an element and returning        <code>Optional[str]</code> (new text or None).</p> required <code>selector</code> <code>str</code> <p>CSS-like selector string to match text elements.</p> <code>'text'</code> <code>max_workers</code> <code>Optional[int]</code> <p>The maximum number of threads to use for parallel execution.          If None or 0 or 1, runs sequentially.</p> <code>None</code> <code>progress_callback</code> <code>Optional[Callable[[], None]]</code> <p>Optional callback function to call after processing each element.</p> <code>None</code> <p>Returns:</p> Type Description <code>Page</code> <p>Self for method chaining.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def update_text(\n    self,\n    transform: Callable[[Any], Optional[str]],\n    selector: str = \"text\",\n    max_workers: Optional[int] = None,\n    progress_callback: Optional[Callable[[], None]] = None,  # Added progress callback\n) -&gt; \"Page\":  # Return self for chaining\n    \"\"\"\n    Applies corrections to text elements on this page\n    using a user-provided callback function, potentially in parallel.\n\n    Finds text elements on this page matching the *selector* argument and\n    calls the ``transform`` for each, passing the element itself.\n    Updates the element's text if the callback returns a new string.\n\n    Args:\n        transform: A function accepting an element and returning\n                   `Optional[str]` (new text or None).\n        selector: CSS-like selector string to match text elements.\n        max_workers: The maximum number of threads to use for parallel execution.\n                     If None or 0 or 1, runs sequentially.\n        progress_callback: Optional callback function to call after processing each element.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    logger.info(\n        f\"Page {self.number}: Starting text update with callback '{transform.__name__}' (max_workers={max_workers}) and selector='{selector}'\"\n    )\n\n    target_elements_collection = self.find_all(selector=selector, apply_exclusions=False)\n    target_elements = target_elements_collection.elements  # Get the list\n\n    if not target_elements:\n        logger.info(f\"Page {self.number}: No text elements found to update.\")\n        return self\n\n    element_pbar = None\n    try:\n        element_pbar = tqdm(\n            total=len(target_elements),\n            desc=f\"Updating text Page {self.number}\",\n            unit=\"element\",\n            leave=False,\n        )\n\n        processed_count = 0\n        updated_count = 0\n        error_count = 0\n\n        # Define the task to be run by the worker thread or sequentially\n        def _process_element_task(element):\n            try:\n                current_text = getattr(element, \"text\", None)\n                # Call the user-provided callback\n                corrected_text = transform(element)\n\n                # Validate result type\n                if corrected_text is not None and not isinstance(corrected_text, str):\n                    logger.warning(\n                        f\"Page {self.number}: Correction callback for element '{getattr(element, 'text', '')[:20]}...' returned non-string, non-None type: {type(corrected_text)}. Skipping update.\"\n                    )\n                    return element, None, None  # Treat as no correction\n\n                return element, corrected_text, None  # Return element, result, no error\n            except Exception as e:\n                logger.error(\n                    f\"Page {self.number}: Error applying correction callback to element '{getattr(element, 'text', '')[:30]}...' ({element.bbox}): {e}\",\n                    exc_info=False,  # Keep log concise\n                )\n                return element, None, e  # Return element, no result, error\n            finally:\n                # --- Update internal tqdm progress bar ---\n                if element_pbar:\n                    element_pbar.update(1)\n                # --- Call user's progress callback --- #\n                if progress_callback:\n                    try:\n                        progress_callback()\n                    except Exception as cb_e:\n                        # Log error in callback itself, but don't stop processing\n                        logger.error(\n                            f\"Page {self.number}: Error executing progress_callback: {cb_e}\",\n                            exc_info=False,\n                        )\n\n        # Choose execution strategy based on max_workers\n        if max_workers is not None and max_workers &gt; 1:\n            # --- Parallel execution --- #\n            logger.info(\n                f\"Page {self.number}: Running text update in parallel with {max_workers} workers.\"\n            )\n            futures = []\n            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n                # Submit all tasks\n                future_to_element = {\n                    executor.submit(_process_element_task, element): element\n                    for element in target_elements\n                }\n\n                # Process results as they complete (progress_callback called by worker)\n                for future in concurrent.futures.as_completed(future_to_element):\n                    processed_count += 1\n                    try:\n                        element, corrected_text, error = future.result()\n                        if error:\n                            error_count += 1\n                            # Error already logged in worker\n                        elif corrected_text is not None:\n                            # Apply correction if text changed\n                            current_text = getattr(element, \"text\", None)\n                            if corrected_text != current_text:\n                                element.text = corrected_text\n                                updated_count += 1\n                    except Exception as exc:\n                        # Catch errors from future.result() itself\n                        element = future_to_element[future]  # Find original element\n                        logger.error(\n                            f\"Page {self.number}: Internal error retrieving correction result for element {element.bbox}: {exc}\",\n                            exc_info=True,\n                        )\n                        error_count += 1\n                        # Note: progress_callback was already called in the worker's finally block\n\n        else:\n            # --- Sequential execution --- #\n            logger.info(f\"Page {self.number}: Running text update sequentially.\")\n            for element in target_elements:\n                # Call the task function directly (it handles progress_callback)\n                processed_count += 1\n                _element, corrected_text, error = _process_element_task(element)\n                if error:\n                    error_count += 1\n                elif corrected_text is not None:\n                    # Apply correction if text changed\n                    current_text = getattr(_element, \"text\", None)\n                    if corrected_text != current_text:\n                        _element.text = corrected_text\n                        updated_count += 1\n\n        logger.info(\n            f\"Page {self.number}: Text update finished. Processed: {processed_count}/{len(target_elements)}, Updated: {updated_count}, Errors: {error_count}.\"\n        )\n\n        return self  # Return self for chaining\n    finally:\n        if element_pbar:\n            element_pbar.close()\n</code></pre> <code>natural_pdf.Page.viewer()</code> <p>Creates and returns an interactive ipywidget for exploring elements on this page.</p> <p>Uses InteractiveViewerWidget.from_page() to create the viewer.</p> <p>Returns:</p> Type Description <code>Optional[InteractiveViewerWidget]</code> <p>A InteractiveViewerWidget instance ready for display in Jupyter,</p> <code>Optional[InteractiveViewerWidget]</code> <p>or None if ipywidgets is not installed or widget creation fails.</p> <p>Raises:</p> Type Description <code># Optional</code> <p>Could raise ImportError instead of returning None</p> <code># ImportError</code> <p>If required dependencies (ipywidgets) are missing.</p> <code>ValueError</code> <p>If image rendering or data preparation fails within from_page.</p> Source code in <code>natural_pdf/core/page.py</code> <pre><code>def viewer(\n    self,\n    # elements_to_render: Optional[List['Element']] = None, # No longer needed, from_page handles it\n    # include_source_types: List[str] = ['word', 'line', 'rect', 'region'] # No longer needed\n) -&gt; Optional[\"InteractiveViewerWidget\"]:  # Return type hint updated\n    \"\"\"\n    Creates and returns an interactive ipywidget for exploring elements on this page.\n\n    Uses InteractiveViewerWidget.from_page() to create the viewer.\n\n    Returns:\n        A InteractiveViewerWidget instance ready for display in Jupyter,\n        or None if ipywidgets is not installed or widget creation fails.\n\n    Raises:\n        # Optional: Could raise ImportError instead of returning None\n        # ImportError: If required dependencies (ipywidgets) are missing.\n        ValueError: If image rendering or data preparation fails within from_page.\n    \"\"\"\n    # Check for availability using the imported flag and class variable\n    if not _IPYWIDGETS_AVAILABLE or InteractiveViewerWidget is None:\n        logger.error(\n            \"Interactive viewer requires 'ipywidgets'. \"\n            'Please install with: pip install \"ipywidgets&gt;=7.0.0,&lt;10.0.0\"'\n        )\n        # raise ImportError(\"ipywidgets not found.\") # Option 1: Raise error\n        return None  # Option 2: Return None gracefully\n\n    # If we reach here, InteractiveViewerWidget should be the actual class\n    try:\n        # Pass self (the Page object) to the factory method\n        return InteractiveViewerWidget.from_page(self)\n    except Exception as e:\n        # Catch potential errors during widget creation (e.g., image rendering)\n        logger.error(\n            f\"Error creating viewer widget from page {self.number}: {e}\", exc_info=True\n        )\n        # raise # Option 1: Re-raise error (might include ValueError from from_page)\n        return None  # Option 2: Return None on creation error\n</code></pre>"},{"location":"api/#natural_pdf.PageCollection","title":"<code>natural_pdf.PageCollection</code>","text":"<p>               Bases: <code>TextMixin</code>, <code>Generic[P]</code>, <code>ApplyMixin</code>, <code>ShapeDetectionMixin</code>, <code>Visualizable</code></p> <p>Represents a collection of Page objects, often from a single PDF document. Provides methods for batch operations on these pages.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>class PageCollection(TextMixin, Generic[P], ApplyMixin, ShapeDetectionMixin, Visualizable):\n    \"\"\"\n    Represents a collection of Page objects, often from a single PDF document.\n    Provides methods for batch operations on these pages.\n    \"\"\"\n\n    def __init__(self, pages: Union[List[P], Sequence[P]]):\n        \"\"\"\n        Initialize a page collection.\n\n        Args:\n            pages: List or sequence of Page objects (can be lazy)\n        \"\"\"\n        # Store the sequence as-is to preserve lazy behavior\n        # Only convert to list if we need list-specific operations\n        if hasattr(pages, \"__iter__\") and hasattr(pages, \"__len__\"):\n            self.pages = pages\n        else:\n            # Fallback for non-sequence types\n            self.pages = list(pages)\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of pages in the collection.\"\"\"\n        return len(self.pages)\n\n    def __getitem__(self, idx) -&gt; Union[P, \"PageCollection[P]\"]:\n        \"\"\"Support indexing and slicing.\"\"\"\n        if isinstance(idx, slice):\n            return PageCollection(self.pages[idx])\n        return self.pages[idx]\n\n    def __iter__(self) -&gt; Iterator[P]:\n        \"\"\"Support iteration.\"\"\"\n        return iter(self.pages)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation showing the page count.\"\"\"\n        return f\"&lt;PageCollection(count={len(self)})&gt;\"\n\n    def _get_items_for_apply(self) -&gt; Iterator[P]:\n        \"\"\"\n        Override ApplyMixin's _get_items_for_apply to preserve lazy behavior.\n\n        Returns an iterator that yields pages on-demand rather than materializing\n        all pages at once, maintaining the lazy loading behavior.\n        \"\"\"\n        return iter(self.pages)\n\n    def _get_page_indices(self) -&gt; List[int]:\n        \"\"\"\n        Get page indices without forcing materialization of pages.\n\n        Returns:\n            List of page indices for the pages in this collection.\n        \"\"\"\n        # Handle different types of page sequences efficiently\n        if hasattr(self.pages, \"_indices\"):\n            # If it's a _LazyPageList (or slice), get indices directly\n            return list(self.pages._indices)\n        else:\n            # Fallback: if pages are already materialized, get indices normally\n            # This will force materialization but only if pages aren't lazy\n            return [p.index for p in self.pages]\n\n    def extract_text(\n        self,\n        keep_blank_chars: bool = True,\n        apply_exclusions: bool = True,\n        strip: Optional[bool] = None,\n        **kwargs,\n    ) -&gt; str:\n        \"\"\"\n        Extract text from all pages in the collection.\n\n        Args:\n            keep_blank_chars: Whether to keep blank characters (default: True)\n            apply_exclusions: Whether to apply exclusion regions (default: True)\n            strip: Whether to strip whitespace from the extracted text.\n            **kwargs: Additional extraction parameters\n\n        Returns:\n            Combined text from all pages\n        \"\"\"\n        texts = []\n        for page in self.pages:\n            text = page.extract_text(\n                keep_blank_chars=keep_blank_chars,\n                apply_exclusions=apply_exclusions,\n                **kwargs,\n            )\n            texts.append(text)\n\n        combined = \"\\n\".join(texts)\n\n        # Default strip behaviour: if caller picks, honour; else respect layout flag passed via kwargs.\n        use_layout = kwargs.get(\"layout\", False)\n        strip_final = strip if strip is not None else (not use_layout)\n\n        if strip_final:\n            combined = \"\\n\".join(line.rstrip() for line in combined.splitlines()).strip()\n\n        return combined\n\n    def apply_ocr(\n        self,\n        engine: Optional[str] = None,\n        # --- Common OCR Parameters (Direct Arguments) ---\n        languages: Optional[List[str]] = None,\n        min_confidence: Optional[float] = None,  # Min confidence threshold\n        device: Optional[str] = None,\n        resolution: Optional[int] = None,  # DPI for rendering\n        apply_exclusions: bool = True,  # New parameter\n        replace: bool = True,  # Whether to replace existing OCR elements\n        # --- Engine-Specific Options ---\n        options: Optional[Any] = None,  # e.g., EasyOCROptions(...)\n    ) -&gt; \"PageCollection[P]\":\n        \"\"\"\n        Applies OCR to all pages within this collection using batch processing.\n\n        This delegates the work to the parent PDF object's `apply_ocr` method.\n\n        Args:\n            engine: Name of the OCR engine (e.g., 'easyocr', 'paddleocr').\n            languages: List of language codes (e.g., ['en', 'fr'], ['en', 'ch']).\n                       **Must be codes understood by the specific selected engine.**\n                       No mapping is performed.\n            min_confidence: Minimum confidence threshold for detected text (0.0 to 1.0).\n            device: Device to run OCR on (e.g., 'cpu', 'cuda', 'mps').\n            resolution: DPI resolution to render page images before OCR (e.g., 150, 300).\n            apply_exclusions: If True (default), render page images for OCR with\n                              excluded areas masked (whited out). If False, OCR\n                              the raw page images without masking exclusions.\n            replace: If True (default), remove any existing OCR elements before\n                    adding new ones. If False, add new OCR elements to existing ones.\n            options: An engine-specific options object (e.g., EasyOCROptions) or dict.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            RuntimeError: If pages lack a parent PDF or parent lacks `apply_ocr`.\n            (Propagates exceptions from PDF.apply_ocr)\n        \"\"\"\n        if not self.pages:\n            logger.warning(\"Cannot apply OCR to an empty PageCollection.\")\n            return self\n\n        # Assume all pages share the same parent PDF object\n        first_page = self.pages[0]\n        if not hasattr(first_page, \"_parent\") or not first_page._parent:\n            raise RuntimeError(\"Pages in this collection do not have a parent PDF reference.\")\n\n        parent_pdf = first_page._parent\n\n        if not hasattr(parent_pdf, \"apply_ocr\") or not callable(parent_pdf.apply_ocr):\n            raise RuntimeError(\"Parent PDF object does not have the required 'apply_ocr' method.\")\n\n        # Get the 0-based indices of the pages in this collection\n        page_indices = self._get_page_indices()\n\n        logger.info(f\"Applying OCR via parent PDF to page indices: {page_indices} in collection.\")\n\n        # Delegate the batch call to the parent PDF object, passing direct args and apply_exclusions\n        parent_pdf.apply_ocr(\n            pages=page_indices,\n            engine=engine,\n            languages=languages,\n            min_confidence=min_confidence,  # Pass the renamed parameter\n            device=device,\n            resolution=resolution,\n            apply_exclusions=apply_exclusions,  # Pass down\n            replace=replace,  # Pass the replace parameter\n            options=options,\n        )\n        # The PDF method modifies the Page objects directly by adding elements.\n\n        return self  # Return self for chaining\n\n    @overload\n    def find(\n        self,\n        *,\n        text: str,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[T]: ...\n\n    @overload\n    def find(\n        self,\n        selector: str,\n        *,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[T]: ...\n\n    def find(\n        self,\n        selector: Optional[str] = None,\n        *,\n        text: Optional[str] = None,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[T]:\n        \"\"\"\n        Find the first element matching the selector OR text across all pages in the collection.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            contains: How to determine if elements are inside: 'all' (fully inside),\n                     'any' (any overlap), or 'center' (center point inside).\n                     (default: \"all\")\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional filter parameters.\n\n        Returns:\n            First matching element or None.\n        \"\"\"\n        # Input validation happens within page.find\n        for page in self.pages:\n            element = page.find(\n                selector=selector,\n                text=text,\n                contains=contains,\n                apply_exclusions=apply_exclusions,\n                regex=regex,\n                case=case,\n                **kwargs,\n            )\n            if element:\n                return element\n        return None\n\n    @overload\n    def find_all(\n        self,\n        *,\n        text: str,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    @overload\n    def find_all(\n        self,\n        selector: str,\n        *,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    def find_all(\n        self,\n        selector: Optional[str] = None,\n        *,\n        text: Optional[str] = None,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Find all elements matching the selector OR text across all pages in the collection.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            contains: How to determine if elements are inside: 'all' (fully inside),\n                     'any' (any overlap), or 'center' (center point inside).\n                     (default: \"all\")\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional filter parameters.\n\n        Returns:\n            ElementCollection with matching elements from all pages.\n        \"\"\"\n        all_elements = []\n        # Input validation happens within page.find_all\n        for page in self.pages:\n            elements = page.find_all(\n                selector=selector,\n                text=text,\n                contains=contains,\n                apply_exclusions=apply_exclusions,\n                regex=regex,\n                case=case,\n                **kwargs,\n            )\n            if elements:\n                all_elements.extend(elements.elements)\n\n        return ElementCollection(all_elements)\n\n    def update_text(\n        self,\n        transform: Callable[[Any], Optional[str]],\n        selector: str = \"text\",\n        max_workers: Optional[int] = None,\n    ) -&gt; \"PageCollection[P]\":\n        \"\"\"\n        Applies corrections to text elements across all pages\n        in this collection using a user-provided callback function, executed\n        in parallel if `max_workers` is specified.\n\n        This method delegates to the parent PDF's `update_text` method,\n        targeting all pages within this collection.\n\n        Args:\n            transform: A function that accepts a single argument (an element\n                       object) and returns `Optional[str]` (new text or None).\n            selector: The attribute name to update. Default is 'text'.\n            max_workers: The maximum number of worker threads to use for parallel\n                         correction on each page. If None, defaults are used.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            RuntimeError: If the collection is empty, pages lack a parent PDF reference,\n                          or the parent PDF lacks the `update_text` method.\n        \"\"\"\n        if not self.pages:\n            logger.warning(\"Cannot update text for an empty PageCollection.\")\n            # Return self even if empty to maintain chaining consistency\n            return self\n\n        # Assume all pages share the same parent PDF object\n        parent_pdf = self.pages[0]._parent\n        if (\n            not parent_pdf\n            or not hasattr(parent_pdf, \"update_text\")\n            or not callable(parent_pdf.update_text)\n        ):\n            raise RuntimeError(\n                \"Parent PDF reference not found or parent PDF lacks the required 'update_text' method.\"\n            )\n\n        page_indices = self._get_page_indices()\n        logger.info(\n            f\"PageCollection: Delegating text update to parent PDF for page indices: {page_indices} with max_workers={max_workers} and selector='{selector}'.\"\n        )\n\n        # Delegate the call to the parent PDF object for the relevant pages\n        # Pass the max_workers parameter down\n        parent_pdf.update_text(\n            transform=transform,\n            pages=page_indices,\n            selector=selector,\n            max_workers=max_workers,\n        )\n\n        return self\n\n    def get_sections(\n        self,\n        start_elements=None,\n        end_elements=None,\n        new_section_on_page_break=False,\n        include_boundaries=\"both\",\n    ) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Extract sections from a page collection based on start/end elements.\n\n        Args:\n            start_elements: Elements or selector string that mark the start of sections (optional)\n            end_elements: Elements or selector string that mark the end of sections (optional)\n            new_section_on_page_break: Whether to start a new section at page boundaries (default: False)\n            include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none' (default: 'both')\n\n        Returns:\n            List of Region objects representing the extracted sections\n\n        Note:\n            You can provide only start_elements, only end_elements, or both.\n            - With only start_elements: sections go from each start to the next start (or end of page)\n            - With only end_elements: sections go from beginning of document/page to each end\n            - With both: sections go from each start to the corresponding end\n        \"\"\"\n        # Find start and end elements across all pages\n        if isinstance(start_elements, str):\n            start_elements = self.find_all(start_elements).elements\n\n        if isinstance(end_elements, str):\n            end_elements = self.find_all(end_elements).elements\n\n        # If no start elements and no end elements, return empty list\n        if not start_elements and not end_elements:\n            return []\n\n        # If there are page break boundaries, we'll need to add them\n        if new_section_on_page_break:\n            # For each page boundary, create virtual \"end\" and \"start\" elements\n            for i in range(len(self.pages) - 1):\n                # Add a virtual \"end\" element at the bottom of the current page\n                page = self.pages[i]\n                # If end_elements is None, initialize it as an empty list\n                if end_elements is None:\n                    end_elements = []\n\n                # Create a region at the bottom of the page as an artificial end marker\n                from natural_pdf.elements.region import Region\n\n                bottom_region = Region(page, (0, page.height - 1, page.width, page.height))\n                bottom_region.is_page_boundary = True  # Mark it as a special boundary\n                end_elements.append(bottom_region)\n\n                # Add a virtual \"start\" element at the top of the next page\n                next_page = self.pages[i + 1]\n                top_region = Region(next_page, (0, 0, next_page.width, 1))\n                top_region.is_page_boundary = True  # Mark it as a special boundary\n                start_elements.append(top_region)\n\n        # Get all elements from all pages and sort them in document order\n        all_elements = []\n        for page in self.pages:\n            elements = page.get_elements()\n            all_elements.extend(elements)\n\n        # Sort by page index, then vertical position, then horizontal position\n        all_elements.sort(key=lambda e: (e.page.index, e.top, e.x0))\n\n        # If we only have end_elements (no start_elements), create implicit start elements\n        if not start_elements and end_elements:\n            from natural_pdf.elements.region import Region\n\n            start_elements = []\n\n            # Add implicit start at the beginning of the first page\n            first_page = self.pages[0]\n            first_start = Region(first_page, (0, 0, first_page.width, 1))\n            first_start.is_implicit_start = True\n            start_elements.append(first_start)\n\n            # For each end element (except the last), add an implicit start after it\n            sorted_end_elements = sorted(end_elements, key=lambda e: (e.page.index, e.top, e.x0))\n            for i, end_elem in enumerate(sorted_end_elements[:-1]):  # Exclude last end element\n                # Create implicit start element right after this end element\n                implicit_start = Region(\n                    end_elem.page, (0, end_elem.bottom, end_elem.page.width, end_elem.bottom + 1)\n                )\n                implicit_start.is_implicit_start = True\n                start_elements.append(implicit_start)\n\n        # Mark section boundaries\n        section_boundaries = []\n\n        # Add start element boundaries\n        for element in start_elements:\n            if element in all_elements:\n                idx = all_elements.index(element)\n                section_boundaries.append(\n                    {\n                        \"index\": idx,\n                        \"element\": element,\n                        \"type\": \"start\",\n                        \"page_idx\": element.page.index,\n                    }\n                )\n            elif hasattr(element, \"is_page_boundary\") and element.is_page_boundary:\n                # This is a virtual page boundary element\n                section_boundaries.append(\n                    {\n                        \"index\": -1,  # Special index for page boundaries\n                        \"element\": element,\n                        \"type\": \"start\",\n                        \"page_idx\": element.page.index,\n                    }\n                )\n            elif hasattr(element, \"is_implicit_start\") and element.is_implicit_start:\n                # This is an implicit start element\n                section_boundaries.append(\n                    {\n                        \"index\": -2,  # Special index for implicit starts\n                        \"element\": element,\n                        \"type\": \"start\",\n                        \"page_idx\": element.page.index,\n                    }\n                )\n\n        # Add end element boundaries if provided\n        if end_elements:\n            for element in end_elements:\n                if element in all_elements:\n                    idx = all_elements.index(element)\n                    section_boundaries.append(\n                        {\n                            \"index\": idx,\n                            \"element\": element,\n                            \"type\": \"end\",\n                            \"page_idx\": element.page.index,\n                        }\n                    )\n                elif hasattr(element, \"is_page_boundary\") and element.is_page_boundary:\n                    # This is a virtual page boundary element\n                    section_boundaries.append(\n                        {\n                            \"index\": -1,  # Special index for page boundaries\n                            \"element\": element,\n                            \"type\": \"end\",\n                            \"page_idx\": element.page.index,\n                        }\n                    )\n\n        # Sort boundaries by page index, then by actual document position\n        def _sort_key(boundary):\n            \"\"\"Sort boundaries by (page_idx, vertical_top, priority).\"\"\"\n            page_idx = boundary[\"page_idx\"]\n            element = boundary[\"element\"]\n\n            # Vertical position on the page\n            y_pos = getattr(element, \"top\", 0.0)\n\n            # Ensure starts come before ends at the same coordinate\n            priority = 0 if boundary[\"type\"] == \"start\" else 1\n\n            return (page_idx, y_pos, priority)\n\n        section_boundaries.sort(key=_sort_key)\n\n        # Generate sections\n        sections = []\n\n        # --- Helper: build a FlowRegion spanning multiple pages ---\n        def _build_flow_region(start_el, end_el):\n            \"\"\"Return a FlowRegion that covers from *start_el* to *end_el* (inclusive).\n            If *end_el* is None, the region continues to the bottom of the last\n            page in this PageCollection.\"\"\"\n            # Local imports to avoid top-level cycles\n            from natural_pdf.elements.region import Region\n            from natural_pdf.flows.element import FlowElement\n            from natural_pdf.flows.flow import Flow\n            from natural_pdf.flows.region import FlowRegion\n\n            start_pg = start_el.page\n            end_pg = end_el.page if end_el is not None else self.pages[-1]\n\n            parts: list[Region] = []\n\n            # Use the actual top of the start element (for implicit starts this is\n            # the bottom of the previous end element) instead of forcing to 0.\n            start_top = start_el.top\n\n            # Slice of first page beginning at *start_top*\n            parts.append(Region(start_pg, (0, start_top, start_pg.width, start_pg.height)))\n\n            # Full middle pages\n            for pg_idx in range(start_pg.index + 1, end_pg.index):\n                mid_pg = self.pages[pg_idx]\n                parts.append(Region(mid_pg, (0, 0, mid_pg.width, mid_pg.height)))\n\n            # Slice of last page (if distinct)\n            if end_pg is not start_pg:\n                bottom = end_el.bottom if end_el is not None else end_pg.height\n                parts.append(Region(end_pg, (0, 0, end_pg.width, bottom)))\n\n            flow = Flow(segments=parts, arrangement=\"vertical\")\n            src_fe = FlowElement(physical_object=start_el, flow=flow)\n            return FlowRegion(\n                flow=flow,\n                constituent_regions=parts,\n                source_flow_element=src_fe,\n                boundary_element_found=end_el,\n            )\n\n        # ------------------------------------------------------------------\n\n        current_start = None\n\n        for i, boundary in enumerate(section_boundaries):\n            # If it's a start boundary and we don't have a current start\n            if boundary[\"type\"] == \"start\" and current_start is None:\n                current_start = boundary\n\n            # If it's an end boundary and we have a current start\n            elif boundary[\"type\"] == \"end\" and current_start is not None:\n                # Create a section from current_start to this boundary\n                start_element = current_start[\"element\"]\n                end_element = boundary[\"element\"]\n\n                # If both elements are on the same page, use the page's get_section_between\n                if start_element.page == end_element.page:\n                    # For implicit start elements, create a region from the top of the page\n                    if hasattr(start_element, \"is_implicit_start\"):\n                        from natural_pdf.elements.region import Region\n\n                        section = Region(\n                            start_element.page,\n                            (0, start_element.top, start_element.page.width, end_element.bottom),\n                        )\n                        section.start_element = start_element\n                        section.boundary_element_found = end_element\n                    else:\n                        section = start_element.page.get_section_between(\n                            start_element, end_element, include_boundaries\n                        )\n                    sections.append(section)\n                else:\n                    # Create FlowRegion spanning pages\n                    flow_region = _build_flow_region(start_element, end_element)\n                    sections.append(flow_region)\n\n                current_start = None\n\n            # If it's another start boundary and we have a current start (for splitting by starts only)\n            elif boundary[\"type\"] == \"start\" and current_start is not None and not end_elements:\n                # Create a section from current_start to just before this boundary\n                start_element = current_start[\"element\"]\n\n                # Find the last element before this boundary on the same page\n                if start_element.page == boundary[\"element\"].page:\n                    # Find elements on this page\n                    page_elements = [e for e in all_elements if e.page == start_element.page]\n                    # Sort by position\n                    page_elements.sort(key=lambda e: (e.top, e.x0))\n\n                    # Find the last element before the boundary\n                    end_idx = (\n                        page_elements.index(boundary[\"element\"]) - 1\n                        if boundary[\"element\"] in page_elements\n                        else -1\n                    )\n                    end_element = page_elements[end_idx] if end_idx &gt;= 0 else None\n\n                    # Create the section\n                    section = start_element.page.get_section_between(\n                        start_element, end_element, include_boundaries\n                    )\n                    sections.append(section)\n                else:\n                    # Cross-page section - create from current_start to the end of its page\n                    from natural_pdf.elements.region import Region\n\n                    start_page = start_element.page\n\n                    # Handle implicit start elements\n                    start_top = start_element.top\n                    region = Region(start_page, (0, start_top, start_page.width, start_page.height))\n                    region.start_element = start_element\n                    sections.append(region)\n\n                current_start = boundary\n\n        # Handle the last section if we have a current start\n        if current_start is not None:\n            start_element = current_start[\"element\"]\n            start_page = start_element.page\n\n            if end_elements:\n                # With end_elements, we need an explicit end - use the last element\n                # on the last page of the collection\n                last_page = self.pages[-1]\n                last_page_elements = [e for e in all_elements if e.page == last_page]\n                last_page_elements.sort(key=lambda e: (e.top, e.x0))\n                end_element = last_page_elements[-1] if last_page_elements else None\n\n                # Create FlowRegion spanning multiple pages using helper\n                flow_region = _build_flow_region(start_element, end_element)\n                sections.append(flow_region)\n            else:\n                # With start_elements only, create a section to the end of the current page\n                from natural_pdf.elements.region import Region\n\n                # Handle implicit start elements\n                start_top = start_element.top\n                region = Region(start_page, (0, start_top, start_page.width, start_page.height))\n                region.start_element = start_element\n                sections.append(region)\n\n        return ElementCollection(sections)\n\n    def _gather_analysis_data(\n        self,\n        analysis_keys: List[str],\n        include_content: bool,\n        include_images: bool,\n        image_dir: Optional[Path],\n        image_format: str,\n        image_resolution: int,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Gather analysis data from all pages in the collection.\n\n        Args:\n            analysis_keys: Keys in the analyses dictionary to export\n            include_content: Whether to include extracted text\n            include_images: Whether to export images\n            image_dir: Directory to save images\n            image_format: Format to save images\n            image_resolution: Resolution for exported images\n\n        Returns:\n            List of dictionaries containing analysis data\n        \"\"\"\n        if not self.elements:\n            logger.warning(\"No pages found in collection\")\n            return []\n\n        all_data = []\n\n        for page in self.elements:\n            # Basic page information\n            page_data = {\n                \"page_number\": page.number,\n                \"page_index\": page.index,\n                \"width\": page.width,\n                \"height\": page.height,\n            }\n\n            # Add PDF information if available\n            if hasattr(page, \"pdf\") and page.pdf:\n                page_data[\"pdf_path\"] = page.pdf.path\n                page_data[\"pdf_filename\"] = Path(page.pdf.path).name\n\n            # Include extracted text if requested\n            if include_content:\n                try:\n                    page_data[\"content\"] = page.extract_text(preserve_whitespace=True)\n                except Exception as e:\n                    logger.error(f\"Error extracting text from page {page.number}: {e}\")\n                    page_data[\"content\"] = \"\"\n\n            # Save image if requested\n            if include_images:\n                try:\n                    # Create image filename\n                    pdf_name = \"unknown\"\n                    if hasattr(page, \"pdf\") and page.pdf:\n                        pdf_name = Path(page.pdf.path).stem\n\n                    image_filename = f\"{pdf_name}_page_{page.number}.{image_format}\"\n                    image_path = image_dir / image_filename\n\n                    # Save image\n                    page.save_image(\n                        str(image_path), resolution=image_resolution, include_highlights=True\n                    )\n\n                    # Add relative path to data\n                    page_data[\"image_path\"] = str(Path(image_path).relative_to(image_dir.parent))\n                except Exception as e:\n                    logger.error(f\"Error saving image for page {page.number}: {e}\")\n                    page_data[\"image_path\"] = None\n\n            # Add analyses data\n            if hasattr(page, \"analyses\") and page.analyses:\n                for key in analysis_keys:\n                    if key not in page.analyses:\n                        raise KeyError(f\"Analysis key '{key}' not found in page {page.number}\")\n\n                    # Get the analysis result\n                    analysis_result = page.analyses[key]\n\n                    # If the result has a to_dict method, use it\n                    if hasattr(analysis_result, \"to_dict\"):\n                        analysis_data = analysis_result.to_dict()\n                    else:\n                        # Otherwise, use the result directly if it's dict-like\n                        try:\n                            analysis_data = dict(analysis_result)\n                        except (TypeError, ValueError):\n                            # Last resort: convert to string\n                            analysis_data = {\"raw_result\": str(analysis_result)}\n\n                    # Add analysis data to page data with the key as prefix\n                    for k, v in analysis_data.items():\n                        page_data[f\"{key}.{k}\"] = v\n\n            all_data.append(page_data)\n\n        return all_data\n\n    # --- Deskew Method --- #\n\n    def deskew(\n        self,\n        resolution: int = 300,\n        detection_resolution: int = 72,\n        force_overwrite: bool = False,\n        **deskew_kwargs,\n    ) -&gt; \"PDF\":  # Changed return type\n        \"\"\"\n        Creates a new, in-memory PDF object containing deskewed versions of the pages\n        in this collection.\n\n        This method delegates the actual processing to the parent PDF object's\n        `deskew` method.\n\n        Important: The returned PDF is image-based. Any existing text, OCR results,\n        annotations, or other elements from the original pages will *not* be carried over.\n\n        Args:\n            resolution: DPI resolution for rendering the output deskewed pages.\n            detection_resolution: DPI resolution used for skew detection if angles are not\n                                  already cached on the page objects.\n            force_overwrite: If False (default), raises a ValueError if any target page\n                             already contains processed elements (text, OCR, regions) to\n                             prevent accidental data loss. Set to True to proceed anyway.\n            **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                             during automatic detection (e.g., `max_angle`, `num_peaks`).\n\n        Returns:\n            A new PDF object representing the deskewed document.\n\n        Raises:\n            ImportError: If 'deskew' or 'img2pdf' libraries are not installed (raised by PDF.deskew).\n            ValueError: If `force_overwrite` is False and target pages contain elements (raised by PDF.deskew),\n                        or if the collection is empty.\n            RuntimeError: If pages lack a parent PDF reference, or the parent PDF lacks the `deskew` method.\n        \"\"\"\n        if not self.pages:\n            logger.warning(\"Cannot deskew an empty PageCollection.\")\n            raise ValueError(\"Cannot deskew an empty PageCollection.\")\n\n        # Assume all pages share the same parent PDF object\n        # Need to hint the type of _parent for type checkers\n        if TYPE_CHECKING:\n            parent_pdf: \"natural_pdf.core.pdf.PDF\" = self.pages[0]._parent\n        else:\n            parent_pdf = self.pages[0]._parent\n\n        if not parent_pdf or not hasattr(parent_pdf, \"deskew\") or not callable(parent_pdf.deskew):\n            raise RuntimeError(\n                \"Parent PDF reference not found or parent PDF lacks the required 'deskew' method.\"\n            )\n\n        # Get the 0-based indices of the pages in this collection\n        page_indices = self._get_page_indices()\n        logger.info(\n            f\"PageCollection: Delegating deskew to parent PDF for page indices: {page_indices}\"\n        )\n\n        # Delegate the call to the parent PDF object for the relevant pages\n        # Pass all relevant arguments through (no output_path anymore)\n        return parent_pdf.deskew(\n            pages=page_indices,\n            resolution=resolution,\n            detection_resolution=detection_resolution,\n            force_overwrite=force_overwrite,\n            **deskew_kwargs,\n        )\n\n    # --- End Deskew Method --- #\n\n    def _get_render_specs(\n        self,\n        mode: Literal[\"show\", \"render\"] = \"show\",\n        color: Optional[Union[str, Tuple[int, int, int]]] = None,\n        highlights: Optional[List[Dict[str, Any]]] = None,\n        crop: Union[bool, Literal[\"content\"]] = False,\n        crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n        **kwargs,\n    ) -&gt; List[RenderSpec]:\n        \"\"\"Get render specifications for this page collection.\n\n        For page collections, we return specs for all pages that will be\n        rendered into a grid layout.\n\n        Args:\n            mode: Rendering mode - 'show' includes highlights, 'render' is clean\n            color: Color for highlighting pages in show mode\n            highlights: Additional highlight groups to show\n            crop: Whether to crop pages\n            crop_bbox: Explicit crop bounds\n            **kwargs: Additional parameters\n\n        Returns:\n            List of RenderSpec objects, one per page\n        \"\"\"\n        specs = []\n\n        # Get max pages from kwargs if specified\n        max_pages = kwargs.get(\"max_pages\")\n        pages_to_render = self.pages[:max_pages] if max_pages else self.pages\n\n        for page in pages_to_render:\n            if hasattr(page, \"_get_render_specs\"):\n                # Page has the new unified rendering\n                page_specs = page._get_render_specs(\n                    mode=mode,\n                    color=color,\n                    highlights=highlights,\n                    crop=crop,\n                    crop_bbox=crop_bbox,\n                    **kwargs,\n                )\n                specs.extend(page_specs)\n            else:\n                # Fallback for pages without unified rendering\n                spec = RenderSpec(page=page)\n                if crop_bbox:\n                    spec.crop_bbox = crop_bbox\n                specs.append(spec)\n\n        return specs\n\n    def save_pdf(\n        self,\n        output_path: Union[str, Path],\n        ocr: bool = False,\n        original: bool = False,\n        dpi: int = 300,\n    ):\n        \"\"\"\n        Saves the pages in this collection to a new PDF file.\n\n        Choose one saving mode:\n        - `ocr=True`: Creates a new, image-based PDF using OCR results. This\n          makes the text generated during the natural-pdf session searchable,\n          but loses original vector content. Requires 'ocr-export' extras.\n        - `original=True`: Extracts the original pages from the source PDF,\n          preserving all vector content, fonts, and annotations. OCR results\n          from the natural-pdf session are NOT included. Requires 'ocr-export' extras.\n\n        Args:\n            output_path: Path to save the new PDF file.\n            ocr: If True, save as a searchable, image-based PDF using OCR data.\n            original: If True, save the original, vector-based pages.\n            dpi: Resolution (dots per inch) used only when ocr=True for\n                 rendering page images and aligning the text layer.\n\n        Raises:\n            ValueError: If the collection is empty, if neither or both 'ocr'\n                        and 'original' are True, or if 'original=True' and\n                        pages originate from different PDFs.\n            ImportError: If required libraries ('pikepdf', 'Pillow')\n                         are not installed for the chosen mode.\n            RuntimeError: If an unexpected error occurs during saving.\n        \"\"\"\n        if not self.pages:\n            raise ValueError(\"Cannot save an empty PageCollection.\")\n\n        if not (ocr ^ original):  # XOR: exactly one must be true\n            raise ValueError(\"Exactly one of 'ocr' or 'original' must be True.\")\n\n        output_path_obj = Path(output_path)\n        output_path_str = str(output_path_obj)\n\n        if ocr:\n            if create_searchable_pdf is None:\n                raise ImportError(\n                    \"Saving with ocr=True requires 'pikepdf' and 'Pillow'. \"\n                    'Install with: pip install \\\\\"natural-pdf[ocr-export]\\\\\"'  # Escaped quotes\n                )\n\n            # Check for non-OCR vector elements (provide a warning)\n            has_vector_elements = False\n            for page in self.pages:\n                # Simplified check for common vector types or non-OCR chars/words\n                if (\n                    hasattr(page, \"rects\")\n                    and page.rects\n                    or hasattr(page, \"lines\")\n                    and page.lines\n                    or hasattr(page, \"curves\")\n                    and page.curves\n                    or (\n                        hasattr(page, \"chars\")\n                        and any(getattr(el, \"source\", None) != \"ocr\" for el in page.chars)\n                    )\n                    or (\n                        hasattr(page, \"words\")\n                        and any(getattr(el, \"source\", None) != \"ocr\" for el in page.words)\n                    )\n                ):\n                    has_vector_elements = True\n                    break\n            if has_vector_elements:\n                logger.warning(\n                    \"Warning: Saving with ocr=True creates an image-based PDF. \"\n                    \"Original vector elements (rects, lines, non-OCR text/chars) \"\n                    \"on selected pages will not be preserved in the output file.\"\n                )\n\n            logger.info(f\"Saving searchable PDF (OCR text layer) to: {output_path_str}\")\n            try:\n                # Delegate to the searchable PDF exporter function\n                # Pass `self` (the PageCollection instance) as the source\n                create_searchable_pdf(self, output_path_str, dpi=dpi)\n                # Success log is now inside create_searchable_pdf if needed, or keep here\n                # logger.info(f\"Successfully saved searchable PDF to: {output_path_str}\")\n            except Exception as e:\n                logger.error(f\"Failed to create searchable PDF: {e}\", exc_info=True)\n                # Re-raise as RuntimeError for consistency, potentially handled in exporter too\n                raise RuntimeError(f\"Failed to create searchable PDF: {e}\") from e\n\n        elif original:\n            # ---&gt; MODIFIED: Call the new exporter\n            if create_original_pdf is None:\n                raise ImportError(\n                    \"Saving with original=True requires 'pikepdf'. \"\n                    'Install with: pip install \\\\\"natural-pdf[ocr-export]\\\\\"'  # Escaped quotes\n                )\n\n            # Check for OCR elements (provide a warning) - keep this check here\n            has_ocr_elements = False\n            for page in self.pages:\n                # Use find_all which returns a collection; check if it's non-empty\n                if hasattr(page, \"find_all\"):\n                    ocr_text_elements = page.find_all(\"text[source=ocr]\")\n                    if ocr_text_elements:  # Check truthiness of collection\n                        has_ocr_elements = True\n                        break\n                elif hasattr(page, \"words\"):  # Fallback check if find_all isn't present?\n                    if any(getattr(el, \"source\", None) == \"ocr\" for el in page.words):\n                        has_ocr_elements = True\n                        break\n\n            if has_ocr_elements:\n                logger.warning(\n                    \"Warning: Saving with original=True preserves original page content. \"\n                    \"OCR text generated in this session will not be included in the saved file.\"\n                )\n\n            logger.info(f\"Saving original pages PDF to: {output_path_str}\")\n            try:\n                # Delegate to the original PDF exporter function\n                # Pass `self` (the PageCollection instance) as the source\n                create_original_pdf(self, output_path_str)\n                # Success log is now inside create_original_pdf\n                # logger.info(f\"Successfully saved original pages PDF to: {output_path_str}\")\n            except Exception as e:\n                # Error logging is handled within create_original_pdf\n                # Re-raise the exception caught from the exporter\n                raise e  # Keep the original exception type (ValueError, RuntimeError, etc.)\n            # &lt;--- END MODIFIED\n\n    def to_flow(\n        self,\n        arrangement: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n        alignment: Literal[\"start\", \"center\", \"end\", \"top\", \"left\", \"bottom\", \"right\"] = \"start\",\n        segment_gap: float = 0.0,\n    ) -&gt; \"Flow\":\n        \"\"\"\n        Convert this PageCollection to a Flow for cross-page operations.\n\n        This enables treating multiple pages as a continuous logical document\n        structure, useful for multi-page tables, articles spanning columns,\n        or any content requiring reading order across page boundaries.\n\n        Args:\n            arrangement: Primary flow direction ('vertical' or 'horizontal').\n                        'vertical' stacks pages top-to-bottom (most common).\n                        'horizontal' arranges pages left-to-right.\n            alignment: Cross-axis alignment for pages of different sizes:\n                      For vertical: 'left'/'start', 'center', 'right'/'end'\n                      For horizontal: 'top'/'start', 'center', 'bottom'/'end'\n            segment_gap: Virtual gap between pages in PDF points (default: 0.0).\n\n        Returns:\n            Flow object that can perform operations across all pages in sequence.\n\n        Example:\n            Multi-page table extraction:\n            ```python\n            pdf = npdf.PDF(\"multi_page_report.pdf\")\n\n            # Create flow for pages 2-4 containing a table\n            table_flow = pdf.pages[1:4].to_flow()\n\n            # Extract table as if it were continuous\n            table_data = table_flow.extract_table()\n            df = table_data.df\n            ```\n\n            Cross-page element search:\n            ```python\n            # Find all headers across multiple pages\n            headers = pdf.pages[5:10].to_flow().find_all('text[size&gt;12]:bold')\n\n            # Analyze layout across pages\n            regions = pdf.pages.to_flow().analyze_layout(engine='yolo')\n            ```\n        \"\"\"\n        from natural_pdf.flows.flow import Flow\n\n        return Flow(\n            segments=self,  # Flow constructor now handles PageCollection\n            arrangement=arrangement,\n            alignment=alignment,\n            segment_gap=segment_gap,\n        )\n\n    def analyze_layout(self, *args, **kwargs) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Analyzes the layout of each page in the collection.\n\n        This method iterates through each page, calls its analyze_layout method,\n        and returns a single ElementCollection containing all the detected layout\n        regions from all pages.\n\n        Args:\n            *args: Positional arguments to pass to each page's analyze_layout method.\n            **kwargs: Keyword arguments to pass to each page's analyze_layout method.\n                      A 'show_progress' kwarg can be included to show a progress bar.\n\n        Returns:\n            An ElementCollection of all detected Region objects.\n        \"\"\"\n        all_regions = []\n\n        show_progress = kwargs.pop(\"show_progress\", True)\n\n        iterator = self.pages\n        if show_progress:\n            try:\n                from tqdm.auto import tqdm\n\n                iterator = tqdm(self.pages, desc=\"Analyzing layout\")\n            except ImportError:\n                pass  # tqdm not installed\n\n        for page in iterator:\n            # Each page's analyze_layout method returns an ElementCollection\n            regions_collection = page.analyze_layout(*args, **kwargs)\n            if regions_collection:\n                all_regions.extend(regions_collection.elements)\n\n        return ElementCollection(all_regions)\n\n    def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n        \"\"\"\n        Create a highlight context for accumulating highlights.\n\n        This allows for clean syntax to show multiple highlight groups:\n\n        Example:\n            with pages.highlights() as h:\n                h.add(pages.find_all('table'), label='tables', color='blue')\n                h.add(pages.find_all('text:bold'), label='bold text', color='red')\n                h.show()\n\n        Or with automatic display:\n            with pages.highlights(show=True) as h:\n                h.add(pages.find_all('table'), label='tables')\n                h.add(pages.find_all('text:bold'), label='bold')\n                # Automatically shows when exiting the context\n\n        Args:\n            show: If True, automatically show highlights when exiting context\n\n        Returns:\n            HighlightContext for accumulating highlights\n        \"\"\"\n        from natural_pdf.core.highlighting_service import HighlightContext\n\n        return HighlightContext(self, show_on_exit=show)\n\n    def groupby(self, by: Union[str, Callable], *, show_progress: bool = True) -&gt; \"PageGroupBy\":\n        \"\"\"\n        Group pages by selector text or callable result.\n\n        Args:\n            by: CSS selector string or callable function\n            show_progress: Whether to show progress bar during computation (default: True)\n\n        Returns:\n            PageGroupBy object supporting iteration and dict-like access\n\n        Examples:\n            # Group by header text\n            for title, pages in pdf.pages.groupby('text[size=16]'):\n                print(f\"Section: {title}\")\n\n            # Group by callable\n            for city, pages in pdf.pages.groupby(lambda p: p.find('text:contains(\"CITY\")').extract_text()):\n                process_city_pages(pages)\n\n            # Quick exploration with indexing\n            grouped = pdf.pages.groupby('text[size=16]')\n            grouped.info()                    # Show all groups\n            first_section = grouped[0]        # First group\n            last_section = grouped[-1]       # Last group\n\n            # Dict-like access by name\n            madison_pages = grouped.get('CITY OF MADISON')\n            madison_pages = grouped['CITY OF MADISON']  # Alternative\n\n            # Disable progress bar for small collections\n            grouped = pdf.pages.groupby('text[size=16]', show_progress=False)\n        \"\"\"\n        from natural_pdf.core.page_groupby import PageGroupBy\n\n        return PageGroupBy(self, by, show_progress=show_progress)\n</code></pre>"},{"location":"api/#natural_pdf.PageCollection-functions","title":"Functions","text":"<code>natural_pdf.PageCollection.__getitem__(idx)</code> <p>Support indexing and slicing.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def __getitem__(self, idx) -&gt; Union[P, \"PageCollection[P]\"]:\n    \"\"\"Support indexing and slicing.\"\"\"\n    if isinstance(idx, slice):\n        return PageCollection(self.pages[idx])\n    return self.pages[idx]\n</code></pre> <code>natural_pdf.PageCollection.__init__(pages)</code> <p>Initialize a page collection.</p> <p>Parameters:</p> Name Type Description Default <code>pages</code> <code>Union[List[P], Sequence[P]]</code> <p>List or sequence of Page objects (can be lazy)</p> required Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def __init__(self, pages: Union[List[P], Sequence[P]]):\n    \"\"\"\n    Initialize a page collection.\n\n    Args:\n        pages: List or sequence of Page objects (can be lazy)\n    \"\"\"\n    # Store the sequence as-is to preserve lazy behavior\n    # Only convert to list if we need list-specific operations\n    if hasattr(pages, \"__iter__\") and hasattr(pages, \"__len__\"):\n        self.pages = pages\n    else:\n        # Fallback for non-sequence types\n        self.pages = list(pages)\n</code></pre> <code>natural_pdf.PageCollection.__iter__()</code> <p>Support iteration.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def __iter__(self) -&gt; Iterator[P]:\n    \"\"\"Support iteration.\"\"\"\n    return iter(self.pages)\n</code></pre> <code>natural_pdf.PageCollection.__len__()</code> <p>Return the number of pages in the collection.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of pages in the collection.\"\"\"\n    return len(self.pages)\n</code></pre> <code>natural_pdf.PageCollection.__repr__()</code> <p>Return a string representation showing the page count.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation showing the page count.\"\"\"\n    return f\"&lt;PageCollection(count={len(self)})&gt;\"\n</code></pre> <code>natural_pdf.PageCollection.analyze_layout(*args, **kwargs)</code> <p>Analyzes the layout of each page in the collection.</p> <p>This method iterates through each page, calls its analyze_layout method, and returns a single ElementCollection containing all the detected layout regions from all pages.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Positional arguments to pass to each page's analyze_layout method.</p> <code>()</code> <code>**kwargs</code> <p>Keyword arguments to pass to each page's analyze_layout method.       A 'show_progress' kwarg can be included to show a progress bar.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection[Region]</code> <p>An ElementCollection of all detected Region objects.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def analyze_layout(self, *args, **kwargs) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Analyzes the layout of each page in the collection.\n\n    This method iterates through each page, calls its analyze_layout method,\n    and returns a single ElementCollection containing all the detected layout\n    regions from all pages.\n\n    Args:\n        *args: Positional arguments to pass to each page's analyze_layout method.\n        **kwargs: Keyword arguments to pass to each page's analyze_layout method.\n                  A 'show_progress' kwarg can be included to show a progress bar.\n\n    Returns:\n        An ElementCollection of all detected Region objects.\n    \"\"\"\n    all_regions = []\n\n    show_progress = kwargs.pop(\"show_progress\", True)\n\n    iterator = self.pages\n    if show_progress:\n        try:\n            from tqdm.auto import tqdm\n\n            iterator = tqdm(self.pages, desc=\"Analyzing layout\")\n        except ImportError:\n            pass  # tqdm not installed\n\n    for page in iterator:\n        # Each page's analyze_layout method returns an ElementCollection\n        regions_collection = page.analyze_layout(*args, **kwargs)\n        if regions_collection:\n            all_regions.extend(regions_collection.elements)\n\n    return ElementCollection(all_regions)\n</code></pre> <code>natural_pdf.PageCollection.apply_ocr(engine=None, languages=None, min_confidence=None, device=None, resolution=None, apply_exclusions=True, replace=True, options=None)</code> <p>Applies OCR to all pages within this collection using batch processing.</p> <p>This delegates the work to the parent PDF object's <code>apply_ocr</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Optional[str]</code> <p>Name of the OCR engine (e.g., 'easyocr', 'paddleocr').</p> <code>None</code> <code>languages</code> <code>Optional[List[str]]</code> <p>List of language codes (e.g., ['en', 'fr'], ['en', 'ch']).        Must be codes understood by the specific selected engine.        No mapping is performed.</p> <code>None</code> <code>min_confidence</code> <code>Optional[float]</code> <p>Minimum confidence threshold for detected text (0.0 to 1.0).</p> <code>None</code> <code>device</code> <code>Optional[str]</code> <p>Device to run OCR on (e.g., 'cpu', 'cuda', 'mps').</p> <code>None</code> <code>resolution</code> <code>Optional[int]</code> <p>DPI resolution to render page images before OCR (e.g., 150, 300).</p> <code>None</code> <code>apply_exclusions</code> <code>bool</code> <p>If True (default), render page images for OCR with               excluded areas masked (whited out). If False, OCR               the raw page images without masking exclusions.</p> <code>True</code> <code>replace</code> <code>bool</code> <p>If True (default), remove any existing OCR elements before     adding new ones. If False, add new OCR elements to existing ones.</p> <code>True</code> <code>options</code> <code>Optional[Any]</code> <p>An engine-specific options object (e.g., EasyOCROptions) or dict.</p> <code>None</code> <p>Returns:</p> Type Description <code>PageCollection[P]</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If pages lack a parent PDF or parent lacks <code>apply_ocr</code>.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def apply_ocr(\n    self,\n    engine: Optional[str] = None,\n    # --- Common OCR Parameters (Direct Arguments) ---\n    languages: Optional[List[str]] = None,\n    min_confidence: Optional[float] = None,  # Min confidence threshold\n    device: Optional[str] = None,\n    resolution: Optional[int] = None,  # DPI for rendering\n    apply_exclusions: bool = True,  # New parameter\n    replace: bool = True,  # Whether to replace existing OCR elements\n    # --- Engine-Specific Options ---\n    options: Optional[Any] = None,  # e.g., EasyOCROptions(...)\n) -&gt; \"PageCollection[P]\":\n    \"\"\"\n    Applies OCR to all pages within this collection using batch processing.\n\n    This delegates the work to the parent PDF object's `apply_ocr` method.\n\n    Args:\n        engine: Name of the OCR engine (e.g., 'easyocr', 'paddleocr').\n        languages: List of language codes (e.g., ['en', 'fr'], ['en', 'ch']).\n                   **Must be codes understood by the specific selected engine.**\n                   No mapping is performed.\n        min_confidence: Minimum confidence threshold for detected text (0.0 to 1.0).\n        device: Device to run OCR on (e.g., 'cpu', 'cuda', 'mps').\n        resolution: DPI resolution to render page images before OCR (e.g., 150, 300).\n        apply_exclusions: If True (default), render page images for OCR with\n                          excluded areas masked (whited out). If False, OCR\n                          the raw page images without masking exclusions.\n        replace: If True (default), remove any existing OCR elements before\n                adding new ones. If False, add new OCR elements to existing ones.\n        options: An engine-specific options object (e.g., EasyOCROptions) or dict.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        RuntimeError: If pages lack a parent PDF or parent lacks `apply_ocr`.\n        (Propagates exceptions from PDF.apply_ocr)\n    \"\"\"\n    if not self.pages:\n        logger.warning(\"Cannot apply OCR to an empty PageCollection.\")\n        return self\n\n    # Assume all pages share the same parent PDF object\n    first_page = self.pages[0]\n    if not hasattr(first_page, \"_parent\") or not first_page._parent:\n        raise RuntimeError(\"Pages in this collection do not have a parent PDF reference.\")\n\n    parent_pdf = first_page._parent\n\n    if not hasattr(parent_pdf, \"apply_ocr\") or not callable(parent_pdf.apply_ocr):\n        raise RuntimeError(\"Parent PDF object does not have the required 'apply_ocr' method.\")\n\n    # Get the 0-based indices of the pages in this collection\n    page_indices = self._get_page_indices()\n\n    logger.info(f\"Applying OCR via parent PDF to page indices: {page_indices} in collection.\")\n\n    # Delegate the batch call to the parent PDF object, passing direct args and apply_exclusions\n    parent_pdf.apply_ocr(\n        pages=page_indices,\n        engine=engine,\n        languages=languages,\n        min_confidence=min_confidence,  # Pass the renamed parameter\n        device=device,\n        resolution=resolution,\n        apply_exclusions=apply_exclusions,  # Pass down\n        replace=replace,  # Pass the replace parameter\n        options=options,\n    )\n    # The PDF method modifies the Page objects directly by adding elements.\n\n    return self  # Return self for chaining\n</code></pre> <code>natural_pdf.PageCollection.deskew(resolution=300, detection_resolution=72, force_overwrite=False, **deskew_kwargs)</code> <p>Creates a new, in-memory PDF object containing deskewed versions of the pages in this collection.</p> <p>This method delegates the actual processing to the parent PDF object's <code>deskew</code> method.</p> <p>Important: The returned PDF is image-based. Any existing text, OCR results, annotations, or other elements from the original pages will not be carried over.</p> <p>Parameters:</p> Name Type Description Default <code>resolution</code> <code>int</code> <p>DPI resolution for rendering the output deskewed pages.</p> <code>300</code> <code>detection_resolution</code> <code>int</code> <p>DPI resolution used for skew detection if angles are not                   already cached on the page objects.</p> <code>72</code> <code>force_overwrite</code> <code>bool</code> <p>If False (default), raises a ValueError if any target page              already contains processed elements (text, OCR, regions) to              prevent accidental data loss. Set to True to proceed anyway.</p> <code>False</code> <code>**deskew_kwargs</code> <p>Additional keyword arguments passed to <code>deskew.determine_skew</code>              during automatic detection (e.g., <code>max_angle</code>, <code>num_peaks</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <code>PDF</code> <p>A new PDF object representing the deskewed document.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If 'deskew' or 'img2pdf' libraries are not installed (raised by PDF.deskew).</p> <code>ValueError</code> <p>If <code>force_overwrite</code> is False and target pages contain elements (raised by PDF.deskew),         or if the collection is empty.</p> <code>RuntimeError</code> <p>If pages lack a parent PDF reference, or the parent PDF lacks the <code>deskew</code> method.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def deskew(\n    self,\n    resolution: int = 300,\n    detection_resolution: int = 72,\n    force_overwrite: bool = False,\n    **deskew_kwargs,\n) -&gt; \"PDF\":  # Changed return type\n    \"\"\"\n    Creates a new, in-memory PDF object containing deskewed versions of the pages\n    in this collection.\n\n    This method delegates the actual processing to the parent PDF object's\n    `deskew` method.\n\n    Important: The returned PDF is image-based. Any existing text, OCR results,\n    annotations, or other elements from the original pages will *not* be carried over.\n\n    Args:\n        resolution: DPI resolution for rendering the output deskewed pages.\n        detection_resolution: DPI resolution used for skew detection if angles are not\n                              already cached on the page objects.\n        force_overwrite: If False (default), raises a ValueError if any target page\n                         already contains processed elements (text, OCR, regions) to\n                         prevent accidental data loss. Set to True to proceed anyway.\n        **deskew_kwargs: Additional keyword arguments passed to `deskew.determine_skew`\n                         during automatic detection (e.g., `max_angle`, `num_peaks`).\n\n    Returns:\n        A new PDF object representing the deskewed document.\n\n    Raises:\n        ImportError: If 'deskew' or 'img2pdf' libraries are not installed (raised by PDF.deskew).\n        ValueError: If `force_overwrite` is False and target pages contain elements (raised by PDF.deskew),\n                    or if the collection is empty.\n        RuntimeError: If pages lack a parent PDF reference, or the parent PDF lacks the `deskew` method.\n    \"\"\"\n    if not self.pages:\n        logger.warning(\"Cannot deskew an empty PageCollection.\")\n        raise ValueError(\"Cannot deskew an empty PageCollection.\")\n\n    # Assume all pages share the same parent PDF object\n    # Need to hint the type of _parent for type checkers\n    if TYPE_CHECKING:\n        parent_pdf: \"natural_pdf.core.pdf.PDF\" = self.pages[0]._parent\n    else:\n        parent_pdf = self.pages[0]._parent\n\n    if not parent_pdf or not hasattr(parent_pdf, \"deskew\") or not callable(parent_pdf.deskew):\n        raise RuntimeError(\n            \"Parent PDF reference not found or parent PDF lacks the required 'deskew' method.\"\n        )\n\n    # Get the 0-based indices of the pages in this collection\n    page_indices = self._get_page_indices()\n    logger.info(\n        f\"PageCollection: Delegating deskew to parent PDF for page indices: {page_indices}\"\n    )\n\n    # Delegate the call to the parent PDF object for the relevant pages\n    # Pass all relevant arguments through (no output_path anymore)\n    return parent_pdf.deskew(\n        pages=page_indices,\n        resolution=resolution,\n        detection_resolution=detection_resolution,\n        force_overwrite=force_overwrite,\n        **deskew_kwargs,\n    )\n</code></pre> <code>natural_pdf.PageCollection.extract_text(keep_blank_chars=True, apply_exclusions=True, strip=None, **kwargs)</code> <p>Extract text from all pages in the collection.</p> <p>Parameters:</p> Name Type Description Default <code>keep_blank_chars</code> <code>bool</code> <p>Whether to keep blank characters (default: True)</p> <code>True</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to apply exclusion regions (default: True)</p> <code>True</code> <code>strip</code> <code>Optional[bool]</code> <p>Whether to strip whitespace from the extracted text.</p> <code>None</code> <code>**kwargs</code> <p>Additional extraction parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Combined text from all pages</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def extract_text(\n    self,\n    keep_blank_chars: bool = True,\n    apply_exclusions: bool = True,\n    strip: Optional[bool] = None,\n    **kwargs,\n) -&gt; str:\n    \"\"\"\n    Extract text from all pages in the collection.\n\n    Args:\n        keep_blank_chars: Whether to keep blank characters (default: True)\n        apply_exclusions: Whether to apply exclusion regions (default: True)\n        strip: Whether to strip whitespace from the extracted text.\n        **kwargs: Additional extraction parameters\n\n    Returns:\n        Combined text from all pages\n    \"\"\"\n    texts = []\n    for page in self.pages:\n        text = page.extract_text(\n            keep_blank_chars=keep_blank_chars,\n            apply_exclusions=apply_exclusions,\n            **kwargs,\n        )\n        texts.append(text)\n\n    combined = \"\\n\".join(texts)\n\n    # Default strip behaviour: if caller picks, honour; else respect layout flag passed via kwargs.\n    use_layout = kwargs.get(\"layout\", False)\n    strip_final = strip if strip is not None else (not use_layout)\n\n    if strip_final:\n        combined = \"\\n\".join(line.rstrip() for line in combined.splitlines()).strip()\n\n    return combined\n</code></pre> <code>natural_pdf.PageCollection.find(selector=None, *, text=None, contains='all', apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find(*, text: str, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[T]\n</code></pre><pre><code>find(selector: str, *, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[T]\n</code></pre> <p>Find the first element matching the selector OR text across all pages in the collection.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>contains</code> <code>str</code> <p>How to determine if elements are inside: 'all' (fully inside),      'any' (any overlap), or 'center' (center point inside).      (default: \"all\")</p> <code>'all'</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional filter parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[T]</code> <p>First matching element or None.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def find(\n    self,\n    selector: Optional[str] = None,\n    *,\n    text: Optional[str] = None,\n    contains: str = \"all\",\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; Optional[T]:\n    \"\"\"\n    Find the first element matching the selector OR text across all pages in the collection.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        contains: How to determine if elements are inside: 'all' (fully inside),\n                 'any' (any overlap), or 'center' (center point inside).\n                 (default: \"all\")\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional filter parameters.\n\n    Returns:\n        First matching element or None.\n    \"\"\"\n    # Input validation happens within page.find\n    for page in self.pages:\n        element = page.find(\n            selector=selector,\n            text=text,\n            contains=contains,\n            apply_exclusions=apply_exclusions,\n            regex=regex,\n            case=case,\n            **kwargs,\n        )\n        if element:\n            return element\n    return None\n</code></pre> <code>natural_pdf.PageCollection.find_all(selector=None, *, text=None, contains='all', apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find_all(*, text: str, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre><pre><code>find_all(selector: str, *, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre> <p>Find all elements matching the selector OR text across all pages in the collection.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>contains</code> <code>str</code> <p>How to determine if elements are inside: 'all' (fully inside),      'any' (any overlap), or 'center' (center point inside).      (default: \"all\")</p> <code>'all'</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional filter parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection with matching elements from all pages.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def find_all(\n    self,\n    selector: Optional[str] = None,\n    *,\n    text: Optional[str] = None,\n    contains: str = \"all\",\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Find all elements matching the selector OR text across all pages in the collection.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        contains: How to determine if elements are inside: 'all' (fully inside),\n                 'any' (any overlap), or 'center' (center point inside).\n                 (default: \"all\")\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional filter parameters.\n\n    Returns:\n        ElementCollection with matching elements from all pages.\n    \"\"\"\n    all_elements = []\n    # Input validation happens within page.find_all\n    for page in self.pages:\n        elements = page.find_all(\n            selector=selector,\n            text=text,\n            contains=contains,\n            apply_exclusions=apply_exclusions,\n            regex=regex,\n            case=case,\n            **kwargs,\n        )\n        if elements:\n            all_elements.extend(elements.elements)\n\n    return ElementCollection(all_elements)\n</code></pre> <code>natural_pdf.PageCollection.get_sections(start_elements=None, end_elements=None, new_section_on_page_break=False, include_boundaries='both')</code> <p>Extract sections from a page collection based on start/end elements.</p> <p>Parameters:</p> Name Type Description Default <code>start_elements</code> <p>Elements or selector string that mark the start of sections (optional)</p> <code>None</code> <code>end_elements</code> <p>Elements or selector string that mark the end of sections (optional)</p> <code>None</code> <code>new_section_on_page_break</code> <p>Whether to start a new section at page boundaries (default: False)</p> <code>False</code> <code>include_boundaries</code> <p>How to include boundary elements: 'start', 'end', 'both', or 'none' (default: 'both')</p> <code>'both'</code> <p>Returns:</p> Type Description <code>ElementCollection[Region]</code> <p>List of Region objects representing the extracted sections</p> Note <p>You can provide only start_elements, only end_elements, or both. - With only start_elements: sections go from each start to the next start (or end of page) - With only end_elements: sections go from beginning of document/page to each end - With both: sections go from each start to the corresponding end</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def get_sections(\n    self,\n    start_elements=None,\n    end_elements=None,\n    new_section_on_page_break=False,\n    include_boundaries=\"both\",\n) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Extract sections from a page collection based on start/end elements.\n\n    Args:\n        start_elements: Elements or selector string that mark the start of sections (optional)\n        end_elements: Elements or selector string that mark the end of sections (optional)\n        new_section_on_page_break: Whether to start a new section at page boundaries (default: False)\n        include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none' (default: 'both')\n\n    Returns:\n        List of Region objects representing the extracted sections\n\n    Note:\n        You can provide only start_elements, only end_elements, or both.\n        - With only start_elements: sections go from each start to the next start (or end of page)\n        - With only end_elements: sections go from beginning of document/page to each end\n        - With both: sections go from each start to the corresponding end\n    \"\"\"\n    # Find start and end elements across all pages\n    if isinstance(start_elements, str):\n        start_elements = self.find_all(start_elements).elements\n\n    if isinstance(end_elements, str):\n        end_elements = self.find_all(end_elements).elements\n\n    # If no start elements and no end elements, return empty list\n    if not start_elements and not end_elements:\n        return []\n\n    # If there are page break boundaries, we'll need to add them\n    if new_section_on_page_break:\n        # For each page boundary, create virtual \"end\" and \"start\" elements\n        for i in range(len(self.pages) - 1):\n            # Add a virtual \"end\" element at the bottom of the current page\n            page = self.pages[i]\n            # If end_elements is None, initialize it as an empty list\n            if end_elements is None:\n                end_elements = []\n\n            # Create a region at the bottom of the page as an artificial end marker\n            from natural_pdf.elements.region import Region\n\n            bottom_region = Region(page, (0, page.height - 1, page.width, page.height))\n            bottom_region.is_page_boundary = True  # Mark it as a special boundary\n            end_elements.append(bottom_region)\n\n            # Add a virtual \"start\" element at the top of the next page\n            next_page = self.pages[i + 1]\n            top_region = Region(next_page, (0, 0, next_page.width, 1))\n            top_region.is_page_boundary = True  # Mark it as a special boundary\n            start_elements.append(top_region)\n\n    # Get all elements from all pages and sort them in document order\n    all_elements = []\n    for page in self.pages:\n        elements = page.get_elements()\n        all_elements.extend(elements)\n\n    # Sort by page index, then vertical position, then horizontal position\n    all_elements.sort(key=lambda e: (e.page.index, e.top, e.x0))\n\n    # If we only have end_elements (no start_elements), create implicit start elements\n    if not start_elements and end_elements:\n        from natural_pdf.elements.region import Region\n\n        start_elements = []\n\n        # Add implicit start at the beginning of the first page\n        first_page = self.pages[0]\n        first_start = Region(first_page, (0, 0, first_page.width, 1))\n        first_start.is_implicit_start = True\n        start_elements.append(first_start)\n\n        # For each end element (except the last), add an implicit start after it\n        sorted_end_elements = sorted(end_elements, key=lambda e: (e.page.index, e.top, e.x0))\n        for i, end_elem in enumerate(sorted_end_elements[:-1]):  # Exclude last end element\n            # Create implicit start element right after this end element\n            implicit_start = Region(\n                end_elem.page, (0, end_elem.bottom, end_elem.page.width, end_elem.bottom + 1)\n            )\n            implicit_start.is_implicit_start = True\n            start_elements.append(implicit_start)\n\n    # Mark section boundaries\n    section_boundaries = []\n\n    # Add start element boundaries\n    for element in start_elements:\n        if element in all_elements:\n            idx = all_elements.index(element)\n            section_boundaries.append(\n                {\n                    \"index\": idx,\n                    \"element\": element,\n                    \"type\": \"start\",\n                    \"page_idx\": element.page.index,\n                }\n            )\n        elif hasattr(element, \"is_page_boundary\") and element.is_page_boundary:\n            # This is a virtual page boundary element\n            section_boundaries.append(\n                {\n                    \"index\": -1,  # Special index for page boundaries\n                    \"element\": element,\n                    \"type\": \"start\",\n                    \"page_idx\": element.page.index,\n                }\n            )\n        elif hasattr(element, \"is_implicit_start\") and element.is_implicit_start:\n            # This is an implicit start element\n            section_boundaries.append(\n                {\n                    \"index\": -2,  # Special index for implicit starts\n                    \"element\": element,\n                    \"type\": \"start\",\n                    \"page_idx\": element.page.index,\n                }\n            )\n\n    # Add end element boundaries if provided\n    if end_elements:\n        for element in end_elements:\n            if element in all_elements:\n                idx = all_elements.index(element)\n                section_boundaries.append(\n                    {\n                        \"index\": idx,\n                        \"element\": element,\n                        \"type\": \"end\",\n                        \"page_idx\": element.page.index,\n                    }\n                )\n            elif hasattr(element, \"is_page_boundary\") and element.is_page_boundary:\n                # This is a virtual page boundary element\n                section_boundaries.append(\n                    {\n                        \"index\": -1,  # Special index for page boundaries\n                        \"element\": element,\n                        \"type\": \"end\",\n                        \"page_idx\": element.page.index,\n                    }\n                )\n\n    # Sort boundaries by page index, then by actual document position\n    def _sort_key(boundary):\n        \"\"\"Sort boundaries by (page_idx, vertical_top, priority).\"\"\"\n        page_idx = boundary[\"page_idx\"]\n        element = boundary[\"element\"]\n\n        # Vertical position on the page\n        y_pos = getattr(element, \"top\", 0.0)\n\n        # Ensure starts come before ends at the same coordinate\n        priority = 0 if boundary[\"type\"] == \"start\" else 1\n\n        return (page_idx, y_pos, priority)\n\n    section_boundaries.sort(key=_sort_key)\n\n    # Generate sections\n    sections = []\n\n    # --- Helper: build a FlowRegion spanning multiple pages ---\n    def _build_flow_region(start_el, end_el):\n        \"\"\"Return a FlowRegion that covers from *start_el* to *end_el* (inclusive).\n        If *end_el* is None, the region continues to the bottom of the last\n        page in this PageCollection.\"\"\"\n        # Local imports to avoid top-level cycles\n        from natural_pdf.elements.region import Region\n        from natural_pdf.flows.element import FlowElement\n        from natural_pdf.flows.flow import Flow\n        from natural_pdf.flows.region import FlowRegion\n\n        start_pg = start_el.page\n        end_pg = end_el.page if end_el is not None else self.pages[-1]\n\n        parts: list[Region] = []\n\n        # Use the actual top of the start element (for implicit starts this is\n        # the bottom of the previous end element) instead of forcing to 0.\n        start_top = start_el.top\n\n        # Slice of first page beginning at *start_top*\n        parts.append(Region(start_pg, (0, start_top, start_pg.width, start_pg.height)))\n\n        # Full middle pages\n        for pg_idx in range(start_pg.index + 1, end_pg.index):\n            mid_pg = self.pages[pg_idx]\n            parts.append(Region(mid_pg, (0, 0, mid_pg.width, mid_pg.height)))\n\n        # Slice of last page (if distinct)\n        if end_pg is not start_pg:\n            bottom = end_el.bottom if end_el is not None else end_pg.height\n            parts.append(Region(end_pg, (0, 0, end_pg.width, bottom)))\n\n        flow = Flow(segments=parts, arrangement=\"vertical\")\n        src_fe = FlowElement(physical_object=start_el, flow=flow)\n        return FlowRegion(\n            flow=flow,\n            constituent_regions=parts,\n            source_flow_element=src_fe,\n            boundary_element_found=end_el,\n        )\n\n    # ------------------------------------------------------------------\n\n    current_start = None\n\n    for i, boundary in enumerate(section_boundaries):\n        # If it's a start boundary and we don't have a current start\n        if boundary[\"type\"] == \"start\" and current_start is None:\n            current_start = boundary\n\n        # If it's an end boundary and we have a current start\n        elif boundary[\"type\"] == \"end\" and current_start is not None:\n            # Create a section from current_start to this boundary\n            start_element = current_start[\"element\"]\n            end_element = boundary[\"element\"]\n\n            # If both elements are on the same page, use the page's get_section_between\n            if start_element.page == end_element.page:\n                # For implicit start elements, create a region from the top of the page\n                if hasattr(start_element, \"is_implicit_start\"):\n                    from natural_pdf.elements.region import Region\n\n                    section = Region(\n                        start_element.page,\n                        (0, start_element.top, start_element.page.width, end_element.bottom),\n                    )\n                    section.start_element = start_element\n                    section.boundary_element_found = end_element\n                else:\n                    section = start_element.page.get_section_between(\n                        start_element, end_element, include_boundaries\n                    )\n                sections.append(section)\n            else:\n                # Create FlowRegion spanning pages\n                flow_region = _build_flow_region(start_element, end_element)\n                sections.append(flow_region)\n\n            current_start = None\n\n        # If it's another start boundary and we have a current start (for splitting by starts only)\n        elif boundary[\"type\"] == \"start\" and current_start is not None and not end_elements:\n            # Create a section from current_start to just before this boundary\n            start_element = current_start[\"element\"]\n\n            # Find the last element before this boundary on the same page\n            if start_element.page == boundary[\"element\"].page:\n                # Find elements on this page\n                page_elements = [e for e in all_elements if e.page == start_element.page]\n                # Sort by position\n                page_elements.sort(key=lambda e: (e.top, e.x0))\n\n                # Find the last element before the boundary\n                end_idx = (\n                    page_elements.index(boundary[\"element\"]) - 1\n                    if boundary[\"element\"] in page_elements\n                    else -1\n                )\n                end_element = page_elements[end_idx] if end_idx &gt;= 0 else None\n\n                # Create the section\n                section = start_element.page.get_section_between(\n                    start_element, end_element, include_boundaries\n                )\n                sections.append(section)\n            else:\n                # Cross-page section - create from current_start to the end of its page\n                from natural_pdf.elements.region import Region\n\n                start_page = start_element.page\n\n                # Handle implicit start elements\n                start_top = start_element.top\n                region = Region(start_page, (0, start_top, start_page.width, start_page.height))\n                region.start_element = start_element\n                sections.append(region)\n\n            current_start = boundary\n\n    # Handle the last section if we have a current start\n    if current_start is not None:\n        start_element = current_start[\"element\"]\n        start_page = start_element.page\n\n        if end_elements:\n            # With end_elements, we need an explicit end - use the last element\n            # on the last page of the collection\n            last_page = self.pages[-1]\n            last_page_elements = [e for e in all_elements if e.page == last_page]\n            last_page_elements.sort(key=lambda e: (e.top, e.x0))\n            end_element = last_page_elements[-1] if last_page_elements else None\n\n            # Create FlowRegion spanning multiple pages using helper\n            flow_region = _build_flow_region(start_element, end_element)\n            sections.append(flow_region)\n        else:\n            # With start_elements only, create a section to the end of the current page\n            from natural_pdf.elements.region import Region\n\n            # Handle implicit start elements\n            start_top = start_element.top\n            region = Region(start_page, (0, start_top, start_page.width, start_page.height))\n            region.start_element = start_element\n            sections.append(region)\n\n    return ElementCollection(sections)\n</code></pre> <code>natural_pdf.PageCollection.groupby(by, *, show_progress=True)</code> <p>Group pages by selector text or callable result.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>Union[str, Callable]</code> <p>CSS selector string or callable function</p> required <code>show_progress</code> <code>bool</code> <p>Whether to show progress bar during computation (default: True)</p> <code>True</code> <p>Returns:</p> Type Description <code>PageGroupBy</code> <p>PageGroupBy object supporting iteration and dict-like access</p> <p>Examples:</p> <code>natural_pdf.PageCollection.highlights(show=False)</code> <p>Create a highlight context for accumulating highlights.</p> <p>This allows for clean syntax to show multiple highlight groups:</p> Example <p>with pages.highlights() as h:     h.add(pages.find_all('table'), label='tables', color='blue')     h.add(pages.find_all('text:bold'), label='bold text', color='red')     h.show()</p> Or with automatic display <p>with pages.highlights(show=True) as h:     h.add(pages.find_all('table'), label='tables')     h.add(pages.find_all('text:bold'), label='bold')     # Automatically shows when exiting the context</p> <p>Parameters:</p> Name Type Description Default <code>show</code> <code>bool</code> <p>If True, automatically show highlights when exiting context</p> <code>False</code> <p>Returns:</p> Type Description <code>HighlightContext</code> <p>HighlightContext for accumulating highlights</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def highlights(self, show: bool = False) -&gt; \"HighlightContext\":\n    \"\"\"\n    Create a highlight context for accumulating highlights.\n\n    This allows for clean syntax to show multiple highlight groups:\n\n    Example:\n        with pages.highlights() as h:\n            h.add(pages.find_all('table'), label='tables', color='blue')\n            h.add(pages.find_all('text:bold'), label='bold text', color='red')\n            h.show()\n\n    Or with automatic display:\n        with pages.highlights(show=True) as h:\n            h.add(pages.find_all('table'), label='tables')\n            h.add(pages.find_all('text:bold'), label='bold')\n            # Automatically shows when exiting the context\n\n    Args:\n        show: If True, automatically show highlights when exiting context\n\n    Returns:\n        HighlightContext for accumulating highlights\n    \"\"\"\n    from natural_pdf.core.highlighting_service import HighlightContext\n\n    return HighlightContext(self, show_on_exit=show)\n</code></pre> <code>natural_pdf.PageCollection.save_pdf(output_path, ocr=False, original=False, dpi=300)</code> <p>Saves the pages in this collection to a new PDF file.</p> <p>Choose one saving mode: - <code>ocr=True</code>: Creates a new, image-based PDF using OCR results. This   makes the text generated during the natural-pdf session searchable,   but loses original vector content. Requires 'ocr-export' extras. - <code>original=True</code>: Extracts the original pages from the source PDF,   preserving all vector content, fonts, and annotations. OCR results   from the natural-pdf session are NOT included. Requires 'ocr-export' extras.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save the new PDF file.</p> required <code>ocr</code> <code>bool</code> <p>If True, save as a searchable, image-based PDF using OCR data.</p> <code>False</code> <code>original</code> <code>bool</code> <p>If True, save the original, vector-based pages.</p> <code>False</code> <code>dpi</code> <code>int</code> <p>Resolution (dots per inch) used only when ocr=True for  rendering page images and aligning the text layer.</p> <code>300</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the collection is empty, if neither or both 'ocr'         and 'original' are True, or if 'original=True' and         pages originate from different PDFs.</p> <code>ImportError</code> <p>If required libraries ('pikepdf', 'Pillow')          are not installed for the chosen mode.</p> <code>RuntimeError</code> <p>If an unexpected error occurs during saving.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def save_pdf(\n    self,\n    output_path: Union[str, Path],\n    ocr: bool = False,\n    original: bool = False,\n    dpi: int = 300,\n):\n    \"\"\"\n    Saves the pages in this collection to a new PDF file.\n\n    Choose one saving mode:\n    - `ocr=True`: Creates a new, image-based PDF using OCR results. This\n      makes the text generated during the natural-pdf session searchable,\n      but loses original vector content. Requires 'ocr-export' extras.\n    - `original=True`: Extracts the original pages from the source PDF,\n      preserving all vector content, fonts, and annotations. OCR results\n      from the natural-pdf session are NOT included. Requires 'ocr-export' extras.\n\n    Args:\n        output_path: Path to save the new PDF file.\n        ocr: If True, save as a searchable, image-based PDF using OCR data.\n        original: If True, save the original, vector-based pages.\n        dpi: Resolution (dots per inch) used only when ocr=True for\n             rendering page images and aligning the text layer.\n\n    Raises:\n        ValueError: If the collection is empty, if neither or both 'ocr'\n                    and 'original' are True, or if 'original=True' and\n                    pages originate from different PDFs.\n        ImportError: If required libraries ('pikepdf', 'Pillow')\n                     are not installed for the chosen mode.\n        RuntimeError: If an unexpected error occurs during saving.\n    \"\"\"\n    if not self.pages:\n        raise ValueError(\"Cannot save an empty PageCollection.\")\n\n    if not (ocr ^ original):  # XOR: exactly one must be true\n        raise ValueError(\"Exactly one of 'ocr' or 'original' must be True.\")\n\n    output_path_obj = Path(output_path)\n    output_path_str = str(output_path_obj)\n\n    if ocr:\n        if create_searchable_pdf is None:\n            raise ImportError(\n                \"Saving with ocr=True requires 'pikepdf' and 'Pillow'. \"\n                'Install with: pip install \\\\\"natural-pdf[ocr-export]\\\\\"'  # Escaped quotes\n            )\n\n        # Check for non-OCR vector elements (provide a warning)\n        has_vector_elements = False\n        for page in self.pages:\n            # Simplified check for common vector types or non-OCR chars/words\n            if (\n                hasattr(page, \"rects\")\n                and page.rects\n                or hasattr(page, \"lines\")\n                and page.lines\n                or hasattr(page, \"curves\")\n                and page.curves\n                or (\n                    hasattr(page, \"chars\")\n                    and any(getattr(el, \"source\", None) != \"ocr\" for el in page.chars)\n                )\n                or (\n                    hasattr(page, \"words\")\n                    and any(getattr(el, \"source\", None) != \"ocr\" for el in page.words)\n                )\n            ):\n                has_vector_elements = True\n                break\n        if has_vector_elements:\n            logger.warning(\n                \"Warning: Saving with ocr=True creates an image-based PDF. \"\n                \"Original vector elements (rects, lines, non-OCR text/chars) \"\n                \"on selected pages will not be preserved in the output file.\"\n            )\n\n        logger.info(f\"Saving searchable PDF (OCR text layer) to: {output_path_str}\")\n        try:\n            # Delegate to the searchable PDF exporter function\n            # Pass `self` (the PageCollection instance) as the source\n            create_searchable_pdf(self, output_path_str, dpi=dpi)\n            # Success log is now inside create_searchable_pdf if needed, or keep here\n            # logger.info(f\"Successfully saved searchable PDF to: {output_path_str}\")\n        except Exception as e:\n            logger.error(f\"Failed to create searchable PDF: {e}\", exc_info=True)\n            # Re-raise as RuntimeError for consistency, potentially handled in exporter too\n            raise RuntimeError(f\"Failed to create searchable PDF: {e}\") from e\n\n    elif original:\n        # ---&gt; MODIFIED: Call the new exporter\n        if create_original_pdf is None:\n            raise ImportError(\n                \"Saving with original=True requires 'pikepdf'. \"\n                'Install with: pip install \\\\\"natural-pdf[ocr-export]\\\\\"'  # Escaped quotes\n            )\n\n        # Check for OCR elements (provide a warning) - keep this check here\n        has_ocr_elements = False\n        for page in self.pages:\n            # Use find_all which returns a collection; check if it's non-empty\n            if hasattr(page, \"find_all\"):\n                ocr_text_elements = page.find_all(\"text[source=ocr]\")\n                if ocr_text_elements:  # Check truthiness of collection\n                    has_ocr_elements = True\n                    break\n            elif hasattr(page, \"words\"):  # Fallback check if find_all isn't present?\n                if any(getattr(el, \"source\", None) == \"ocr\" for el in page.words):\n                    has_ocr_elements = True\n                    break\n\n        if has_ocr_elements:\n            logger.warning(\n                \"Warning: Saving with original=True preserves original page content. \"\n                \"OCR text generated in this session will not be included in the saved file.\"\n            )\n\n        logger.info(f\"Saving original pages PDF to: {output_path_str}\")\n        try:\n            # Delegate to the original PDF exporter function\n            # Pass `self` (the PageCollection instance) as the source\n            create_original_pdf(self, output_path_str)\n            # Success log is now inside create_original_pdf\n            # logger.info(f\"Successfully saved original pages PDF to: {output_path_str}\")\n        except Exception as e:\n            # Error logging is handled within create_original_pdf\n            # Re-raise the exception caught from the exporter\n            raise e  # Keep the original exception type (ValueError, RuntimeError, etc.)\n</code></pre> <code>natural_pdf.PageCollection.to_flow(arrangement='vertical', alignment='start', segment_gap=0.0)</code> <p>Convert this PageCollection to a Flow for cross-page operations.</p> <p>This enables treating multiple pages as a continuous logical document structure, useful for multi-page tables, articles spanning columns, or any content requiring reading order across page boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>arrangement</code> <code>Literal['vertical', 'horizontal']</code> <p>Primary flow direction ('vertical' or 'horizontal').         'vertical' stacks pages top-to-bottom (most common).         'horizontal' arranges pages left-to-right.</p> <code>'vertical'</code> <code>alignment</code> <code>Literal['start', 'center', 'end', 'top', 'left', 'bottom', 'right']</code> <p>Cross-axis alignment for pages of different sizes:       For vertical: 'left'/'start', 'center', 'right'/'end'       For horizontal: 'top'/'start', 'center', 'bottom'/'end'</p> <code>'start'</code> <code>segment_gap</code> <code>float</code> <p>Virtual gap between pages in PDF points (default: 0.0).</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Flow</code> <p>Flow object that can perform operations across all pages in sequence.</p> Example <p>Multi-page table extraction: <pre><code>pdf = npdf.PDF(\"multi_page_report.pdf\")\n\n# Create flow for pages 2-4 containing a table\ntable_flow = pdf.pages[1:4].to_flow()\n\n# Extract table as if it were continuous\ntable_data = table_flow.extract_table()\ndf = table_data.df\n</code></pre></p> <p>Cross-page element search: <pre><code># Find all headers across multiple pages\nheaders = pdf.pages[5:10].to_flow().find_all('text[size&gt;12]:bold')\n\n# Analyze layout across pages\nregions = pdf.pages.to_flow().analyze_layout(engine='yolo')\n</code></pre></p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def to_flow(\n    self,\n    arrangement: Literal[\"vertical\", \"horizontal\"] = \"vertical\",\n    alignment: Literal[\"start\", \"center\", \"end\", \"top\", \"left\", \"bottom\", \"right\"] = \"start\",\n    segment_gap: float = 0.0,\n) -&gt; \"Flow\":\n    \"\"\"\n    Convert this PageCollection to a Flow for cross-page operations.\n\n    This enables treating multiple pages as a continuous logical document\n    structure, useful for multi-page tables, articles spanning columns,\n    or any content requiring reading order across page boundaries.\n\n    Args:\n        arrangement: Primary flow direction ('vertical' or 'horizontal').\n                    'vertical' stacks pages top-to-bottom (most common).\n                    'horizontal' arranges pages left-to-right.\n        alignment: Cross-axis alignment for pages of different sizes:\n                  For vertical: 'left'/'start', 'center', 'right'/'end'\n                  For horizontal: 'top'/'start', 'center', 'bottom'/'end'\n        segment_gap: Virtual gap between pages in PDF points (default: 0.0).\n\n    Returns:\n        Flow object that can perform operations across all pages in sequence.\n\n    Example:\n        Multi-page table extraction:\n        ```python\n        pdf = npdf.PDF(\"multi_page_report.pdf\")\n\n        # Create flow for pages 2-4 containing a table\n        table_flow = pdf.pages[1:4].to_flow()\n\n        # Extract table as if it were continuous\n        table_data = table_flow.extract_table()\n        df = table_data.df\n        ```\n\n        Cross-page element search:\n        ```python\n        # Find all headers across multiple pages\n        headers = pdf.pages[5:10].to_flow().find_all('text[size&gt;12]:bold')\n\n        # Analyze layout across pages\n        regions = pdf.pages.to_flow().analyze_layout(engine='yolo')\n        ```\n    \"\"\"\n    from natural_pdf.flows.flow import Flow\n\n    return Flow(\n        segments=self,  # Flow constructor now handles PageCollection\n        arrangement=arrangement,\n        alignment=alignment,\n        segment_gap=segment_gap,\n    )\n</code></pre> <code>natural_pdf.PageCollection.update_text(transform, selector='text', max_workers=None)</code> <p>Applies corrections to text elements across all pages in this collection using a user-provided callback function, executed in parallel if <code>max_workers</code> is specified.</p> <p>This method delegates to the parent PDF's <code>update_text</code> method, targeting all pages within this collection.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>Callable[[Any], Optional[str]]</code> <p>A function that accepts a single argument (an element        object) and returns <code>Optional[str]</code> (new text or None).</p> required <code>selector</code> <code>str</code> <p>The attribute name to update. Default is 'text'.</p> <code>'text'</code> <code>max_workers</code> <code>Optional[int]</code> <p>The maximum number of worker threads to use for parallel          correction on each page. If None, defaults are used.</p> <code>None</code> <p>Returns:</p> Type Description <code>PageCollection[P]</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the collection is empty, pages lack a parent PDF reference,           or the parent PDF lacks the <code>update_text</code> method.</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def update_text(\n    self,\n    transform: Callable[[Any], Optional[str]],\n    selector: str = \"text\",\n    max_workers: Optional[int] = None,\n) -&gt; \"PageCollection[P]\":\n    \"\"\"\n    Applies corrections to text elements across all pages\n    in this collection using a user-provided callback function, executed\n    in parallel if `max_workers` is specified.\n\n    This method delegates to the parent PDF's `update_text` method,\n    targeting all pages within this collection.\n\n    Args:\n        transform: A function that accepts a single argument (an element\n                   object) and returns `Optional[str]` (new text or None).\n        selector: The attribute name to update. Default is 'text'.\n        max_workers: The maximum number of worker threads to use for parallel\n                     correction on each page. If None, defaults are used.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        RuntimeError: If the collection is empty, pages lack a parent PDF reference,\n                      or the parent PDF lacks the `update_text` method.\n    \"\"\"\n    if not self.pages:\n        logger.warning(\"Cannot update text for an empty PageCollection.\")\n        # Return self even if empty to maintain chaining consistency\n        return self\n\n    # Assume all pages share the same parent PDF object\n    parent_pdf = self.pages[0]._parent\n    if (\n        not parent_pdf\n        or not hasattr(parent_pdf, \"update_text\")\n        or not callable(parent_pdf.update_text)\n    ):\n        raise RuntimeError(\n            \"Parent PDF reference not found or parent PDF lacks the required 'update_text' method.\"\n        )\n\n    page_indices = self._get_page_indices()\n    logger.info(\n        f\"PageCollection: Delegating text update to parent PDF for page indices: {page_indices} with max_workers={max_workers} and selector='{selector}'.\"\n    )\n\n    # Delegate the call to the parent PDF object for the relevant pages\n    # Pass the max_workers parameter down\n    parent_pdf.update_text(\n        transform=transform,\n        pages=page_indices,\n        selector=selector,\n        max_workers=max_workers,\n    )\n\n    return self\n</code></pre>"},{"location":"api/#natural_pdf.PageCollection.groupby--group-by-header-text","title":"Group by header text","text":"<p>for title, pages in pdf.pages.groupby('text[size=16]'):     print(f\"Section: {title}\")</p>"},{"location":"api/#natural_pdf.PageCollection.groupby--group-by-callable","title":"Group by callable","text":"<p>for city, pages in pdf.pages.groupby(lambda p: p.find('text:contains(\"CITY\")').extract_text()):     process_city_pages(pages)</p>"},{"location":"api/#natural_pdf.PageCollection.groupby--quick-exploration-with-indexing","title":"Quick exploration with indexing","text":"<p>grouped = pdf.pages.groupby('text[size=16]') grouped.info()                    # Show all groups first_section = grouped[0]        # First group last_section = grouped[-1]       # Last group</p>"},{"location":"api/#natural_pdf.PageCollection.groupby--dict-like-access-by-name","title":"Dict-like access by name","text":"<p>madison_pages = grouped.get('CITY OF MADISON') madison_pages = grouped['CITY OF MADISON']  # Alternative</p>"},{"location":"api/#natural_pdf.PageCollection.groupby--disable-progress-bar-for-small-collections","title":"Disable progress bar for small collections","text":"<p>grouped = pdf.pages.groupby('text[size=16]', show_progress=False)</p> Source code in <code>natural_pdf/core/page_collection.py</code> <pre><code>def groupby(self, by: Union[str, Callable], *, show_progress: bool = True) -&gt; \"PageGroupBy\":\n    \"\"\"\n    Group pages by selector text or callable result.\n\n    Args:\n        by: CSS selector string or callable function\n        show_progress: Whether to show progress bar during computation (default: True)\n\n    Returns:\n        PageGroupBy object supporting iteration and dict-like access\n\n    Examples:\n        # Group by header text\n        for title, pages in pdf.pages.groupby('text[size=16]'):\n            print(f\"Section: {title}\")\n\n        # Group by callable\n        for city, pages in pdf.pages.groupby(lambda p: p.find('text:contains(\"CITY\")').extract_text()):\n            process_city_pages(pages)\n\n        # Quick exploration with indexing\n        grouped = pdf.pages.groupby('text[size=16]')\n        grouped.info()                    # Show all groups\n        first_section = grouped[0]        # First group\n        last_section = grouped[-1]       # Last group\n\n        # Dict-like access by name\n        madison_pages = grouped.get('CITY OF MADISON')\n        madison_pages = grouped['CITY OF MADISON']  # Alternative\n\n        # Disable progress bar for small collections\n        grouped = pdf.pages.groupby('text[size=16]', show_progress=False)\n    \"\"\"\n    from natural_pdf.core.page_groupby import PageGroupBy\n\n    return PageGroupBy(self, by, show_progress=show_progress)\n</code></pre>"},{"location":"api/#natural_pdf.Region","title":"<code>natural_pdf.Region</code>","text":"<p>               Bases: <code>TextMixin</code>, <code>DirectionalMixin</code>, <code>ClassificationMixin</code>, <code>ExtractionMixin</code>, <code>ShapeDetectionMixin</code>, <code>DescribeMixin</code>, <code>Visualizable</code></p> <p>Represents a rectangular region on a page.</p> <p>Regions are fundamental building blocks in natural-pdf that define rectangular areas of a page for analysis, extraction, and navigation. They can be created manually or automatically through spatial navigation methods like .below(), .above(), .left(), and .right() from elements or other regions.</p> <p>Regions integrate multiple analysis capabilities through mixins and provide: - Element filtering and collection within the region boundary - OCR processing for the region area - Table detection and extraction - AI-powered classification and structured data extraction - Visual rendering and debugging capabilities - Text extraction with spatial awareness</p> <p>The Region class supports both rectangular and polygonal boundaries, making it suitable for complex document layouts and irregular shapes detected by layout analysis algorithms.</p> <p>Attributes:</p> Name Type Description <code>page</code> <code>Page</code> <p>Reference to the parent Page object.</p> <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box tuple (x0, top, x1, bottom) in PDF coordinates.</p> <code>x0</code> <code>float</code> <p>Left x-coordinate.</p> <code>top</code> <code>float</code> <p>Top y-coordinate (minimum y).</p> <code>x1</code> <code>float</code> <p>Right x-coordinate.</p> <code>bottom</code> <code>float</code> <p>Bottom y-coordinate (maximum y).</p> <code>width</code> <code>float</code> <p>Region width (x1 - x0).</p> <code>height</code> <code>float</code> <p>Region height (bottom - top).</p> <code>polygon</code> <code>List[Tuple[float, float]]</code> <p>List of coordinate points for non-rectangular regions.</p> <code>label</code> <p>Optional descriptive label for the region.</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Dictionary for storing analysis results and custom data.</p> Example <p>Creating regions: <pre><code>pdf = npdf.PDF(\"document.pdf\")\npage = pdf.pages[0]\n\n# Manual region creation\nheader_region = page.region(0, 0, page.width, 100)\n\n# Spatial navigation from elements\nsummary_text = page.find('text:contains(\"Summary\")')\ncontent_region = summary_text.below(until='text[size&gt;12]:bold')\n\n# Extract content from region\ntables = content_region.extract_table()\ntext = content_region.get_text()\n</code></pre></p> <p>Advanced usage: <pre><code># OCR processing\nregion.apply_ocr(engine='easyocr', resolution=300)\n\n# AI-powered extraction\ndata = region.extract_structured_data(MySchema)\n\n# Visual debugging\nregion.show(highlights=['tables', 'text'])\n</code></pre></p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>class Region(\n    TextMixin,\n    DirectionalMixin,\n    ClassificationMixin,\n    ExtractionMixin,\n    ShapeDetectionMixin,\n    DescribeMixin,\n    Visualizable,\n):\n    \"\"\"Represents a rectangular region on a page.\n\n    Regions are fundamental building blocks in natural-pdf that define rectangular\n    areas of a page for analysis, extraction, and navigation. They can be created\n    manually or automatically through spatial navigation methods like .below(), .above(),\n    .left(), and .right() from elements or other regions.\n\n    Regions integrate multiple analysis capabilities through mixins and provide:\n    - Element filtering and collection within the region boundary\n    - OCR processing for the region area\n    - Table detection and extraction\n    - AI-powered classification and structured data extraction\n    - Visual rendering and debugging capabilities\n    - Text extraction with spatial awareness\n\n    The Region class supports both rectangular and polygonal boundaries, making it\n    suitable for complex document layouts and irregular shapes detected by layout\n    analysis algorithms.\n\n    Attributes:\n        page: Reference to the parent Page object.\n        bbox: Bounding box tuple (x0, top, x1, bottom) in PDF coordinates.\n        x0: Left x-coordinate.\n        top: Top y-coordinate (minimum y).\n        x1: Right x-coordinate.\n        bottom: Bottom y-coordinate (maximum y).\n        width: Region width (x1 - x0).\n        height: Region height (bottom - top).\n        polygon: List of coordinate points for non-rectangular regions.\n        label: Optional descriptive label for the region.\n        metadata: Dictionary for storing analysis results and custom data.\n\n    Example:\n        Creating regions:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n        page = pdf.pages[0]\n\n        # Manual region creation\n        header_region = page.region(0, 0, page.width, 100)\n\n        # Spatial navigation from elements\n        summary_text = page.find('text:contains(\"Summary\")')\n        content_region = summary_text.below(until='text[size&gt;12]:bold')\n\n        # Extract content from region\n        tables = content_region.extract_table()\n        text = content_region.get_text()\n        ```\n\n        Advanced usage:\n        ```python\n        # OCR processing\n        region.apply_ocr(engine='easyocr', resolution=300)\n\n        # AI-powered extraction\n        data = region.extract_structured_data(MySchema)\n\n        # Visual debugging\n        region.show(highlights=['tables', 'text'])\n        ```\n    \"\"\"\n\n    def __init__(\n        self,\n        page: \"Page\",\n        bbox: Tuple[float, float, float, float],\n        polygon: List[Tuple[float, float]] = None,\n        parent=None,\n        label: Optional[str] = None,\n    ):\n        \"\"\"Initialize a region.\n\n        Creates a Region object that represents a rectangular or polygonal area on a page.\n        Regions are used for spatial navigation, content extraction, and analysis operations.\n\n        Args:\n            page: Parent Page object that contains this region and provides access\n                to document elements and analysis capabilities.\n            bbox: Bounding box coordinates as (x0, top, x1, bottom) tuple in PDF\n                coordinate system (points, with origin at bottom-left).\n            polygon: Optional list of coordinate points [(x1,y1), (x2,y2), ...] for\n                non-rectangular regions. If provided, the region will use polygon-based\n                intersection calculations instead of simple rectangle overlap.\n            parent: Optional parent region for hierarchical document structure.\n                Useful for maintaining tree-like relationships between regions.\n            label: Optional descriptive label for the region, useful for debugging\n                and identification in complex workflows.\n\n        Example:\n            ```python\n            pdf = npdf.PDF(\"document.pdf\")\n            page = pdf.pages[0]\n\n            # Rectangular region\n            header = Region(page, (0, 0, page.width, 100), label=\"header\")\n\n            # Polygonal region (from layout detection)\n            table_polygon = [(50, 100), (300, 100), (300, 400), (50, 400)]\n            table_region = Region(page, (50, 100, 300, 400),\n                                polygon=table_polygon, label=\"table\")\n            ```\n\n        Note:\n            Regions are typically created through page methods like page.region() or\n            spatial navigation methods like element.below(). Direct instantiation is\n            used mainly for advanced workflows or layout analysis integration.\n        \"\"\"\n        self._page = page\n        self._bbox = bbox\n        self._polygon = polygon\n\n        self.metadata: Dict[str, Any] = {}\n        # Analysis results live under self.metadata['analysis'] via property\n\n        # Standard attributes for all elements\n        self.object_type = \"region\"  # For selector compatibility\n\n        # Layout detection attributes\n        self.region_type = None\n        self.normalized_type = None\n        self.confidence = None\n        self.model = None\n\n        # Region management attributes\n        self.name = None\n        self.label = label\n        self.source = None  # Will be set by creation methods\n\n        # Hierarchy support for nested document structure\n        self.parent_region = parent\n        self.child_regions = []\n        self.text_content = None  # Direct text content (e.g., from Docling)\n        self.associated_text_elements = []  # Native text elements that overlap with this region\n\n    def _get_render_specs(\n        self,\n        mode: Literal[\"show\", \"render\"] = \"show\",\n        color: Optional[Union[str, Tuple[int, int, int]]] = None,\n        highlights: Optional[List[Dict[str, Any]]] = None,\n        crop: Union[bool, Literal[\"content\"]] = True,  # Default to True for regions\n        crop_bbox: Optional[Tuple[float, float, float, float]] = None,\n        **kwargs,\n    ) -&gt; List[RenderSpec]:\n        \"\"\"Get render specifications for this region.\n\n        Args:\n            mode: Rendering mode - 'show' includes highlights, 'render' is clean\n            color: Color for highlighting this region in show mode\n            highlights: Additional highlight groups to show\n            crop: Whether to crop to this region\n            crop_bbox: Explicit crop bounds (overrides region bounds)\n            **kwargs: Additional parameters\n\n        Returns:\n            List containing a single RenderSpec for this region's page\n        \"\"\"\n        from typing import Literal\n\n        spec = RenderSpec(page=self.page)\n\n        # Handle cropping\n        if crop_bbox:\n            spec.crop_bbox = crop_bbox\n        elif crop:\n            # Crop to this region's bounds\n            spec.crop_bbox = self.bbox\n\n        # Add highlights in show mode\n        if mode == \"show\":\n            # Highlight this region\n            if color or mode == \"show\":  # Always highlight in show mode\n                spec.add_highlight(\n                    bbox=self.bbox,\n                    polygon=self.polygon if self.has_polygon else None,\n                    color=color or \"blue\",\n                    label=self.label or self.name or \"Region\",\n                )\n\n            # Add additional highlight groups if provided\n            if highlights:\n                for group in highlights:\n                    elements = group.get(\"elements\", [])\n                    group_color = group.get(\"color\", color)\n                    group_label = group.get(\"label\")\n\n                    for elem in elements:\n                        spec.add_highlight(element=elem, color=group_color, label=group_label)\n\n        return [spec]\n\n    def _direction(\n        self,\n        direction: str,\n        size: Optional[float] = None,\n        cross_size: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Region-specific wrapper around :py:meth:`DirectionalMixin._direction`.\n\n        It performs any pre-processing required by *Region* (none currently),\n        delegates the core geometry work to the mix-in implementation via\n        ``super()``, then attaches region-level metadata before returning the\n        new :class:`Region` instance.\n        \"\"\"\n\n        # Delegate to the shared implementation on DirectionalMixin\n        region = super()._direction(\n            direction=direction,\n            size=size,\n            cross_size=cross_size,\n            include_source=include_source,\n            until=until,\n            include_endpoint=include_endpoint,\n            **kwargs,\n        )\n\n        # Post-process: make sure callers can trace lineage and flags\n        region.source_element = self\n        region.includes_source = include_source\n\n        return region\n\n    def above(\n        self,\n        height: Optional[float] = None,\n        width: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Select region above this region.\n\n        Args:\n            height: Height of the region above, in points\n            width: Width mode - \"full\" for full page width or \"element\" for element width\n            include_source: Whether to include this region in the result (default: False)\n            until: Optional selector string to specify an upper boundary element\n            include_endpoint: Whether to include the boundary element in the region (default: True)\n            **kwargs: Additional parameters\n\n        Returns:\n            Region object representing the area above\n        \"\"\"\n        return self._direction(\n            direction=\"above\",\n            size=height,\n            cross_size=width,\n            include_source=include_source,\n            until=until,\n            include_endpoint=include_endpoint,\n            **kwargs,\n        )\n\n    def below(\n        self,\n        height: Optional[float] = None,\n        width: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Select region below this region.\n\n        Args:\n            height: Height of the region below, in points\n            width: Width mode - \"full\" for full page width or \"element\" for element width\n            include_source: Whether to include this region in the result (default: False)\n            until: Optional selector string to specify a lower boundary element\n            include_endpoint: Whether to include the boundary element in the region (default: True)\n            **kwargs: Additional parameters\n\n        Returns:\n            Region object representing the area below\n        \"\"\"\n        return self._direction(\n            direction=\"below\",\n            size=height,\n            cross_size=width,\n            include_source=include_source,\n            until=until,\n            include_endpoint=include_endpoint,\n            **kwargs,\n        )\n\n    def left(\n        self,\n        width: Optional[float] = None,\n        height: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Select region to the left of this region.\n\n        Args:\n            width: Width of the region to the left, in points\n            height: Height mode - \"full\" for full page height or \"element\" for element height\n            include_source: Whether to include this region in the result (default: False)\n            until: Optional selector string to specify a left boundary element\n            include_endpoint: Whether to include the boundary element in the region (default: True)\n            **kwargs: Additional parameters\n\n        Returns:\n            Region object representing the area to the left\n        \"\"\"\n        return self._direction(\n            direction=\"left\",\n            size=width,\n            cross_size=height,\n            include_source=include_source,\n            until=until,\n            include_endpoint=include_endpoint,\n            **kwargs,\n        )\n\n    def right(\n        self,\n        width: Optional[float] = None,\n        height: str = \"full\",\n        include_source: bool = False,\n        until: Optional[str] = None,\n        include_endpoint: bool = True,\n        **kwargs,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Select region to the right of this region.\n\n        Args:\n            width: Width of the region to the right, in points\n            height: Height mode - \"full\" for full page height or \"element\" for element height\n            include_source: Whether to include this region in the result (default: False)\n            until: Optional selector string to specify a right boundary element\n            include_endpoint: Whether to include the boundary element in the region (default: True)\n            **kwargs: Additional parameters\n\n        Returns:\n            Region object representing the area to the right\n        \"\"\"\n        return self._direction(\n            direction=\"right\",\n            size=width,\n            cross_size=height,\n            include_source=include_source,\n            until=until,\n            include_endpoint=include_endpoint,\n            **kwargs,\n        )\n\n    @property\n    def type(self) -&gt; str:\n        \"\"\"Element type.\"\"\"\n        # Return the specific type if detected (e.g., from layout analysis)\n        # or 'region' as a default.\n        return self.region_type or \"region\"  # Prioritize specific region_type if set\n\n    @property\n    def page(self) -&gt; \"Page\":\n        \"\"\"Get the parent page.\"\"\"\n        return self._page\n\n    @property\n    def bbox(self) -&gt; Tuple[float, float, float, float]:\n        \"\"\"Get the bounding box as (x0, top, x1, bottom).\"\"\"\n        return self._bbox\n\n    @property\n    def x0(self) -&gt; float:\n        \"\"\"Get the left coordinate.\"\"\"\n        return self._bbox[0]\n\n    @property\n    def top(self) -&gt; float:\n        \"\"\"Get the top coordinate.\"\"\"\n        return self._bbox[1]\n\n    @property\n    def x1(self) -&gt; float:\n        \"\"\"Get the right coordinate.\"\"\"\n        return self._bbox[2]\n\n    @property\n    def bottom(self) -&gt; float:\n        \"\"\"Get the bottom coordinate.\"\"\"\n        return self._bbox[3]\n\n    @property\n    def width(self) -&gt; float:\n        \"\"\"Get the width of the region.\"\"\"\n        return self.x1 - self.x0\n\n    @property\n    def height(self) -&gt; float:\n        \"\"\"Get the height of the region.\"\"\"\n        return self.bottom - self.top\n\n    @property\n    def has_polygon(self) -&gt; bool:\n        \"\"\"Check if this region has polygon coordinates.\"\"\"\n        return self._polygon is not None and len(self._polygon) &gt;= 3\n\n    @property\n    def polygon(self) -&gt; List[Tuple[float, float]]:\n        \"\"\"Get polygon coordinates if available, otherwise return rectangle corners.\"\"\"\n        if self._polygon:\n            return self._polygon\n        else:\n            # Create rectangle corners from bbox as fallback\n            return [\n                (self.x0, self.top),  # top-left\n                (self.x1, self.top),  # top-right\n                (self.x1, self.bottom),  # bottom-right\n                (self.x0, self.bottom),  # bottom-left\n            ]\n\n    def _is_point_in_polygon(self, x: float, y: float) -&gt; bool:\n        \"\"\"\n        Check if a point is inside the polygon using ray casting algorithm.\n\n        Args:\n            x: X coordinate of the point\n            y: Y coordinate of the point\n\n        Returns:\n            bool: True if the point is inside the polygon\n        \"\"\"\n        if not self.has_polygon:\n            return (self.x0 &lt;= x &lt;= self.x1) and (self.top &lt;= y &lt;= self.bottom)\n\n        # Ray casting algorithm\n        inside = False\n        j = len(self.polygon) - 1\n\n        for i in range(len(self.polygon)):\n            if ((self.polygon[i][1] &gt; y) != (self.polygon[j][1] &gt; y)) and (\n                x\n                &lt; (self.polygon[j][0] - self.polygon[i][0])\n                * (y - self.polygon[i][1])\n                / (self.polygon[j][1] - self.polygon[i][1])\n                + self.polygon[i][0]\n            ):\n                inside = not inside\n            j = i\n\n        return inside\n\n    def is_point_inside(self, x: float, y: float) -&gt; bool:\n        \"\"\"\n        Check if a point is inside this region using ray casting algorithm for polygons.\n\n        Args:\n            x: X coordinate of the point\n            y: Y coordinate of the point\n\n        Returns:\n            bool: True if the point is inside the region\n        \"\"\"\n        if not self.has_polygon:\n            return (self.x0 &lt;= x &lt;= self.x1) and (self.top &lt;= y &lt;= self.bottom)\n\n        # Ray casting algorithm\n        inside = False\n        j = len(self.polygon) - 1\n\n        for i in range(len(self.polygon)):\n            if ((self.polygon[i][1] &gt; y) != (self.polygon[j][1] &gt; y)) and (\n                x\n                &lt; (self.polygon[j][0] - self.polygon[i][0])\n                * (y - self.polygon[i][1])\n                / (self.polygon[j][1] - self.polygon[i][1])\n                + self.polygon[i][0]\n            ):\n                inside = not inside\n            j = i\n\n        return inside\n\n    def is_element_center_inside(self, element: \"Element\") -&gt; bool:\n        \"\"\"\n        Check if the center point of an element's bounding box is inside this region.\n\n        Args:\n            element: Element to check\n\n        Returns:\n            True if the element's center point is inside the region, False otherwise.\n        \"\"\"\n        # Check if element is on the same page\n        if not hasattr(element, \"page\") or element.page != self._page:\n            return False\n\n        # Ensure element has necessary attributes\n        if not all(hasattr(element, attr) for attr in [\"x0\", \"x1\", \"top\", \"bottom\"]):\n            logger.warning(\n                f\"Element {element} lacks bounding box attributes. Cannot check center point.\"\n            )\n            return False  # Cannot determine position\n\n        # Calculate center point\n        center_x = (element.x0 + element.x1) / 2\n        center_y = (element.top + element.bottom) / 2\n\n        # Use the existing is_point_inside check\n        return self.is_point_inside(center_x, center_y)\n\n    def _is_element_in_region(self, element: \"Element\", use_boundary_tolerance=True) -&gt; bool:\n        \"\"\"\n        Check if an element intersects or is contained within this region.\n\n        Args:\n            element: Element to check\n            use_boundary_tolerance: Whether to apply a small tolerance for boundary elements\n\n        Returns:\n            True if the element is in the region, False otherwise\n        \"\"\"\n        # Check if element is on the same page\n        if not hasattr(element, \"page\") or element.page != self._page:\n            return False\n\n        return self.is_element_center_inside(element)\n        # return self.intersects(element)\n\n    def contains(self, element: \"Element\") -&gt; bool:\n        \"\"\"\n        Check if this region completely contains an element.\n\n        Args:\n            element: Element to check\n\n        Returns:\n            True if the element is completely contained within the region, False otherwise\n        \"\"\"\n        # Check if element is on the same page\n        if not hasattr(element, \"page\") or element.page != self._page:\n            return False\n\n        # Ensure element has necessary attributes\n        if not all(hasattr(element, attr) for attr in [\"x0\", \"x1\", \"top\", \"bottom\"]):\n            return False  # Cannot determine position\n\n        # For rectangular regions, check if element's bbox is fully inside region's bbox\n        if not self.has_polygon:\n            return (\n                self.x0 &lt;= element.x0\n                and element.x1 &lt;= self.x1\n                and self.top &lt;= element.top\n                and element.bottom &lt;= self.bottom\n            )\n\n        # For polygon regions, check if all corners of the element are inside the polygon\n        element_corners = [\n            (element.x0, element.top),  # top-left\n            (element.x1, element.top),  # top-right\n            (element.x1, element.bottom),  # bottom-right\n            (element.x0, element.bottom),  # bottom-left\n        ]\n\n        return all(self.is_point_inside(x, y) for x, y in element_corners)\n\n    def intersects(self, element: \"Element\") -&gt; bool:\n        \"\"\"\n        Check if this region intersects with an element (any overlap).\n\n        Args:\n            element: Element to check\n\n        Returns:\n            True if the element overlaps with the region at all, False otherwise\n        \"\"\"\n        # Check if element is on the same page\n        if not hasattr(element, \"page\") or element.page != self._page:\n            return False\n\n        # Ensure element has necessary attributes\n        if not all(hasattr(element, attr) for attr in [\"x0\", \"x1\", \"top\", \"bottom\"]):\n            return False  # Cannot determine position\n\n        # For rectangular regions, check for bbox overlap\n        if not self.has_polygon:\n            return (\n                self.x0 &lt; element.x1\n                and self.x1 &gt; element.x0\n                and self.top &lt; element.bottom\n                and self.bottom &gt; element.top\n            )\n\n        # For polygon regions, check if any corner of the element is inside the polygon\n        element_corners = [\n            (element.x0, element.top),  # top-left\n            (element.x1, element.top),  # top-right\n            (element.x1, element.bottom),  # bottom-right\n            (element.x0, element.bottom),  # bottom-left\n        ]\n\n        # First check if any element corner is inside the polygon\n        if any(self.is_point_inside(x, y) for x, y in element_corners):\n            return True\n\n        # Also check if any polygon corner is inside the element's rectangle\n        for x, y in self.polygon:\n            if element.x0 &lt;= x &lt;= element.x1 and element.top &lt;= y &lt;= element.bottom:\n                return True\n\n        # Also check if any polygon edge intersects with any rectangle edge\n        # This is a simplification - for complex cases, we'd need a full polygon-rectangle\n        # intersection algorithm\n\n        # For now, return True if bounding boxes overlap (approximation for polygon-rectangle case)\n        return (\n            self.x0 &lt; element.x1\n            and self.x1 &gt; element.x0\n            and self.top &lt; element.bottom\n            and self.bottom &gt; element.top\n        )\n\n    def highlight(\n        self,\n        label: Optional[str] = None,\n        color: Optional[Union[Tuple, str]] = None,\n        use_color_cycling: bool = False,\n        annotate: Optional[List[str]] = None,\n        existing: str = \"append\",\n    ) -&gt; \"Region\":\n        \"\"\"\n        Highlight this region on the page.\n\n        Args:\n            label: Optional label for the highlight\n            color: Color tuple/string for the highlight, or None to use automatic color\n            use_color_cycling: Force color cycling even with no label (default: False)\n            annotate: List of attribute names to display on the highlight (e.g., ['confidence', 'type'])\n            existing: How to handle existing highlights ('append' or 'replace').\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Access the highlighter service correctly\n        highlighter = self.page._highlighter\n\n        # Prepare common arguments\n        highlight_args = {\n            \"page_index\": self.page.index,\n            \"color\": color,\n            \"label\": label,\n            \"use_color_cycling\": use_color_cycling,\n            \"element\": self,  # Pass the region itself so attributes can be accessed\n            \"annotate\": annotate,\n            \"existing\": existing,\n        }\n\n        # Call the appropriate service method\n        if self.has_polygon:\n            highlight_args[\"polygon\"] = self.polygon\n            highlighter.add_polygon(**highlight_args)\n        else:\n            highlight_args[\"bbox\"] = self.bbox\n            highlighter.add(**highlight_args)\n\n        return self\n\n    def save(\n        self,\n        filename: str,\n        resolution: Optional[float] = None,\n        labels: bool = True,\n        legend_position: str = \"right\",\n    ) -&gt; \"Region\":\n        \"\"\"\n        Save the page with this region highlighted to an image file.\n\n        Args:\n            filename: Path to save the image to\n            resolution: Resolution in DPI for rendering (default: uses global options, fallback to 144 DPI)\n            labels: Whether to include a legend for labels\n            legend_position: Position of the legend\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Apply global options as defaults\n        import natural_pdf\n\n        if resolution is None:\n            if natural_pdf.options.image.resolution is not None:\n                resolution = natural_pdf.options.image.resolution\n            else:\n                resolution = 144  # Default resolution when none specified\n\n        # Highlight this region if not already highlighted\n        self.highlight()\n\n        # Save the highlighted image\n        self._page.save_image(\n            filename, resolution=resolution, labels=labels, legend_position=legend_position\n        )\n        return self\n\n    def save_image(\n        self,\n        filename: str,\n        resolution: Optional[float] = None,\n        crop: bool = False,\n        include_highlights: bool = True,\n        **kwargs,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Save an image of just this region to a file.\n\n        Args:\n            filename: Path to save the image to\n            resolution: Resolution in DPI for rendering (default: uses global options, fallback to 144 DPI)\n            crop: If True, only crop the region without highlighting its boundaries\n            include_highlights: Whether to include existing highlights (default: True)\n            **kwargs: Additional parameters for rendering\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        # Apply global options as defaults\n        import natural_pdf\n\n        if resolution is None:\n            if natural_pdf.options.image.resolution is not None:\n                resolution = natural_pdf.options.image.resolution\n            else:\n                resolution = 144  # Default resolution when none specified\n\n        # Use export() to save the image\n        if include_highlights:\n            # With highlights, use export() which includes them\n            self.export(\n                path=filename,\n                resolution=resolution,\n                crop=crop,\n                **kwargs,\n            )\n        else:\n            # Without highlights, use render() and save manually\n            image = self.render(resolution=resolution, crop=crop, **kwargs)\n            if image:\n                image.save(filename)\n            else:\n                logger.error(f\"Failed to render region image for saving to {filename}\")\n\n        return self\n\n    def trim(\n        self,\n        padding: int = 1,\n        threshold: float = 0.95,\n        resolution: Optional[float] = None,\n        pre_shrink: float = 0.5,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Trim visual whitespace from the edges of this region.\n\n        Similar to Python's string .strip() method, but for visual whitespace in the region image.\n        Uses pixel analysis to detect rows/columns that are predominantly whitespace.\n\n        Args:\n            padding: Number of pixels to keep as padding after trimming (default: 1)\n            threshold: Threshold for considering a row/column as whitespace (0.0-1.0, default: 0.95)\n                      Higher values mean more strict whitespace detection.\n                      E.g., 0.95 means if 95% of pixels in a row/column are white, consider it whitespace.\n            resolution: Resolution for image rendering in DPI (default: uses global options, fallback to 144 DPI)\n            pre_shrink: Amount to shrink region before trimming, then expand back after (default: 0.5)\n                       This helps avoid detecting box borders/slivers as content.\n\n        Returns\n        ------\n\n        New Region with visual whitespace trimmed from all edges\n\n        Examples\n        --------\n\n        ```python\n        # Basic trimming with 1 pixel padding and 0.5px pre-shrink\n        trimmed = region.trim()\n\n        # More aggressive trimming with no padding and no pre-shrink\n        tight = region.trim(padding=0, threshold=0.9, pre_shrink=0)\n\n        # Conservative trimming with more padding\n        loose = region.trim(padding=3, threshold=0.98)\n        ```\n        \"\"\"\n        # Apply global options as defaults\n        import natural_pdf\n\n        if resolution is None:\n            if natural_pdf.options.image.resolution is not None:\n                resolution = natural_pdf.options.image.resolution\n            else:\n                resolution = 144  # Default resolution when none specified\n\n        # Pre-shrink the region to avoid box slivers\n        work_region = (\n            self.expand(left=-pre_shrink, right=-pre_shrink, top=-pre_shrink, bottom=-pre_shrink)\n            if pre_shrink &gt; 0\n            else self\n        )\n\n        # Get the region image\n        # Use render() for clean image without highlights, with cropping\n        image = work_region.render(resolution=resolution, crop=True)\n\n        if image is None:\n            logger.warning(\n                f\"Region {self.bbox}: Could not generate image for trimming. Returning original region.\"\n            )\n            return self\n\n        # Convert to grayscale for easier analysis\n        import numpy as np\n\n        # Convert PIL image to numpy array\n        img_array = np.array(image.convert(\"L\"))  # Convert to grayscale\n        height, width = img_array.shape\n\n        if height == 0 or width == 0:\n            logger.warning(\n                f\"Region {self.bbox}: Image has zero dimensions. Returning original region.\"\n            )\n            return self\n\n        # Normalize pixel values to 0-1 range (255 = white = 1.0, 0 = black = 0.0)\n        normalized = img_array.astype(np.float32) / 255.0\n\n        # Find content boundaries by analyzing row and column averages\n\n        # Analyze rows (horizontal strips) to find top and bottom boundaries\n        row_averages = np.mean(normalized, axis=1)  # Average each row\n        content_rows = row_averages &lt; threshold  # True where there's content (not whitespace)\n\n        # Find first and last rows with content\n        content_row_indices = np.where(content_rows)[0]\n        if len(content_row_indices) == 0:\n            # No content found, return a minimal region at the center\n            logger.warning(\n                f\"Region {self.bbox}: No content detected during trimming. Returning center point.\"\n            )\n            center_x = (self.x0 + self.x1) / 2\n            center_y = (self.top + self.bottom) / 2\n            return Region(self.page, (center_x, center_y, center_x, center_y))\n\n        top_content_row = max(0, content_row_indices[0] - padding)\n        bottom_content_row = min(height - 1, content_row_indices[-1] + padding)\n\n        # Analyze columns (vertical strips) to find left and right boundaries\n        col_averages = np.mean(normalized, axis=0)  # Average each column\n        content_cols = col_averages &lt; threshold  # True where there's content\n\n        content_col_indices = np.where(content_cols)[0]\n        if len(content_col_indices) == 0:\n            # No content found in columns either\n            logger.warning(\n                f\"Region {self.bbox}: No column content detected during trimming. Returning center point.\"\n            )\n            center_x = (self.x0 + self.x1) / 2\n            center_y = (self.top + self.bottom) / 2\n            return Region(self.page, (center_x, center_y, center_x, center_y))\n\n        left_content_col = max(0, content_col_indices[0] - padding)\n        right_content_col = min(width - 1, content_col_indices[-1] + padding)\n\n        # Convert trimmed pixel coordinates back to PDF coordinates\n        scale_factor = resolution / 72.0  # Scale factor used in render()\n\n        # Calculate new PDF coordinates and ensure they are Python floats\n        trimmed_x0 = float(work_region.x0 + (left_content_col / scale_factor))\n        trimmed_top = float(work_region.top + (top_content_row / scale_factor))\n        trimmed_x1 = float(\n            work_region.x0 + ((right_content_col + 1) / scale_factor)\n        )  # +1 because we want inclusive right edge\n        trimmed_bottom = float(\n            work_region.top + ((bottom_content_row + 1) / scale_factor)\n        )  # +1 because we want inclusive bottom edge\n\n        # Ensure the trimmed region doesn't exceed the work region boundaries\n        final_x0 = max(work_region.x0, trimmed_x0)\n        final_top = max(work_region.top, trimmed_top)\n        final_x1 = min(work_region.x1, trimmed_x1)\n        final_bottom = min(work_region.bottom, trimmed_bottom)\n\n        # Ensure valid coordinates (width &gt; 0, height &gt; 0)\n        if final_x1 &lt;= final_x0 or final_bottom &lt;= final_top:\n            logger.warning(\n                f\"Region {self.bbox}: Trimming resulted in invalid dimensions. Returning original region.\"\n            )\n            return self\n\n        # Create the trimmed region\n        trimmed_region = Region(self.page, (final_x0, final_top, final_x1, final_bottom))\n\n        # Expand back by the pre_shrink amount to restore original positioning\n        if pre_shrink &gt; 0:\n            trimmed_region = trimmed_region.expand(\n                left=pre_shrink, right=pre_shrink, top=pre_shrink, bottom=pre_shrink\n            )\n\n        # Copy relevant metadata\n        trimmed_region.region_type = self.region_type\n        trimmed_region.normalized_type = self.normalized_type\n        trimmed_region.confidence = self.confidence\n        trimmed_region.model = self.model\n        trimmed_region.name = self.name\n        trimmed_region.label = self.label\n        trimmed_region.source = \"trimmed\"  # Indicate this is a derived region\n        trimmed_region.parent_region = self\n\n        logger.debug(\n            f\"Region {self.bbox}: Trimmed to {trimmed_region.bbox} (padding={padding}, threshold={threshold}, pre_shrink={pre_shrink})\"\n        )\n        return trimmed_region\n\n    def clip(\n        self,\n        obj: Optional[Any] = None,\n        left: Optional[float] = None,\n        top: Optional[float] = None,\n        right: Optional[float] = None,\n        bottom: Optional[float] = None,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Clip this region to specific bounds, either from another object with bbox or explicit coordinates.\n\n        The clipped region will be constrained to not exceed the specified boundaries.\n        You can provide either an object with bounding box properties, specific coordinates, or both.\n        When both are provided, explicit coordinates take precedence.\n\n        Args:\n            obj: Optional object with bbox properties (Region, Element, TextElement, etc.)\n            left: Optional left boundary (x0) to clip to\n            top: Optional top boundary to clip to\n            right: Optional right boundary (x1) to clip to\n            bottom: Optional bottom boundary to clip to\n\n        Returns:\n            New Region with bounds clipped to the specified constraints\n\n        Examples:\n            # Clip to another region's bounds\n            clipped = region.clip(container_region)\n\n            # Clip to any element's bounds\n            clipped = region.clip(text_element)\n\n            # Clip to specific coordinates\n            clipped = region.clip(left=100, right=400)\n\n            # Mix object bounds with specific overrides\n            clipped = region.clip(obj=container, bottom=page.height/2)\n        \"\"\"\n        from natural_pdf.elements.base import extract_bbox\n\n        # Start with current region bounds\n        clip_x0 = self.x0\n        clip_top = self.top\n        clip_x1 = self.x1\n        clip_bottom = self.bottom\n\n        # Apply object constraints if provided\n        if obj is not None:\n            obj_bbox = extract_bbox(obj)\n            if obj_bbox is not None:\n                obj_x0, obj_top, obj_x1, obj_bottom = obj_bbox\n                # Constrain to the intersection with the provided object\n                clip_x0 = max(clip_x0, obj_x0)\n                clip_top = max(clip_top, obj_top)\n                clip_x1 = min(clip_x1, obj_x1)\n                clip_bottom = min(clip_bottom, obj_bottom)\n            else:\n                logger.warning(\n                    f\"Region {self.bbox}: Cannot extract bbox from clipping object {type(obj)}. \"\n                    \"Object must have bbox property or x0/top/x1/bottom attributes.\"\n                )\n\n        # Apply explicit coordinate constraints (these take precedence)\n        if left is not None:\n            clip_x0 = max(clip_x0, left)\n        if top is not None:\n            clip_top = max(clip_top, top)\n        if right is not None:\n            clip_x1 = min(clip_x1, right)\n        if bottom is not None:\n            clip_bottom = min(clip_bottom, bottom)\n\n        # Ensure valid coordinates\n        if clip_x1 &lt;= clip_x0 or clip_bottom &lt;= clip_top:\n            logger.warning(\n                f\"Region {self.bbox}: Clipping resulted in invalid dimensions \"\n                f\"({clip_x0}, {clip_top}, {clip_x1}, {clip_bottom}). Returning minimal region.\"\n            )\n            # Return a minimal region at the clip area's top-left\n            return Region(self.page, (clip_x0, clip_top, clip_x0, clip_top))\n\n        # Create the clipped region\n        clipped_region = Region(self.page, (clip_x0, clip_top, clip_x1, clip_bottom))\n\n        # Copy relevant metadata\n        clipped_region.region_type = self.region_type\n        clipped_region.normalized_type = self.normalized_type\n        clipped_region.confidence = self.confidence\n        clipped_region.model = self.model\n        clipped_region.name = self.name\n        clipped_region.label = self.label\n        clipped_region.source = \"clipped\"  # Indicate this is a derived region\n        clipped_region.parent_region = self\n\n        logger.debug(\n            f\"Region {self.bbox}: Clipped to {clipped_region.bbox} \"\n            f\"(constraints: obj={type(obj).__name__ if obj else None}, \"\n            f\"left={left}, top={top}, right={right}, bottom={bottom})\"\n        )\n        return clipped_region\n\n    def get_elements(\n        self, selector: Optional[str] = None, apply_exclusions=True, **kwargs\n    ) -&gt; List[\"Element\"]:\n        \"\"\"\n        Get all elements within this region.\n\n        Args:\n            selector: Optional selector to filter elements\n            apply_exclusions: Whether to apply exclusion regions\n            **kwargs: Additional parameters for element filtering\n\n        Returns:\n            List of elements in the region\n        \"\"\"\n        if selector:\n            # Find elements on the page matching the selector\n            page_elements = self.page.find_all(\n                selector, apply_exclusions=apply_exclusions, **kwargs\n            )\n            # Filter those elements to only include ones within this region\n            return [e for e in page_elements if self._is_element_in_region(e)]\n        else:\n            # Get all elements from the page\n            page_elements = self.page.get_elements(apply_exclusions=apply_exclusions)\n            # Filter to elements in this region\n            return [e for e in page_elements if self._is_element_in_region(e)]\n\n    def extract_text(\n        self, apply_exclusions=True, debug=False, content_filter=None, **kwargs\n    ) -&gt; str:\n        \"\"\"\n        Extract text from this region, respecting page exclusions and using pdfplumber's\n        layout engine (chars_to_textmap).\n\n        Args:\n            apply_exclusions: Whether to apply exclusion regions defined on the parent page.\n            debug: Enable verbose debugging output for filtering steps.\n            content_filter: Optional content filter to exclude specific text patterns. Can be:\n                - A regex pattern string (characters matching the pattern are EXCLUDED)\n                - A callable that takes text and returns True to KEEP the character\n                - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n            **kwargs: Additional layout parameters passed directly to pdfplumber's\n                      `chars_to_textmap` function (e.g., layout, x_density, y_density).\n                      See Page.extract_text docstring for more.\n\n        Returns:\n            Extracted text as string, potentially with layout-based spacing.\n        \"\"\"\n        # Allow 'debug_exclusions' for backward compatibility\n        debug = kwargs.get(\"debug\", debug or kwargs.get(\"debug_exclusions\", False))\n        logger.debug(f\"Region {self.bbox}: extract_text called with kwargs: {kwargs}\")\n\n        # 1. Get Word Elements potentially within this region (initial broad phase)\n        # Optimization: Could use spatial query if page elements were indexed\n        page_words = self.page.words  # Get all words from the page\n\n        # 2. Gather all character dicts from words potentially in region\n        # We filter precisely in filter_chars_spatially\n        all_char_dicts = []\n        for word in page_words:\n            # Quick bbox check to avoid processing words clearly outside\n            if get_bbox_overlap(self.bbox, word.bbox) is not None:\n                all_char_dicts.extend(getattr(word, \"_char_dicts\", []))\n\n        if not all_char_dicts:\n            logger.debug(f\"Region {self.bbox}: No character dicts found overlapping region bbox.\")\n            return \"\"\n\n        # 3. Get Relevant Exclusions (overlapping this region)\n        apply_exclusions_flag = kwargs.get(\"apply_exclusions\", apply_exclusions)\n        exclusion_regions = []\n        if apply_exclusions_flag and self._page._exclusions:\n            all_page_exclusions = self._page._get_exclusion_regions(\n                include_callable=True, debug=debug\n            )\n            overlapping_exclusions = []\n            for excl in all_page_exclusions:\n                if get_bbox_overlap(self.bbox, excl.bbox) is not None:\n                    overlapping_exclusions.append(excl)\n            exclusion_regions = overlapping_exclusions\n            if debug:\n                logger.debug(\n                    f\"Region {self.bbox}: Applying {len(exclusion_regions)} overlapping exclusions.\"\n                )\n        elif debug:\n            logger.debug(f\"Region {self.bbox}: Not applying exclusions.\")\n\n        # 4. Spatially Filter Characters using Utility\n        # Pass self as the target_region for precise polygon checks etc.\n        filtered_chars = filter_chars_spatially(\n            char_dicts=all_char_dicts,\n            exclusion_regions=exclusion_regions,\n            target_region=self,  # Pass self!\n            debug=debug,\n        )\n\n        # 5. Generate Text Layout using Utility\n        # Add content_filter to kwargs if provided\n        final_kwargs = kwargs.copy()\n        if content_filter is not None:\n            final_kwargs[\"content_filter\"] = content_filter\n\n        result = generate_text_layout(\n            char_dicts=filtered_chars,\n            layout_context_bbox=self.bbox,  # Use region's bbox for context\n            user_kwargs=final_kwargs,  # Pass kwargs including content_filter\n        )\n\n        logger.debug(f\"Region {self.bbox}: extract_text finished, result length: {len(result)}.\")\n        return result\n\n    def extract_table(\n        self,\n        method: Optional[str] = None,  # Make method optional\n        table_settings: Optional[dict] = None,  # Use Optional\n        use_ocr: bool = False,\n        ocr_config: Optional[dict] = None,  # Use Optional\n        text_options: Optional[Dict] = None,\n        cell_extraction_func: Optional[Callable[[\"Region\"], Optional[str]]] = None,\n        # --- NEW: Add tqdm control option --- #\n        show_progress: bool = False,  # Controls progress bar for text method\n        content_filter: Optional[\n            Union[str, Callable[[str], bool], List[str]]\n        ] = None,  # NEW: Content filtering\n    ) -&gt; TableResult:  # Return type allows Optional[str] for cells\n        \"\"\"\n        Extract a table from this region.\n\n        Args:\n            method: Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).\n                    'stream' is an alias for 'pdfplumber' with text-based strategies (equivalent to\n                    setting `vertical_strategy` and `horizontal_strategy` to 'text').\n                    'lattice' is an alias for 'pdfplumber' with line-based strategies (equivalent to\n                    setting `vertical_strategy` and `horizontal_strategy` to 'lines').\n            table_settings: Settings for pdfplumber table extraction (used with 'pdfplumber', 'stream', or 'lattice' methods).\n            use_ocr: Whether to use OCR for text extraction (currently only applicable with 'tatr' method).\n            ocr_config: OCR configuration parameters.\n            text_options: Dictionary of options for the 'text' method, corresponding to arguments\n                          of analyze_text_table_structure (e.g., snap_tolerance, expand_bbox).\n            cell_extraction_func: Optional callable function that takes a cell Region object\n                                  and returns its string content. Overrides default text extraction\n                                  for the 'text' method.\n            show_progress: If True, display a progress bar during cell text extraction for the 'text' method.\n            content_filter: Optional content filter to apply during cell text extraction. Can be:\n                - A regex pattern string (characters matching the pattern are EXCLUDED)\n                - A callable that takes text and returns True to KEEP the character\n                - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n                Works with all extraction methods by filtering cell content.\n\n        Returns:\n            Table data as a list of rows, where each row is a list of cell values (str or None).\n        \"\"\"\n        # Default settings if none provided\n        if table_settings is None:\n            table_settings = {}\n        if text_options is None:\n            text_options = {}  # Initialize empty dict\n\n        # Auto-detect method if not specified\n        if method is None:\n            # If this is a TATR-detected region, use TATR method\n            if hasattr(self, \"model\") and self.model == \"tatr\" and self.region_type == \"table\":\n                effective_method = \"tatr\"\n            else:\n                # Try lattice first, then fall back to stream if no meaningful results\n                logger.debug(f\"Region {self.bbox}: Auto-detecting table extraction method...\")\n\n                # --- NEW: Prefer already-created table_cell regions if they exist --- #\n                try:\n                    cell_regions_in_table = [\n                        c\n                        for c in self.page.find_all(\n                            \"region[type=table_cell]\", apply_exclusions=False\n                        )\n                        if self.intersects(c)\n                    ]\n                except Exception as _cells_err:\n                    cell_regions_in_table = []  # Fallback silently\n\n                if cell_regions_in_table:\n                    logger.debug(\n                        f\"Region {self.bbox}: Found {len(cell_regions_in_table)} pre-computed table_cell regions \u2013 using 'cells' method.\"\n                    )\n                    return TableResult(\n                        self._extract_table_from_cells(\n                            cell_regions_in_table, content_filter=content_filter\n                        )\n                    )\n\n                # --------------------------------------------------------------- #\n\n                try:\n                    logger.debug(f\"Region {self.bbox}: Trying 'lattice' method first...\")\n                    lattice_result = self.extract_table(\n                        \"lattice\", table_settings=table_settings.copy()\n                    )\n\n                    # Check if lattice found meaningful content\n                    if (\n                        lattice_result\n                        and len(lattice_result) &gt; 0\n                        and any(\n                            any(cell and cell.strip() for cell in row if cell)\n                            for row in lattice_result\n                        )\n                    ):\n                        logger.debug(\n                            f\"Region {self.bbox}: 'lattice' method found table with {len(lattice_result)} rows\"\n                        )\n                        return lattice_result\n                    else:\n                        logger.debug(\n                            f\"Region {self.bbox}: 'lattice' method found no meaningful content\"\n                        )\n                except Exception as e:\n                    logger.debug(f\"Region {self.bbox}: 'lattice' method failed: {e}\")\n\n                # Fall back to stream\n                logger.debug(f\"Region {self.bbox}: Falling back to 'stream' method...\")\n                return self.extract_table(\"stream\", table_settings=table_settings.copy())\n        else:\n            effective_method = method\n\n        # Handle method aliases for pdfplumber\n        if effective_method == \"stream\":\n            logger.debug(\"Using 'stream' method alias for 'pdfplumber' with text-based strategies.\")\n            effective_method = \"pdfplumber\"\n            # Set default text strategies if not already provided by the user\n            table_settings.setdefault(\"vertical_strategy\", \"text\")\n            table_settings.setdefault(\"horizontal_strategy\", \"text\")\n        elif effective_method == \"lattice\":\n            logger.debug(\n                \"Using 'lattice' method alias for 'pdfplumber' with line-based strategies.\"\n            )\n            effective_method = \"pdfplumber\"\n            # Set default line strategies if not already provided by the user\n            table_settings.setdefault(\"vertical_strategy\", \"lines\")\n            table_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n        # -------------------------------------------------------------\n        # Auto-inject tolerances when text-based strategies are requested.\n        # This must happen AFTER alias handling (so strategies are final)\n        # and BEFORE we delegate to _extract_table_* helpers.\n        # -------------------------------------------------------------\n        if \"text\" in (\n            table_settings.get(\"vertical_strategy\"),\n            table_settings.get(\"horizontal_strategy\"),\n        ):\n            page_cfg = getattr(self.page, \"_config\", {})\n            # Ensure text_* tolerances passed to pdfplumber\n            if \"text_x_tolerance\" not in table_settings and \"x_tolerance\" not in table_settings:\n                if page_cfg.get(\"x_tolerance\") is not None:\n                    table_settings[\"text_x_tolerance\"] = page_cfg[\"x_tolerance\"]\n            if \"text_y_tolerance\" not in table_settings and \"y_tolerance\" not in table_settings:\n                if page_cfg.get(\"y_tolerance\") is not None:\n                    table_settings[\"text_y_tolerance\"] = page_cfg[\"y_tolerance\"]\n\n            # Snap / join tolerances (~ line spacing)\n            if \"snap_tolerance\" not in table_settings and \"snap_x_tolerance\" not in table_settings:\n                snap = max(1, round((page_cfg.get(\"y_tolerance\", 1)) * 0.9))\n                table_settings[\"snap_tolerance\"] = snap\n            if \"join_tolerance\" not in table_settings and \"join_x_tolerance\" not in table_settings:\n                table_settings[\"join_tolerance\"] = table_settings[\"snap_tolerance\"]\n\n        logger.debug(f\"Region {self.bbox}: Extracting table using method '{effective_method}'\")\n\n        # Use the selected method\n        if effective_method == \"tatr\":\n            table_rows = self._extract_table_tatr(\n                use_ocr=use_ocr, ocr_config=ocr_config, content_filter=content_filter\n            )\n        elif effective_method == \"text\":\n            current_text_options = text_options.copy()\n            current_text_options[\"cell_extraction_func\"] = cell_extraction_func\n            current_text_options[\"show_progress\"] = show_progress\n            current_text_options[\"content_filter\"] = content_filter\n            table_rows = self._extract_table_text(**current_text_options)\n        elif effective_method == \"pdfplumber\":\n            table_rows = self._extract_table_plumber(table_settings, content_filter=content_filter)\n        else:\n            raise ValueError(\n                f\"Unknown table extraction method: '{method}'. Choose from 'tatr', 'pdfplumber', 'text', 'stream', 'lattice'.\"\n            )\n\n        return TableResult(table_rows)\n\n    def extract_tables(\n        self,\n        method: Optional[str] = None,\n        table_settings: Optional[dict] = None,\n    ) -&gt; List[List[List[str]]]:\n        \"\"\"\n        Extract all tables from this region using pdfplumber-based methods.\n\n        Note: Only 'pdfplumber', 'stream', and 'lattice' methods are supported for extract_tables.\n        'tatr' and 'text' methods are designed for single table extraction only.\n\n        Args:\n            method: Method to use: 'pdfplumber', 'stream', 'lattice', or None (auto-detect).\n                    'stream' uses text-based strategies, 'lattice' uses line-based strategies.\n            table_settings: Settings for pdfplumber table extraction.\n\n        Returns:\n            List of tables, where each table is a list of rows, and each row is a list of cell values.\n        \"\"\"\n        if table_settings is None:\n            table_settings = {}\n\n        # Auto-detect method if not specified (try lattice first, then stream)\n        if method is None:\n            logger.debug(f\"Region {self.bbox}: Auto-detecting tables extraction method...\")\n\n            # Try lattice first\n            try:\n                lattice_settings = table_settings.copy()\n                lattice_settings.setdefault(\"vertical_strategy\", \"lines\")\n                lattice_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n                logger.debug(f\"Region {self.bbox}: Trying 'lattice' method first for tables...\")\n                lattice_result = self._extract_tables_plumber(lattice_settings)\n\n                # Check if lattice found meaningful tables\n                if (\n                    lattice_result\n                    and len(lattice_result) &gt; 0\n                    and any(\n                        any(\n                            any(cell and cell.strip() for cell in row if cell)\n                            for row in table\n                            if table\n                        )\n                        for table in lattice_result\n                    )\n                ):\n                    logger.debug(\n                        f\"Region {self.bbox}: 'lattice' method found {len(lattice_result)} tables\"\n                    )\n                    return lattice_result\n                else:\n                    logger.debug(f\"Region {self.bbox}: 'lattice' method found no meaningful tables\")\n\n            except Exception as e:\n                logger.debug(f\"Region {self.bbox}: 'lattice' method failed: {e}\")\n\n            # Fall back to stream\n            logger.debug(f\"Region {self.bbox}: Falling back to 'stream' method for tables...\")\n            stream_settings = table_settings.copy()\n            stream_settings.setdefault(\"vertical_strategy\", \"text\")\n            stream_settings.setdefault(\"horizontal_strategy\", \"text\")\n\n            return self._extract_tables_plumber(stream_settings)\n\n        effective_method = method\n\n        # Handle method aliases\n        if effective_method == \"stream\":\n            logger.debug(\"Using 'stream' method alias for 'pdfplumber' with text-based strategies.\")\n            effective_method = \"pdfplumber\"\n            table_settings.setdefault(\"vertical_strategy\", \"text\")\n            table_settings.setdefault(\"horizontal_strategy\", \"text\")\n        elif effective_method == \"lattice\":\n            logger.debug(\n                \"Using 'lattice' method alias for 'pdfplumber' with line-based strategies.\"\n            )\n            effective_method = \"pdfplumber\"\n            table_settings.setdefault(\"vertical_strategy\", \"lines\")\n            table_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n        # Use the selected method\n        if effective_method == \"pdfplumber\":\n            return self._extract_tables_plumber(table_settings)\n        else:\n            raise ValueError(\n                f\"Unknown tables extraction method: '{method}'. Choose from 'pdfplumber', 'stream', 'lattice'.\"\n            )\n\n    def _extract_tables_plumber(self, table_settings: dict) -&gt; List[List[List[str]]]:\n        \"\"\"\n        Extract all tables using pdfplumber's table extraction.\n\n        Args:\n            table_settings: Settings for pdfplumber table extraction\n\n        Returns:\n            List of tables, where each table is a list of rows, and each row is a list of cell values\n        \"\"\"\n        # Inject global PDF-level text tolerances if not explicitly present\n        pdf_cfg = getattr(self.page, \"_config\", getattr(self.page._parent, \"_config\", {}))\n        _uses_text = \"text\" in (\n            table_settings.get(\"vertical_strategy\"),\n            table_settings.get(\"horizontal_strategy\"),\n        )\n        if (\n            _uses_text\n            and \"text_x_tolerance\" not in table_settings\n            and \"x_tolerance\" not in table_settings\n        ):\n            x_tol = pdf_cfg.get(\"x_tolerance\")\n            if x_tol is not None:\n                table_settings.setdefault(\"text_x_tolerance\", x_tol)\n        if (\n            _uses_text\n            and \"text_y_tolerance\" not in table_settings\n            and \"y_tolerance\" not in table_settings\n        ):\n            y_tol = pdf_cfg.get(\"y_tolerance\")\n            if y_tol is not None:\n                table_settings.setdefault(\"text_y_tolerance\", y_tol)\n\n        if (\n            _uses_text\n            and \"snap_tolerance\" not in table_settings\n            and \"snap_x_tolerance\" not in table_settings\n        ):\n            snap = max(1, round((pdf_cfg.get(\"y_tolerance\", 1)) * 0.9))\n            table_settings.setdefault(\"snap_tolerance\", snap)\n        if (\n            _uses_text\n            and \"join_tolerance\" not in table_settings\n            and \"join_x_tolerance\" not in table_settings\n        ):\n            join = table_settings.get(\"snap_tolerance\", 1)\n            table_settings.setdefault(\"join_tolerance\", join)\n            table_settings.setdefault(\"join_x_tolerance\", join)\n            table_settings.setdefault(\"join_y_tolerance\", join)\n\n        # -------------------------------------------------------------\n        # Apply char-level exclusion filtering, if any exclusions are\n        # defined on the parent Page.  We create a lightweight\n        # pdfplumber.Page copy whose .chars list omits characters that\n        # fall inside any exclusion Region.  Other object types are\n        # left untouched for now (\"chars-only\" strategy).\n        # -------------------------------------------------------------\n        base_plumber_page = self.page._page\n\n        if getattr(self.page, \"_exclusions\", None):\n            # Resolve exclusion Regions (callables already evaluated)\n            exclusion_regions = self.page._get_exclusion_regions(include_callable=True)\n\n            def _keep_char(obj):\n                \"\"\"Return True if pdfplumber obj should be kept.\"\"\"\n                if obj.get(\"object_type\") != \"char\":\n                    # Keep non-char objects unchanged \u2013 lattice grids etc.\n                    return True\n\n                # Compute character centre point\n                cx = (obj[\"x0\"] + obj[\"x1\"]) / 2.0\n                cy = (obj[\"top\"] + obj[\"bottom\"]) / 2.0\n\n                # Reject if the centre lies inside ANY exclusion Region\n                for reg in exclusion_regions:\n                    if reg.x0 &lt;= cx &lt;= reg.x1 and reg.top &lt;= cy &lt;= reg.bottom:\n                        return False\n                return True\n\n            try:\n                filtered_page = base_plumber_page.filter(_keep_char)\n            except Exception as _filter_err:\n                # Fallback \u2013 if filtering fails, log and proceed unfiltered\n                logger.warning(\n                    f\"Region {self.bbox}: Failed to filter pdfplumber chars for exclusions: {_filter_err}\"\n                )\n                filtered_page = base_plumber_page\n        else:\n            filtered_page = base_plumber_page\n\n        cropped = filtered_page.crop(self.bbox)\n\n        # Extract all tables from the cropped area\n        tables = cropped.extract_tables(table_settings)\n\n        # Apply RTL text processing to all tables\n        if tables:\n            processed_tables = []\n            for table in tables:\n                processed_table = []\n                for row in table:\n                    processed_row = []\n                    for cell in row:\n                        if cell is not None:\n                            # Apply RTL text processing to each cell\n                            rtl_processed_cell = self._apply_rtl_processing_to_text(cell)\n                            processed_row.append(rtl_processed_cell)\n                        else:\n                            processed_row.append(cell)\n                    processed_table.append(processed_row)\n                processed_tables.append(processed_table)\n            return processed_tables\n\n        # Return empty list if no tables found\n        return []\n\n    def _extract_table_plumber(self, table_settings: dict, content_filter=None) -&gt; List[List[str]]:\n        \"\"\"\n        Extract table using pdfplumber's table extraction.\n        This method extracts the largest table within the region.\n\n        Args:\n            table_settings: Settings for pdfplumber table extraction\n            content_filter: Optional content filter to apply to cell values\n\n        Returns:\n            Table data as a list of rows, where each row is a list of cell values\n        \"\"\"\n        # Inject global PDF-level text tolerances if not explicitly present\n        pdf_cfg = getattr(self.page, \"_config\", getattr(self.page._parent, \"_config\", {}))\n        _uses_text = \"text\" in (\n            table_settings.get(\"vertical_strategy\"),\n            table_settings.get(\"horizontal_strategy\"),\n        )\n        if (\n            _uses_text\n            and \"text_x_tolerance\" not in table_settings\n            and \"x_tolerance\" not in table_settings\n        ):\n            x_tol = pdf_cfg.get(\"x_tolerance\")\n            if x_tol is not None:\n                table_settings.setdefault(\"text_x_tolerance\", x_tol)\n        if (\n            _uses_text\n            and \"text_y_tolerance\" not in table_settings\n            and \"y_tolerance\" not in table_settings\n        ):\n            y_tol = pdf_cfg.get(\"y_tolerance\")\n            if y_tol is not None:\n                table_settings.setdefault(\"text_y_tolerance\", y_tol)\n\n        # -------------------------------------------------------------\n        # Apply char-level exclusion filtering (chars only) just like in\n        # _extract_tables_plumber so header/footer text does not appear\n        # in extracted tables.\n        # -------------------------------------------------------------\n        base_plumber_page = self.page._page\n\n        if getattr(self.page, \"_exclusions\", None):\n            exclusion_regions = self.page._get_exclusion_regions(include_callable=True)\n\n            def _keep_char(obj):\n                if obj.get(\"object_type\") != \"char\":\n                    return True\n                cx = (obj[\"x0\"] + obj[\"x1\"]) / 2.0\n                cy = (obj[\"top\"] + obj[\"bottom\"]) / 2.0\n                for reg in exclusion_regions:\n                    if reg.x0 &lt;= cx &lt;= reg.x1 and reg.top &lt;= cy &lt;= reg.bottom:\n                        return False\n                return True\n\n            try:\n                filtered_page = base_plumber_page.filter(_keep_char)\n            except Exception as _filter_err:\n                logger.warning(\n                    f\"Region {self.bbox}: Failed to filter pdfplumber chars for exclusions (single table): {_filter_err}\"\n                )\n                filtered_page = base_plumber_page\n        else:\n            filtered_page = base_plumber_page\n\n        # Now crop the (possibly filtered) page to the region bbox\n        cropped = filtered_page.crop(self.bbox)\n\n        # Extract the single largest table from the cropped area\n        table = cropped.extract_table(table_settings)\n\n        # Return the table or an empty list if none found\n        if table:\n            # Apply RTL text processing and content filtering if provided\n            processed_table = []\n            for row in table:\n                processed_row = []\n                for cell in row:\n                    if cell is not None:\n                        # Apply RTL text processing first\n                        rtl_processed_cell = self._apply_rtl_processing_to_text(cell)\n\n                        # Then apply content filter if provided\n                        if content_filter is not None:\n                            filtered_cell = self._apply_content_filter_to_text(\n                                rtl_processed_cell, content_filter\n                            )\n                            processed_row.append(filtered_cell)\n                        else:\n                            processed_row.append(rtl_processed_cell)\n                    else:\n                        processed_row.append(cell)\n                processed_table.append(processed_row)\n            return processed_table\n        return []\n\n    def _extract_table_tatr(\n        self, use_ocr=False, ocr_config=None, content_filter=None\n    ) -&gt; List[List[str]]:\n        \"\"\"\n        Extract table using TATR structure detection.\n\n        Args:\n            use_ocr: Whether to apply OCR to each cell for better text extraction\n            ocr_config: Optional OCR configuration parameters\n            content_filter: Optional content filter to apply to cell values\n\n        Returns:\n            Table data as a list of rows, where each row is a list of cell values\n        \"\"\"\n        # Find all rows and headers in this table\n        rows = self.page.find_all(f\"region[type=table-row][model=tatr]\")\n        headers = self.page.find_all(f\"region[type=table-column-header][model=tatr]\")\n        columns = self.page.find_all(f\"region[type=table-column][model=tatr]\")\n\n        # Filter to only include rows/headers/columns that overlap with this table region\n        def is_in_table(region):\n            # Check for overlap - simplifying to center point for now\n            region_center_x = (region.x0 + region.x1) / 2\n            region_center_y = (region.top + region.bottom) / 2\n            return (\n                self.x0 &lt;= region_center_x &lt;= self.x1 and self.top &lt;= region_center_y &lt;= self.bottom\n            )\n\n        rows = [row for row in rows if is_in_table(row)]\n        headers = [header for header in headers if is_in_table(header)]\n        columns = [column for column in columns if is_in_table(column)]\n\n        # Sort rows by vertical position (top to bottom)\n        rows.sort(key=lambda r: r.top)\n\n        # Sort columns by horizontal position (left to right)\n        columns.sort(key=lambda c: c.x0)\n\n        # Create table data structure\n        table_data = []\n\n        # Prepare OCR config if needed\n        if use_ocr:\n            # Default OCR config focuses on small text with low confidence\n            default_ocr_config = {\n                \"enabled\": True,\n                \"min_confidence\": 0.1,  # Lower than default to catch more text\n                \"detection_params\": {\n                    \"text_threshold\": 0.1,  # Lower threshold for low-contrast text\n                    \"link_threshold\": 0.1,  # Lower threshold for connecting text components\n                },\n            }\n\n            # Merge with provided config if any\n            if ocr_config:\n                if isinstance(ocr_config, dict):\n                    # Update default config with provided values\n                    for key, value in ocr_config.items():\n                        if (\n                            isinstance(value, dict)\n                            and key in default_ocr_config\n                            and isinstance(default_ocr_config[key], dict)\n                        ):\n                            # Merge nested dicts\n                            default_ocr_config[key].update(value)\n                        else:\n                            # Replace value\n                            default_ocr_config[key] = value\n                else:\n                    # Not a dict, use as is\n                    default_ocr_config = ocr_config\n\n            # Use the merged config\n            ocr_config = default_ocr_config\n\n        # Add header row if headers were detected\n        if headers:\n            header_texts = []\n            for header in headers:\n                if use_ocr:\n                    # Try OCR for better text extraction\n                    ocr_elements = header.apply_ocr(**ocr_config)\n                    if ocr_elements:\n                        ocr_text = \" \".join(e.text for e in ocr_elements).strip()\n                        if ocr_text:\n                            header_texts.append(ocr_text)\n                            continue\n\n                # Fallback to normal extraction\n                header_text = header.extract_text().strip()\n                if content_filter is not None:\n                    header_text = self._apply_content_filter_to_text(header_text, content_filter)\n                header_texts.append(header_text)\n            table_data.append(header_texts)\n\n        # Process rows\n        for row in rows:\n            row_cells = []\n\n            # If we have columns, use them to extract cells\n            if columns:\n                for column in columns:\n                    # Create a cell region at the intersection of row and column\n                    cell_bbox = (column.x0, row.top, column.x1, row.bottom)\n\n                    # Create a region for this cell\n                    from natural_pdf.elements.region import (  # Import here to avoid circular imports\n                        Region,\n                    )\n\n                    cell_region = Region(self.page, cell_bbox)\n\n                    # Extract text from the cell\n                    if use_ocr:\n                        # Apply OCR to the cell\n                        ocr_elements = cell_region.apply_ocr(**ocr_config)\n                        if ocr_elements:\n                            # Get text from OCR elements\n                            ocr_text = \" \".join(e.text for e in ocr_elements).strip()\n                            if ocr_text:\n                                row_cells.append(ocr_text)\n                                continue\n\n                    # Fallback to normal extraction\n                    cell_text = cell_region.extract_text().strip()\n                    if content_filter is not None:\n                        cell_text = self._apply_content_filter_to_text(cell_text, content_filter)\n                    row_cells.append(cell_text)\n            else:\n                # No column information, just extract the whole row text\n                if use_ocr:\n                    # Try OCR on the whole row\n                    ocr_elements = row.apply_ocr(**ocr_config)\n                    if ocr_elements:\n                        ocr_text = \" \".join(e.text for e in ocr_elements).strip()\n                        if ocr_text:\n                            row_cells.append(ocr_text)\n                            continue\n\n                # Fallback to normal extraction\n                row_text = row.extract_text().strip()\n                if content_filter is not None:\n                    row_text = self._apply_content_filter_to_text(row_text, content_filter)\n                row_cells.append(row_text)\n\n            table_data.append(row_cells)\n\n        return table_data\n\n    def _extract_table_text(self, **text_options) -&gt; List[List[Optional[str]]]:\n        \"\"\"\n        Extracts table content based on text alignment analysis.\n\n        Args:\n            **text_options: Options passed to analyze_text_table_structure,\n                          plus optional 'cell_extraction_func', 'coordinate_grouping_tolerance',\n                          'show_progress', and 'content_filter'.\n\n        Returns:\n            Table data as list of lists of strings (or None for empty cells).\n        \"\"\"\n        cell_extraction_func = text_options.pop(\"cell_extraction_func\", None)\n        # --- Get show_progress option --- #\n        show_progress = text_options.pop(\"show_progress\", False)\n        # --- Get content_filter option --- #\n        content_filter = text_options.pop(\"content_filter\", None)\n\n        # Analyze structure first (or use cached results)\n        if \"text_table_structure\" in self.analyses:\n            analysis_results = self.analyses[\"text_table_structure\"]\n            logger.debug(\"Using cached text table structure analysis results.\")\n        else:\n            analysis_results = self.analyze_text_table_structure(**text_options)\n\n        if analysis_results is None or not analysis_results.get(\"cells\"):\n            logger.warning(f\"Region {self.bbox}: No cells found using 'text' method.\")\n            return []\n\n        cell_dicts = analysis_results[\"cells\"]\n\n        # --- Grid Reconstruction Logic --- #\n        if not cell_dicts:\n            return []\n\n        # 1. Get unique sorted top and left coordinates (cell boundaries)\n        coord_tolerance = text_options.get(\"coordinate_grouping_tolerance\", 1)\n        tops = sorted(\n            list(set(round(c[\"top\"] / coord_tolerance) * coord_tolerance for c in cell_dicts))\n        )\n        lefts = sorted(\n            list(set(round(c[\"left\"] / coord_tolerance) * coord_tolerance for c in cell_dicts))\n        )\n\n        # Refine boundaries (cluster_coords helper remains the same)\n        def cluster_coords(coords):\n            if not coords:\n                return []\n            clustered = []\n            current_cluster = [coords[0]]\n            for c in coords[1:]:\n                if abs(c - current_cluster[-1]) &lt;= coord_tolerance:\n                    current_cluster.append(c)\n                else:\n                    clustered.append(min(current_cluster))\n                    current_cluster = [c]\n            clustered.append(min(current_cluster))\n            return clustered\n\n        unique_tops = cluster_coords(tops)\n        unique_lefts = cluster_coords(lefts)\n\n        # Determine iterable for tqdm\n        cell_iterator = cell_dicts\n        if show_progress:\n            # Only wrap if progress should be shown\n            cell_iterator = tqdm(\n                cell_dicts,\n                desc=f\"Extracting text from {len(cell_dicts)} cells (text method)\",\n                unit=\"cell\",\n                leave=False,  # Optional: Keep bar after completion\n            )\n        # --- End tqdm Setup --- #\n\n        # 2. Create a lookup map for cell text: {(rounded_top, rounded_left): cell_text}\n        cell_text_map = {}\n        # --- Use the potentially wrapped iterator --- #\n        for cell_data in cell_iterator:\n            try:\n                cell_region = self.page.region(**cell_data)\n                cell_value = None  # Initialize\n                if callable(cell_extraction_func):\n                    try:\n                        cell_value = cell_extraction_func(cell_region)\n                        if not isinstance(cell_value, (str, type(None))):\n                            logger.warning(\n                                f\"Custom cell_extraction_func returned non-string/None type ({type(cell_value)}) for cell {cell_data}. Treating as None.\"\n                            )\n                            cell_value = None\n                    except Exception as func_err:\n                        logger.error(\n                            f\"Error executing custom cell_extraction_func for cell {cell_data}: {func_err}\",\n                            exc_info=True,\n                        )\n                        cell_value = None\n                else:\n                    cell_value = cell_region.extract_text(\n                        layout=False, apply_exclusions=False, content_filter=content_filter\n                    ).strip()\n\n                rounded_top = round(cell_data[\"top\"] / coord_tolerance) * coord_tolerance\n                rounded_left = round(cell_data[\"left\"] / coord_tolerance) * coord_tolerance\n                cell_text_map[(rounded_top, rounded_left)] = cell_value\n            except Exception as e:\n                logger.warning(f\"Could not process cell {cell_data} for text extraction: {e}\")\n\n        # 3. Build the final list-of-lists table (loop remains the same)\n        final_table = []\n        for row_top in unique_tops:\n            row_data = []\n            for col_left in unique_lefts:\n                best_match_key = None\n                min_dist_sq = float(\"inf\")\n                for map_top, map_left in cell_text_map.keys():\n                    if (\n                        abs(map_top - row_top) &lt;= coord_tolerance\n                        and abs(map_left - col_left) &lt;= coord_tolerance\n                    ):\n                        dist_sq = (map_top - row_top) ** 2 + (map_left - col_left) ** 2\n                        if dist_sq &lt; min_dist_sq:\n                            min_dist_sq = dist_sq\n                            best_match_key = (map_top, map_left)\n                cell_value = cell_text_map.get(best_match_key)\n                row_data.append(cell_value)\n            final_table.append(row_data)\n\n        return final_table\n\n    # --- END MODIFIED METHOD --- #\n\n    @overload\n    def find(\n        self,\n        *,\n        text: str,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[\"Element\"]: ...\n\n    @overload\n    def find(\n        self,\n        selector: str,\n        *,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[\"Element\"]: ...\n\n    def find(\n        self,\n        selector: Optional[str] = None,  # Now optional\n        *,\n        text: Optional[str] = None,  # New text parameter\n        contains: str = \"all\",  # New parameter for containment behavior\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; Optional[\"Element\"]:\n        \"\"\"\n        Find the first element in this region matching the selector OR text content.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            contains: How to determine if elements are inside: 'all' (fully inside),\n                     'any' (any overlap), or 'center' (center point inside).\n                     (default: \"all\")\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional parameters for element filtering.\n\n        Returns:\n            First matching element or None.\n        \"\"\"\n        # Delegate validation and selector construction to find_all\n        elements = self.find_all(\n            selector=selector,\n            text=text,\n            contains=contains,\n            apply_exclusions=apply_exclusions,\n            regex=regex,\n            case=case,\n            **kwargs,\n        )\n        return elements.first if elements else None\n\n    @overload\n    def find_all(\n        self,\n        *,\n        text: str,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    @overload\n    def find_all(\n        self,\n        selector: str,\n        *,\n        contains: str = \"all\",\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\": ...\n\n    def find_all(\n        self,\n        selector: Optional[str] = None,  # Now optional\n        *,\n        text: Optional[str] = None,  # New text parameter\n        contains: str = \"all\",  # New parameter to control inside/overlap behavior\n        apply_exclusions: bool = True,\n        regex: bool = False,\n        case: bool = True,\n        **kwargs,\n    ) -&gt; \"ElementCollection\":\n        \"\"\"\n        Find all elements in this region matching the selector OR text content.\n\n        Provide EITHER `selector` OR `text`, but not both.\n\n        Args:\n            selector: CSS-like selector string.\n            text: Text content to search for (equivalent to 'text:contains(...)').\n            contains: How to determine if elements are inside: 'all' (fully inside),\n                     'any' (any overlap), or 'center' (center point inside).\n                     (default: \"all\")\n            apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n            regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n            case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n            **kwargs: Additional parameters for element filtering.\n\n        Returns:\n            ElementCollection with matching elements.\n        \"\"\"\n        from natural_pdf.elements.element_collection import ElementCollection\n\n        if selector is not None and text is not None:\n            raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n        if selector is None and text is None:\n            raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n        # Validate contains parameter\n        if contains not in [\"all\", \"any\", \"center\"]:\n            raise ValueError(\n                f\"Invalid contains value: {contains}. Must be 'all', 'any', or 'center'\"\n            )\n\n        # Construct selector if 'text' is provided\n        effective_selector = \"\"\n        if text is not None:\n            escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n            effective_selector = f'text:contains(\"{escaped_text}\")'\n            logger.debug(\n                f\"Using text shortcut: find_all(text='{text}') -&gt; find_all('{effective_selector}')\"\n            )\n        elif selector is not None:\n            effective_selector = selector\n        else:\n            raise ValueError(\"Internal error: No selector or text provided.\")\n\n        # Normal case: Region is on a single page\n        try:\n            # Parse the final selector string\n            selector_obj = parse_selector(effective_selector)\n\n            # Get all potentially relevant elements from the page\n            # Let the page handle its exclusion logic if needed\n            potential_elements = self.page.find_all(\n                selector=effective_selector,\n                apply_exclusions=apply_exclusions,\n                regex=regex,\n                case=case,\n                **kwargs,\n            )\n\n            # Filter these elements based on the specified containment method\n            region_bbox = self.bbox\n            matching_elements = []\n\n            if contains == \"all\":  # Fully inside (strict)\n                matching_elements = [\n                    el\n                    for el in potential_elements\n                    if el.x0 &gt;= region_bbox[0]\n                    and el.top &gt;= region_bbox[1]\n                    and el.x1 &lt;= region_bbox[2]\n                    and el.bottom &lt;= region_bbox[3]\n                ]\n            elif contains == \"any\":  # Any overlap\n                matching_elements = [el for el in potential_elements if self.intersects(el)]\n            elif contains == \"center\":  # Center point inside\n                matching_elements = [\n                    el for el in potential_elements if self.is_element_center_inside(el)\n                ]\n\n            return ElementCollection(matching_elements)\n\n        except Exception as e:\n            logger.error(f\"Error during find_all in region: {e}\", exc_info=True)\n            return ElementCollection([])\n\n    def apply_ocr(self, replace=True, **ocr_params) -&gt; \"Region\":\n        \"\"\"\n        Apply OCR to this region and return the created text elements.\n\n        This method supports two modes:\n        1. **Built-in OCR Engines** (default) \u2013 identical to previous behaviour. Pass typical\n           parameters like ``engine='easyocr'`` or ``languages=['en']`` and the method will\n           route the request through :class:`OCRManager`.\n        2. **Custom OCR Function** \u2013 pass a *callable* under the keyword ``function`` (or\n           ``ocr_function``). The callable will receive *this* Region instance and should\n           return the extracted text (``str``) or ``None``.  Internally the call is\n           delegated to :pymeth:`apply_custom_ocr` so the same logic (replacement, element\n           creation, etc.) is re-used.\n\n        Examples\n        ---------\n        ```python\n        def llm_ocr(region):\n            image = region.render(resolution=300, crop=True)\n            return my_llm_client.ocr(image)\n        region.apply_ocr(function=llm_ocr)\n        ```\n\n        Args:\n            replace: Whether to remove existing OCR elements first (default ``True``).\n            **ocr_params: Parameters for the built-in OCR manager *or* the special\n                          ``function``/``ocr_function`` keyword to trigger custom mode.\n\n        Returns\n        -------\n            Self \u2013 for chaining.\n        \"\"\"\n        # --- Custom OCR function path --------------------------------------------------\n        custom_func = ocr_params.pop(\"function\", None) or ocr_params.pop(\"ocr_function\", None)\n        if callable(custom_func):\n            # Delegate to the specialised helper while preserving key kwargs\n            return self.apply_custom_ocr(\n                ocr_function=custom_func,\n                source_label=ocr_params.pop(\"source_label\", \"custom-ocr\"),\n                replace=replace,\n                confidence=ocr_params.pop(\"confidence\", None),\n                add_to_page=ocr_params.pop(\"add_to_page\", True),\n            )\n\n        # --- Original built-in OCR engine path (unchanged except docstring) ------------\n        # Ensure OCRManager is available\n        if not hasattr(self.page._parent, \"_ocr_manager\") or self.page._parent._ocr_manager is None:\n            logger.error(\"OCRManager not available on parent PDF. Cannot apply OCR to region.\")\n            return self\n\n        # If replace is True, find and remove existing OCR elements in this region\n        if replace:\n            logger.info(\n                f\"Region {self.bbox}: Removing existing OCR elements before applying new OCR.\"\n            )\n\n            # --- Robust removal: iterate through all OCR elements on the page and\n            #     remove those that overlap this region. This avoids reliance on\n            #     identity\u2010based look-ups that can break if the ElementManager\n            #     rebuilt its internal lists.\n\n            removed_count = 0\n\n            # Helper to remove a single element safely\n            def _safe_remove(elem):\n                nonlocal removed_count\n                success = False\n                if hasattr(elem, \"page\") and hasattr(elem.page, \"_element_mgr\"):\n                    etype = getattr(elem, \"object_type\", \"word\")\n                    if etype == \"word\":\n                        etype_key = \"words\"\n                    elif etype == \"char\":\n                        etype_key = \"chars\"\n                    else:\n                        etype_key = etype + \"s\" if not etype.endswith(\"s\") else etype\n                    try:\n                        success = elem.page._element_mgr.remove_element(elem, etype_key)\n                    except Exception:\n                        success = False\n                if success:\n                    removed_count += 1\n\n            # Remove OCR WORD elements overlapping region\n            for word in list(self.page._element_mgr.words):\n                if getattr(word, \"source\", None) == \"ocr\" and self.intersects(word):\n                    _safe_remove(word)\n\n            # Remove OCR CHAR dicts overlapping region\n            for char in list(self.page._element_mgr.chars):\n                # char can be dict or TextElement; normalise\n                char_src = (\n                    char.get(\"source\") if isinstance(char, dict) else getattr(char, \"source\", None)\n                )\n                if char_src == \"ocr\":\n                    # Rough bbox for dicts\n                    if isinstance(char, dict):\n                        cx0, ctop, cx1, cbottom = (\n                            char.get(\"x0\", 0),\n                            char.get(\"top\", 0),\n                            char.get(\"x1\", 0),\n                            char.get(\"bottom\", 0),\n                        )\n                    else:\n                        cx0, ctop, cx1, cbottom = char.x0, char.top, char.x1, char.bottom\n                    # Quick overlap check\n                    if not (\n                        cx1 &lt; self.x0 or cx0 &gt; self.x1 or cbottom &lt; self.top or ctop &gt; self.bottom\n                    ):\n                        _safe_remove(char)\n\n            logger.info(\n                f\"Region {self.bbox}: Removed {removed_count} existing OCR elements (words &amp; chars) before re-applying OCR.\"\n            )\n\n        ocr_mgr = self.page._parent._ocr_manager\n\n        # Determine rendering resolution from parameters\n        final_resolution = ocr_params.get(\"resolution\")\n        if final_resolution is None and hasattr(self.page, \"_parent\") and self.page._parent:\n            final_resolution = getattr(self.page._parent, \"_config\", {}).get(\"resolution\", 150)\n        elif final_resolution is None:\n            final_resolution = 150\n        logger.debug(\n            f\"Region {self.bbox}: Applying OCR with resolution {final_resolution} DPI and params: {ocr_params}\"\n        )\n\n        # Render the page region to an image using the determined resolution\n        try:\n            # Use render() for clean image without highlights, with cropping\n            region_image = self.render(resolution=final_resolution, crop=True)\n            if not region_image:\n                logger.error(\"Failed to render region to image for OCR.\")\n                return self\n            logger.debug(f\"Region rendered to image size: {region_image.size}\")\n        except Exception as e:\n            logger.error(f\"Error rendering region to image for OCR: {e}\", exc_info=True)\n            return self\n\n        # Prepare args for the OCR Manager\n        manager_args = {\n            \"images\": region_image,\n            \"engine\": ocr_params.get(\"engine\"),\n            \"languages\": ocr_params.get(\"languages\"),\n            \"min_confidence\": ocr_params.get(\"min_confidence\"),\n            \"device\": ocr_params.get(\"device\"),\n            \"options\": ocr_params.get(\"options\"),\n            \"detect_only\": ocr_params.get(\"detect_only\"),\n        }\n        manager_args = {k: v for k, v in manager_args.items() if v is not None}\n\n        # Run OCR on this region's image using the manager\n        results = ocr_mgr.apply_ocr(**manager_args)\n        if not isinstance(results, list):\n            logger.error(\n                f\"OCRManager returned unexpected type for single region image: {type(results)}\"\n            )\n            return self\n        logger.debug(f\"Region OCR processing returned {len(results)} results.\")\n\n        # Convert results to TextElements\n        scale_x = self.width / region_image.width if region_image.width &gt; 0 else 1.0\n        scale_y = self.height / region_image.height if region_image.height &gt; 0 else 1.0\n        logger.debug(f\"Region OCR scaling factors (PDF/Img): x={scale_x:.2f}, y={scale_y:.2f}\")\n        created_elements = []\n        for result in results:\n            try:\n                img_x0, img_top, img_x1, img_bottom = map(float, result[\"bbox\"])\n                pdf_height = (img_bottom - img_top) * scale_y\n                page_x0 = self.x0 + (img_x0 * scale_x)\n                page_top = self.top + (img_top * scale_y)\n                page_x1 = self.x0 + (img_x1 * scale_x)\n                page_bottom = self.top + (img_bottom * scale_y)\n                raw_conf = result.get(\"confidence\")\n                # Convert confidence to float unless it is None/invalid\n                try:\n                    confidence_val = float(raw_conf) if raw_conf is not None else None\n                except (TypeError, ValueError):\n                    confidence_val = None\n\n                text_val = result.get(\"text\")  # May legitimately be None in detect_only mode\n\n                element_data = {\n                    \"text\": text_val,\n                    \"x0\": page_x0,\n                    \"top\": page_top,\n                    \"x1\": page_x1,\n                    \"bottom\": page_bottom,\n                    \"width\": page_x1 - page_x0,\n                    \"height\": page_bottom - page_top,\n                    \"object_type\": \"word\",\n                    \"source\": \"ocr\",\n                    \"confidence\": confidence_val,\n                    \"fontname\": \"OCR\",\n                    \"size\": round(pdf_height) if pdf_height &gt; 0 else 10.0,\n                    \"page_number\": self.page.number,\n                    \"bold\": False,\n                    \"italic\": False,\n                    \"upright\": True,\n                    \"doctop\": page_top + self.page._page.initial_doctop,\n                }\n                ocr_char_dict = element_data.copy()\n                ocr_char_dict[\"object_type\"] = \"char\"\n                ocr_char_dict.setdefault(\"adv\", ocr_char_dict.get(\"width\", 0))\n                element_data[\"_char_dicts\"] = [ocr_char_dict]\n                from natural_pdf.elements.text import TextElement\n\n                elem = TextElement(element_data, self.page)\n                created_elements.append(elem)\n                self.page._element_mgr.add_element(elem, element_type=\"words\")\n                self.page._element_mgr.add_element(ocr_char_dict, element_type=\"chars\")\n            except Exception as e:\n                logger.error(\n                    f\"Failed to convert region OCR result to element: {result}. Error: {e}\",\n                    exc_info=True,\n                )\n        logger.info(f\"Region {self.bbox}: Added {len(created_elements)} elements from OCR.\")\n        return self\n\n    def apply_custom_ocr(\n        self,\n        ocr_function: Callable[[\"Region\"], Optional[str]],\n        source_label: str = \"custom-ocr\",\n        replace: bool = True,\n        confidence: Optional[float] = None,\n        add_to_page: bool = True,\n    ) -&gt; \"Region\":\n        \"\"\"\n        Apply a custom OCR function to this region and create text elements from the results.\n\n        This is useful when you want to use a custom OCR method (e.g., an LLM API,\n        specialized OCR service, or any custom logic) instead of the built-in OCR engines.\n\n        Args:\n            ocr_function: A callable that takes a Region and returns the OCR'd text (or None).\n                          The function receives this region as its argument and should return\n                          the extracted text as a string, or None if no text was found.\n            source_label: Label to identify the source of these text elements (default: \"custom-ocr\").\n                          This will be set as the 'source' attribute on created elements.\n            replace: If True (default), removes existing OCR elements in this region before\n                     adding new ones. If False, adds new OCR elements alongside existing ones.\n            confidence: Optional confidence score for the OCR result (0.0-1.0).\n                        If None, defaults to 1.0 if text is returned, 0.0 if None is returned.\n            add_to_page: If True (default), adds the created text element to the page.\n                         If False, creates the element but doesn't add it to the page.\n\n        Returns:\n            Self for method chaining.\n\n        Example:\n            # Using with an LLM\n            def ocr_with_llm(region):\n                image = region.render(resolution=300, crop=True)\n                # Call your LLM API here\n                return llm_client.ocr(image)\n\n            region.apply_custom_ocr(ocr_with_llm)\n\n            # Using with a custom OCR service\n            def ocr_with_service(region):\n                img_bytes = region.render(crop=True).tobytes()\n                response = ocr_service.process(img_bytes)\n                return response.text\n\n            region.apply_custom_ocr(ocr_with_service, source_label=\"my-ocr-service\")\n        \"\"\"\n        # If replace is True, remove existing OCR elements in this region\n        if replace:\n            logger.info(\n                f\"Region {self.bbox}: Removing existing OCR elements before applying custom OCR.\"\n            )\n\n            removed_count = 0\n\n            # Helper to remove a single element safely\n            def _safe_remove(elem):\n                nonlocal removed_count\n                success = False\n                if hasattr(elem, \"page\") and hasattr(elem.page, \"_element_mgr\"):\n                    etype = getattr(elem, \"object_type\", \"word\")\n                    if etype == \"word\":\n                        etype_key = \"words\"\n                    elif etype == \"char\":\n                        etype_key = \"chars\"\n                    else:\n                        etype_key = etype + \"s\" if not etype.endswith(\"s\") else etype\n                    try:\n                        success = elem.page._element_mgr.remove_element(elem, etype_key)\n                    except Exception:\n                        success = False\n                if success:\n                    removed_count += 1\n\n            # Remove ALL OCR elements overlapping this region\n            # Remove elements with source==\"ocr\" (built-in OCR) or matching the source_label (previous custom OCR)\n            for word in list(self.page._element_mgr.words):\n                word_source = getattr(word, \"source\", \"\")\n                # Match built-in OCR behavior: remove elements with source \"ocr\" exactly\n                # Also remove elements with the same source_label to avoid duplicates\n                if (word_source == \"ocr\" or word_source == source_label) and self.intersects(word):\n                    _safe_remove(word)\n\n            # Also remove char dicts if needed (matching built-in OCR)\n            for char in list(self.page._element_mgr.chars):\n                # char can be dict or TextElement; normalize\n                char_src = (\n                    char.get(\"source\") if isinstance(char, dict) else getattr(char, \"source\", None)\n                )\n                if char_src == \"ocr\" or char_src == source_label:\n                    # Rough bbox for dicts\n                    if isinstance(char, dict):\n                        cx0, ctop, cx1, cbottom = (\n                            char.get(\"x0\", 0),\n                            char.get(\"top\", 0),\n                            char.get(\"x1\", 0),\n                            char.get(\"bottom\", 0),\n                        )\n                    else:\n                        cx0, ctop, cx1, cbottom = char.x0, char.top, char.x1, char.bottom\n                    # Quick overlap check\n                    if not (\n                        cx1 &lt; self.x0 or cx0 &gt; self.x1 or cbottom &lt; self.top or ctop &gt; self.bottom\n                    ):\n                        _safe_remove(char)\n\n            if removed_count &gt; 0:\n                logger.info(f\"Region {self.bbox}: Removed {removed_count} existing OCR elements.\")\n\n        # Call the custom OCR function\n        try:\n            logger.debug(f\"Region {self.bbox}: Calling custom OCR function...\")\n            ocr_text = ocr_function(self)\n\n            if ocr_text is not None and not isinstance(ocr_text, str):\n                logger.warning(\n                    f\"Custom OCR function returned non-string type ({type(ocr_text)}). \"\n                    f\"Converting to string.\"\n                )\n                ocr_text = str(ocr_text)\n\n        except Exception as e:\n            logger.error(\n                f\"Error calling custom OCR function for region {self.bbox}: {e}\", exc_info=True\n            )\n            return self\n\n        # Create text element if we got text\n        if ocr_text is not None:\n            # Use the to_text_element method to create the element\n            text_element = self.to_text_element(\n                text_content=ocr_text,\n                source_label=source_label,\n                confidence=confidence,\n                add_to_page=add_to_page,\n            )\n\n            logger.info(\n                f\"Region {self.bbox}: Created text element with {len(ocr_text)} chars\"\n                f\"{' and added to page' if add_to_page else ''}\"\n            )\n        else:\n            logger.debug(f\"Region {self.bbox}: Custom OCR function returned None (no text found)\")\n\n        return self\n\n    def get_section_between(self, start_element=None, end_element=None, include_boundaries=\"both\"):\n        \"\"\"\n        Get a section between two elements within this region.\n\n        Args:\n            start_element: Element marking the start of the section\n            end_element: Element marking the end of the section\n            include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none'\n\n        Returns:\n            Region representing the section\n        \"\"\"\n        # Get elements only within this region first\n        elements = self.get_elements()\n\n        # If no elements, return self or empty region?\n        if not elements:\n            logger.warning(\n                f\"get_section_between called on region {self.bbox} with no contained elements.\"\n            )\n            # Return an empty region at the start of the parent region\n            return Region(self.page, (self.x0, self.top, self.x0, self.top))\n\n        # Sort elements in reading order\n        elements.sort(key=lambda e: (e.top, e.x0))\n\n        # Find start index\n        start_idx = 0\n        if start_element:\n            try:\n                start_idx = elements.index(start_element)\n            except ValueError:\n                # Start element not in region, use first element\n                logger.debug(\"Start element not found in region, using first element.\")\n                start_element = elements[0]  # Use the actual first element\n                start_idx = 0\n        else:\n            start_element = elements[0]  # Default start is first element\n\n        # Find end index\n        end_idx = len(elements) - 1\n        if end_element:\n            try:\n                end_idx = elements.index(end_element)\n            except ValueError:\n                # End element not in region, use last element\n                logger.debug(\"End element not found in region, using last element.\")\n                end_element = elements[-1]  # Use the actual last element\n                end_idx = len(elements) - 1\n        else:\n            end_element = elements[-1]  # Default end is last element\n\n        # Adjust indexes based on boundary inclusion\n        start_element_for_bbox = start_element\n        end_element_for_bbox = end_element\n\n        if include_boundaries == \"none\":\n            start_idx += 1\n            end_idx -= 1\n            start_element_for_bbox = elements[start_idx] if start_idx &lt;= end_idx else None\n            end_element_for_bbox = elements[end_idx] if start_idx &lt;= end_idx else None\n        elif include_boundaries == \"start\":\n            end_idx -= 1\n            end_element_for_bbox = elements[end_idx] if start_idx &lt;= end_idx else None\n        elif include_boundaries == \"end\":\n            start_idx += 1\n            start_element_for_bbox = elements[start_idx] if start_idx &lt;= end_idx else None\n\n        # Ensure valid indexes\n        start_idx = max(0, start_idx)\n        end_idx = min(len(elements) - 1, end_idx)\n\n        # If no valid elements in range, return empty region\n        if start_idx &gt; end_idx or start_element_for_bbox is None or end_element_for_bbox is None:\n            logger.debug(\"No valid elements in range for get_section_between.\")\n            # Return an empty region positioned at the start element boundary\n            anchor = start_element if start_element else self\n            return Region(self.page, (anchor.x0, anchor.top, anchor.x0, anchor.top))\n\n        # Get elements in range based on adjusted indices\n        section_elements = elements[start_idx : end_idx + 1]\n\n        # Create bounding box around the ELEMENTS included based on indices\n        x0 = min(e.x0 for e in section_elements)\n        top = min(e.top for e in section_elements)\n        x1 = max(e.x1 for e in section_elements)\n        bottom = max(e.bottom for e in section_elements)\n\n        # Create new region\n        section = Region(self.page, (x0, top, x1, bottom))\n        # Store the original boundary elements for reference\n        section.start_element = start_element\n        section.end_element = end_element\n\n        return section\n\n    def get_sections(\n        self, start_elements=None, end_elements=None, include_boundaries=\"both\"\n    ) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Get sections within this region based on start/end elements.\n\n        Args:\n            start_elements: Elements or selector string that mark the start of sections\n            end_elements: Elements or selector string that mark the end of sections\n            include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none'\n\n        Returns:\n            List of Region objects representing the extracted sections\n        \"\"\"\n        from natural_pdf.elements.element_collection import ElementCollection\n\n        # Process string selectors to find elements WITHIN THIS REGION\n        if isinstance(start_elements, str):\n            start_elements = self.find_all(start_elements)  # Use region's find_all\n            if hasattr(start_elements, \"elements\"):\n                start_elements = start_elements.elements\n\n        if isinstance(end_elements, str):\n            end_elements = self.find_all(end_elements)  # Use region's find_all\n            if hasattr(end_elements, \"elements\"):\n                end_elements = end_elements.elements\n\n        # Ensure start_elements is a list (or similar iterable)\n        if start_elements is None or not hasattr(start_elements, \"__iter__\"):\n            logger.warning(\n                \"get_sections requires valid start_elements (selector or list). Returning empty.\"\n            )\n            return []\n        # Ensure end_elements is a list if provided\n        if end_elements is not None and not hasattr(end_elements, \"__iter__\"):\n            logger.warning(\"end_elements must be iterable if provided. Ignoring.\")\n            end_elements = []\n        elif end_elements is None:\n            end_elements = []\n\n        # If no start elements found within the region, return empty list\n        if not start_elements:\n            return []\n\n        # Sort all elements within the region in reading order\n        all_elements_in_region = self.get_elements()\n        all_elements_in_region.sort(key=lambda e: (e.top, e.x0))\n\n        if not all_elements_in_region:\n            return []  # Cannot create sections if region is empty\n\n        # Map elements to their indices in the sorted list\n        element_to_index = {el: i for i, el in enumerate(all_elements_in_region)}\n\n        # Mark section boundaries using indices from the sorted list\n        section_boundaries = []\n\n        # Add start element indexes\n        for element in start_elements:\n            idx = element_to_index.get(element)\n            if idx is not None:\n                section_boundaries.append({\"index\": idx, \"element\": element, \"type\": \"start\"})\n            # else: Element found by selector might not be geometrically in region? Log warning?\n\n        # Add end element indexes if provided\n        for element in end_elements:\n            idx = element_to_index.get(element)\n            if idx is not None:\n                section_boundaries.append({\"index\": idx, \"element\": element, \"type\": \"end\"})\n\n        # Sort boundaries by index (document order within the region)\n        section_boundaries.sort(key=lambda x: x[\"index\"])\n\n        # Generate sections\n        sections = []\n        current_start_boundary = None\n\n        for i, boundary in enumerate(section_boundaries):\n            # If it's a start boundary and we don't have a current start\n            if boundary[\"type\"] == \"start\" and current_start_boundary is None:\n                current_start_boundary = boundary\n\n            # If it's an end boundary and we have a current start\n            elif boundary[\"type\"] == \"end\" and current_start_boundary is not None:\n                # Create a section from current_start to this boundary\n                start_element = current_start_boundary[\"element\"]\n                end_element = boundary[\"element\"]\n                # Use the helper, ensuring elements are from within the region\n                section = self.get_section_between(start_element, end_element, include_boundaries)\n                sections.append(section)\n                current_start_boundary = None  # Reset\n\n            # If it's another start boundary and we have a current start (split by starts only)\n            elif (\n                boundary[\"type\"] == \"start\"\n                and current_start_boundary is not None\n                and not end_elements\n            ):\n                # End the previous section just before this start boundary\n                start_element = current_start_boundary[\"element\"]\n                # Find the element immediately preceding this start in the sorted list\n                end_idx = boundary[\"index\"] - 1\n                if end_idx &gt;= 0 and end_idx &gt;= current_start_boundary[\"index\"]:\n                    end_element = all_elements_in_region[end_idx]\n                    section = self.get_section_between(\n                        start_element, end_element, include_boundaries\n                    )\n                    sections.append(section)\n                # Else: Section started and ended by consecutive start elements? Create empty?\n                # For now, just reset and start new section\n\n                # Start the new section\n                current_start_boundary = boundary\n\n        # Handle the last section if we have a current start\n        if current_start_boundary is not None:\n            start_element = current_start_boundary[\"element\"]\n            # End at the last element within the region\n            end_element = all_elements_in_region[-1]\n            section = self.get_section_between(start_element, end_element, include_boundaries)\n            sections.append(section)\n\n        return ElementCollection(sections)\n\n    def create_cells(self):\n        \"\"\"\n        Create cell regions for a detected table by intersecting its\n        row and column regions, and add them to the page.\n\n        Assumes child row and column regions are already present on the page.\n\n        Returns:\n            Self for method chaining.\n        \"\"\"\n        # Ensure this is called on a table region\n        if self.region_type not in (\n            \"table\",\n            \"tableofcontents\",\n        ):  # Allow for ToC which might have structure\n            raise ValueError(\n                f\"create_cells should be called on a 'table' or 'tableofcontents' region, not '{self.region_type}'\"\n            )\n\n        # Find rows and columns associated with this page\n        # Remove the model-specific filter\n        rows = self.page.find_all(\"region[type=table-row]\")\n        columns = self.page.find_all(\"region[type=table-column]\")\n\n        # Filter to only include those that overlap with this table region\n        def is_in_table(element):\n            # Use a simple overlap check (more robust than just center point)\n            # Check if element's bbox overlaps with self.bbox\n            return (\n                hasattr(element, \"bbox\")\n                and element.x0 &lt; self.x1  # Ensure element has bbox\n                and element.x1 &gt; self.x0\n                and element.top &lt; self.bottom\n                and element.bottom &gt; self.top\n            )\n\n        table_rows = [r for r in rows if is_in_table(r)]\n        table_columns = [c for c in columns if is_in_table(c)]\n\n        if not table_rows or not table_columns:\n            # Use page's logger if available\n            logger_instance = getattr(self._page, \"logger\", logger)\n            logger_instance.warning(\n                f\"Region {self.bbox}: Cannot create cells. No overlapping row or column regions found.\"\n            )\n            return self  # Return self even if no cells created\n\n        # Sort rows and columns\n        table_rows.sort(key=lambda r: r.top)\n        table_columns.sort(key=lambda c: c.x0)\n\n        # Create cells and add them to the page's element manager\n        created_count = 0\n        for row in table_rows:\n            for column in table_columns:\n                # Calculate intersection bbox for the cell\n                cell_x0 = max(row.x0, column.x0)\n                cell_y0 = max(row.top, column.top)\n                cell_x1 = min(row.x1, column.x1)\n                cell_y1 = min(row.bottom, column.bottom)\n\n                # Only create a cell if the intersection is valid (positive width/height)\n                if cell_x1 &gt; cell_x0 and cell_y1 &gt; cell_y0:\n                    # Create cell region at the intersection\n                    cell = self.page.create_region(cell_x0, cell_y0, cell_x1, cell_y1)\n                    # Set metadata\n                    cell.source = \"derived\"\n                    cell.region_type = \"table-cell\"  # Explicitly set type\n                    cell.normalized_type = \"table-cell\"  # And normalized type\n                    # Inherit model from the parent table region\n                    cell.model = self.model\n                    cell.parent_region = self  # Link cell to parent table region\n\n                    # Add the cell region to the page's element manager\n                    self.page._element_mgr.add_region(cell)\n                    created_count += 1\n\n        # Optional: Add created cells to the table region's children\n        # self.child_regions.extend(cells_created_in_this_call) # Needs list management\n\n        logger_instance = getattr(self._page, \"logger\", logger)\n        logger_instance.info(\n            f\"Region {self.bbox} (Model: {self.model}): Created and added {created_count} cell regions.\"\n        )\n\n        return self  # Return self for chaining\n\n    def ask(\n        self,\n        question: Union[str, List[str], Tuple[str, ...]],\n        min_confidence: float = 0.1,\n        model: str = None,\n        debug: bool = False,\n        **kwargs,\n    ) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n        \"\"\"\n        Ask a question about the region content using document QA.\n\n        This method uses a document question answering model to extract answers from the region content.\n        It leverages both textual content and layout information for better understanding.\n\n        Args:\n            question: The question to ask about the region content\n            min_confidence: Minimum confidence threshold for answers (0.0-1.0)\n            model: Optional model name to use for QA (if None, uses default model)\n            **kwargs: Additional parameters to pass to the QA engine\n\n        Returns:\n            Dictionary with answer details: {\n                \"answer\": extracted text,\n                \"confidence\": confidence score,\n                \"found\": whether an answer was found,\n                \"page_num\": page number,\n                \"region\": reference to this region,\n                \"source_elements\": list of elements that contain the answer (if found)\n            }\n        \"\"\"\n        try:\n            from natural_pdf.qa.document_qa import get_qa_engine\n        except ImportError:\n            logger.error(\n                \"Question answering requires optional dependencies. Install with `pip install natural-pdf[ai]`\"\n            )\n            return {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": self.page.number,\n                \"source_elements\": [],\n                \"region\": self,\n            }\n\n        # Get or initialize QA engine with specified model\n        try:\n            qa_engine = get_qa_engine(model_name=model) if model else get_qa_engine()\n        except Exception as e:\n            logger.error(f\"Failed to initialize QA engine (model: {model}): {e}\", exc_info=True)\n            return {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": self.page.number,\n                \"source_elements\": [],\n                \"region\": self,\n            }\n\n        # Ask the question using the QA engine\n        try:\n            return qa_engine.ask_pdf_region(\n                self, question, min_confidence=min_confidence, debug=debug, **kwargs\n            )\n        except Exception as e:\n            logger.error(f\"Error during qa_engine.ask_pdf_region: {e}\", exc_info=True)\n            return {\n                \"answer\": None,\n                \"confidence\": 0.0,\n                \"found\": False,\n                \"page_num\": self.page.number,\n                \"source_elements\": [],\n                \"region\": self,\n            }\n\n    def add_child(self, child):\n        \"\"\"\n        Add a child region to this region.\n\n        Used for hierarchical document structure when using models like Docling\n        that understand document hierarchy.\n\n        Args:\n            child: Region object to add as a child\n\n        Returns:\n            Self for method chaining\n        \"\"\"\n        self.child_regions.append(child)\n        child.parent_region = self\n        return self\n\n    def get_children(self, selector=None):\n        \"\"\"\n        Get immediate child regions, optionally filtered by selector.\n\n        Args:\n            selector: Optional selector to filter children\n\n        Returns:\n            List of child regions matching the selector\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(\"natural_pdf.elements.region\")\n\n        if selector is None:\n            return self.child_regions\n\n        # Use existing selector parser to filter\n        try:\n            selector_obj = parse_selector(selector)\n            filter_func = selector_to_filter_func(selector_obj)  # Removed region=self\n            matched = [child for child in self.child_regions if filter_func(child)]\n            logger.debug(\n                f\"get_children: found {len(matched)} of {len(self.child_regions)} children matching '{selector}'\"\n            )\n            return matched\n        except Exception as e:\n            logger.error(f\"Error applying selector in get_children: {e}\", exc_info=True)\n            return []  # Return empty list on error\n\n    def get_descendants(self, selector=None):\n        \"\"\"\n        Get all descendant regions (children, grandchildren, etc.), optionally filtered by selector.\n\n        Args:\n            selector: Optional selector to filter descendants\n\n        Returns:\n            List of descendant regions matching the selector\n        \"\"\"\n        import logging\n\n        logger = logging.getLogger(\"natural_pdf.elements.region\")\n\n        all_descendants = []\n        queue = list(self.child_regions)  # Start with direct children\n\n        while queue:\n            current = queue.pop(0)\n            all_descendants.append(current)\n            # Add current's children to the queue for processing\n            if hasattr(current, \"child_regions\"):\n                queue.extend(current.child_regions)\n\n        logger.debug(f\"get_descendants: found {len(all_descendants)} total descendants\")\n\n        # Filter by selector if provided\n        if selector is not None:\n            try:\n                selector_obj = parse_selector(selector)\n                filter_func = selector_to_filter_func(selector_obj)  # Removed region=self\n                matched = [desc for desc in all_descendants if filter_func(desc)]\n                logger.debug(f\"get_descendants: filtered to {len(matched)} matching '{selector}'\")\n                return matched\n            except Exception as e:\n                logger.error(f\"Error applying selector in get_descendants: {e}\", exc_info=True)\n                return []  # Return empty list on error\n\n        return all_descendants\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the region.\"\"\"\n        poly_info = \" (Polygon)\" if self.has_polygon else \"\"\n        name_info = f\" name='{self.name}'\" if self.name else \"\"\n        type_info = f\" type='{self.region_type}'\" if self.region_type else \"\"\n        source_info = f\" source='{self.source}'\" if self.source else \"\"\n        return f\"&lt;Region{name_info}{type_info}{source_info} bbox={self.bbox}{poly_info}&gt;\"\n\n    def update_text(\n        self,\n        transform: Callable[[Any], Optional[str]],\n        *,\n        selector: str = \"text\",\n        apply_exclusions: bool = False,\n    ) -&gt; \"Region\":\n        \"\"\"Apply *transform* to every text element matched by *selector* inside this region.\n\n        The heavy lifting is delegated to :py:meth:`TextMixin.update_text`; this\n        override simply ensures the search is scoped to the region.\n        \"\"\"\n\n        return TextMixin.update_text(\n            self, transform, selector=selector, apply_exclusions=apply_exclusions\n        )\n\n    # --- Classification Mixin Implementation --- #\n    def _get_classification_manager(self) -&gt; \"ClassificationManager\":\n        if (\n            not hasattr(self, \"page\")\n            or not hasattr(self.page, \"pdf\")\n            or not hasattr(self.page.pdf, \"get_manager\")\n        ):\n            raise AttributeError(\n                \"ClassificationManager cannot be accessed: Parent Page, PDF, or get_manager method missing.\"\n            )\n        try:\n            # Use the PDF's manager registry accessor via page\n            return self.page.pdf.get_manager(\"classification\")\n        except (ValueError, RuntimeError, AttributeError) as e:\n            # Wrap potential errors from get_manager for clarity\n            raise AttributeError(\n                f\"Failed to get ClassificationManager from PDF via Page: {e}\"\n            ) from e\n\n    def _get_classification_content(\n        self, model_type: str, **kwargs\n    ) -&gt; Union[str, \"Image\"]:  # Use \"Image\" for lazy import\n        if model_type == \"text\":\n            text_content = self.extract_text(layout=False)  # Simple join for classification\n            if not text_content or text_content.isspace():\n                raise ValueError(\"Cannot classify region with 'text' model: No text content found.\")\n            return text_content\n        elif model_type == \"vision\":\n            # Get resolution from manager/kwargs if possible, else default\n            # We access manager via the method to ensure it's available\n            manager = self._get_classification_manager()\n            default_resolution = 150  # Manager doesn't store default res, set here\n            # Note: classify() passes resolution via **kwargs if user specifies\n            resolution = (\n                kwargs.get(\"resolution\", default_resolution)\n                if \"kwargs\" in locals()\n                else default_resolution\n            )\n\n            img = self.render(\n                resolution=resolution,\n                crop=True,  # Just the region content\n            )\n            if img is None:\n                raise ValueError(\n                    \"Cannot classify region with 'vision' model: Failed to render image.\"\n                )\n            return img\n        else:\n            raise ValueError(f\"Unsupported model_type for classification: {model_type}\")\n\n    def _get_metadata_storage(self) -&gt; Dict[str, Any]:\n        # Ensure metadata exists\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            self.metadata = {}\n        return self.metadata\n\n    # --- End Classification Mixin Implementation --- #\n\n    # --- NEW METHOD: analyze_text_table_structure ---\n    def analyze_text_table_structure(\n        self,\n        snap_tolerance: int = 10,\n        join_tolerance: int = 3,\n        min_words_vertical: int = 3,\n        min_words_horizontal: int = 1,\n        intersection_tolerance: int = 3,\n        expand_bbox: Optional[Dict[str, int]] = None,\n        **kwargs,\n    ) -&gt; Optional[Dict]:\n        \"\"\"\n        Analyzes the text elements within the region (or slightly expanded area)\n        to find potential table structure (lines, cells) using text alignment logic\n        adapted from pdfplumber.\n\n        Args:\n            snap_tolerance: Tolerance for snapping parallel lines.\n            join_tolerance: Tolerance for joining collinear lines.\n            min_words_vertical: Minimum words needed to define a vertical line.\n            min_words_horizontal: Minimum words needed to define a horizontal line.\n            intersection_tolerance: Tolerance for detecting line intersections.\n            expand_bbox: Optional dictionary to expand the search area slightly beyond\n                         the region's exact bounds (e.g., {'left': 5, 'right': 5}).\n            **kwargs: Additional keyword arguments passed to\n                      find_text_based_tables (e.g., specific x/y tolerances).\n\n        Returns:\n            A dictionary containing 'horizontal_edges', 'vertical_edges', 'cells' (list of dicts),\n            and 'intersections', or None if pdfplumber is unavailable or an error occurs.\n        \"\"\"\n\n        # Determine the search region (expand if requested)\n        search_region = self\n        if expand_bbox and isinstance(expand_bbox, dict):\n            try:\n                search_region = self.expand(**expand_bbox)\n                logger.debug(\n                    f\"Expanded search region for text table analysis to: {search_region.bbox}\"\n                )\n            except Exception as e:\n                logger.warning(f\"Could not expand region bbox: {e}. Using original region.\")\n                search_region = self\n\n        # Find text elements within the search region\n        text_elements = search_region.find_all(\n            \"text\", apply_exclusions=False\n        )  # Use unfiltered text\n        if not text_elements:\n            logger.info(f\"Region {self.bbox}: No text elements found for text table analysis.\")\n            return {\"horizontal_edges\": [], \"vertical_edges\": [], \"cells\": [], \"intersections\": {}}\n\n        # Extract bounding boxes\n        bboxes = [element.bbox for element in text_elements if hasattr(element, \"bbox\")]\n        if not bboxes:\n            logger.info(f\"Region {self.bbox}: No bboxes extracted from text elements.\")\n            return {\"horizontal_edges\": [], \"vertical_edges\": [], \"cells\": [], \"intersections\": {}}\n\n        # Call the utility function\n        try:\n            analysis_results = find_text_based_tables(\n                bboxes=bboxes,\n                snap_tolerance=snap_tolerance,\n                join_tolerance=join_tolerance,\n                min_words_vertical=min_words_vertical,\n                min_words_horizontal=min_words_horizontal,\n                intersection_tolerance=intersection_tolerance,\n                **kwargs,  # Pass through any extra specific tolerance args\n            )\n            # Store results in the region's analyses cache\n            self.analyses[\"text_table_structure\"] = analysis_results\n            return analysis_results\n        except ImportError:\n            logger.error(\"pdfplumber library is required for 'text' table analysis but not found.\")\n            return None\n        except Exception as e:\n            logger.error(f\"Error during text-based table analysis: {e}\", exc_info=True)\n            return None\n\n    # --- END NEW METHOD ---\n\n    # --- NEW METHOD: get_text_table_cells ---\n    def get_text_table_cells(\n        self,\n        snap_tolerance: int = 10,\n        join_tolerance: int = 3,\n        min_words_vertical: int = 3,\n        min_words_horizontal: int = 1,\n        intersection_tolerance: int = 3,\n        expand_bbox: Optional[Dict[str, int]] = None,\n        **kwargs,\n    ) -&gt; \"ElementCollection[Region]\":\n        \"\"\"\n        Analyzes text alignment to find table cells and returns them as\n        temporary Region objects without adding them to the page.\n\n        Args:\n            snap_tolerance: Tolerance for snapping parallel lines.\n            join_tolerance: Tolerance for joining collinear lines.\n            min_words_vertical: Minimum words needed to define a vertical line.\n            min_words_horizontal: Minimum words needed to define a horizontal line.\n            intersection_tolerance: Tolerance for detecting line intersections.\n            expand_bbox: Optional dictionary to expand the search area slightly beyond\n                         the region's exact bounds (e.g., {'left': 5, 'right': 5}).\n            **kwargs: Additional keyword arguments passed to\n                      find_text_based_tables (e.g., specific x/y tolerances).\n\n        Returns:\n            An ElementCollection containing temporary Region objects for each detected cell,\n            or an empty ElementCollection if no cells are found or an error occurs.\n        \"\"\"\n        from natural_pdf.elements.element_collection import ElementCollection\n\n        # 1. Perform the analysis (or use cached results)\n        if \"text_table_structure\" in self.analyses:\n            analysis_results = self.analyses[\"text_table_structure\"]\n            logger.debug(\"get_text_table_cells: Using cached analysis results.\")\n        else:\n            analysis_results = self.analyze_text_table_structure(\n                snap_tolerance=snap_tolerance,\n                join_tolerance=join_tolerance,\n                min_words_vertical=min_words_vertical,\n                min_words_horizontal=min_words_horizontal,\n                intersection_tolerance=intersection_tolerance,\n                expand_bbox=expand_bbox,\n                **kwargs,\n            )\n\n        # 2. Check if analysis was successful and cells were found\n        if analysis_results is None or not analysis_results.get(\"cells\"):\n            logger.info(f\"Region {self.bbox}: No cells found by text table analysis.\")\n            return ElementCollection([])  # Return empty collection\n\n        # 3. Create temporary Region objects for each cell dictionary\n        cell_regions = []\n        for cell_data in analysis_results[\"cells\"]:\n            try:\n                # Use page.region to create the region object\n                # It expects left, top, right, bottom keys\n                cell_region = self.page.region(**cell_data)\n\n                # Set metadata on the temporary region\n                cell_region.region_type = \"table-cell\"\n                cell_region.normalized_type = \"table-cell\"\n                cell_region.model = \"pdfplumber-text\"\n                cell_region.source = \"volatile\"  # Indicate it's not managed/persistent\n                cell_region.parent_region = self  # Link back to the region it came from\n\n                cell_regions.append(cell_region)\n            except Exception as e:\n                logger.warning(f\"Could not create Region object for cell data {cell_data}: {e}\")\n\n        # 4. Return the list wrapped in an ElementCollection\n        logger.debug(f\"get_text_table_cells: Created {len(cell_regions)} temporary cell regions.\")\n        return ElementCollection(cell_regions)\n\n    # --- END NEW METHOD ---\n\n    def to_text_element(\n        self,\n        text_content: Optional[Union[str, Callable[[\"Region\"], Optional[str]]]] = None,\n        source_label: str = \"derived_from_region\",\n        object_type: str = \"word\",  # Or \"char\", controls how it's categorized\n        default_font_size: float = 10.0,\n        default_font_name: str = \"RegionContent\",\n        confidence: Optional[float] = None,  # Allow overriding confidence\n        add_to_page: bool = False,  # NEW: Option to add to page\n    ) -&gt; \"TextElement\":\n        \"\"\"\n        Creates a new TextElement object based on this region's geometry.\n\n        The text for the new TextElement can be provided directly,\n        generated by a callback function, or left as None.\n\n        Args:\n            text_content:\n                - If a string, this will be the text of the new TextElement.\n                - If a callable, it will be called with this region instance\n                  and its return value (a string or None) will be the text.\n                - If None (default), the TextElement's text will be None.\n            source_label: The 'source' attribute for the new TextElement.\n            object_type: The 'object_type' for the TextElement's data dict\n                         (e.g., \"word\", \"char\").\n            default_font_size: Placeholder font size if text is generated.\n            default_font_name: Placeholder font name if text is generated.\n            confidence: Confidence score for the text. If text_content is None,\n                        defaults to 0.0. If text is provided/generated, defaults to 1.0\n                        unless specified.\n            add_to_page: If True, the created TextElement will be added to the\n                         region's parent page. (Default: False)\n\n        Returns:\n            A new TextElement instance.\n\n        Raises:\n            ValueError: If the region does not have a valid 'page' attribute.\n        \"\"\"\n        actual_text: Optional[str] = None\n        if isinstance(text_content, str):\n            actual_text = text_content\n        elif callable(text_content):\n            try:\n                actual_text = text_content(self)\n            except Exception as e:\n                logger.error(\n                    f\"Error executing text_content callback for region {self.bbox}: {e}\",\n                    exc_info=True,\n                )\n                actual_text = None  # Ensure actual_text is None on error\n\n        final_confidence = confidence\n        if final_confidence is None:\n            final_confidence = 1.0 if actual_text is not None and actual_text.strip() else 0.0\n\n        if not hasattr(self, \"page\") or self.page is None:\n            raise ValueError(\"Region must have a valid 'page' attribute to create a TextElement.\")\n\n        # Create character dictionaries for the text\n        char_dicts = []\n        if actual_text:\n            # Create a single character dict that spans the entire region\n            # This is a simplified approach - OCR engines typically create one per character\n            char_dict = {\n                \"text\": actual_text,\n                \"x0\": self.x0,\n                \"top\": self.top,\n                \"x1\": self.x1,\n                \"bottom\": self.bottom,\n                \"width\": self.width,\n                \"height\": self.height,\n                \"object_type\": \"char\",\n                \"page_number\": self.page.page_number,\n                \"fontname\": default_font_name,\n                \"size\": default_font_size,\n                \"upright\": True,\n                \"direction\": 1,\n                \"adv\": self.width,\n                \"source\": source_label,\n                \"confidence\": final_confidence,\n                \"stroking_color\": (0, 0, 0),\n                \"non_stroking_color\": (0, 0, 0),\n            }\n            char_dicts.append(char_dict)\n\n        elem_data = {\n            \"text\": actual_text,\n            \"x0\": self.x0,\n            \"top\": self.top,\n            \"x1\": self.x1,\n            \"bottom\": self.bottom,\n            \"width\": self.width,\n            \"height\": self.height,\n            \"object_type\": object_type,\n            \"page_number\": self.page.page_number,\n            \"stroking_color\": getattr(self, \"stroking_color\", (0, 0, 0)),\n            \"non_stroking_color\": getattr(self, \"non_stroking_color\", (0, 0, 0)),\n            \"fontname\": default_font_name,\n            \"size\": default_font_size,\n            \"upright\": True,\n            \"direction\": 1,\n            \"adv\": self.width,\n            \"source\": source_label,\n            \"confidence\": final_confidence,\n            \"_char_dicts\": char_dicts,\n        }\n        text_element = TextElement(elem_data, self.page)\n\n        if add_to_page:\n            if hasattr(self.page, \"_element_mgr\") and self.page._element_mgr is not None:\n                add_as_type = (\n                    \"words\"\n                    if object_type == \"word\"\n                    else \"chars\" if object_type == \"char\" else object_type\n                )\n                # REMOVED try-except block around add_element\n                self.page._element_mgr.add_element(text_element, element_type=add_as_type)\n                logger.debug(\n                    f\"TextElement created from region {self.bbox} and added to page {self.page.page_number} as {add_as_type}.\"\n                )\n                # Also add character dictionaries to the chars collection\n                if char_dicts and object_type == \"word\":\n                    for char_dict in char_dicts:\n                        self.page._element_mgr.add_element(char_dict, element_type=\"chars\")\n            else:\n                page_num_str = (\n                    str(self.page.page_number) if hasattr(self.page, \"page_number\") else \"N/A\"\n                )\n                logger.warning(\n                    f\"Cannot add TextElement to page: Page {page_num_str} for region {self.bbox} is missing '_element_mgr'.\"\n                )\n\n        return text_element\n\n    # ------------------------------------------------------------------\n    # Unified analysis storage (maps to metadata[\"analysis\"])\n    # ------------------------------------------------------------------\n\n    @property\n    def analyses(self) -&gt; Dict[str, Any]:\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            self.metadata = {}\n        return self.metadata.setdefault(\"analysis\", {})\n\n    @analyses.setter\n    def analyses(self, value: Dict[str, Any]):\n        if not hasattr(self, \"metadata\") or self.metadata is None:\n            self.metadata = {}\n        self.metadata[\"analysis\"] = value\n\n    # ------------------------------------------------------------------\n    # New helper: build table from pre-computed table_cell regions\n    # ------------------------------------------------------------------\n\n    def _extract_table_from_cells(\n        self, cell_regions: List[\"Region\"], content_filter=None\n    ) -&gt; List[List[Optional[str]]]:\n        \"\"\"Construct a table (list-of-lists) from table_cell regions.\n\n        This assumes each cell Region has metadata.row_index / col_index as written by\n        detect_table_structure_from_lines().  If these keys are missing we will\n        fall back to sorting by geometry.\n\n        Args:\n            cell_regions: List of table cell Region objects to extract text from\n            content_filter: Optional content filter to apply to cell text extraction\n        \"\"\"\n        if not cell_regions:\n            return []\n\n        # Attempt to use explicit indices first\n        all_row_idxs = []\n        all_col_idxs = []\n        for cell in cell_regions:\n            try:\n                r_idx = int(cell.metadata.get(\"row_index\"))\n                c_idx = int(cell.metadata.get(\"col_index\"))\n                all_row_idxs.append(r_idx)\n                all_col_idxs.append(c_idx)\n            except Exception:\n                # Not all cells have indices \u2013 clear the lists so we switch to geometric sorting\n                all_row_idxs = []\n                all_col_idxs = []\n                break\n\n        if all_row_idxs and all_col_idxs:\n            num_rows = max(all_row_idxs) + 1\n            num_cols = max(all_col_idxs) + 1\n\n            # Initialise blank grid\n            table_grid: List[List[Optional[str]]] = [[None] * num_cols for _ in range(num_rows)]\n\n            for cell in cell_regions:\n                try:\n                    r_idx = int(cell.metadata.get(\"row_index\"))\n                    c_idx = int(cell.metadata.get(\"col_index\"))\n                    text_val = cell.extract_text(\n                        layout=False, apply_exclusions=True, content_filter=content_filter\n                    ).strip()\n                    table_grid[r_idx][c_idx] = text_val if text_val else None\n                except Exception as _err:\n                    # Skip problematic cell\n                    continue\n\n            return table_grid\n\n        # ------------------------------------------------------------------\n        # Fallback: derive order purely from geometry if indices are absent\n        # ------------------------------------------------------------------\n        # Sort unique centers to define ordering\n        try:\n            import numpy as np\n        except ImportError:\n            logger.warning(\"NumPy required for geometric cell ordering; returning empty result.\")\n            return []\n\n        # Build arrays of centers\n        centers = np.array([[(c.x0 + c.x1) / 2.0, (c.top + c.bottom) / 2.0] for c in cell_regions])\n        xs = centers[:, 0]\n        ys = centers[:, 1]\n\n        # Cluster unique row Y positions and column X positions with a tolerance\n        def _cluster(vals, tol=1.0):\n            sorted_vals = np.sort(vals)\n            groups = [[sorted_vals[0]]]\n            for v in sorted_vals[1:]:\n                if abs(v - groups[-1][-1]) &lt;= tol:\n                    groups[-1].append(v)\n                else:\n                    groups.append([v])\n            return [np.mean(g) for g in groups]\n\n        row_centers = _cluster(ys)\n        col_centers = _cluster(xs)\n\n        num_rows = len(row_centers)\n        num_cols = len(col_centers)\n\n        table_grid: List[List[Optional[str]]] = [[None] * num_cols for _ in range(num_rows)]\n\n        # Assign each cell to nearest row &amp; col center\n        for cell, (cx, cy) in zip(cell_regions, centers):\n            row_idx = int(np.argmin([abs(cy - rc) for rc in row_centers]))\n            col_idx = int(np.argmin([abs(cx - cc) for cc in col_centers]))\n\n            text_val = cell.extract_text(\n                layout=False, apply_exclusions=False, content_filter=content_filter\n            ).strip()\n            table_grid[row_idx][col_idx] = text_val if text_val else None\n\n        return table_grid\n\n    def _apply_rtl_processing_to_text(self, text: str) -&gt; str:\n        \"\"\"\n        Apply RTL (Right-to-Left) text processing to a string.\n\n        This converts visual order text (as stored in PDFs) to logical order\n        for proper display of Arabic, Hebrew, and other RTL scripts.\n\n        Args:\n            text: Input text string in visual order\n\n        Returns:\n            Text string in logical order\n        \"\"\"\n        if not text or not text.strip():\n            return text\n\n        # Quick check for RTL characters - if none found, return as-is\n        import unicodedata\n\n        def _contains_rtl(s):\n            return any(unicodedata.bidirectional(ch) in (\"R\", \"AL\", \"AN\") for ch in s)\n\n        if not _contains_rtl(text):\n            return text\n\n        try:\n            from bidi.algorithm import get_display  # type: ignore\n\n            from natural_pdf.utils.bidi_mirror import mirror_brackets\n\n            # Apply BiDi algorithm to convert from visual to logical order\n            # Process line by line to handle mixed content properly\n            processed_lines = []\n            for line in text.split(\"\\n\"):\n                if line.strip():\n                    # Determine base direction for this line\n                    base_dir = \"R\" if _contains_rtl(line) else \"L\"\n                    logical_line = get_display(line, base_dir=base_dir)\n                    # Apply bracket mirroring for correct logical order\n                    processed_lines.append(mirror_brackets(logical_line))\n                else:\n                    processed_lines.append(line)\n\n            return \"\\n\".join(processed_lines)\n\n        except (ImportError, Exception):\n            # If bidi library is not available or fails, return original text\n            return text\n\n    def _apply_content_filter_to_text(self, text: str, content_filter) -&gt; str:\n        \"\"\"\n        Apply content filter to a text string.\n\n        Args:\n            text: Input text string\n            content_filter: Content filter (regex, callable, or list of regexes)\n\n        Returns:\n            Filtered text string\n        \"\"\"\n        if not text or content_filter is None:\n            return text\n\n        import re\n\n        if isinstance(content_filter, str):\n            # Single regex pattern - remove matching parts\n            try:\n                return re.sub(content_filter, \"\", text)\n            except re.error:\n                return text  # Invalid regex, return original\n\n        elif isinstance(content_filter, list):\n            # List of regex patterns - remove parts matching ANY pattern\n            try:\n                result = text\n                for pattern in content_filter:\n                    result = re.sub(pattern, \"\", result)\n                return result\n            except re.error:\n                return text  # Invalid regex, return original\n\n        elif callable(content_filter):\n            # Callable filter - apply to individual characters\n            try:\n                filtered_chars = []\n                for char in text:\n                    if content_filter(char):\n                        filtered_chars.append(char)\n                return \"\".join(filtered_chars)\n            except Exception:\n                return text  # Function error, return original\n\n        return text\n\n    # ------------------------------------------------------------------\n    # Interactive Viewer Support\n    # ------------------------------------------------------------------\n\n    def viewer(\n        self,\n        *,\n        resolution: int = 150,\n        include_chars: bool = False,\n        include_attributes: Optional[List[str]] = None,\n    ) -&gt; Optional[\"InteractiveViewerWidget\"]:\n        \"\"\"Create an interactive ipywidget viewer for **this specific region**.\n\n        The method renders the region to an image (cropped to the region bounds) and\n        overlays all elements that intersect the region (optionally excluding noisy\n        character-level elements).  The resulting widget offers the same zoom / pan\n        experience as :py:meth:`Page.viewer` but scoped to the region.\n\n        Parameters\n        ----------\n        resolution : int, default 150\n            Rendering resolution (DPI).  This should match the value used by the\n            page-level viewer so element scaling is accurate.\n        include_chars : bool, default False\n            Whether to include individual *char* elements in the overlay.  These\n            are often too dense for a meaningful visualisation so are skipped by\n            default.\n        include_attributes : list[str], optional\n            Additional element attributes to expose in the info panel (on top of\n            the default set used by the page viewer).\n\n        Returns\n        -------\n        InteractiveViewerWidget | None\n            The widget instance, or ``None`` if *ipywidgets* is not installed or\n            an error occurred during creation.\n        \"\"\"\n\n        # ------------------------------------------------------------------\n        # Dependency / environment checks\n        # ------------------------------------------------------------------\n        if not _IPYWIDGETS_AVAILABLE or InteractiveViewerWidget is None:\n            logger.error(\n                \"Interactive viewer requires 'ipywidgets'. \"\n                'Please install with: pip install \"ipywidgets&gt;=7.0.0,&lt;10.0.0\"'\n            )\n            return None\n\n        try:\n            # ------------------------------------------------------------------\n            # Render region image (cropped) and encode as data URI\n            # ------------------------------------------------------------------\n            import base64\n            from io import BytesIO\n\n            # Use unified render() with crop=True to obtain just the region\n            img = self.render(resolution=resolution, crop=True)\n            if img is None:\n                logger.error(f\"Failed to render image for region {self.bbox} viewer.\")\n                return None\n\n            buf = BytesIO()\n            img.save(buf, format=\"PNG\")\n            img_str = base64.b64encode(buf.getvalue()).decode()\n            image_uri = f\"data:image/png;base64,{img_str}\"\n\n            # ------------------------------------------------------------------\n            # Prepare element overlay data (coordinates relative to region)\n            # ------------------------------------------------------------------\n            scale = resolution / 72.0  # Same convention as page viewer\n\n            # Gather elements intersecting the region\n            region_elements = self.get_elements(apply_exclusions=False)\n\n            # Optionally filter out chars\n            if not include_chars:\n                region_elements = [\n                    el for el in region_elements if str(getattr(el, \"type\", \"\")).lower() != \"char\"\n                ]\n\n            default_attrs = [\n                \"text\",\n                \"fontname\",\n                \"size\",\n                \"bold\",\n                \"italic\",\n                \"color\",\n                \"linewidth\",\n                \"is_horizontal\",\n                \"is_vertical\",\n                \"source\",\n                \"confidence\",\n                \"label\",\n                \"model\",\n                \"upright\",\n                \"direction\",\n            ]\n\n            if include_attributes:\n                default_attrs.extend([a for a in include_attributes if a not in default_attrs])\n\n            elements_json: List[dict] = []\n            for idx, el in enumerate(region_elements):\n                try:\n                    # Calculate coordinates relative to region bbox and apply scale\n                    x0 = (el.x0 - self.x0) * scale\n                    y0 = (el.top - self.top) * scale\n                    x1 = (el.x1 - self.x0) * scale\n                    y1 = (el.bottom - self.top) * scale\n\n                    elem_dict = {\n                        \"id\": idx,\n                        \"type\": getattr(el, \"type\", \"unknown\"),\n                        \"x0\": round(x0, 2),\n                        \"y0\": round(y0, 2),\n                        \"x1\": round(x1, 2),\n                        \"y1\": round(y1, 2),\n                        \"width\": round(x1 - x0, 2),\n                        \"height\": round(y1 - y0, 2),\n                    }\n\n                    # Add requested / default attributes\n                    for attr_name in default_attrs:\n                        if hasattr(el, attr_name):\n                            val = getattr(el, attr_name)\n                            # Ensure JSON serialisable\n                            if not isinstance(val, (str, int, float, bool, list, dict, type(None))):\n                                val = str(val)\n                            elem_dict[attr_name] = val\n                    elements_json.append(elem_dict)\n                except Exception as e:\n                    logger.warning(f\"Error preparing element {idx} for region viewer: {e}\")\n\n            viewer_data = {\"page_image\": image_uri, \"elements\": elements_json}\n\n            # ------------------------------------------------------------------\n            # Instantiate the widget directly using the prepared data\n            # ------------------------------------------------------------------\n            return InteractiveViewerWidget(pdf_data=viewer_data)\n\n        except Exception as e:\n            logger.error(f\"Error creating viewer for region {self.bbox}: {e}\", exc_info=True)\n            return None\n</code></pre>"},{"location":"api/#natural_pdf.Region-attributes","title":"Attributes","text":"<code>natural_pdf.Region.bbox</code> <code>property</code> <p>Get the bounding box as (x0, top, x1, bottom).</p> <code>natural_pdf.Region.bottom</code> <code>property</code> <p>Get the bottom coordinate.</p> <code>natural_pdf.Region.has_polygon</code> <code>property</code> <p>Check if this region has polygon coordinates.</p> <code>natural_pdf.Region.height</code> <code>property</code> <p>Get the height of the region.</p> <code>natural_pdf.Region.page</code> <code>property</code> <p>Get the parent page.</p> <code>natural_pdf.Region.polygon</code> <code>property</code> <p>Get polygon coordinates if available, otherwise return rectangle corners.</p> <code>natural_pdf.Region.top</code> <code>property</code> <p>Get the top coordinate.</p> <code>natural_pdf.Region.type</code> <code>property</code> <p>Element type.</p> <code>natural_pdf.Region.width</code> <code>property</code> <p>Get the width of the region.</p> <code>natural_pdf.Region.x0</code> <code>property</code> <p>Get the left coordinate.</p> <code>natural_pdf.Region.x1</code> <code>property</code> <p>Get the right coordinate.</p>"},{"location":"api/#natural_pdf.Region-functions","title":"Functions","text":"<code>natural_pdf.Region.__init__(page, bbox, polygon=None, parent=None, label=None)</code> <p>Initialize a region.</p> <p>Creates a Region object that represents a rectangular or polygonal area on a page. Regions are used for spatial navigation, content extraction, and analysis operations.</p> <p>Parameters:</p> Name Type Description Default <code>page</code> <code>Page</code> <p>Parent Page object that contains this region and provides access to document elements and analysis capabilities.</p> required <code>bbox</code> <code>Tuple[float, float, float, float]</code> <p>Bounding box coordinates as (x0, top, x1, bottom) tuple in PDF coordinate system (points, with origin at bottom-left).</p> required <code>polygon</code> <code>List[Tuple[float, float]]</code> <p>Optional list of coordinate points [(x1,y1), (x2,y2), ...] for non-rectangular regions. If provided, the region will use polygon-based intersection calculations instead of simple rectangle overlap.</p> <code>None</code> <code>parent</code> <p>Optional parent region for hierarchical document structure. Useful for maintaining tree-like relationships between regions.</p> <code>None</code> <code>label</code> <code>Optional[str]</code> <p>Optional descriptive label for the region, useful for debugging and identification in complex workflows.</p> <code>None</code> Example <pre><code>pdf = npdf.PDF(\"document.pdf\")\npage = pdf.pages[0]\n\n# Rectangular region\nheader = Region(page, (0, 0, page.width, 100), label=\"header\")\n\n# Polygonal region (from layout detection)\ntable_polygon = [(50, 100), (300, 100), (300, 400), (50, 400)]\ntable_region = Region(page, (50, 100, 300, 400),\n                    polygon=table_polygon, label=\"table\")\n</code></pre> Note <p>Regions are typically created through page methods like page.region() or spatial navigation methods like element.below(). Direct instantiation is used mainly for advanced workflows or layout analysis integration.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def __init__(\n    self,\n    page: \"Page\",\n    bbox: Tuple[float, float, float, float],\n    polygon: List[Tuple[float, float]] = None,\n    parent=None,\n    label: Optional[str] = None,\n):\n    \"\"\"Initialize a region.\n\n    Creates a Region object that represents a rectangular or polygonal area on a page.\n    Regions are used for spatial navigation, content extraction, and analysis operations.\n\n    Args:\n        page: Parent Page object that contains this region and provides access\n            to document elements and analysis capabilities.\n        bbox: Bounding box coordinates as (x0, top, x1, bottom) tuple in PDF\n            coordinate system (points, with origin at bottom-left).\n        polygon: Optional list of coordinate points [(x1,y1), (x2,y2), ...] for\n            non-rectangular regions. If provided, the region will use polygon-based\n            intersection calculations instead of simple rectangle overlap.\n        parent: Optional parent region for hierarchical document structure.\n            Useful for maintaining tree-like relationships between regions.\n        label: Optional descriptive label for the region, useful for debugging\n            and identification in complex workflows.\n\n    Example:\n        ```python\n        pdf = npdf.PDF(\"document.pdf\")\n        page = pdf.pages[0]\n\n        # Rectangular region\n        header = Region(page, (0, 0, page.width, 100), label=\"header\")\n\n        # Polygonal region (from layout detection)\n        table_polygon = [(50, 100), (300, 100), (300, 400), (50, 400)]\n        table_region = Region(page, (50, 100, 300, 400),\n                            polygon=table_polygon, label=\"table\")\n        ```\n\n    Note:\n        Regions are typically created through page methods like page.region() or\n        spatial navigation methods like element.below(). Direct instantiation is\n        used mainly for advanced workflows or layout analysis integration.\n    \"\"\"\n    self._page = page\n    self._bbox = bbox\n    self._polygon = polygon\n\n    self.metadata: Dict[str, Any] = {}\n    # Analysis results live under self.metadata['analysis'] via property\n\n    # Standard attributes for all elements\n    self.object_type = \"region\"  # For selector compatibility\n\n    # Layout detection attributes\n    self.region_type = None\n    self.normalized_type = None\n    self.confidence = None\n    self.model = None\n\n    # Region management attributes\n    self.name = None\n    self.label = label\n    self.source = None  # Will be set by creation methods\n\n    # Hierarchy support for nested document structure\n    self.parent_region = parent\n    self.child_regions = []\n    self.text_content = None  # Direct text content (e.g., from Docling)\n    self.associated_text_elements = []  # Native text elements that overlap with this region\n</code></pre> <code>natural_pdf.Region.__repr__()</code> <p>String representation of the region.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the region.\"\"\"\n    poly_info = \" (Polygon)\" if self.has_polygon else \"\"\n    name_info = f\" name='{self.name}'\" if self.name else \"\"\n    type_info = f\" type='{self.region_type}'\" if self.region_type else \"\"\n    source_info = f\" source='{self.source}'\" if self.source else \"\"\n    return f\"&lt;Region{name_info}{type_info}{source_info} bbox={self.bbox}{poly_info}&gt;\"\n</code></pre> <code>natural_pdf.Region.above(height=None, width='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Select region above this region.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>Optional[float]</code> <p>Height of the region above, in points</p> <code>None</code> <code>width</code> <code>str</code> <p>Width mode - \"full\" for full page width or \"element\" for element width</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this region in the result (default: False)</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify an upper boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region (default: True)</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Region</code> <p>Region object representing the area above</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def above(\n    self,\n    height: Optional[float] = None,\n    width: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"Region\":\n    \"\"\"\n    Select region above this region.\n\n    Args:\n        height: Height of the region above, in points\n        width: Width mode - \"full\" for full page width or \"element\" for element width\n        include_source: Whether to include this region in the result (default: False)\n        until: Optional selector string to specify an upper boundary element\n        include_endpoint: Whether to include the boundary element in the region (default: True)\n        **kwargs: Additional parameters\n\n    Returns:\n        Region object representing the area above\n    \"\"\"\n    return self._direction(\n        direction=\"above\",\n        size=height,\n        cross_size=width,\n        include_source=include_source,\n        until=until,\n        include_endpoint=include_endpoint,\n        **kwargs,\n    )\n</code></pre> <code>natural_pdf.Region.add_child(child)</code> <p>Add a child region to this region.</p> <p>Used for hierarchical document structure when using models like Docling that understand document hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>child</code> <p>Region object to add as a child</p> required <p>Returns:</p> Type Description <p>Self for method chaining</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def add_child(self, child):\n    \"\"\"\n    Add a child region to this region.\n\n    Used for hierarchical document structure when using models like Docling\n    that understand document hierarchy.\n\n    Args:\n        child: Region object to add as a child\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    self.child_regions.append(child)\n    child.parent_region = self\n    return self\n</code></pre> <code>natural_pdf.Region.analyze_text_table_structure(snap_tolerance=10, join_tolerance=3, min_words_vertical=3, min_words_horizontal=1, intersection_tolerance=3, expand_bbox=None, **kwargs)</code> <p>Analyzes the text elements within the region (or slightly expanded area) to find potential table structure (lines, cells) using text alignment logic adapted from pdfplumber.</p> <p>Parameters:</p> Name Type Description Default <code>snap_tolerance</code> <code>int</code> <p>Tolerance for snapping parallel lines.</p> <code>10</code> <code>join_tolerance</code> <code>int</code> <p>Tolerance for joining collinear lines.</p> <code>3</code> <code>min_words_vertical</code> <code>int</code> <p>Minimum words needed to define a vertical line.</p> <code>3</code> <code>min_words_horizontal</code> <code>int</code> <p>Minimum words needed to define a horizontal line.</p> <code>1</code> <code>intersection_tolerance</code> <code>int</code> <p>Tolerance for detecting line intersections.</p> <code>3</code> <code>expand_bbox</code> <code>Optional[Dict[str, int]]</code> <p>Optional dictionary to expand the search area slightly beyond          the region's exact bounds (e.g., {'left': 5, 'right': 5}).</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to       find_text_based_tables (e.g., specific x/y tolerances).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Dict]</code> <p>A dictionary containing 'horizontal_edges', 'vertical_edges', 'cells' (list of dicts),</p> <code>Optional[Dict]</code> <p>and 'intersections', or None if pdfplumber is unavailable or an error occurs.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def analyze_text_table_structure(\n    self,\n    snap_tolerance: int = 10,\n    join_tolerance: int = 3,\n    min_words_vertical: int = 3,\n    min_words_horizontal: int = 1,\n    intersection_tolerance: int = 3,\n    expand_bbox: Optional[Dict[str, int]] = None,\n    **kwargs,\n) -&gt; Optional[Dict]:\n    \"\"\"\n    Analyzes the text elements within the region (or slightly expanded area)\n    to find potential table structure (lines, cells) using text alignment logic\n    adapted from pdfplumber.\n\n    Args:\n        snap_tolerance: Tolerance for snapping parallel lines.\n        join_tolerance: Tolerance for joining collinear lines.\n        min_words_vertical: Minimum words needed to define a vertical line.\n        min_words_horizontal: Minimum words needed to define a horizontal line.\n        intersection_tolerance: Tolerance for detecting line intersections.\n        expand_bbox: Optional dictionary to expand the search area slightly beyond\n                     the region's exact bounds (e.g., {'left': 5, 'right': 5}).\n        **kwargs: Additional keyword arguments passed to\n                  find_text_based_tables (e.g., specific x/y tolerances).\n\n    Returns:\n        A dictionary containing 'horizontal_edges', 'vertical_edges', 'cells' (list of dicts),\n        and 'intersections', or None if pdfplumber is unavailable or an error occurs.\n    \"\"\"\n\n    # Determine the search region (expand if requested)\n    search_region = self\n    if expand_bbox and isinstance(expand_bbox, dict):\n        try:\n            search_region = self.expand(**expand_bbox)\n            logger.debug(\n                f\"Expanded search region for text table analysis to: {search_region.bbox}\"\n            )\n        except Exception as e:\n            logger.warning(f\"Could not expand region bbox: {e}. Using original region.\")\n            search_region = self\n\n    # Find text elements within the search region\n    text_elements = search_region.find_all(\n        \"text\", apply_exclusions=False\n    )  # Use unfiltered text\n    if not text_elements:\n        logger.info(f\"Region {self.bbox}: No text elements found for text table analysis.\")\n        return {\"horizontal_edges\": [], \"vertical_edges\": [], \"cells\": [], \"intersections\": {}}\n\n    # Extract bounding boxes\n    bboxes = [element.bbox for element in text_elements if hasattr(element, \"bbox\")]\n    if not bboxes:\n        logger.info(f\"Region {self.bbox}: No bboxes extracted from text elements.\")\n        return {\"horizontal_edges\": [], \"vertical_edges\": [], \"cells\": [], \"intersections\": {}}\n\n    # Call the utility function\n    try:\n        analysis_results = find_text_based_tables(\n            bboxes=bboxes,\n            snap_tolerance=snap_tolerance,\n            join_tolerance=join_tolerance,\n            min_words_vertical=min_words_vertical,\n            min_words_horizontal=min_words_horizontal,\n            intersection_tolerance=intersection_tolerance,\n            **kwargs,  # Pass through any extra specific tolerance args\n        )\n        # Store results in the region's analyses cache\n        self.analyses[\"text_table_structure\"] = analysis_results\n        return analysis_results\n    except ImportError:\n        logger.error(\"pdfplumber library is required for 'text' table analysis but not found.\")\n        return None\n    except Exception as e:\n        logger.error(f\"Error during text-based table analysis: {e}\", exc_info=True)\n        return None\n</code></pre> <code>natural_pdf.Region.apply_custom_ocr(ocr_function, source_label='custom-ocr', replace=True, confidence=None, add_to_page=True)</code> <p>Apply a custom OCR function to this region and create text elements from the results.</p> <p>This is useful when you want to use a custom OCR method (e.g., an LLM API, specialized OCR service, or any custom logic) instead of the built-in OCR engines.</p> <p>Parameters:</p> Name Type Description Default <code>ocr_function</code> <code>Callable[[Region], Optional[str]]</code> <p>A callable that takes a Region and returns the OCR'd text (or None).           The function receives this region as its argument and should return           the extracted text as a string, or None if no text was found.</p> required <code>source_label</code> <code>str</code> <p>Label to identify the source of these text elements (default: \"custom-ocr\").           This will be set as the 'source' attribute on created elements.</p> <code>'custom-ocr'</code> <code>replace</code> <code>bool</code> <p>If True (default), removes existing OCR elements in this region before      adding new ones. If False, adds new OCR elements alongside existing ones.</p> <code>True</code> <code>confidence</code> <code>Optional[float]</code> <p>Optional confidence score for the OCR result (0.0-1.0).         If None, defaults to 1.0 if text is returned, 0.0 if None is returned.</p> <code>None</code> <code>add_to_page</code> <code>bool</code> <p>If True (default), adds the created text element to the page.          If False, creates the element but doesn't add it to the page.</p> <code>True</code> <p>Returns:</p> Type Description <code>Region</code> <p>Self for method chaining.</p> Example Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def apply_custom_ocr(\n    self,\n    ocr_function: Callable[[\"Region\"], Optional[str]],\n    source_label: str = \"custom-ocr\",\n    replace: bool = True,\n    confidence: Optional[float] = None,\n    add_to_page: bool = True,\n) -&gt; \"Region\":\n    \"\"\"\n    Apply a custom OCR function to this region and create text elements from the results.\n\n    This is useful when you want to use a custom OCR method (e.g., an LLM API,\n    specialized OCR service, or any custom logic) instead of the built-in OCR engines.\n\n    Args:\n        ocr_function: A callable that takes a Region and returns the OCR'd text (or None).\n                      The function receives this region as its argument and should return\n                      the extracted text as a string, or None if no text was found.\n        source_label: Label to identify the source of these text elements (default: \"custom-ocr\").\n                      This will be set as the 'source' attribute on created elements.\n        replace: If True (default), removes existing OCR elements in this region before\n                 adding new ones. If False, adds new OCR elements alongside existing ones.\n        confidence: Optional confidence score for the OCR result (0.0-1.0).\n                    If None, defaults to 1.0 if text is returned, 0.0 if None is returned.\n        add_to_page: If True (default), adds the created text element to the page.\n                     If False, creates the element but doesn't add it to the page.\n\n    Returns:\n        Self for method chaining.\n\n    Example:\n        # Using with an LLM\n        def ocr_with_llm(region):\n            image = region.render(resolution=300, crop=True)\n            # Call your LLM API here\n            return llm_client.ocr(image)\n\n        region.apply_custom_ocr(ocr_with_llm)\n\n        # Using with a custom OCR service\n        def ocr_with_service(region):\n            img_bytes = region.render(crop=True).tobytes()\n            response = ocr_service.process(img_bytes)\n            return response.text\n\n        region.apply_custom_ocr(ocr_with_service, source_label=\"my-ocr-service\")\n    \"\"\"\n    # If replace is True, remove existing OCR elements in this region\n    if replace:\n        logger.info(\n            f\"Region {self.bbox}: Removing existing OCR elements before applying custom OCR.\"\n        )\n\n        removed_count = 0\n\n        # Helper to remove a single element safely\n        def _safe_remove(elem):\n            nonlocal removed_count\n            success = False\n            if hasattr(elem, \"page\") and hasattr(elem.page, \"_element_mgr\"):\n                etype = getattr(elem, \"object_type\", \"word\")\n                if etype == \"word\":\n                    etype_key = \"words\"\n                elif etype == \"char\":\n                    etype_key = \"chars\"\n                else:\n                    etype_key = etype + \"s\" if not etype.endswith(\"s\") else etype\n                try:\n                    success = elem.page._element_mgr.remove_element(elem, etype_key)\n                except Exception:\n                    success = False\n            if success:\n                removed_count += 1\n\n        # Remove ALL OCR elements overlapping this region\n        # Remove elements with source==\"ocr\" (built-in OCR) or matching the source_label (previous custom OCR)\n        for word in list(self.page._element_mgr.words):\n            word_source = getattr(word, \"source\", \"\")\n            # Match built-in OCR behavior: remove elements with source \"ocr\" exactly\n            # Also remove elements with the same source_label to avoid duplicates\n            if (word_source == \"ocr\" or word_source == source_label) and self.intersects(word):\n                _safe_remove(word)\n\n        # Also remove char dicts if needed (matching built-in OCR)\n        for char in list(self.page._element_mgr.chars):\n            # char can be dict or TextElement; normalize\n            char_src = (\n                char.get(\"source\") if isinstance(char, dict) else getattr(char, \"source\", None)\n            )\n            if char_src == \"ocr\" or char_src == source_label:\n                # Rough bbox for dicts\n                if isinstance(char, dict):\n                    cx0, ctop, cx1, cbottom = (\n                        char.get(\"x0\", 0),\n                        char.get(\"top\", 0),\n                        char.get(\"x1\", 0),\n                        char.get(\"bottom\", 0),\n                    )\n                else:\n                    cx0, ctop, cx1, cbottom = char.x0, char.top, char.x1, char.bottom\n                # Quick overlap check\n                if not (\n                    cx1 &lt; self.x0 or cx0 &gt; self.x1 or cbottom &lt; self.top or ctop &gt; self.bottom\n                ):\n                    _safe_remove(char)\n\n        if removed_count &gt; 0:\n            logger.info(f\"Region {self.bbox}: Removed {removed_count} existing OCR elements.\")\n\n    # Call the custom OCR function\n    try:\n        logger.debug(f\"Region {self.bbox}: Calling custom OCR function...\")\n        ocr_text = ocr_function(self)\n\n        if ocr_text is not None and not isinstance(ocr_text, str):\n            logger.warning(\n                f\"Custom OCR function returned non-string type ({type(ocr_text)}). \"\n                f\"Converting to string.\"\n            )\n            ocr_text = str(ocr_text)\n\n    except Exception as e:\n        logger.error(\n            f\"Error calling custom OCR function for region {self.bbox}: {e}\", exc_info=True\n        )\n        return self\n\n    # Create text element if we got text\n    if ocr_text is not None:\n        # Use the to_text_element method to create the element\n        text_element = self.to_text_element(\n            text_content=ocr_text,\n            source_label=source_label,\n            confidence=confidence,\n            add_to_page=add_to_page,\n        )\n\n        logger.info(\n            f\"Region {self.bbox}: Created text element with {len(ocr_text)} chars\"\n            f\"{' and added to page' if add_to_page else ''}\"\n        )\n    else:\n        logger.debug(f\"Region {self.bbox}: Custom OCR function returned None (no text found)\")\n\n    return self\n</code></pre> <code>natural_pdf.Region.apply_ocr(replace=True, **ocr_params)</code> <p>Apply OCR to this region and return the created text elements.</p> <p>This method supports two modes: 1. Built-in OCR Engines (default) \u2013 identical to previous behaviour. Pass typical    parameters like <code>engine='easyocr'</code> or <code>languages=['en']</code> and the method will    route the request through :class:<code>OCRManager</code>. 2. Custom OCR Function \u2013 pass a callable under the keyword <code>function</code> (or    <code>ocr_function</code>). The callable will receive this Region instance and should    return the extracted text (<code>str</code>) or <code>None</code>.  Internally the call is    delegated to :pymeth:<code>apply_custom_ocr</code> so the same logic (replacement, element    creation, etc.) is re-used.</p> <code>natural_pdf.Region.ask(question, min_confidence=0.1, model=None, debug=False, **kwargs)</code> <p>Ask a question about the region content using document QA.</p> <p>This method uses a document question answering model to extract answers from the region content. It leverages both textual content and layout information for better understanding.</p> <p>Parameters:</p> Name Type Description Default <code>question</code> <code>Union[str, List[str], Tuple[str, ...]]</code> <p>The question to ask about the region content</p> required <code>min_confidence</code> <code>float</code> <p>Minimum confidence threshold for answers (0.0-1.0)</p> <code>0.1</code> <code>model</code> <code>str</code> <p>Optional model name to use for QA (if None, uses default model)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters to pass to the QA engine</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>Dictionary with answer details: { \"answer\": extracted text, \"confidence\": confidence score, \"found\": whether an answer was found, \"page_num\": page number, \"region\": reference to this region, \"source_elements\": list of elements that contain the answer (if found)</p> <code>Union[Dict[str, Any], List[Dict[str, Any]]]</code> <p>}</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def ask(\n    self,\n    question: Union[str, List[str], Tuple[str, ...]],\n    min_confidence: float = 0.1,\n    model: str = None,\n    debug: bool = False,\n    **kwargs,\n) -&gt; Union[Dict[str, Any], List[Dict[str, Any]]]:\n    \"\"\"\n    Ask a question about the region content using document QA.\n\n    This method uses a document question answering model to extract answers from the region content.\n    It leverages both textual content and layout information for better understanding.\n\n    Args:\n        question: The question to ask about the region content\n        min_confidence: Minimum confidence threshold for answers (0.0-1.0)\n        model: Optional model name to use for QA (if None, uses default model)\n        **kwargs: Additional parameters to pass to the QA engine\n\n    Returns:\n        Dictionary with answer details: {\n            \"answer\": extracted text,\n            \"confidence\": confidence score,\n            \"found\": whether an answer was found,\n            \"page_num\": page number,\n            \"region\": reference to this region,\n            \"source_elements\": list of elements that contain the answer (if found)\n        }\n    \"\"\"\n    try:\n        from natural_pdf.qa.document_qa import get_qa_engine\n    except ImportError:\n        logger.error(\n            \"Question answering requires optional dependencies. Install with `pip install natural-pdf[ai]`\"\n        )\n        return {\n            \"answer\": None,\n            \"confidence\": 0.0,\n            \"found\": False,\n            \"page_num\": self.page.number,\n            \"source_elements\": [],\n            \"region\": self,\n        }\n\n    # Get or initialize QA engine with specified model\n    try:\n        qa_engine = get_qa_engine(model_name=model) if model else get_qa_engine()\n    except Exception as e:\n        logger.error(f\"Failed to initialize QA engine (model: {model}): {e}\", exc_info=True)\n        return {\n            \"answer\": None,\n            \"confidence\": 0.0,\n            \"found\": False,\n            \"page_num\": self.page.number,\n            \"source_elements\": [],\n            \"region\": self,\n        }\n\n    # Ask the question using the QA engine\n    try:\n        return qa_engine.ask_pdf_region(\n            self, question, min_confidence=min_confidence, debug=debug, **kwargs\n        )\n    except Exception as e:\n        logger.error(f\"Error during qa_engine.ask_pdf_region: {e}\", exc_info=True)\n        return {\n            \"answer\": None,\n            \"confidence\": 0.0,\n            \"found\": False,\n            \"page_num\": self.page.number,\n            \"source_elements\": [],\n            \"region\": self,\n        }\n</code></pre> <code>natural_pdf.Region.below(height=None, width='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Select region below this region.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>Optional[float]</code> <p>Height of the region below, in points</p> <code>None</code> <code>width</code> <code>str</code> <p>Width mode - \"full\" for full page width or \"element\" for element width</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this region in the result (default: False)</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify a lower boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region (default: True)</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Region</code> <p>Region object representing the area below</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def below(\n    self,\n    height: Optional[float] = None,\n    width: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"Region\":\n    \"\"\"\n    Select region below this region.\n\n    Args:\n        height: Height of the region below, in points\n        width: Width mode - \"full\" for full page width or \"element\" for element width\n        include_source: Whether to include this region in the result (default: False)\n        until: Optional selector string to specify a lower boundary element\n        include_endpoint: Whether to include the boundary element in the region (default: True)\n        **kwargs: Additional parameters\n\n    Returns:\n        Region object representing the area below\n    \"\"\"\n    return self._direction(\n        direction=\"below\",\n        size=height,\n        cross_size=width,\n        include_source=include_source,\n        until=until,\n        include_endpoint=include_endpoint,\n        **kwargs,\n    )\n</code></pre> <code>natural_pdf.Region.clip(obj=None, left=None, top=None, right=None, bottom=None)</code> <p>Clip this region to specific bounds, either from another object with bbox or explicit coordinates.</p> <p>The clipped region will be constrained to not exceed the specified boundaries. You can provide either an object with bounding box properties, specific coordinates, or both. When both are provided, explicit coordinates take precedence.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Optional[Any]</code> <p>Optional object with bbox properties (Region, Element, TextElement, etc.)</p> <code>None</code> <code>left</code> <code>Optional[float]</code> <p>Optional left boundary (x0) to clip to</p> <code>None</code> <code>top</code> <code>Optional[float]</code> <p>Optional top boundary to clip to</p> <code>None</code> <code>right</code> <code>Optional[float]</code> <p>Optional right boundary (x1) to clip to</p> <code>None</code> <code>bottom</code> <code>Optional[float]</code> <p>Optional bottom boundary to clip to</p> <code>None</code> <p>Returns:</p> Type Description <code>Region</code> <p>New Region with bounds clipped to the specified constraints</p> <p>Examples:</p> <code>natural_pdf.Region.contains(element)</code> <p>Check if this region completely contains an element.</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>Element</code> <p>Element to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the element is completely contained within the region, False otherwise</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def contains(self, element: \"Element\") -&gt; bool:\n    \"\"\"\n    Check if this region completely contains an element.\n\n    Args:\n        element: Element to check\n\n    Returns:\n        True if the element is completely contained within the region, False otherwise\n    \"\"\"\n    # Check if element is on the same page\n    if not hasattr(element, \"page\") or element.page != self._page:\n        return False\n\n    # Ensure element has necessary attributes\n    if not all(hasattr(element, attr) for attr in [\"x0\", \"x1\", \"top\", \"bottom\"]):\n        return False  # Cannot determine position\n\n    # For rectangular regions, check if element's bbox is fully inside region's bbox\n    if not self.has_polygon:\n        return (\n            self.x0 &lt;= element.x0\n            and element.x1 &lt;= self.x1\n            and self.top &lt;= element.top\n            and element.bottom &lt;= self.bottom\n        )\n\n    # For polygon regions, check if all corners of the element are inside the polygon\n    element_corners = [\n        (element.x0, element.top),  # top-left\n        (element.x1, element.top),  # top-right\n        (element.x1, element.bottom),  # bottom-right\n        (element.x0, element.bottom),  # bottom-left\n    ]\n\n    return all(self.is_point_inside(x, y) for x, y in element_corners)\n</code></pre> <code>natural_pdf.Region.create_cells()</code> <p>Create cell regions for a detected table by intersecting its row and column regions, and add them to the page.</p> <p>Assumes child row and column regions are already present on the page.</p> <p>Returns:</p> Type Description <p>Self for method chaining.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def create_cells(self):\n    \"\"\"\n    Create cell regions for a detected table by intersecting its\n    row and column regions, and add them to the page.\n\n    Assumes child row and column regions are already present on the page.\n\n    Returns:\n        Self for method chaining.\n    \"\"\"\n    # Ensure this is called on a table region\n    if self.region_type not in (\n        \"table\",\n        \"tableofcontents\",\n    ):  # Allow for ToC which might have structure\n        raise ValueError(\n            f\"create_cells should be called on a 'table' or 'tableofcontents' region, not '{self.region_type}'\"\n        )\n\n    # Find rows and columns associated with this page\n    # Remove the model-specific filter\n    rows = self.page.find_all(\"region[type=table-row]\")\n    columns = self.page.find_all(\"region[type=table-column]\")\n\n    # Filter to only include those that overlap with this table region\n    def is_in_table(element):\n        # Use a simple overlap check (more robust than just center point)\n        # Check if element's bbox overlaps with self.bbox\n        return (\n            hasattr(element, \"bbox\")\n            and element.x0 &lt; self.x1  # Ensure element has bbox\n            and element.x1 &gt; self.x0\n            and element.top &lt; self.bottom\n            and element.bottom &gt; self.top\n        )\n\n    table_rows = [r for r in rows if is_in_table(r)]\n    table_columns = [c for c in columns if is_in_table(c)]\n\n    if not table_rows or not table_columns:\n        # Use page's logger if available\n        logger_instance = getattr(self._page, \"logger\", logger)\n        logger_instance.warning(\n            f\"Region {self.bbox}: Cannot create cells. No overlapping row or column regions found.\"\n        )\n        return self  # Return self even if no cells created\n\n    # Sort rows and columns\n    table_rows.sort(key=lambda r: r.top)\n    table_columns.sort(key=lambda c: c.x0)\n\n    # Create cells and add them to the page's element manager\n    created_count = 0\n    for row in table_rows:\n        for column in table_columns:\n            # Calculate intersection bbox for the cell\n            cell_x0 = max(row.x0, column.x0)\n            cell_y0 = max(row.top, column.top)\n            cell_x1 = min(row.x1, column.x1)\n            cell_y1 = min(row.bottom, column.bottom)\n\n            # Only create a cell if the intersection is valid (positive width/height)\n            if cell_x1 &gt; cell_x0 and cell_y1 &gt; cell_y0:\n                # Create cell region at the intersection\n                cell = self.page.create_region(cell_x0, cell_y0, cell_x1, cell_y1)\n                # Set metadata\n                cell.source = \"derived\"\n                cell.region_type = \"table-cell\"  # Explicitly set type\n                cell.normalized_type = \"table-cell\"  # And normalized type\n                # Inherit model from the parent table region\n                cell.model = self.model\n                cell.parent_region = self  # Link cell to parent table region\n\n                # Add the cell region to the page's element manager\n                self.page._element_mgr.add_region(cell)\n                created_count += 1\n\n    # Optional: Add created cells to the table region's children\n    # self.child_regions.extend(cells_created_in_this_call) # Needs list management\n\n    logger_instance = getattr(self._page, \"logger\", logger)\n    logger_instance.info(\n        f\"Region {self.bbox} (Model: {self.model}): Created and added {created_count} cell regions.\"\n    )\n\n    return self  # Return self for chaining\n</code></pre> <code>natural_pdf.Region.extract_table(method=None, table_settings=None, use_ocr=False, ocr_config=None, text_options=None, cell_extraction_func=None, show_progress=False, content_filter=None)</code> <p>Extract a table from this region.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Optional[str]</code> <p>Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).     'stream' is an alias for 'pdfplumber' with text-based strategies (equivalent to     setting <code>vertical_strategy</code> and <code>horizontal_strategy</code> to 'text').     'lattice' is an alias for 'pdfplumber' with line-based strategies (equivalent to     setting <code>vertical_strategy</code> and <code>horizontal_strategy</code> to 'lines').</p> <code>None</code> <code>table_settings</code> <code>Optional[dict]</code> <p>Settings for pdfplumber table extraction (used with 'pdfplumber', 'stream', or 'lattice' methods).</p> <code>None</code> <code>use_ocr</code> <code>bool</code> <p>Whether to use OCR for text extraction (currently only applicable with 'tatr' method).</p> <code>False</code> <code>ocr_config</code> <code>Optional[dict]</code> <p>OCR configuration parameters.</p> <code>None</code> <code>text_options</code> <code>Optional[Dict]</code> <p>Dictionary of options for the 'text' method, corresponding to arguments           of analyze_text_table_structure (e.g., snap_tolerance, expand_bbox).</p> <code>None</code> <code>cell_extraction_func</code> <code>Optional[Callable[[Region], Optional[str]]]</code> <p>Optional callable function that takes a cell Region object                   and returns its string content. Overrides default text extraction                   for the 'text' method.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>If True, display a progress bar during cell text extraction for the 'text' method.</p> <code>False</code> <code>content_filter</code> <code>Optional[Union[str, Callable[[str], bool], List[str]]]</code> <p>Optional content filter to apply during cell text extraction. Can be: - A regex pattern string (characters matching the pattern are EXCLUDED) - A callable that takes text and returns True to KEEP the character - A list of regex patterns (characters matching ANY pattern are EXCLUDED) Works with all extraction methods by filtering cell content.</p> <code>None</code> <p>Returns:</p> Type Description <code>TableResult</code> <p>Table data as a list of rows, where each row is a list of cell values (str or None).</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def extract_table(\n    self,\n    method: Optional[str] = None,  # Make method optional\n    table_settings: Optional[dict] = None,  # Use Optional\n    use_ocr: bool = False,\n    ocr_config: Optional[dict] = None,  # Use Optional\n    text_options: Optional[Dict] = None,\n    cell_extraction_func: Optional[Callable[[\"Region\"], Optional[str]]] = None,\n    # --- NEW: Add tqdm control option --- #\n    show_progress: bool = False,  # Controls progress bar for text method\n    content_filter: Optional[\n        Union[str, Callable[[str], bool], List[str]]\n    ] = None,  # NEW: Content filtering\n) -&gt; TableResult:  # Return type allows Optional[str] for cells\n    \"\"\"\n    Extract a table from this region.\n\n    Args:\n        method: Method to use: 'tatr', 'pdfplumber', 'text', 'stream', 'lattice', or None (auto-detect).\n                'stream' is an alias for 'pdfplumber' with text-based strategies (equivalent to\n                setting `vertical_strategy` and `horizontal_strategy` to 'text').\n                'lattice' is an alias for 'pdfplumber' with line-based strategies (equivalent to\n                setting `vertical_strategy` and `horizontal_strategy` to 'lines').\n        table_settings: Settings for pdfplumber table extraction (used with 'pdfplumber', 'stream', or 'lattice' methods).\n        use_ocr: Whether to use OCR for text extraction (currently only applicable with 'tatr' method).\n        ocr_config: OCR configuration parameters.\n        text_options: Dictionary of options for the 'text' method, corresponding to arguments\n                      of analyze_text_table_structure (e.g., snap_tolerance, expand_bbox).\n        cell_extraction_func: Optional callable function that takes a cell Region object\n                              and returns its string content. Overrides default text extraction\n                              for the 'text' method.\n        show_progress: If True, display a progress bar during cell text extraction for the 'text' method.\n        content_filter: Optional content filter to apply during cell text extraction. Can be:\n            - A regex pattern string (characters matching the pattern are EXCLUDED)\n            - A callable that takes text and returns True to KEEP the character\n            - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n            Works with all extraction methods by filtering cell content.\n\n    Returns:\n        Table data as a list of rows, where each row is a list of cell values (str or None).\n    \"\"\"\n    # Default settings if none provided\n    if table_settings is None:\n        table_settings = {}\n    if text_options is None:\n        text_options = {}  # Initialize empty dict\n\n    # Auto-detect method if not specified\n    if method is None:\n        # If this is a TATR-detected region, use TATR method\n        if hasattr(self, \"model\") and self.model == \"tatr\" and self.region_type == \"table\":\n            effective_method = \"tatr\"\n        else:\n            # Try lattice first, then fall back to stream if no meaningful results\n            logger.debug(f\"Region {self.bbox}: Auto-detecting table extraction method...\")\n\n            # --- NEW: Prefer already-created table_cell regions if they exist --- #\n            try:\n                cell_regions_in_table = [\n                    c\n                    for c in self.page.find_all(\n                        \"region[type=table_cell]\", apply_exclusions=False\n                    )\n                    if self.intersects(c)\n                ]\n            except Exception as _cells_err:\n                cell_regions_in_table = []  # Fallback silently\n\n            if cell_regions_in_table:\n                logger.debug(\n                    f\"Region {self.bbox}: Found {len(cell_regions_in_table)} pre-computed table_cell regions \u2013 using 'cells' method.\"\n                )\n                return TableResult(\n                    self._extract_table_from_cells(\n                        cell_regions_in_table, content_filter=content_filter\n                    )\n                )\n\n            # --------------------------------------------------------------- #\n\n            try:\n                logger.debug(f\"Region {self.bbox}: Trying 'lattice' method first...\")\n                lattice_result = self.extract_table(\n                    \"lattice\", table_settings=table_settings.copy()\n                )\n\n                # Check if lattice found meaningful content\n                if (\n                    lattice_result\n                    and len(lattice_result) &gt; 0\n                    and any(\n                        any(cell and cell.strip() for cell in row if cell)\n                        for row in lattice_result\n                    )\n                ):\n                    logger.debug(\n                        f\"Region {self.bbox}: 'lattice' method found table with {len(lattice_result)} rows\"\n                    )\n                    return lattice_result\n                else:\n                    logger.debug(\n                        f\"Region {self.bbox}: 'lattice' method found no meaningful content\"\n                    )\n            except Exception as e:\n                logger.debug(f\"Region {self.bbox}: 'lattice' method failed: {e}\")\n\n            # Fall back to stream\n            logger.debug(f\"Region {self.bbox}: Falling back to 'stream' method...\")\n            return self.extract_table(\"stream\", table_settings=table_settings.copy())\n    else:\n        effective_method = method\n\n    # Handle method aliases for pdfplumber\n    if effective_method == \"stream\":\n        logger.debug(\"Using 'stream' method alias for 'pdfplumber' with text-based strategies.\")\n        effective_method = \"pdfplumber\"\n        # Set default text strategies if not already provided by the user\n        table_settings.setdefault(\"vertical_strategy\", \"text\")\n        table_settings.setdefault(\"horizontal_strategy\", \"text\")\n    elif effective_method == \"lattice\":\n        logger.debug(\n            \"Using 'lattice' method alias for 'pdfplumber' with line-based strategies.\"\n        )\n        effective_method = \"pdfplumber\"\n        # Set default line strategies if not already provided by the user\n        table_settings.setdefault(\"vertical_strategy\", \"lines\")\n        table_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n    # -------------------------------------------------------------\n    # Auto-inject tolerances when text-based strategies are requested.\n    # This must happen AFTER alias handling (so strategies are final)\n    # and BEFORE we delegate to _extract_table_* helpers.\n    # -------------------------------------------------------------\n    if \"text\" in (\n        table_settings.get(\"vertical_strategy\"),\n        table_settings.get(\"horizontal_strategy\"),\n    ):\n        page_cfg = getattr(self.page, \"_config\", {})\n        # Ensure text_* tolerances passed to pdfplumber\n        if \"text_x_tolerance\" not in table_settings and \"x_tolerance\" not in table_settings:\n            if page_cfg.get(\"x_tolerance\") is not None:\n                table_settings[\"text_x_tolerance\"] = page_cfg[\"x_tolerance\"]\n        if \"text_y_tolerance\" not in table_settings and \"y_tolerance\" not in table_settings:\n            if page_cfg.get(\"y_tolerance\") is not None:\n                table_settings[\"text_y_tolerance\"] = page_cfg[\"y_tolerance\"]\n\n        # Snap / join tolerances (~ line spacing)\n        if \"snap_tolerance\" not in table_settings and \"snap_x_tolerance\" not in table_settings:\n            snap = max(1, round((page_cfg.get(\"y_tolerance\", 1)) * 0.9))\n            table_settings[\"snap_tolerance\"] = snap\n        if \"join_tolerance\" not in table_settings and \"join_x_tolerance\" not in table_settings:\n            table_settings[\"join_tolerance\"] = table_settings[\"snap_tolerance\"]\n\n    logger.debug(f\"Region {self.bbox}: Extracting table using method '{effective_method}'\")\n\n    # Use the selected method\n    if effective_method == \"tatr\":\n        table_rows = self._extract_table_tatr(\n            use_ocr=use_ocr, ocr_config=ocr_config, content_filter=content_filter\n        )\n    elif effective_method == \"text\":\n        current_text_options = text_options.copy()\n        current_text_options[\"cell_extraction_func\"] = cell_extraction_func\n        current_text_options[\"show_progress\"] = show_progress\n        current_text_options[\"content_filter\"] = content_filter\n        table_rows = self._extract_table_text(**current_text_options)\n    elif effective_method == \"pdfplumber\":\n        table_rows = self._extract_table_plumber(table_settings, content_filter=content_filter)\n    else:\n        raise ValueError(\n            f\"Unknown table extraction method: '{method}'. Choose from 'tatr', 'pdfplumber', 'text', 'stream', 'lattice'.\"\n        )\n\n    return TableResult(table_rows)\n</code></pre> <code>natural_pdf.Region.extract_tables(method=None, table_settings=None)</code> <p>Extract all tables from this region using pdfplumber-based methods.</p> <p>Note: Only 'pdfplumber', 'stream', and 'lattice' methods are supported for extract_tables. 'tatr' and 'text' methods are designed for single table extraction only.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>Optional[str]</code> <p>Method to use: 'pdfplumber', 'stream', 'lattice', or None (auto-detect).     'stream' uses text-based strategies, 'lattice' uses line-based strategies.</p> <code>None</code> <code>table_settings</code> <code>Optional[dict]</code> <p>Settings for pdfplumber table extraction.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[List[str]]]</code> <p>List of tables, where each table is a list of rows, and each row is a list of cell values.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def extract_tables(\n    self,\n    method: Optional[str] = None,\n    table_settings: Optional[dict] = None,\n) -&gt; List[List[List[str]]]:\n    \"\"\"\n    Extract all tables from this region using pdfplumber-based methods.\n\n    Note: Only 'pdfplumber', 'stream', and 'lattice' methods are supported for extract_tables.\n    'tatr' and 'text' methods are designed for single table extraction only.\n\n    Args:\n        method: Method to use: 'pdfplumber', 'stream', 'lattice', or None (auto-detect).\n                'stream' uses text-based strategies, 'lattice' uses line-based strategies.\n        table_settings: Settings for pdfplumber table extraction.\n\n    Returns:\n        List of tables, where each table is a list of rows, and each row is a list of cell values.\n    \"\"\"\n    if table_settings is None:\n        table_settings = {}\n\n    # Auto-detect method if not specified (try lattice first, then stream)\n    if method is None:\n        logger.debug(f\"Region {self.bbox}: Auto-detecting tables extraction method...\")\n\n        # Try lattice first\n        try:\n            lattice_settings = table_settings.copy()\n            lattice_settings.setdefault(\"vertical_strategy\", \"lines\")\n            lattice_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n            logger.debug(f\"Region {self.bbox}: Trying 'lattice' method first for tables...\")\n            lattice_result = self._extract_tables_plumber(lattice_settings)\n\n            # Check if lattice found meaningful tables\n            if (\n                lattice_result\n                and len(lattice_result) &gt; 0\n                and any(\n                    any(\n                        any(cell and cell.strip() for cell in row if cell)\n                        for row in table\n                        if table\n                    )\n                    for table in lattice_result\n                )\n            ):\n                logger.debug(\n                    f\"Region {self.bbox}: 'lattice' method found {len(lattice_result)} tables\"\n                )\n                return lattice_result\n            else:\n                logger.debug(f\"Region {self.bbox}: 'lattice' method found no meaningful tables\")\n\n        except Exception as e:\n            logger.debug(f\"Region {self.bbox}: 'lattice' method failed: {e}\")\n\n        # Fall back to stream\n        logger.debug(f\"Region {self.bbox}: Falling back to 'stream' method for tables...\")\n        stream_settings = table_settings.copy()\n        stream_settings.setdefault(\"vertical_strategy\", \"text\")\n        stream_settings.setdefault(\"horizontal_strategy\", \"text\")\n\n        return self._extract_tables_plumber(stream_settings)\n\n    effective_method = method\n\n    # Handle method aliases\n    if effective_method == \"stream\":\n        logger.debug(\"Using 'stream' method alias for 'pdfplumber' with text-based strategies.\")\n        effective_method = \"pdfplumber\"\n        table_settings.setdefault(\"vertical_strategy\", \"text\")\n        table_settings.setdefault(\"horizontal_strategy\", \"text\")\n    elif effective_method == \"lattice\":\n        logger.debug(\n            \"Using 'lattice' method alias for 'pdfplumber' with line-based strategies.\"\n        )\n        effective_method = \"pdfplumber\"\n        table_settings.setdefault(\"vertical_strategy\", \"lines\")\n        table_settings.setdefault(\"horizontal_strategy\", \"lines\")\n\n    # Use the selected method\n    if effective_method == \"pdfplumber\":\n        return self._extract_tables_plumber(table_settings)\n    else:\n        raise ValueError(\n            f\"Unknown tables extraction method: '{method}'. Choose from 'pdfplumber', 'stream', 'lattice'.\"\n        )\n</code></pre> <code>natural_pdf.Region.extract_text(apply_exclusions=True, debug=False, content_filter=None, **kwargs)</code> <p>Extract text from this region, respecting page exclusions and using pdfplumber's layout engine (chars_to_textmap).</p> <p>Parameters:</p> Name Type Description Default <code>apply_exclusions</code> <p>Whether to apply exclusion regions defined on the parent page.</p> <code>True</code> <code>debug</code> <p>Enable verbose debugging output for filtering steps.</p> <code>False</code> <code>content_filter</code> <p>Optional content filter to exclude specific text patterns. Can be: - A regex pattern string (characters matching the pattern are EXCLUDED) - A callable that takes text and returns True to KEEP the character - A list of regex patterns (characters matching ANY pattern are EXCLUDED)</p> <code>None</code> <code>**kwargs</code> <p>Additional layout parameters passed directly to pdfplumber's       <code>chars_to_textmap</code> function (e.g., layout, x_density, y_density).       See Page.extract_text docstring for more.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Extracted text as string, potentially with layout-based spacing.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def extract_text(\n    self, apply_exclusions=True, debug=False, content_filter=None, **kwargs\n) -&gt; str:\n    \"\"\"\n    Extract text from this region, respecting page exclusions and using pdfplumber's\n    layout engine (chars_to_textmap).\n\n    Args:\n        apply_exclusions: Whether to apply exclusion regions defined on the parent page.\n        debug: Enable verbose debugging output for filtering steps.\n        content_filter: Optional content filter to exclude specific text patterns. Can be:\n            - A regex pattern string (characters matching the pattern are EXCLUDED)\n            - A callable that takes text and returns True to KEEP the character\n            - A list of regex patterns (characters matching ANY pattern are EXCLUDED)\n        **kwargs: Additional layout parameters passed directly to pdfplumber's\n                  `chars_to_textmap` function (e.g., layout, x_density, y_density).\n                  See Page.extract_text docstring for more.\n\n    Returns:\n        Extracted text as string, potentially with layout-based spacing.\n    \"\"\"\n    # Allow 'debug_exclusions' for backward compatibility\n    debug = kwargs.get(\"debug\", debug or kwargs.get(\"debug_exclusions\", False))\n    logger.debug(f\"Region {self.bbox}: extract_text called with kwargs: {kwargs}\")\n\n    # 1. Get Word Elements potentially within this region (initial broad phase)\n    # Optimization: Could use spatial query if page elements were indexed\n    page_words = self.page.words  # Get all words from the page\n\n    # 2. Gather all character dicts from words potentially in region\n    # We filter precisely in filter_chars_spatially\n    all_char_dicts = []\n    for word in page_words:\n        # Quick bbox check to avoid processing words clearly outside\n        if get_bbox_overlap(self.bbox, word.bbox) is not None:\n            all_char_dicts.extend(getattr(word, \"_char_dicts\", []))\n\n    if not all_char_dicts:\n        logger.debug(f\"Region {self.bbox}: No character dicts found overlapping region bbox.\")\n        return \"\"\n\n    # 3. Get Relevant Exclusions (overlapping this region)\n    apply_exclusions_flag = kwargs.get(\"apply_exclusions\", apply_exclusions)\n    exclusion_regions = []\n    if apply_exclusions_flag and self._page._exclusions:\n        all_page_exclusions = self._page._get_exclusion_regions(\n            include_callable=True, debug=debug\n        )\n        overlapping_exclusions = []\n        for excl in all_page_exclusions:\n            if get_bbox_overlap(self.bbox, excl.bbox) is not None:\n                overlapping_exclusions.append(excl)\n        exclusion_regions = overlapping_exclusions\n        if debug:\n            logger.debug(\n                f\"Region {self.bbox}: Applying {len(exclusion_regions)} overlapping exclusions.\"\n            )\n    elif debug:\n        logger.debug(f\"Region {self.bbox}: Not applying exclusions.\")\n\n    # 4. Spatially Filter Characters using Utility\n    # Pass self as the target_region for precise polygon checks etc.\n    filtered_chars = filter_chars_spatially(\n        char_dicts=all_char_dicts,\n        exclusion_regions=exclusion_regions,\n        target_region=self,  # Pass self!\n        debug=debug,\n    )\n\n    # 5. Generate Text Layout using Utility\n    # Add content_filter to kwargs if provided\n    final_kwargs = kwargs.copy()\n    if content_filter is not None:\n        final_kwargs[\"content_filter\"] = content_filter\n\n    result = generate_text_layout(\n        char_dicts=filtered_chars,\n        layout_context_bbox=self.bbox,  # Use region's bbox for context\n        user_kwargs=final_kwargs,  # Pass kwargs including content_filter\n    )\n\n    logger.debug(f\"Region {self.bbox}: extract_text finished, result length: {len(result)}.\")\n    return result\n</code></pre> <code>natural_pdf.Region.find(selector=None, *, text=None, contains='all', apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find(*, text: str, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[Element]\n</code></pre><pre><code>find(selector: str, *, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; Optional[Element]\n</code></pre> <p>Find the first element in this region matching the selector OR text content.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>contains</code> <code>str</code> <p>How to determine if elements are inside: 'all' (fully inside),      'any' (any overlap), or 'center' (center point inside).      (default: \"all\")</p> <code>'all'</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters for element filtering.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Element]</code> <p>First matching element or None.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def find(\n    self,\n    selector: Optional[str] = None,  # Now optional\n    *,\n    text: Optional[str] = None,  # New text parameter\n    contains: str = \"all\",  # New parameter for containment behavior\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; Optional[\"Element\"]:\n    \"\"\"\n    Find the first element in this region matching the selector OR text content.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        contains: How to determine if elements are inside: 'all' (fully inside),\n                 'any' (any overlap), or 'center' (center point inside).\n                 (default: \"all\")\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional parameters for element filtering.\n\n    Returns:\n        First matching element or None.\n    \"\"\"\n    # Delegate validation and selector construction to find_all\n    elements = self.find_all(\n        selector=selector,\n        text=text,\n        contains=contains,\n        apply_exclusions=apply_exclusions,\n        regex=regex,\n        case=case,\n        **kwargs,\n    )\n    return elements.first if elements else None\n</code></pre> <code>natural_pdf.Region.find_all(selector=None, *, text=None, contains='all', apply_exclusions=True, regex=False, case=True, **kwargs)</code> <pre><code>find_all(*, text: str, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre><pre><code>find_all(selector: str, *, contains: str = 'all', apply_exclusions: bool = True, regex: bool = False, case: bool = True, **kwargs) -&gt; ElementCollection\n</code></pre> <p>Find all elements in this region matching the selector OR text content.</p> <p>Provide EITHER <code>selector</code> OR <code>text</code>, but not both.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>CSS-like selector string.</p> <code>None</code> <code>text</code> <code>Optional[str]</code> <p>Text content to search for (equivalent to 'text:contains(...)').</p> <code>None</code> <code>contains</code> <code>str</code> <p>How to determine if elements are inside: 'all' (fully inside),      'any' (any overlap), or 'center' (center point inside).      (default: \"all\")</p> <code>'all'</code> <code>apply_exclusions</code> <code>bool</code> <p>Whether to exclude elements in exclusion regions (default: True).</p> <code>True</code> <code>regex</code> <code>bool</code> <p>Whether to use regex for text search (<code>selector</code> or <code>text</code>) (default: False).</p> <code>False</code> <code>case</code> <code>bool</code> <p>Whether to do case-sensitive text search (<code>selector</code> or <code>text</code>) (default: True).</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters for element filtering.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection</code> <p>ElementCollection with matching elements.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def find_all(\n    self,\n    selector: Optional[str] = None,  # Now optional\n    *,\n    text: Optional[str] = None,  # New text parameter\n    contains: str = \"all\",  # New parameter to control inside/overlap behavior\n    apply_exclusions: bool = True,\n    regex: bool = False,\n    case: bool = True,\n    **kwargs,\n) -&gt; \"ElementCollection\":\n    \"\"\"\n    Find all elements in this region matching the selector OR text content.\n\n    Provide EITHER `selector` OR `text`, but not both.\n\n    Args:\n        selector: CSS-like selector string.\n        text: Text content to search for (equivalent to 'text:contains(...)').\n        contains: How to determine if elements are inside: 'all' (fully inside),\n                 'any' (any overlap), or 'center' (center point inside).\n                 (default: \"all\")\n        apply_exclusions: Whether to exclude elements in exclusion regions (default: True).\n        regex: Whether to use regex for text search (`selector` or `text`) (default: False).\n        case: Whether to do case-sensitive text search (`selector` or `text`) (default: True).\n        **kwargs: Additional parameters for element filtering.\n\n    Returns:\n        ElementCollection with matching elements.\n    \"\"\"\n    from natural_pdf.elements.element_collection import ElementCollection\n\n    if selector is not None and text is not None:\n        raise ValueError(\"Provide either 'selector' or 'text', not both.\")\n    if selector is None and text is None:\n        raise ValueError(\"Provide either 'selector' or 'text'.\")\n\n    # Validate contains parameter\n    if contains not in [\"all\", \"any\", \"center\"]:\n        raise ValueError(\n            f\"Invalid contains value: {contains}. Must be 'all', 'any', or 'center'\"\n        )\n\n    # Construct selector if 'text' is provided\n    effective_selector = \"\"\n    if text is not None:\n        escaped_text = text.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n        effective_selector = f'text:contains(\"{escaped_text}\")'\n        logger.debug(\n            f\"Using text shortcut: find_all(text='{text}') -&gt; find_all('{effective_selector}')\"\n        )\n    elif selector is not None:\n        effective_selector = selector\n    else:\n        raise ValueError(\"Internal error: No selector or text provided.\")\n\n    # Normal case: Region is on a single page\n    try:\n        # Parse the final selector string\n        selector_obj = parse_selector(effective_selector)\n\n        # Get all potentially relevant elements from the page\n        # Let the page handle its exclusion logic if needed\n        potential_elements = self.page.find_all(\n            selector=effective_selector,\n            apply_exclusions=apply_exclusions,\n            regex=regex,\n            case=case,\n            **kwargs,\n        )\n\n        # Filter these elements based on the specified containment method\n        region_bbox = self.bbox\n        matching_elements = []\n\n        if contains == \"all\":  # Fully inside (strict)\n            matching_elements = [\n                el\n                for el in potential_elements\n                if el.x0 &gt;= region_bbox[0]\n                and el.top &gt;= region_bbox[1]\n                and el.x1 &lt;= region_bbox[2]\n                and el.bottom &lt;= region_bbox[3]\n            ]\n        elif contains == \"any\":  # Any overlap\n            matching_elements = [el for el in potential_elements if self.intersects(el)]\n        elif contains == \"center\":  # Center point inside\n            matching_elements = [\n                el for el in potential_elements if self.is_element_center_inside(el)\n            ]\n\n        return ElementCollection(matching_elements)\n\n    except Exception as e:\n        logger.error(f\"Error during find_all in region: {e}\", exc_info=True)\n        return ElementCollection([])\n</code></pre> <code>natural_pdf.Region.get_children(selector=None)</code> <p>Get immediate child regions, optionally filtered by selector.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <p>Optional selector to filter children</p> <code>None</code> <p>Returns:</p> Type Description <p>List of child regions matching the selector</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def get_children(self, selector=None):\n    \"\"\"\n    Get immediate child regions, optionally filtered by selector.\n\n    Args:\n        selector: Optional selector to filter children\n\n    Returns:\n        List of child regions matching the selector\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(\"natural_pdf.elements.region\")\n\n    if selector is None:\n        return self.child_regions\n\n    # Use existing selector parser to filter\n    try:\n        selector_obj = parse_selector(selector)\n        filter_func = selector_to_filter_func(selector_obj)  # Removed region=self\n        matched = [child for child in self.child_regions if filter_func(child)]\n        logger.debug(\n            f\"get_children: found {len(matched)} of {len(self.child_regions)} children matching '{selector}'\"\n        )\n        return matched\n    except Exception as e:\n        logger.error(f\"Error applying selector in get_children: {e}\", exc_info=True)\n        return []  # Return empty list on error\n</code></pre> <code>natural_pdf.Region.get_descendants(selector=None)</code> <p>Get all descendant regions (children, grandchildren, etc.), optionally filtered by selector.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <p>Optional selector to filter descendants</p> <code>None</code> <p>Returns:</p> Type Description <p>List of descendant regions matching the selector</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def get_descendants(self, selector=None):\n    \"\"\"\n    Get all descendant regions (children, grandchildren, etc.), optionally filtered by selector.\n\n    Args:\n        selector: Optional selector to filter descendants\n\n    Returns:\n        List of descendant regions matching the selector\n    \"\"\"\n    import logging\n\n    logger = logging.getLogger(\"natural_pdf.elements.region\")\n\n    all_descendants = []\n    queue = list(self.child_regions)  # Start with direct children\n\n    while queue:\n        current = queue.pop(0)\n        all_descendants.append(current)\n        # Add current's children to the queue for processing\n        if hasattr(current, \"child_regions\"):\n            queue.extend(current.child_regions)\n\n    logger.debug(f\"get_descendants: found {len(all_descendants)} total descendants\")\n\n    # Filter by selector if provided\n    if selector is not None:\n        try:\n            selector_obj = parse_selector(selector)\n            filter_func = selector_to_filter_func(selector_obj)  # Removed region=self\n            matched = [desc for desc in all_descendants if filter_func(desc)]\n            logger.debug(f\"get_descendants: filtered to {len(matched)} matching '{selector}'\")\n            return matched\n        except Exception as e:\n            logger.error(f\"Error applying selector in get_descendants: {e}\", exc_info=True)\n            return []  # Return empty list on error\n\n    return all_descendants\n</code></pre> <code>natural_pdf.Region.get_elements(selector=None, apply_exclusions=True, **kwargs)</code> <p>Get all elements within this region.</p> <p>Parameters:</p> Name Type Description Default <code>selector</code> <code>Optional[str]</code> <p>Optional selector to filter elements</p> <code>None</code> <code>apply_exclusions</code> <p>Whether to apply exclusion regions</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters for element filtering</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Element]</code> <p>List of elements in the region</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def get_elements(\n    self, selector: Optional[str] = None, apply_exclusions=True, **kwargs\n) -&gt; List[\"Element\"]:\n    \"\"\"\n    Get all elements within this region.\n\n    Args:\n        selector: Optional selector to filter elements\n        apply_exclusions: Whether to apply exclusion regions\n        **kwargs: Additional parameters for element filtering\n\n    Returns:\n        List of elements in the region\n    \"\"\"\n    if selector:\n        # Find elements on the page matching the selector\n        page_elements = self.page.find_all(\n            selector, apply_exclusions=apply_exclusions, **kwargs\n        )\n        # Filter those elements to only include ones within this region\n        return [e for e in page_elements if self._is_element_in_region(e)]\n    else:\n        # Get all elements from the page\n        page_elements = self.page.get_elements(apply_exclusions=apply_exclusions)\n        # Filter to elements in this region\n        return [e for e in page_elements if self._is_element_in_region(e)]\n</code></pre> <code>natural_pdf.Region.get_section_between(start_element=None, end_element=None, include_boundaries='both')</code> <p>Get a section between two elements within this region.</p> <p>Parameters:</p> Name Type Description Default <code>start_element</code> <p>Element marking the start of the section</p> <code>None</code> <code>end_element</code> <p>Element marking the end of the section</p> <code>None</code> <code>include_boundaries</code> <p>How to include boundary elements: 'start', 'end', 'both', or 'none'</p> <code>'both'</code> <p>Returns:</p> Type Description <p>Region representing the section</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def get_section_between(self, start_element=None, end_element=None, include_boundaries=\"both\"):\n    \"\"\"\n    Get a section between two elements within this region.\n\n    Args:\n        start_element: Element marking the start of the section\n        end_element: Element marking the end of the section\n        include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none'\n\n    Returns:\n        Region representing the section\n    \"\"\"\n    # Get elements only within this region first\n    elements = self.get_elements()\n\n    # If no elements, return self or empty region?\n    if not elements:\n        logger.warning(\n            f\"get_section_between called on region {self.bbox} with no contained elements.\"\n        )\n        # Return an empty region at the start of the parent region\n        return Region(self.page, (self.x0, self.top, self.x0, self.top))\n\n    # Sort elements in reading order\n    elements.sort(key=lambda e: (e.top, e.x0))\n\n    # Find start index\n    start_idx = 0\n    if start_element:\n        try:\n            start_idx = elements.index(start_element)\n        except ValueError:\n            # Start element not in region, use first element\n            logger.debug(\"Start element not found in region, using first element.\")\n            start_element = elements[0]  # Use the actual first element\n            start_idx = 0\n    else:\n        start_element = elements[0]  # Default start is first element\n\n    # Find end index\n    end_idx = len(elements) - 1\n    if end_element:\n        try:\n            end_idx = elements.index(end_element)\n        except ValueError:\n            # End element not in region, use last element\n            logger.debug(\"End element not found in region, using last element.\")\n            end_element = elements[-1]  # Use the actual last element\n            end_idx = len(elements) - 1\n    else:\n        end_element = elements[-1]  # Default end is last element\n\n    # Adjust indexes based on boundary inclusion\n    start_element_for_bbox = start_element\n    end_element_for_bbox = end_element\n\n    if include_boundaries == \"none\":\n        start_idx += 1\n        end_idx -= 1\n        start_element_for_bbox = elements[start_idx] if start_idx &lt;= end_idx else None\n        end_element_for_bbox = elements[end_idx] if start_idx &lt;= end_idx else None\n    elif include_boundaries == \"start\":\n        end_idx -= 1\n        end_element_for_bbox = elements[end_idx] if start_idx &lt;= end_idx else None\n    elif include_boundaries == \"end\":\n        start_idx += 1\n        start_element_for_bbox = elements[start_idx] if start_idx &lt;= end_idx else None\n\n    # Ensure valid indexes\n    start_idx = max(0, start_idx)\n    end_idx = min(len(elements) - 1, end_idx)\n\n    # If no valid elements in range, return empty region\n    if start_idx &gt; end_idx or start_element_for_bbox is None or end_element_for_bbox is None:\n        logger.debug(\"No valid elements in range for get_section_between.\")\n        # Return an empty region positioned at the start element boundary\n        anchor = start_element if start_element else self\n        return Region(self.page, (anchor.x0, anchor.top, anchor.x0, anchor.top))\n\n    # Get elements in range based on adjusted indices\n    section_elements = elements[start_idx : end_idx + 1]\n\n    # Create bounding box around the ELEMENTS included based on indices\n    x0 = min(e.x0 for e in section_elements)\n    top = min(e.top for e in section_elements)\n    x1 = max(e.x1 for e in section_elements)\n    bottom = max(e.bottom for e in section_elements)\n\n    # Create new region\n    section = Region(self.page, (x0, top, x1, bottom))\n    # Store the original boundary elements for reference\n    section.start_element = start_element\n    section.end_element = end_element\n\n    return section\n</code></pre> <code>natural_pdf.Region.get_sections(start_elements=None, end_elements=None, include_boundaries='both')</code> <p>Get sections within this region based on start/end elements.</p> <p>Parameters:</p> Name Type Description Default <code>start_elements</code> <p>Elements or selector string that mark the start of sections</p> <code>None</code> <code>end_elements</code> <p>Elements or selector string that mark the end of sections</p> <code>None</code> <code>include_boundaries</code> <p>How to include boundary elements: 'start', 'end', 'both', or 'none'</p> <code>'both'</code> <p>Returns:</p> Type Description <code>ElementCollection[Region]</code> <p>List of Region objects representing the extracted sections</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def get_sections(\n    self, start_elements=None, end_elements=None, include_boundaries=\"both\"\n) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Get sections within this region based on start/end elements.\n\n    Args:\n        start_elements: Elements or selector string that mark the start of sections\n        end_elements: Elements or selector string that mark the end of sections\n        include_boundaries: How to include boundary elements: 'start', 'end', 'both', or 'none'\n\n    Returns:\n        List of Region objects representing the extracted sections\n    \"\"\"\n    from natural_pdf.elements.element_collection import ElementCollection\n\n    # Process string selectors to find elements WITHIN THIS REGION\n    if isinstance(start_elements, str):\n        start_elements = self.find_all(start_elements)  # Use region's find_all\n        if hasattr(start_elements, \"elements\"):\n            start_elements = start_elements.elements\n\n    if isinstance(end_elements, str):\n        end_elements = self.find_all(end_elements)  # Use region's find_all\n        if hasattr(end_elements, \"elements\"):\n            end_elements = end_elements.elements\n\n    # Ensure start_elements is a list (or similar iterable)\n    if start_elements is None or not hasattr(start_elements, \"__iter__\"):\n        logger.warning(\n            \"get_sections requires valid start_elements (selector or list). Returning empty.\"\n        )\n        return []\n    # Ensure end_elements is a list if provided\n    if end_elements is not None and not hasattr(end_elements, \"__iter__\"):\n        logger.warning(\"end_elements must be iterable if provided. Ignoring.\")\n        end_elements = []\n    elif end_elements is None:\n        end_elements = []\n\n    # If no start elements found within the region, return empty list\n    if not start_elements:\n        return []\n\n    # Sort all elements within the region in reading order\n    all_elements_in_region = self.get_elements()\n    all_elements_in_region.sort(key=lambda e: (e.top, e.x0))\n\n    if not all_elements_in_region:\n        return []  # Cannot create sections if region is empty\n\n    # Map elements to their indices in the sorted list\n    element_to_index = {el: i for i, el in enumerate(all_elements_in_region)}\n\n    # Mark section boundaries using indices from the sorted list\n    section_boundaries = []\n\n    # Add start element indexes\n    for element in start_elements:\n        idx = element_to_index.get(element)\n        if idx is not None:\n            section_boundaries.append({\"index\": idx, \"element\": element, \"type\": \"start\"})\n        # else: Element found by selector might not be geometrically in region? Log warning?\n\n    # Add end element indexes if provided\n    for element in end_elements:\n        idx = element_to_index.get(element)\n        if idx is not None:\n            section_boundaries.append({\"index\": idx, \"element\": element, \"type\": \"end\"})\n\n    # Sort boundaries by index (document order within the region)\n    section_boundaries.sort(key=lambda x: x[\"index\"])\n\n    # Generate sections\n    sections = []\n    current_start_boundary = None\n\n    for i, boundary in enumerate(section_boundaries):\n        # If it's a start boundary and we don't have a current start\n        if boundary[\"type\"] == \"start\" and current_start_boundary is None:\n            current_start_boundary = boundary\n\n        # If it's an end boundary and we have a current start\n        elif boundary[\"type\"] == \"end\" and current_start_boundary is not None:\n            # Create a section from current_start to this boundary\n            start_element = current_start_boundary[\"element\"]\n            end_element = boundary[\"element\"]\n            # Use the helper, ensuring elements are from within the region\n            section = self.get_section_between(start_element, end_element, include_boundaries)\n            sections.append(section)\n            current_start_boundary = None  # Reset\n\n        # If it's another start boundary and we have a current start (split by starts only)\n        elif (\n            boundary[\"type\"] == \"start\"\n            and current_start_boundary is not None\n            and not end_elements\n        ):\n            # End the previous section just before this start boundary\n            start_element = current_start_boundary[\"element\"]\n            # Find the element immediately preceding this start in the sorted list\n            end_idx = boundary[\"index\"] - 1\n            if end_idx &gt;= 0 and end_idx &gt;= current_start_boundary[\"index\"]:\n                end_element = all_elements_in_region[end_idx]\n                section = self.get_section_between(\n                    start_element, end_element, include_boundaries\n                )\n                sections.append(section)\n            # Else: Section started and ended by consecutive start elements? Create empty?\n            # For now, just reset and start new section\n\n            # Start the new section\n            current_start_boundary = boundary\n\n    # Handle the last section if we have a current start\n    if current_start_boundary is not None:\n        start_element = current_start_boundary[\"element\"]\n        # End at the last element within the region\n        end_element = all_elements_in_region[-1]\n        section = self.get_section_between(start_element, end_element, include_boundaries)\n        sections.append(section)\n\n    return ElementCollection(sections)\n</code></pre> <code>natural_pdf.Region.get_text_table_cells(snap_tolerance=10, join_tolerance=3, min_words_vertical=3, min_words_horizontal=1, intersection_tolerance=3, expand_bbox=None, **kwargs)</code> <p>Analyzes text alignment to find table cells and returns them as temporary Region objects without adding them to the page.</p> <p>Parameters:</p> Name Type Description Default <code>snap_tolerance</code> <code>int</code> <p>Tolerance for snapping parallel lines.</p> <code>10</code> <code>join_tolerance</code> <code>int</code> <p>Tolerance for joining collinear lines.</p> <code>3</code> <code>min_words_vertical</code> <code>int</code> <p>Minimum words needed to define a vertical line.</p> <code>3</code> <code>min_words_horizontal</code> <code>int</code> <p>Minimum words needed to define a horizontal line.</p> <code>1</code> <code>intersection_tolerance</code> <code>int</code> <p>Tolerance for detecting line intersections.</p> <code>3</code> <code>expand_bbox</code> <code>Optional[Dict[str, int]]</code> <p>Optional dictionary to expand the search area slightly beyond          the region's exact bounds (e.g., {'left': 5, 'right': 5}).</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to       find_text_based_tables (e.g., specific x/y tolerances).</p> <code>{}</code> <p>Returns:</p> Type Description <code>ElementCollection[Region]</code> <p>An ElementCollection containing temporary Region objects for each detected cell,</p> <code>ElementCollection[Region]</code> <p>or an empty ElementCollection if no cells are found or an error occurs.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def get_text_table_cells(\n    self,\n    snap_tolerance: int = 10,\n    join_tolerance: int = 3,\n    min_words_vertical: int = 3,\n    min_words_horizontal: int = 1,\n    intersection_tolerance: int = 3,\n    expand_bbox: Optional[Dict[str, int]] = None,\n    **kwargs,\n) -&gt; \"ElementCollection[Region]\":\n    \"\"\"\n    Analyzes text alignment to find table cells and returns them as\n    temporary Region objects without adding them to the page.\n\n    Args:\n        snap_tolerance: Tolerance for snapping parallel lines.\n        join_tolerance: Tolerance for joining collinear lines.\n        min_words_vertical: Minimum words needed to define a vertical line.\n        min_words_horizontal: Minimum words needed to define a horizontal line.\n        intersection_tolerance: Tolerance for detecting line intersections.\n        expand_bbox: Optional dictionary to expand the search area slightly beyond\n                     the region's exact bounds (e.g., {'left': 5, 'right': 5}).\n        **kwargs: Additional keyword arguments passed to\n                  find_text_based_tables (e.g., specific x/y tolerances).\n\n    Returns:\n        An ElementCollection containing temporary Region objects for each detected cell,\n        or an empty ElementCollection if no cells are found or an error occurs.\n    \"\"\"\n    from natural_pdf.elements.element_collection import ElementCollection\n\n    # 1. Perform the analysis (or use cached results)\n    if \"text_table_structure\" in self.analyses:\n        analysis_results = self.analyses[\"text_table_structure\"]\n        logger.debug(\"get_text_table_cells: Using cached analysis results.\")\n    else:\n        analysis_results = self.analyze_text_table_structure(\n            snap_tolerance=snap_tolerance,\n            join_tolerance=join_tolerance,\n            min_words_vertical=min_words_vertical,\n            min_words_horizontal=min_words_horizontal,\n            intersection_tolerance=intersection_tolerance,\n            expand_bbox=expand_bbox,\n            **kwargs,\n        )\n\n    # 2. Check if analysis was successful and cells were found\n    if analysis_results is None or not analysis_results.get(\"cells\"):\n        logger.info(f\"Region {self.bbox}: No cells found by text table analysis.\")\n        return ElementCollection([])  # Return empty collection\n\n    # 3. Create temporary Region objects for each cell dictionary\n    cell_regions = []\n    for cell_data in analysis_results[\"cells\"]:\n        try:\n            # Use page.region to create the region object\n            # It expects left, top, right, bottom keys\n            cell_region = self.page.region(**cell_data)\n\n            # Set metadata on the temporary region\n            cell_region.region_type = \"table-cell\"\n            cell_region.normalized_type = \"table-cell\"\n            cell_region.model = \"pdfplumber-text\"\n            cell_region.source = \"volatile\"  # Indicate it's not managed/persistent\n            cell_region.parent_region = self  # Link back to the region it came from\n\n            cell_regions.append(cell_region)\n        except Exception as e:\n            logger.warning(f\"Could not create Region object for cell data {cell_data}: {e}\")\n\n    # 4. Return the list wrapped in an ElementCollection\n    logger.debug(f\"get_text_table_cells: Created {len(cell_regions)} temporary cell regions.\")\n    return ElementCollection(cell_regions)\n</code></pre> <code>natural_pdf.Region.highlight(label=None, color=None, use_color_cycling=False, annotate=None, existing='append')</code> <p>Highlight this region on the page.</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Optional[str]</code> <p>Optional label for the highlight</p> <code>None</code> <code>color</code> <code>Optional[Union[Tuple, str]]</code> <p>Color tuple/string for the highlight, or None to use automatic color</p> <code>None</code> <code>use_color_cycling</code> <code>bool</code> <p>Force color cycling even with no label (default: False)</p> <code>False</code> <code>annotate</code> <code>Optional[List[str]]</code> <p>List of attribute names to display on the highlight (e.g., ['confidence', 'type'])</p> <code>None</code> <code>existing</code> <code>str</code> <p>How to handle existing highlights ('append' or 'replace').</p> <code>'append'</code> <p>Returns:</p> Type Description <code>Region</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def highlight(\n    self,\n    label: Optional[str] = None,\n    color: Optional[Union[Tuple, str]] = None,\n    use_color_cycling: bool = False,\n    annotate: Optional[List[str]] = None,\n    existing: str = \"append\",\n) -&gt; \"Region\":\n    \"\"\"\n    Highlight this region on the page.\n\n    Args:\n        label: Optional label for the highlight\n        color: Color tuple/string for the highlight, or None to use automatic color\n        use_color_cycling: Force color cycling even with no label (default: False)\n        annotate: List of attribute names to display on the highlight (e.g., ['confidence', 'type'])\n        existing: How to handle existing highlights ('append' or 'replace').\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Access the highlighter service correctly\n    highlighter = self.page._highlighter\n\n    # Prepare common arguments\n    highlight_args = {\n        \"page_index\": self.page.index,\n        \"color\": color,\n        \"label\": label,\n        \"use_color_cycling\": use_color_cycling,\n        \"element\": self,  # Pass the region itself so attributes can be accessed\n        \"annotate\": annotate,\n        \"existing\": existing,\n    }\n\n    # Call the appropriate service method\n    if self.has_polygon:\n        highlight_args[\"polygon\"] = self.polygon\n        highlighter.add_polygon(**highlight_args)\n    else:\n        highlight_args[\"bbox\"] = self.bbox\n        highlighter.add(**highlight_args)\n\n    return self\n</code></pre> <code>natural_pdf.Region.intersects(element)</code> <p>Check if this region intersects with an element (any overlap).</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>Element</code> <p>Element to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the element overlaps with the region at all, False otherwise</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def intersects(self, element: \"Element\") -&gt; bool:\n    \"\"\"\n    Check if this region intersects with an element (any overlap).\n\n    Args:\n        element: Element to check\n\n    Returns:\n        True if the element overlaps with the region at all, False otherwise\n    \"\"\"\n    # Check if element is on the same page\n    if not hasattr(element, \"page\") or element.page != self._page:\n        return False\n\n    # Ensure element has necessary attributes\n    if not all(hasattr(element, attr) for attr in [\"x0\", \"x1\", \"top\", \"bottom\"]):\n        return False  # Cannot determine position\n\n    # For rectangular regions, check for bbox overlap\n    if not self.has_polygon:\n        return (\n            self.x0 &lt; element.x1\n            and self.x1 &gt; element.x0\n            and self.top &lt; element.bottom\n            and self.bottom &gt; element.top\n        )\n\n    # For polygon regions, check if any corner of the element is inside the polygon\n    element_corners = [\n        (element.x0, element.top),  # top-left\n        (element.x1, element.top),  # top-right\n        (element.x1, element.bottom),  # bottom-right\n        (element.x0, element.bottom),  # bottom-left\n    ]\n\n    # First check if any element corner is inside the polygon\n    if any(self.is_point_inside(x, y) for x, y in element_corners):\n        return True\n\n    # Also check if any polygon corner is inside the element's rectangle\n    for x, y in self.polygon:\n        if element.x0 &lt;= x &lt;= element.x1 and element.top &lt;= y &lt;= element.bottom:\n            return True\n\n    # Also check if any polygon edge intersects with any rectangle edge\n    # This is a simplification - for complex cases, we'd need a full polygon-rectangle\n    # intersection algorithm\n\n    # For now, return True if bounding boxes overlap (approximation for polygon-rectangle case)\n    return (\n        self.x0 &lt; element.x1\n        and self.x1 &gt; element.x0\n        and self.top &lt; element.bottom\n        and self.bottom &gt; element.top\n    )\n</code></pre> <code>natural_pdf.Region.is_element_center_inside(element)</code> <p>Check if the center point of an element's bounding box is inside this region.</p> <p>Parameters:</p> Name Type Description Default <code>element</code> <code>Element</code> <p>Element to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the element's center point is inside the region, False otherwise.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def is_element_center_inside(self, element: \"Element\") -&gt; bool:\n    \"\"\"\n    Check if the center point of an element's bounding box is inside this region.\n\n    Args:\n        element: Element to check\n\n    Returns:\n        True if the element's center point is inside the region, False otherwise.\n    \"\"\"\n    # Check if element is on the same page\n    if not hasattr(element, \"page\") or element.page != self._page:\n        return False\n\n    # Ensure element has necessary attributes\n    if not all(hasattr(element, attr) for attr in [\"x0\", \"x1\", \"top\", \"bottom\"]):\n        logger.warning(\n            f\"Element {element} lacks bounding box attributes. Cannot check center point.\"\n        )\n        return False  # Cannot determine position\n\n    # Calculate center point\n    center_x = (element.x0 + element.x1) / 2\n    center_y = (element.top + element.bottom) / 2\n\n    # Use the existing is_point_inside check\n    return self.is_point_inside(center_x, center_y)\n</code></pre> <code>natural_pdf.Region.is_point_inside(x, y)</code> <p>Check if a point is inside this region using ray casting algorithm for polygons.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>X coordinate of the point</p> required <code>y</code> <code>float</code> <p>Y coordinate of the point</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the point is inside the region</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def is_point_inside(self, x: float, y: float) -&gt; bool:\n    \"\"\"\n    Check if a point is inside this region using ray casting algorithm for polygons.\n\n    Args:\n        x: X coordinate of the point\n        y: Y coordinate of the point\n\n    Returns:\n        bool: True if the point is inside the region\n    \"\"\"\n    if not self.has_polygon:\n        return (self.x0 &lt;= x &lt;= self.x1) and (self.top &lt;= y &lt;= self.bottom)\n\n    # Ray casting algorithm\n    inside = False\n    j = len(self.polygon) - 1\n\n    for i in range(len(self.polygon)):\n        if ((self.polygon[i][1] &gt; y) != (self.polygon[j][1] &gt; y)) and (\n            x\n            &lt; (self.polygon[j][0] - self.polygon[i][0])\n            * (y - self.polygon[i][1])\n            / (self.polygon[j][1] - self.polygon[i][1])\n            + self.polygon[i][0]\n        ):\n            inside = not inside\n        j = i\n\n    return inside\n</code></pre> <code>natural_pdf.Region.left(width=None, height='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Select region to the left of this region.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[float]</code> <p>Width of the region to the left, in points</p> <code>None</code> <code>height</code> <code>str</code> <p>Height mode - \"full\" for full page height or \"element\" for element height</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this region in the result (default: False)</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify a left boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region (default: True)</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Region</code> <p>Region object representing the area to the left</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def left(\n    self,\n    width: Optional[float] = None,\n    height: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"Region\":\n    \"\"\"\n    Select region to the left of this region.\n\n    Args:\n        width: Width of the region to the left, in points\n        height: Height mode - \"full\" for full page height or \"element\" for element height\n        include_source: Whether to include this region in the result (default: False)\n        until: Optional selector string to specify a left boundary element\n        include_endpoint: Whether to include the boundary element in the region (default: True)\n        **kwargs: Additional parameters\n\n    Returns:\n        Region object representing the area to the left\n    \"\"\"\n    return self._direction(\n        direction=\"left\",\n        size=width,\n        cross_size=height,\n        include_source=include_source,\n        until=until,\n        include_endpoint=include_endpoint,\n        **kwargs,\n    )\n</code></pre> <code>natural_pdf.Region.right(width=None, height='full', include_source=False, until=None, include_endpoint=True, **kwargs)</code> <p>Select region to the right of this region.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>Optional[float]</code> <p>Width of the region to the right, in points</p> <code>None</code> <code>height</code> <code>str</code> <p>Height mode - \"full\" for full page height or \"element\" for element height</p> <code>'full'</code> <code>include_source</code> <code>bool</code> <p>Whether to include this region in the result (default: False)</p> <code>False</code> <code>until</code> <code>Optional[str]</code> <p>Optional selector string to specify a right boundary element</p> <code>None</code> <code>include_endpoint</code> <code>bool</code> <p>Whether to include the boundary element in the region (default: True)</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>Region</code> <p>Region object representing the area to the right</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def right(\n    self,\n    width: Optional[float] = None,\n    height: str = \"full\",\n    include_source: bool = False,\n    until: Optional[str] = None,\n    include_endpoint: bool = True,\n    **kwargs,\n) -&gt; \"Region\":\n    \"\"\"\n    Select region to the right of this region.\n\n    Args:\n        width: Width of the region to the right, in points\n        height: Height mode - \"full\" for full page height or \"element\" for element height\n        include_source: Whether to include this region in the result (default: False)\n        until: Optional selector string to specify a right boundary element\n        include_endpoint: Whether to include the boundary element in the region (default: True)\n        **kwargs: Additional parameters\n\n    Returns:\n        Region object representing the area to the right\n    \"\"\"\n    return self._direction(\n        direction=\"right\",\n        size=width,\n        cross_size=height,\n        include_source=include_source,\n        until=until,\n        include_endpoint=include_endpoint,\n        **kwargs,\n    )\n</code></pre> <code>natural_pdf.Region.save(filename, resolution=None, labels=True, legend_position='right')</code> <p>Save the page with this region highlighted to an image file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to save the image to</p> required <code>resolution</code> <code>Optional[float]</code> <p>Resolution in DPI for rendering (default: uses global options, fallback to 144 DPI)</p> <code>None</code> <code>labels</code> <code>bool</code> <p>Whether to include a legend for labels</p> <code>True</code> <code>legend_position</code> <code>str</code> <p>Position of the legend</p> <code>'right'</code> <p>Returns:</p> Type Description <code>Region</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def save(\n    self,\n    filename: str,\n    resolution: Optional[float] = None,\n    labels: bool = True,\n    legend_position: str = \"right\",\n) -&gt; \"Region\":\n    \"\"\"\n    Save the page with this region highlighted to an image file.\n\n    Args:\n        filename: Path to save the image to\n        resolution: Resolution in DPI for rendering (default: uses global options, fallback to 144 DPI)\n        labels: Whether to include a legend for labels\n        legend_position: Position of the legend\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Apply global options as defaults\n    import natural_pdf\n\n    if resolution is None:\n        if natural_pdf.options.image.resolution is not None:\n            resolution = natural_pdf.options.image.resolution\n        else:\n            resolution = 144  # Default resolution when none specified\n\n    # Highlight this region if not already highlighted\n    self.highlight()\n\n    # Save the highlighted image\n    self._page.save_image(\n        filename, resolution=resolution, labels=labels, legend_position=legend_position\n    )\n    return self\n</code></pre> <code>natural_pdf.Region.save_image(filename, resolution=None, crop=False, include_highlights=True, **kwargs)</code> <p>Save an image of just this region to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to save the image to</p> required <code>resolution</code> <code>Optional[float]</code> <p>Resolution in DPI for rendering (default: uses global options, fallback to 144 DPI)</p> <code>None</code> <code>crop</code> <code>bool</code> <p>If True, only crop the region without highlighting its boundaries</p> <code>False</code> <code>include_highlights</code> <code>bool</code> <p>Whether to include existing highlights (default: True)</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters for rendering</p> <code>{}</code> <p>Returns:</p> Type Description <code>Region</code> <p>Self for method chaining</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def save_image(\n    self,\n    filename: str,\n    resolution: Optional[float] = None,\n    crop: bool = False,\n    include_highlights: bool = True,\n    **kwargs,\n) -&gt; \"Region\":\n    \"\"\"\n    Save an image of just this region to a file.\n\n    Args:\n        filename: Path to save the image to\n        resolution: Resolution in DPI for rendering (default: uses global options, fallback to 144 DPI)\n        crop: If True, only crop the region without highlighting its boundaries\n        include_highlights: Whether to include existing highlights (default: True)\n        **kwargs: Additional parameters for rendering\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    # Apply global options as defaults\n    import natural_pdf\n\n    if resolution is None:\n        if natural_pdf.options.image.resolution is not None:\n            resolution = natural_pdf.options.image.resolution\n        else:\n            resolution = 144  # Default resolution when none specified\n\n    # Use export() to save the image\n    if include_highlights:\n        # With highlights, use export() which includes them\n        self.export(\n            path=filename,\n            resolution=resolution,\n            crop=crop,\n            **kwargs,\n        )\n    else:\n        # Without highlights, use render() and save manually\n        image = self.render(resolution=resolution, crop=crop, **kwargs)\n        if image:\n            image.save(filename)\n        else:\n            logger.error(f\"Failed to render region image for saving to {filename}\")\n\n    return self\n</code></pre> <code>natural_pdf.Region.to_text_element(text_content=None, source_label='derived_from_region', object_type='word', default_font_size=10.0, default_font_name='RegionContent', confidence=None, add_to_page=False)</code> <p>Creates a new TextElement object based on this region's geometry.</p> <p>The text for the new TextElement can be provided directly, generated by a callback function, or left as None.</p> <p>Parameters:</p> Name Type Description Default <code>text_content</code> <code>Optional[Union[str, Callable[[Region], Optional[str]]]]</code> <ul> <li>If a string, this will be the text of the new TextElement.</li> <li>If a callable, it will be called with this region instance   and its return value (a string or None) will be the text.</li> <li>If None (default), the TextElement's text will be None.</li> </ul> <code>None</code> <code>source_label</code> <code>str</code> <p>The 'source' attribute for the new TextElement.</p> <code>'derived_from_region'</code> <code>object_type</code> <code>str</code> <p>The 'object_type' for the TextElement's data dict          (e.g., \"word\", \"char\").</p> <code>'word'</code> <code>default_font_size</code> <code>float</code> <p>Placeholder font size if text is generated.</p> <code>10.0</code> <code>default_font_name</code> <code>str</code> <p>Placeholder font name if text is generated.</p> <code>'RegionContent'</code> <code>confidence</code> <code>Optional[float]</code> <p>Confidence score for the text. If text_content is None,         defaults to 0.0. If text is provided/generated, defaults to 1.0         unless specified.</p> <code>None</code> <code>add_to_page</code> <code>bool</code> <p>If True, the created TextElement will be added to the          region's parent page. (Default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>TextElement</code> <p>A new TextElement instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the region does not have a valid 'page' attribute.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def to_text_element(\n    self,\n    text_content: Optional[Union[str, Callable[[\"Region\"], Optional[str]]]] = None,\n    source_label: str = \"derived_from_region\",\n    object_type: str = \"word\",  # Or \"char\", controls how it's categorized\n    default_font_size: float = 10.0,\n    default_font_name: str = \"RegionContent\",\n    confidence: Optional[float] = None,  # Allow overriding confidence\n    add_to_page: bool = False,  # NEW: Option to add to page\n) -&gt; \"TextElement\":\n    \"\"\"\n    Creates a new TextElement object based on this region's geometry.\n\n    The text for the new TextElement can be provided directly,\n    generated by a callback function, or left as None.\n\n    Args:\n        text_content:\n            - If a string, this will be the text of the new TextElement.\n            - If a callable, it will be called with this region instance\n              and its return value (a string or None) will be the text.\n            - If None (default), the TextElement's text will be None.\n        source_label: The 'source' attribute for the new TextElement.\n        object_type: The 'object_type' for the TextElement's data dict\n                     (e.g., \"word\", \"char\").\n        default_font_size: Placeholder font size if text is generated.\n        default_font_name: Placeholder font name if text is generated.\n        confidence: Confidence score for the text. If text_content is None,\n                    defaults to 0.0. If text is provided/generated, defaults to 1.0\n                    unless specified.\n        add_to_page: If True, the created TextElement will be added to the\n                     region's parent page. (Default: False)\n\n    Returns:\n        A new TextElement instance.\n\n    Raises:\n        ValueError: If the region does not have a valid 'page' attribute.\n    \"\"\"\n    actual_text: Optional[str] = None\n    if isinstance(text_content, str):\n        actual_text = text_content\n    elif callable(text_content):\n        try:\n            actual_text = text_content(self)\n        except Exception as e:\n            logger.error(\n                f\"Error executing text_content callback for region {self.bbox}: {e}\",\n                exc_info=True,\n            )\n            actual_text = None  # Ensure actual_text is None on error\n\n    final_confidence = confidence\n    if final_confidence is None:\n        final_confidence = 1.0 if actual_text is not None and actual_text.strip() else 0.0\n\n    if not hasattr(self, \"page\") or self.page is None:\n        raise ValueError(\"Region must have a valid 'page' attribute to create a TextElement.\")\n\n    # Create character dictionaries for the text\n    char_dicts = []\n    if actual_text:\n        # Create a single character dict that spans the entire region\n        # This is a simplified approach - OCR engines typically create one per character\n        char_dict = {\n            \"text\": actual_text,\n            \"x0\": self.x0,\n            \"top\": self.top,\n            \"x1\": self.x1,\n            \"bottom\": self.bottom,\n            \"width\": self.width,\n            \"height\": self.height,\n            \"object_type\": \"char\",\n            \"page_number\": self.page.page_number,\n            \"fontname\": default_font_name,\n            \"size\": default_font_size,\n            \"upright\": True,\n            \"direction\": 1,\n            \"adv\": self.width,\n            \"source\": source_label,\n            \"confidence\": final_confidence,\n            \"stroking_color\": (0, 0, 0),\n            \"non_stroking_color\": (0, 0, 0),\n        }\n        char_dicts.append(char_dict)\n\n    elem_data = {\n        \"text\": actual_text,\n        \"x0\": self.x0,\n        \"top\": self.top,\n        \"x1\": self.x1,\n        \"bottom\": self.bottom,\n        \"width\": self.width,\n        \"height\": self.height,\n        \"object_type\": object_type,\n        \"page_number\": self.page.page_number,\n        \"stroking_color\": getattr(self, \"stroking_color\", (0, 0, 0)),\n        \"non_stroking_color\": getattr(self, \"non_stroking_color\", (0, 0, 0)),\n        \"fontname\": default_font_name,\n        \"size\": default_font_size,\n        \"upright\": True,\n        \"direction\": 1,\n        \"adv\": self.width,\n        \"source\": source_label,\n        \"confidence\": final_confidence,\n        \"_char_dicts\": char_dicts,\n    }\n    text_element = TextElement(elem_data, self.page)\n\n    if add_to_page:\n        if hasattr(self.page, \"_element_mgr\") and self.page._element_mgr is not None:\n            add_as_type = (\n                \"words\"\n                if object_type == \"word\"\n                else \"chars\" if object_type == \"char\" else object_type\n            )\n            # REMOVED try-except block around add_element\n            self.page._element_mgr.add_element(text_element, element_type=add_as_type)\n            logger.debug(\n                f\"TextElement created from region {self.bbox} and added to page {self.page.page_number} as {add_as_type}.\"\n            )\n            # Also add character dictionaries to the chars collection\n            if char_dicts and object_type == \"word\":\n                for char_dict in char_dicts:\n                    self.page._element_mgr.add_element(char_dict, element_type=\"chars\")\n        else:\n            page_num_str = (\n                str(self.page.page_number) if hasattr(self.page, \"page_number\") else \"N/A\"\n            )\n            logger.warning(\n                f\"Cannot add TextElement to page: Page {page_num_str} for region {self.bbox} is missing '_element_mgr'.\"\n            )\n\n    return text_element\n</code></pre> <code>natural_pdf.Region.trim(padding=1, threshold=0.95, resolution=None, pre_shrink=0.5)</code> <p>Trim visual whitespace from the edges of this region.</p> <p>Similar to Python's string .strip() method, but for visual whitespace in the region image. Uses pixel analysis to detect rows/columns that are predominantly whitespace.</p> <p>Parameters:</p> Name Type Description Default <code>padding</code> <code>int</code> <p>Number of pixels to keep as padding after trimming (default: 1)</p> <code>1</code> <code>threshold</code> <code>float</code> <p>Threshold for considering a row/column as whitespace (0.0-1.0, default: 0.95)       Higher values mean more strict whitespace detection.       E.g., 0.95 means if 95% of pixels in a row/column are white, consider it whitespace.</p> <code>0.95</code> <code>resolution</code> <code>Optional[float]</code> <p>Resolution for image rendering in DPI (default: uses global options, fallback to 144 DPI)</p> <code>None</code> <code>pre_shrink</code> <code>float</code> <p>Amount to shrink region before trimming, then expand back after (default: 0.5)        This helps avoid detecting box borders/slivers as content.</p> <code>0.5</code> <code>natural_pdf.Region.update_text(transform, *, selector='text', apply_exclusions=False)</code> <p>Apply transform to every text element matched by selector inside this region.</p> <p>The heavy lifting is delegated to :py:meth:<code>TextMixin.update_text</code>; this override simply ensures the search is scoped to the region.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def update_text(\n    self,\n    transform: Callable[[Any], Optional[str]],\n    *,\n    selector: str = \"text\",\n    apply_exclusions: bool = False,\n) -&gt; \"Region\":\n    \"\"\"Apply *transform* to every text element matched by *selector* inside this region.\n\n    The heavy lifting is delegated to :py:meth:`TextMixin.update_text`; this\n    override simply ensures the search is scoped to the region.\n    \"\"\"\n\n    return TextMixin.update_text(\n        self, transform, selector=selector, apply_exclusions=apply_exclusions\n    )\n</code></pre> <code>natural_pdf.Region.viewer(*, resolution=150, include_chars=False, include_attributes=None)</code> <p>Create an interactive ipywidget viewer for this specific region.</p> <p>The method renders the region to an image (cropped to the region bounds) and overlays all elements that intersect the region (optionally excluding noisy character-level elements).  The resulting widget offers the same zoom / pan experience as :py:meth:<code>Page.viewer</code> but scoped to the region.</p>"},{"location":"api/#natural_pdf.Region.apply_custom_ocr--using-with-an-llm","title":"Using with an LLM","text":"<p>def ocr_with_llm(region):     image = region.render(resolution=300, crop=True)     # Call your LLM API here     return llm_client.ocr(image)</p> <p>region.apply_custom_ocr(ocr_with_llm)</p>"},{"location":"api/#natural_pdf.Region.apply_custom_ocr--using-with-a-custom-ocr-service","title":"Using with a custom OCR service","text":"<p>def ocr_with_service(region):     img_bytes = region.render(crop=True).tobytes()     response = ocr_service.process(img_bytes)     return response.text</p> <p>region.apply_custom_ocr(ocr_with_service, source_label=\"my-ocr-service\")</p>"},{"location":"api/#natural_pdf.Region.apply_ocr--examples","title":"Examples","text":"<pre><code>def llm_ocr(region):\n    image = region.render(resolution=300, crop=True)\n    return my_llm_client.ocr(image)\nregion.apply_ocr(function=llm_ocr)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>replace</code> <p>Whether to remove existing OCR elements first (default <code>True</code>).</p> <code>True</code> <code>**ocr_params</code> <p>Parameters for the built-in OCR manager or the special           <code>function</code>/<code>ocr_function</code> keyword to trigger custom mode.</p> <code>{}</code>"},{"location":"api/#natural_pdf.Region.apply_ocr--returns","title":"Returns","text":"<pre><code>Self \u2013 for chaining.\n</code></pre> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def apply_ocr(self, replace=True, **ocr_params) -&gt; \"Region\":\n    \"\"\"\n    Apply OCR to this region and return the created text elements.\n\n    This method supports two modes:\n    1. **Built-in OCR Engines** (default) \u2013 identical to previous behaviour. Pass typical\n       parameters like ``engine='easyocr'`` or ``languages=['en']`` and the method will\n       route the request through :class:`OCRManager`.\n    2. **Custom OCR Function** \u2013 pass a *callable* under the keyword ``function`` (or\n       ``ocr_function``). The callable will receive *this* Region instance and should\n       return the extracted text (``str``) or ``None``.  Internally the call is\n       delegated to :pymeth:`apply_custom_ocr` so the same logic (replacement, element\n       creation, etc.) is re-used.\n\n    Examples\n    ---------\n    ```python\n    def llm_ocr(region):\n        image = region.render(resolution=300, crop=True)\n        return my_llm_client.ocr(image)\n    region.apply_ocr(function=llm_ocr)\n    ```\n\n    Args:\n        replace: Whether to remove existing OCR elements first (default ``True``).\n        **ocr_params: Parameters for the built-in OCR manager *or* the special\n                      ``function``/``ocr_function`` keyword to trigger custom mode.\n\n    Returns\n    -------\n        Self \u2013 for chaining.\n    \"\"\"\n    # --- Custom OCR function path --------------------------------------------------\n    custom_func = ocr_params.pop(\"function\", None) or ocr_params.pop(\"ocr_function\", None)\n    if callable(custom_func):\n        # Delegate to the specialised helper while preserving key kwargs\n        return self.apply_custom_ocr(\n            ocr_function=custom_func,\n            source_label=ocr_params.pop(\"source_label\", \"custom-ocr\"),\n            replace=replace,\n            confidence=ocr_params.pop(\"confidence\", None),\n            add_to_page=ocr_params.pop(\"add_to_page\", True),\n        )\n\n    # --- Original built-in OCR engine path (unchanged except docstring) ------------\n    # Ensure OCRManager is available\n    if not hasattr(self.page._parent, \"_ocr_manager\") or self.page._parent._ocr_manager is None:\n        logger.error(\"OCRManager not available on parent PDF. Cannot apply OCR to region.\")\n        return self\n\n    # If replace is True, find and remove existing OCR elements in this region\n    if replace:\n        logger.info(\n            f\"Region {self.bbox}: Removing existing OCR elements before applying new OCR.\"\n        )\n\n        # --- Robust removal: iterate through all OCR elements on the page and\n        #     remove those that overlap this region. This avoids reliance on\n        #     identity\u2010based look-ups that can break if the ElementManager\n        #     rebuilt its internal lists.\n\n        removed_count = 0\n\n        # Helper to remove a single element safely\n        def _safe_remove(elem):\n            nonlocal removed_count\n            success = False\n            if hasattr(elem, \"page\") and hasattr(elem.page, \"_element_mgr\"):\n                etype = getattr(elem, \"object_type\", \"word\")\n                if etype == \"word\":\n                    etype_key = \"words\"\n                elif etype == \"char\":\n                    etype_key = \"chars\"\n                else:\n                    etype_key = etype + \"s\" if not etype.endswith(\"s\") else etype\n                try:\n                    success = elem.page._element_mgr.remove_element(elem, etype_key)\n                except Exception:\n                    success = False\n            if success:\n                removed_count += 1\n\n        # Remove OCR WORD elements overlapping region\n        for word in list(self.page._element_mgr.words):\n            if getattr(word, \"source\", None) == \"ocr\" and self.intersects(word):\n                _safe_remove(word)\n\n        # Remove OCR CHAR dicts overlapping region\n        for char in list(self.page._element_mgr.chars):\n            # char can be dict or TextElement; normalise\n            char_src = (\n                char.get(\"source\") if isinstance(char, dict) else getattr(char, \"source\", None)\n            )\n            if char_src == \"ocr\":\n                # Rough bbox for dicts\n                if isinstance(char, dict):\n                    cx0, ctop, cx1, cbottom = (\n                        char.get(\"x0\", 0),\n                        char.get(\"top\", 0),\n                        char.get(\"x1\", 0),\n                        char.get(\"bottom\", 0),\n                    )\n                else:\n                    cx0, ctop, cx1, cbottom = char.x0, char.top, char.x1, char.bottom\n                # Quick overlap check\n                if not (\n                    cx1 &lt; self.x0 or cx0 &gt; self.x1 or cbottom &lt; self.top or ctop &gt; self.bottom\n                ):\n                    _safe_remove(char)\n\n        logger.info(\n            f\"Region {self.bbox}: Removed {removed_count} existing OCR elements (words &amp; chars) before re-applying OCR.\"\n        )\n\n    ocr_mgr = self.page._parent._ocr_manager\n\n    # Determine rendering resolution from parameters\n    final_resolution = ocr_params.get(\"resolution\")\n    if final_resolution is None and hasattr(self.page, \"_parent\") and self.page._parent:\n        final_resolution = getattr(self.page._parent, \"_config\", {}).get(\"resolution\", 150)\n    elif final_resolution is None:\n        final_resolution = 150\n    logger.debug(\n        f\"Region {self.bbox}: Applying OCR with resolution {final_resolution} DPI and params: {ocr_params}\"\n    )\n\n    # Render the page region to an image using the determined resolution\n    try:\n        # Use render() for clean image without highlights, with cropping\n        region_image = self.render(resolution=final_resolution, crop=True)\n        if not region_image:\n            logger.error(\"Failed to render region to image for OCR.\")\n            return self\n        logger.debug(f\"Region rendered to image size: {region_image.size}\")\n    except Exception as e:\n        logger.error(f\"Error rendering region to image for OCR: {e}\", exc_info=True)\n        return self\n\n    # Prepare args for the OCR Manager\n    manager_args = {\n        \"images\": region_image,\n        \"engine\": ocr_params.get(\"engine\"),\n        \"languages\": ocr_params.get(\"languages\"),\n        \"min_confidence\": ocr_params.get(\"min_confidence\"),\n        \"device\": ocr_params.get(\"device\"),\n        \"options\": ocr_params.get(\"options\"),\n        \"detect_only\": ocr_params.get(\"detect_only\"),\n    }\n    manager_args = {k: v for k, v in manager_args.items() if v is not None}\n\n    # Run OCR on this region's image using the manager\n    results = ocr_mgr.apply_ocr(**manager_args)\n    if not isinstance(results, list):\n        logger.error(\n            f\"OCRManager returned unexpected type for single region image: {type(results)}\"\n        )\n        return self\n    logger.debug(f\"Region OCR processing returned {len(results)} results.\")\n\n    # Convert results to TextElements\n    scale_x = self.width / region_image.width if region_image.width &gt; 0 else 1.0\n    scale_y = self.height / region_image.height if region_image.height &gt; 0 else 1.0\n    logger.debug(f\"Region OCR scaling factors (PDF/Img): x={scale_x:.2f}, y={scale_y:.2f}\")\n    created_elements = []\n    for result in results:\n        try:\n            img_x0, img_top, img_x1, img_bottom = map(float, result[\"bbox\"])\n            pdf_height = (img_bottom - img_top) * scale_y\n            page_x0 = self.x0 + (img_x0 * scale_x)\n            page_top = self.top + (img_top * scale_y)\n            page_x1 = self.x0 + (img_x1 * scale_x)\n            page_bottom = self.top + (img_bottom * scale_y)\n            raw_conf = result.get(\"confidence\")\n            # Convert confidence to float unless it is None/invalid\n            try:\n                confidence_val = float(raw_conf) if raw_conf is not None else None\n            except (TypeError, ValueError):\n                confidence_val = None\n\n            text_val = result.get(\"text\")  # May legitimately be None in detect_only mode\n\n            element_data = {\n                \"text\": text_val,\n                \"x0\": page_x0,\n                \"top\": page_top,\n                \"x1\": page_x1,\n                \"bottom\": page_bottom,\n                \"width\": page_x1 - page_x0,\n                \"height\": page_bottom - page_top,\n                \"object_type\": \"word\",\n                \"source\": \"ocr\",\n                \"confidence\": confidence_val,\n                \"fontname\": \"OCR\",\n                \"size\": round(pdf_height) if pdf_height &gt; 0 else 10.0,\n                \"page_number\": self.page.number,\n                \"bold\": False,\n                \"italic\": False,\n                \"upright\": True,\n                \"doctop\": page_top + self.page._page.initial_doctop,\n            }\n            ocr_char_dict = element_data.copy()\n            ocr_char_dict[\"object_type\"] = \"char\"\n            ocr_char_dict.setdefault(\"adv\", ocr_char_dict.get(\"width\", 0))\n            element_data[\"_char_dicts\"] = [ocr_char_dict]\n            from natural_pdf.elements.text import TextElement\n\n            elem = TextElement(element_data, self.page)\n            created_elements.append(elem)\n            self.page._element_mgr.add_element(elem, element_type=\"words\")\n            self.page._element_mgr.add_element(ocr_char_dict, element_type=\"chars\")\n        except Exception as e:\n            logger.error(\n                f\"Failed to convert region OCR result to element: {result}. Error: {e}\",\n                exc_info=True,\n            )\n    logger.info(f\"Region {self.bbox}: Added {len(created_elements)} elements from OCR.\")\n    return self\n</code></pre>"},{"location":"api/#natural_pdf.Region.clip--clip-to-another-regions-bounds","title":"Clip to another region's bounds","text":"<p>clipped = region.clip(container_region)</p>"},{"location":"api/#natural_pdf.Region.clip--clip-to-any-elements-bounds","title":"Clip to any element's bounds","text":"<p>clipped = region.clip(text_element)</p>"},{"location":"api/#natural_pdf.Region.clip--clip-to-specific-coordinates","title":"Clip to specific coordinates","text":"<p>clipped = region.clip(left=100, right=400)</p>"},{"location":"api/#natural_pdf.Region.clip--mix-object-bounds-with-specific-overrides","title":"Mix object bounds with specific overrides","text":"<p>clipped = region.clip(obj=container, bottom=page.height/2)</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def clip(\n    self,\n    obj: Optional[Any] = None,\n    left: Optional[float] = None,\n    top: Optional[float] = None,\n    right: Optional[float] = None,\n    bottom: Optional[float] = None,\n) -&gt; \"Region\":\n    \"\"\"\n    Clip this region to specific bounds, either from another object with bbox or explicit coordinates.\n\n    The clipped region will be constrained to not exceed the specified boundaries.\n    You can provide either an object with bounding box properties, specific coordinates, or both.\n    When both are provided, explicit coordinates take precedence.\n\n    Args:\n        obj: Optional object with bbox properties (Region, Element, TextElement, etc.)\n        left: Optional left boundary (x0) to clip to\n        top: Optional top boundary to clip to\n        right: Optional right boundary (x1) to clip to\n        bottom: Optional bottom boundary to clip to\n\n    Returns:\n        New Region with bounds clipped to the specified constraints\n\n    Examples:\n        # Clip to another region's bounds\n        clipped = region.clip(container_region)\n\n        # Clip to any element's bounds\n        clipped = region.clip(text_element)\n\n        # Clip to specific coordinates\n        clipped = region.clip(left=100, right=400)\n\n        # Mix object bounds with specific overrides\n        clipped = region.clip(obj=container, bottom=page.height/2)\n    \"\"\"\n    from natural_pdf.elements.base import extract_bbox\n\n    # Start with current region bounds\n    clip_x0 = self.x0\n    clip_top = self.top\n    clip_x1 = self.x1\n    clip_bottom = self.bottom\n\n    # Apply object constraints if provided\n    if obj is not None:\n        obj_bbox = extract_bbox(obj)\n        if obj_bbox is not None:\n            obj_x0, obj_top, obj_x1, obj_bottom = obj_bbox\n            # Constrain to the intersection with the provided object\n            clip_x0 = max(clip_x0, obj_x0)\n            clip_top = max(clip_top, obj_top)\n            clip_x1 = min(clip_x1, obj_x1)\n            clip_bottom = min(clip_bottom, obj_bottom)\n        else:\n            logger.warning(\n                f\"Region {self.bbox}: Cannot extract bbox from clipping object {type(obj)}. \"\n                \"Object must have bbox property or x0/top/x1/bottom attributes.\"\n            )\n\n    # Apply explicit coordinate constraints (these take precedence)\n    if left is not None:\n        clip_x0 = max(clip_x0, left)\n    if top is not None:\n        clip_top = max(clip_top, top)\n    if right is not None:\n        clip_x1 = min(clip_x1, right)\n    if bottom is not None:\n        clip_bottom = min(clip_bottom, bottom)\n\n    # Ensure valid coordinates\n    if clip_x1 &lt;= clip_x0 or clip_bottom &lt;= clip_top:\n        logger.warning(\n            f\"Region {self.bbox}: Clipping resulted in invalid dimensions \"\n            f\"({clip_x0}, {clip_top}, {clip_x1}, {clip_bottom}). Returning minimal region.\"\n        )\n        # Return a minimal region at the clip area's top-left\n        return Region(self.page, (clip_x0, clip_top, clip_x0, clip_top))\n\n    # Create the clipped region\n    clipped_region = Region(self.page, (clip_x0, clip_top, clip_x1, clip_bottom))\n\n    # Copy relevant metadata\n    clipped_region.region_type = self.region_type\n    clipped_region.normalized_type = self.normalized_type\n    clipped_region.confidence = self.confidence\n    clipped_region.model = self.model\n    clipped_region.name = self.name\n    clipped_region.label = self.label\n    clipped_region.source = \"clipped\"  # Indicate this is a derived region\n    clipped_region.parent_region = self\n\n    logger.debug(\n        f\"Region {self.bbox}: Clipped to {clipped_region.bbox} \"\n        f\"(constraints: obj={type(obj).__name__ if obj else None}, \"\n        f\"left={left}, top={top}, right={right}, bottom={bottom})\"\n    )\n    return clipped_region\n</code></pre>"},{"location":"api/#natural_pdf.Region.trim--returns","title":"Returns","text":"<p>New Region with visual whitespace trimmed from all edges</p>"},{"location":"api/#natural_pdf.Region.trim--examples","title":"Examples","text":"<pre><code># Basic trimming with 1 pixel padding and 0.5px pre-shrink\ntrimmed = region.trim()\n\n# More aggressive trimming with no padding and no pre-shrink\ntight = region.trim(padding=0, threshold=0.9, pre_shrink=0)\n\n# Conservative trimming with more padding\nloose = region.trim(padding=3, threshold=0.98)\n</code></pre> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def trim(\n    self,\n    padding: int = 1,\n    threshold: float = 0.95,\n    resolution: Optional[float] = None,\n    pre_shrink: float = 0.5,\n) -&gt; \"Region\":\n    \"\"\"\n    Trim visual whitespace from the edges of this region.\n\n    Similar to Python's string .strip() method, but for visual whitespace in the region image.\n    Uses pixel analysis to detect rows/columns that are predominantly whitespace.\n\n    Args:\n        padding: Number of pixels to keep as padding after trimming (default: 1)\n        threshold: Threshold for considering a row/column as whitespace (0.0-1.0, default: 0.95)\n                  Higher values mean more strict whitespace detection.\n                  E.g., 0.95 means if 95% of pixels in a row/column are white, consider it whitespace.\n        resolution: Resolution for image rendering in DPI (default: uses global options, fallback to 144 DPI)\n        pre_shrink: Amount to shrink region before trimming, then expand back after (default: 0.5)\n                   This helps avoid detecting box borders/slivers as content.\n\n    Returns\n    ------\n\n    New Region with visual whitespace trimmed from all edges\n\n    Examples\n    --------\n\n    ```python\n    # Basic trimming with 1 pixel padding and 0.5px pre-shrink\n    trimmed = region.trim()\n\n    # More aggressive trimming with no padding and no pre-shrink\n    tight = region.trim(padding=0, threshold=0.9, pre_shrink=0)\n\n    # Conservative trimming with more padding\n    loose = region.trim(padding=3, threshold=0.98)\n    ```\n    \"\"\"\n    # Apply global options as defaults\n    import natural_pdf\n\n    if resolution is None:\n        if natural_pdf.options.image.resolution is not None:\n            resolution = natural_pdf.options.image.resolution\n        else:\n            resolution = 144  # Default resolution when none specified\n\n    # Pre-shrink the region to avoid box slivers\n    work_region = (\n        self.expand(left=-pre_shrink, right=-pre_shrink, top=-pre_shrink, bottom=-pre_shrink)\n        if pre_shrink &gt; 0\n        else self\n    )\n\n    # Get the region image\n    # Use render() for clean image without highlights, with cropping\n    image = work_region.render(resolution=resolution, crop=True)\n\n    if image is None:\n        logger.warning(\n            f\"Region {self.bbox}: Could not generate image for trimming. Returning original region.\"\n        )\n        return self\n\n    # Convert to grayscale for easier analysis\n    import numpy as np\n\n    # Convert PIL image to numpy array\n    img_array = np.array(image.convert(\"L\"))  # Convert to grayscale\n    height, width = img_array.shape\n\n    if height == 0 or width == 0:\n        logger.warning(\n            f\"Region {self.bbox}: Image has zero dimensions. Returning original region.\"\n        )\n        return self\n\n    # Normalize pixel values to 0-1 range (255 = white = 1.0, 0 = black = 0.0)\n    normalized = img_array.astype(np.float32) / 255.0\n\n    # Find content boundaries by analyzing row and column averages\n\n    # Analyze rows (horizontal strips) to find top and bottom boundaries\n    row_averages = np.mean(normalized, axis=1)  # Average each row\n    content_rows = row_averages &lt; threshold  # True where there's content (not whitespace)\n\n    # Find first and last rows with content\n    content_row_indices = np.where(content_rows)[0]\n    if len(content_row_indices) == 0:\n        # No content found, return a minimal region at the center\n        logger.warning(\n            f\"Region {self.bbox}: No content detected during trimming. Returning center point.\"\n        )\n        center_x = (self.x0 + self.x1) / 2\n        center_y = (self.top + self.bottom) / 2\n        return Region(self.page, (center_x, center_y, center_x, center_y))\n\n    top_content_row = max(0, content_row_indices[0] - padding)\n    bottom_content_row = min(height - 1, content_row_indices[-1] + padding)\n\n    # Analyze columns (vertical strips) to find left and right boundaries\n    col_averages = np.mean(normalized, axis=0)  # Average each column\n    content_cols = col_averages &lt; threshold  # True where there's content\n\n    content_col_indices = np.where(content_cols)[0]\n    if len(content_col_indices) == 0:\n        # No content found in columns either\n        logger.warning(\n            f\"Region {self.bbox}: No column content detected during trimming. Returning center point.\"\n        )\n        center_x = (self.x0 + self.x1) / 2\n        center_y = (self.top + self.bottom) / 2\n        return Region(self.page, (center_x, center_y, center_x, center_y))\n\n    left_content_col = max(0, content_col_indices[0] - padding)\n    right_content_col = min(width - 1, content_col_indices[-1] + padding)\n\n    # Convert trimmed pixel coordinates back to PDF coordinates\n    scale_factor = resolution / 72.0  # Scale factor used in render()\n\n    # Calculate new PDF coordinates and ensure they are Python floats\n    trimmed_x0 = float(work_region.x0 + (left_content_col / scale_factor))\n    trimmed_top = float(work_region.top + (top_content_row / scale_factor))\n    trimmed_x1 = float(\n        work_region.x0 + ((right_content_col + 1) / scale_factor)\n    )  # +1 because we want inclusive right edge\n    trimmed_bottom = float(\n        work_region.top + ((bottom_content_row + 1) / scale_factor)\n    )  # +1 because we want inclusive bottom edge\n\n    # Ensure the trimmed region doesn't exceed the work region boundaries\n    final_x0 = max(work_region.x0, trimmed_x0)\n    final_top = max(work_region.top, trimmed_top)\n    final_x1 = min(work_region.x1, trimmed_x1)\n    final_bottom = min(work_region.bottom, trimmed_bottom)\n\n    # Ensure valid coordinates (width &gt; 0, height &gt; 0)\n    if final_x1 &lt;= final_x0 or final_bottom &lt;= final_top:\n        logger.warning(\n            f\"Region {self.bbox}: Trimming resulted in invalid dimensions. Returning original region.\"\n        )\n        return self\n\n    # Create the trimmed region\n    trimmed_region = Region(self.page, (final_x0, final_top, final_x1, final_bottom))\n\n    # Expand back by the pre_shrink amount to restore original positioning\n    if pre_shrink &gt; 0:\n        trimmed_region = trimmed_region.expand(\n            left=pre_shrink, right=pre_shrink, top=pre_shrink, bottom=pre_shrink\n        )\n\n    # Copy relevant metadata\n    trimmed_region.region_type = self.region_type\n    trimmed_region.normalized_type = self.normalized_type\n    trimmed_region.confidence = self.confidence\n    trimmed_region.model = self.model\n    trimmed_region.name = self.name\n    trimmed_region.label = self.label\n    trimmed_region.source = \"trimmed\"  # Indicate this is a derived region\n    trimmed_region.parent_region = self\n\n    logger.debug(\n        f\"Region {self.bbox}: Trimmed to {trimmed_region.bbox} (padding={padding}, threshold={threshold}, pre_shrink={pre_shrink})\"\n    )\n    return trimmed_region\n</code></pre>"},{"location":"api/#natural_pdf.Region.viewer--parameters","title":"Parameters","text":"<p>resolution : int, default 150     Rendering resolution (DPI).  This should match the value used by the     page-level viewer so element scaling is accurate. include_chars : bool, default False     Whether to include individual char elements in the overlay.  These     are often too dense for a meaningful visualisation so are skipped by     default. include_attributes : list[str], optional     Additional element attributes to expose in the info panel (on top of     the default set used by the page viewer).</p>"},{"location":"api/#natural_pdf.Region.viewer--returns","title":"Returns","text":"<p>InteractiveViewerWidget | None     The widget instance, or <code>None</code> if ipywidgets is not installed or     an error occurred during creation.</p> Source code in <code>natural_pdf/elements/region.py</code> <pre><code>def viewer(\n    self,\n    *,\n    resolution: int = 150,\n    include_chars: bool = False,\n    include_attributes: Optional[List[str]] = None,\n) -&gt; Optional[\"InteractiveViewerWidget\"]:\n    \"\"\"Create an interactive ipywidget viewer for **this specific region**.\n\n    The method renders the region to an image (cropped to the region bounds) and\n    overlays all elements that intersect the region (optionally excluding noisy\n    character-level elements).  The resulting widget offers the same zoom / pan\n    experience as :py:meth:`Page.viewer` but scoped to the region.\n\n    Parameters\n    ----------\n    resolution : int, default 150\n        Rendering resolution (DPI).  This should match the value used by the\n        page-level viewer so element scaling is accurate.\n    include_chars : bool, default False\n        Whether to include individual *char* elements in the overlay.  These\n        are often too dense for a meaningful visualisation so are skipped by\n        default.\n    include_attributes : list[str], optional\n        Additional element attributes to expose in the info panel (on top of\n        the default set used by the page viewer).\n\n    Returns\n    -------\n    InteractiveViewerWidget | None\n        The widget instance, or ``None`` if *ipywidgets* is not installed or\n        an error occurred during creation.\n    \"\"\"\n\n    # ------------------------------------------------------------------\n    # Dependency / environment checks\n    # ------------------------------------------------------------------\n    if not _IPYWIDGETS_AVAILABLE or InteractiveViewerWidget is None:\n        logger.error(\n            \"Interactive viewer requires 'ipywidgets'. \"\n            'Please install with: pip install \"ipywidgets&gt;=7.0.0,&lt;10.0.0\"'\n        )\n        return None\n\n    try:\n        # ------------------------------------------------------------------\n        # Render region image (cropped) and encode as data URI\n        # ------------------------------------------------------------------\n        import base64\n        from io import BytesIO\n\n        # Use unified render() with crop=True to obtain just the region\n        img = self.render(resolution=resolution, crop=True)\n        if img is None:\n            logger.error(f\"Failed to render image for region {self.bbox} viewer.\")\n            return None\n\n        buf = BytesIO()\n        img.save(buf, format=\"PNG\")\n        img_str = base64.b64encode(buf.getvalue()).decode()\n        image_uri = f\"data:image/png;base64,{img_str}\"\n\n        # ------------------------------------------------------------------\n        # Prepare element overlay data (coordinates relative to region)\n        # ------------------------------------------------------------------\n        scale = resolution / 72.0  # Same convention as page viewer\n\n        # Gather elements intersecting the region\n        region_elements = self.get_elements(apply_exclusions=False)\n\n        # Optionally filter out chars\n        if not include_chars:\n            region_elements = [\n                el for el in region_elements if str(getattr(el, \"type\", \"\")).lower() != \"char\"\n            ]\n\n        default_attrs = [\n            \"text\",\n            \"fontname\",\n            \"size\",\n            \"bold\",\n            \"italic\",\n            \"color\",\n            \"linewidth\",\n            \"is_horizontal\",\n            \"is_vertical\",\n            \"source\",\n            \"confidence\",\n            \"label\",\n            \"model\",\n            \"upright\",\n            \"direction\",\n        ]\n\n        if include_attributes:\n            default_attrs.extend([a for a in include_attributes if a not in default_attrs])\n\n        elements_json: List[dict] = []\n        for idx, el in enumerate(region_elements):\n            try:\n                # Calculate coordinates relative to region bbox and apply scale\n                x0 = (el.x0 - self.x0) * scale\n                y0 = (el.top - self.top) * scale\n                x1 = (el.x1 - self.x0) * scale\n                y1 = (el.bottom - self.top) * scale\n\n                elem_dict = {\n                    \"id\": idx,\n                    \"type\": getattr(el, \"type\", \"unknown\"),\n                    \"x0\": round(x0, 2),\n                    \"y0\": round(y0, 2),\n                    \"x1\": round(x1, 2),\n                    \"y1\": round(y1, 2),\n                    \"width\": round(x1 - x0, 2),\n                    \"height\": round(y1 - y0, 2),\n                }\n\n                # Add requested / default attributes\n                for attr_name in default_attrs:\n                    if hasattr(el, attr_name):\n                        val = getattr(el, attr_name)\n                        # Ensure JSON serialisable\n                        if not isinstance(val, (str, int, float, bool, list, dict, type(None))):\n                            val = str(val)\n                        elem_dict[attr_name] = val\n                elements_json.append(elem_dict)\n            except Exception as e:\n                logger.warning(f\"Error preparing element {idx} for region viewer: {e}\")\n\n        viewer_data = {\"page_image\": image_uri, \"elements\": elements_json}\n\n        # ------------------------------------------------------------------\n        # Instantiate the widget directly using the prepared data\n        # ------------------------------------------------------------------\n        return InteractiveViewerWidget(pdf_data=viewer_data)\n\n    except Exception as e:\n        logger.error(f\"Error creating viewer for region {self.bbox}: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"api/#natural_pdf-functions","title":"Functions","text":""},{"location":"api/#natural_pdf.configure_logging","title":"<code>natural_pdf.configure_logging(level=logging.INFO, handler=None)</code>","text":"<p>Configure logging for the natural_pdf package.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <p>Logging level (e.g., logging.INFO, logging.DEBUG)</p> <code>INFO</code> <code>handler</code> <p>Optional custom handler. Defaults to a StreamHandler.</p> <code>None</code> Source code in <code>natural_pdf/__init__.py</code> <pre><code>def configure_logging(level=logging.INFO, handler=None):\n    \"\"\"Configure logging for the natural_pdf package.\n\n    Args:\n        level: Logging level (e.g., logging.INFO, logging.DEBUG)\n        handler: Optional custom handler. Defaults to a StreamHandler.\n    \"\"\"\n    # Avoid adding duplicate handlers\n    if any(isinstance(h, logging.StreamHandler) for h in logger.handlers):\n        return\n\n    if handler is None:\n        handler = logging.StreamHandler()\n        formatter = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n        handler.setFormatter(formatter)\n\n    logger.addHandler(handler)\n    logger.setLevel(level)\n\n    logger.propagate = False\n</code></pre>"},{"location":"categorizing-documents/","title":"Categorizing Pages and Regions","text":"<p>Natural PDF allows you to automatically categorize pages or specific regions within a page using machine learning models. This is incredibly useful for filtering large collections of documents or understanding the structure and content of individual PDFs.</p>"},{"location":"categorizing-documents/#installation","title":"Installation","text":"<p>To use the classification features, you need to install the optional dependencies:</p> <pre><code>pip install \"natural-pdf[ai]\"\n</code></pre> <p>This installs necessary libraries like <code>torch</code>, <code>transformers</code>, and others.</p>"},{"location":"categorizing-documents/#core-concept-the-classify-method","title":"Core Concept: The <code>.classify()</code> Method","text":"<p>The primary way to perform categorization is using the <code>.classify()</code> method available on <code>Page</code> and <code>Region</code> objects.</p> <pre><code>from natural_pdf import PDF\n\n# Example: Classify a Page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\nlabels = [\"invoice\", \"letter\", \"report cover\", \"data table\"]\npage.classify(labels, using=\"text\")\n\n# Access the top result\nprint(f\"Top Category: {page.category}\")\nprint(f\"Confidence: {page.category_confidence:.3f}\")\n</code></pre> <p>Key Arguments:</p> <ul> <li><code>labels</code> (required): A list of strings representing the potential labels you want to classify the item into.</li> <li><code>using</code> (optional): Specifies which classification model or strategy to use. Defaults to <code>\"text\"</code>.<ul> <li><code>\"text\"</code>: Uses a text-based model (default: <code>facebook/bart-large-mnli</code>) suitable for classifying based on language content.</li> <li><code>\"vision\"</code>: Uses a vision-based model (default: <code>openai/clip-vit-base-patch32</code>) suitable for classifying based on visual layout and appearance.</li> <li>Specific Model ID: You can provide a Hugging Face model ID (e.g., <code>\"google/siglip-base-patch16-224\"</code>, <code>\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\"</code>) compatible with zero-shot text or image classification. The library attempts to infer whether it's text or vision, but you might need <code>using</code>.</li> </ul> </li> <li><code>model</code> (optional): Explicitly model ID (HuggingFace repo name)</li> <li><code>min_confidence</code> (optional): A float between 0.0 and 1.0. Only labels with a confidence score greater than or equal to this threshold will be included in the results (default: 0.0).</li> </ul>"},{"location":"categorizing-documents/#text-vs-vision-classification","title":"Text vs. Vision Classification","text":"<p>Choosing the right model type depends on your goal:</p>"},{"location":"categorizing-documents/#text-classification-usingtext","title":"Text Classification (<code>using=\"text\"</code>)","text":"<ul> <li>How it works: Extracts the text from the page or region and analyzes the language content.</li> <li>Best for:<ul> <li>Topic Identification: Determining what a page or section is about (e.g., \"budget discussion,\" \"environmental impact,\" \"legal terms\").</li> <li>Content-Driven Document Types: Identifying document types primarily defined by their text (e.g., emails, meeting minutes, news articles, reports).</li> </ul> </li> <li>Data Journalism Example: You have thousands of pages of government reports. You can use text classification to find all pages discussing \"public health funding\" or classify paragraphs within environmental impact statements to find mentions of specific endangered species.</li> </ul> <pre><code># Find pages related to finance\nfinancial_labels = [\"budget\", \"revenue\", \"expenditure\", \"forecast\"]\npdf.classify_pages(financial_labels, using=\"text\")\nbudget_pages = [p for p in pdf.pages if p.category == \"budget\"]\n</code></pre>"},{"location":"categorizing-documents/#vision-classification-usingvision","title":"Vision Classification (<code>using=\"vision\"</code>)","text":"<ul> <li>How it works: Renders the page or region as an image and analyzes its visual layout, structure, and appearance.</li> <li>Best for:<ul> <li>Layout-Driven Document Types: Identifying documents recognizable by their structure (e.g., invoices, receipts, forms, presentation slides, title pages).</li> <li>Identifying Visual Elements: Distinguishing between pages dominated by text, tables, charts, or images.</li> </ul> </li> <li>Data Journalism Example: You have a scanned archive of campaign finance filings containing various document types. You can use vision classification to quickly isolate all the pages that look like donation receipts or expenditure forms, even if the OCR quality is poor.</li> </ul> <pre><code># Find pages that look like invoices or receipts\nvisual_labels = [\"invoice\", \"receipt\", \"letter\", \"form\"]\npage.classify(visual_labels, using=\"vision\")\nif page.category in [\"invoice\", \"receipt\"]:\n    print(f\"Page {page.number} looks like an invoice or receipt.\")\n</code></pre>"},{"location":"categorizing-documents/#classifying-specific-objects","title":"Classifying Specific Objects","text":""},{"location":"categorizing-documents/#pages-pageclassify","title":"Pages (<code>page.classify(...)</code>)","text":"<p>Classifying a whole page is useful for sorting documents or identifying the overall purpose of a page within a larger document.</p> <pre><code># Classify the first page\npage = pdf.pages[0]\npage_types = [\"cover page\", \"table of contents\", \"chapter start\", \"appendix\"]\npage.classify(page_types, using=\"vision\") # Vision often good for page structure\nprint(f\"Page 1 Type: {page.category}\")\n</code></pre>"},{"location":"categorizing-documents/#regions-regionclassify","title":"Regions (<code>region.classify(...)</code>)","text":"<p>Classifying a specific region allows for more granular analysis within a page. You might first detect regions using Layout Analysis and then classify those regions.</p> <pre><code># Assume layout analysis has run, find paragraphs\nparagraphs = page.find_all(\"region[type=paragraph]\")\nif paragraphs:\n    # Classify the topic of the first paragraph\n    topic_labels = [\"introduction\", \"methodology\", \"results\", \"conclusion\"]\n    # Use text model for topic\n    paragraphs[0].classify(topic_labels, using=\"text\")\n    print(f\"First paragraph category: {paragraphs[0].category}\")\n</code></pre>"},{"location":"categorizing-documents/#accessing-classification-results","title":"Accessing Classification Results","text":"<p>After running <code>.classify()</code>, you can access the results:</p> <ul> <li><code>page.category</code> or <code>region.category</code>: Returns the string label of the category with the highest confidence score from the last classification run. Returns <code>None</code> if no classification has been run or no category met the threshold.</li> <li><code>page.category_confidence</code> or <code>region.category_confidence</code>: Returns the float confidence score (0.0-1.0) for the top category. Returns <code>None</code> otherwise.</li> <li><code>page.classification_results</code> or <code>region.classification_results</code>: Returns the full result dictionary stored in the object's <code>.metadata['classification']</code>, containing the model used, engine type, labels provided, timestamp, and a list of all scores above the threshold sorted by confidence. Returns <code>None</code> if no classification has been run.</li> </ul> <pre><code>results = page.classify([\"invoice\", \"letter\"], using=\"text\", min_confidence=0.5)\n\nif page.category == \"invoice\":\n    print(f\"Found an invoice with confidence {page.category_confidence:.2f}\")\n\n# See all results above the threshold\n# print(page.classification_results['scores'])\n</code></pre>"},{"location":"categorizing-documents/#classifying-collections","title":"Classifying Collections","text":"<p>For batch processing, use the <code>.classify_all()</code> method on <code>PDFCollection</code> or <code>ElementCollection</code> objects. This displays a progress bar tracking individual items (pages or elements).</p>"},{"location":"categorizing-documents/#pdfcollection-collectionclassify_all","title":"PDFCollection (<code>collection.classify_all(...)</code>)","text":"<p>Classifies pages across all PDFs in the collection. Use <code>max_workers</code> for parallel processing across different PDF files.</p> <pre><code>collection = natural_pdf.PDFCollection.from_directory(\"./documents/\")\nlabels = [\"form\", \"datasheet\", \"image\", \"text document\"]\n\n# Classify all pages using vision model, processing 4 PDFs concurrently\ncollection.classify_all(labels, using=\"vision\", max_workers=4)\n\n# Filter PDFs containing forms\nform_pdfs = []\nfor pdf in collection:\n    if any(p.category == \"form\" for p in pdf.pages if p.category):\n        form_pdfs.append(pdf.path)\n    pdf.close() # Remember to close PDFs\n\nprint(f\"Found forms in: {form_pdfs}\")\n</code></pre>"},{"location":"categorizing-documents/#elementcollection-element_collectionclassify_all","title":"ElementCollection (<code>element_collection.classify_all(...)</code>)","text":"<p>Classifies all classifiable elements (currently <code>Page</code> and <code>Region</code>) within the collection.</p> <pre><code># Assume 'pdf' is loaded and 'layout_regions' is an ElementCollection of Regions\nlayout_regions = pdf.find_all(\"region\")\nregion_types = [\"paragraph\", \"list\", \"table\", \"figure\", \"caption\"]\n\n# Classify all detected regions based on vision\nlayout_regions.classify_all(region_types, model=\"vision\")\n\n# Count table regions using filter()\ntable_regions = layout_regions.filter(lambda region: region.category == \"table\")\nprint(f\"Found {len(table_regions)} regions classified as tables.\")\n</code></pre>"},{"location":"data-extraction/","title":"Pulling Structured Data from PDFs","text":"<p>Ever had a pile of invoices, reports, or forms where you need to extract the same pieces of information from each one? That's where structured data extraction shines. Instead of manually copying invoice numbers and dates, you can tell Natural PDF exactly what information you want and let it find those details automatically.</p> <p>You'll need more than the basic install for this: <pre><code># Install the OpenAI (or compatible) client library\npip install openai\n\n# Or pull in the full AI stack (classification, QA, search, etc.)\npip install \"natural_pdf[ai]\"\n</code></pre></p>"},{"location":"data-extraction/#the-simple-approach-just-tell-it-what-you-want","title":"The Simple Approach: Just Tell It What You Want","text":"<p>Don't want to mess around with schemas? Just make a list of what you're looking for:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Extract data using just a list - no schema required!\ndata = page.extract(schema=[\"site\", \"date\", \"violation count\", \"inspector\"]).extracted()\n\nprint(data.site)  # \"ACME Manufacturing Plant\"\nprint(data.date)  # \"2024-03-15\"\nprint(data.violation_count)  # \"3\"\n</code></pre> <p>Natural PDF automatically builds a schema behind the scenes and extracts the data. Each field becomes a string, and you get confidence scores for free:</p> <pre><code># Check how confident the extraction was\nprint(data.site_confidence)  # 0.89\nprint(data.date_confidence)  # 0.95\n</code></pre> <p>This works completely offline - no API keys or internet connection needed. It uses a local document question-answering model that understands both text and layout.</p>"},{"location":"data-extraction/#working-offline-no-internet-required","title":"Working Offline (No Internet Required)","text":"<p>Maybe you're dealing with sensitive documents or just don't want to send everything to the cloud:</p> <pre><code># This works completely offline\npage.extract(schema=[\"company\", \"total\", \"due_date\"])\n</code></pre> <p>The offline engine is pretty smart - it looks at both the text content and how things are visually laid out on the page. For sketchy results, you can set a confidence threshold:</p> <pre><code># Only accept answers the model is confident about\npage.extract(schema=[\"amount\", \"date\"], min_confidence=0.8)\n</code></pre> <p>If an answer falls below your threshold, it gets set to <code>None</code> instead of giving you questionable data.</p> <p>Want to use a local LLM instead? Tools like LM Studio or Msty can run models locally with an OpenAI-compatible API:</p> <pre><code>from openai import OpenAI\n\n# Point to your local LLM server\nclient = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")\n\npage.extract(schema=InvoiceSchema, client=client)\n</code></pre> <p>Just heads up - local LLMs are much slower than the document QA approach for simple extractions!</p>"},{"location":"data-extraction/#building-custom-schemas","title":"Building Custom Schemas","text":"<p>For more complex extractions, you can define exactly what you want using Pydantic:</p> <pre><code>from natural_pdf import PDF\nfrom pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Set up your LLM client (using Gemini here)\nclient = OpenAI(\n    api_key=\"YOUR_API_KEY\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" \n)\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Define exactly what you want to extract\nclass ReportInfo(BaseModel):\n    inspection_number: str = Field(description=\"The main report identifier\")\n    inspection_date: str = Field(description=\"When the inspection happened\")\n    inspection_service: str = Field(description=\"Name of inspection service\")\n    site_name: str = Field(description=\"Location that was inspected\")\n    summary: str = Field(description=\"Visit summary\")\n    violation_count: int = Field(description=\"Number of violations found\")\n\n# Extract the data\npage.extract(schema=ReportInfo, client=client, model=\"gemini-2.5-flash\") \n\n# Get your structured data\nreport_data = page.extracted() \nprint(report_data.inspection_number)\nprint(report_data.violation_count)\n</code></pre>"},{"location":"data-extraction/#managing-multiple-extractions","title":"Managing Multiple Extractions","text":"<ul> <li>Results get stored under the key <code>\"default-structured\"</code> by default</li> <li>Use <code>analysis_key</code> to store multiple different extractions from the same document</li> <li>Trying to extract with an existing key will fail unless you use <code>overwrite=True</code></li> </ul> <pre><code># Extract using a specific key\npage.extract(InvoiceInfo, client=client, analysis_key=\"invoice_header\")\n\n# Access that specific extraction\nheader_data = page.extracted(analysis_key=\"invoice_header\") \ncompany = page.extracted('company_name', analysis_key=\"invoice_header\")\n</code></pre>"},{"location":"data-extraction/#text-vs-vision-extraction","title":"Text vs Vision Extraction","text":"<p>You can choose how to send the document to the LLM:</p> <ul> <li><code>using='text'</code> (default): Sends the text content with layout preserved</li> <li><code>using='vision'</code>: Sends an image of the page</li> </ul> <pre><code># Send text content (faster, cheaper)\npage.extract(schema=MySchema, client=client, using='text')\n\n# Send page image (better for visual layouts)\npage.extract(schema=MySchema, client=client, using='vision')\n</code></pre>"},{"location":"data-extraction/#processing-multiple-documents","title":"Processing Multiple Documents","text":"<p>The extraction methods work on any part of a PDF - regions, pages, collections - making it easy to process lots of documents:</p> <pre><code># Extract from a specific region\nheader_region.extract(InvoiceInfo, client=client)\ncompany = header_region.extracted('company_name')\n\n# Process multiple pages at once\nresults = pdf.pages[:5].apply(\n    lambda page: page.extract(\n        schema=InvoiceInfo, \n        client=client, \n        analysis_key=\"page_invoice_info\"\n    )\n)\n\n# Access results for any page\npdf.pages[0].extracted('company_name', analysis_key=\"page_invoice_info\")\n</code></pre> <p>This approach lets you turn unstructured PDF content into clean, structured data you can actually work with.</p>"},{"location":"describe/","title":"Describe Functionality","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.describe()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.describe() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Describe all elements on the page\npage.find_all('text').describe()\n</pre> # Describe all elements on the page page.find_all('text').describe() Out[2]: In\u00a0[3]: Copied! <pre># Describe all elements on the page\npage.find_all('rect').describe()\n</pre> # Describe all elements on the page page.find_all('rect').describe() Out[3]: In\u00a0[4]: Copied! <pre>page.find_all('text').inspect()\n</pre> page.find_all('text').inspect() Out[4]: In\u00a0[5]: Copied! <pre>page.find_all('line').inspect()\n</pre> page.find_all('line').inspect() Out[5]:"},{"location":"describe/#describe-functionality","title":"Describe Functionality\u00b6","text":"<p>The <code>describe()</code> and <code>inspect()</code> methods provide an easy way to understand the contents of your PDF elements without having to visualize them as images.</p>"},{"location":"describe/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Get a summary of an entire page:</p>"},{"location":"describe/#page-1-summary","title":"Page 1 Summary\u00b6","text":"<p>Elements:</p> <ul> <li>text: 44 elements</li> <li>line: 21 elements</li> <li>rect: 8 elements</li> </ul> <p>Text Analysis:</p> <ul> <li>typography:<ul> <li>fonts:<ul> <li>Helvetica: 44</li> </ul> </li> <li>sizes:<ul> <li>10.0pt: 40</li> <li>8.0pt: 3</li> <li>12.0pt: 1</li> </ul> </li> <li>styles: 9 bold</li> <li>colors:<ul> <li>black: 43</li> <li>other: 1</li> </ul> </li> </ul> </li> </ul>"},{"location":"describe/#element-collection-summaries","title":"Element collection summaries\u00b6","text":"<p>You can describe element collections on a page with <code>.describe()</code>.</p>"},{"location":"describe/#collection-summary-44-elements","title":"Collection Summary (44 elements)\u00b6","text":"<p>Typography:</p> <ul> <li>fonts:<ul> <li>Helvetica: 44</li> </ul> </li> <li>sizes:<ul> <li>10.0pt: 40</li> <li>8.0pt: 3</li> <li>12.0pt: 1</li> </ul> </li> <li>styles: 9 bold</li> <li>colors:<ul> <li>black: 43</li> <li>other: 1</li> </ul> </li> </ul>"},{"location":"describe/#collection-summary-8-elements","title":"Collection Summary (8 elements)\u00b6","text":"<p>Size Stats:</p> <ul> <li>width range: 8-180</li> <li>height range: 8-35</li> <li>avg area: 844 sq pts</li> </ul> <p>Styles:</p> <ul> <li>stroke widths:<ul> <li>0.5: 7</li> </ul> </li> <li>colors:<ul> <li>black: 8</li> </ul> </li> </ul>"},{"location":"describe/#inspecting-lists-of-elements","title":"Inspecting lists of elements\u00b6","text":"<p>For more detail, you can view specific details of element collections with <code>inspect()</code>.</p>"},{"location":"describe/#collection-inspection-44-elements","title":"Collection Inspection (44 elements)\u00b6","text":""},{"location":"describe/#word-elements","title":"Word Elements\u00b6","text":"text x0 top x1 bottom font_family size bold italic source confidence color Jungle Health and Safety Inspection Service 385 36 542 44 Helvetica 8 False False native 1.00 #000000 INS-UP70N51NCL41R 385 46 466 54 Helvetica 8 False False native 1.00 #ff0000 Site: 50 84 74 94 Helvetica 10 True False native 1.00 #000000 Durham\u2019s Meatpacking 74 84 182 94 Helvetica 10 False False native 1.00 #000000 Chicago, Ill. 182 84 235 94 Helvetica 10 False False native 1.00 #000000 Date: 50 104 81 114 Helvetica 10 True False native 1.00 #000000 February 3, 1905 81 104 157 114 Helvetica 10 False False native 1.00 #000000 Violation Count: 50 124 130 134 Helvetica 10 True False native 1.00 #000000 7 130 124 136 134 Helvetica 10 False False native 1.00 #000000 Summary: 50 144 102 154 Helvetica 10 True False native 1.00 #000000 Worst of any, however, were the fertilizer men, an... 102 144 506 154 Helvetica 10 False False native 1.00 #000000 These people could not be shown to the visitor - f... 50 160 512 170 Helvetica 10 False False native 1.00 #000000 visitor at a hundred yards, and as for the other m... 50 176 491 186 Helvetica 10 False False native 1.00 #000000 some of which there were open vats near the level ... 50 192 496 202 Helvetica 10 False False native 1.00 #000000 into the vats; and when they were fished out, ther... 50 208 465 218 Helvetica 10 False False native 1.00 #000000 exhibiting - sometimes they would be overlooked fo... 50 224 492 234 Helvetica 10 False False native 1.00 #000000 to the world as Durham\u2019s Pure Leaf Lard! 50 240 232 250 Helvetica 10 False False native 1.00 #000000 Violations 50 372 107 384 Helvetica 12 True False native 1.00 #000000 Statute 55 398 89 408 Helvetica 10 True False native 1.00 #000000 Description 105 398 160 408 Helvetica 10 True False native 1.00 #000000 Level 455 398 481 408 Helvetica 10 True False native 1.00 #000000 Repeat? 505 398 544 408 Helvetica 10 True False native 1.00 #000000 4.12.7 55 418 83 428 Helvetica 10 False False native 1.00 #000000 Unsanitary Working Conditions. 105 418 245 428 Helvetica 10 False False native 1.00 #000000 Critical 455 418 486 428 Helvetica 10 False False native 1.00 #000000 5.8.3 55 438 77 448 Helvetica 10 False False native 1.00 #000000 Inadequate Protective Equipment. 105 438 256 448 Helvetica 10 False False native 1.00 #000000 Serious 455 438 489 448 Helvetica 10 False False native 1.00 #000000 6.3.9 55 458 77 468 Helvetica 10 False False native 1.00 #000000 Ineffective Injury Prevention. 105 458 231 468 Helvetica 10 False False native 1.00 #000000 <p>Showing 30 of 44 elements (pass limit= to see more)</p>"},{"location":"describe/#collection-inspection-21-elements","title":"Collection Inspection (21 elements)\u00b6","text":""},{"location":"describe/#line-elements","title":"Line Elements\u00b6","text":"x0 top x1 bottom width is_horizontal is_vertical 50 352 550 352 2 True False 50 392 550 392 0 True False 50 392 50 552 0 False True 100 392 100 552 0 False True 450 392 450 552 0 False True 500 392 500 552 0 False True 550 392 550 552 0 False True 50 412 550 412 0 True False 520 418 528 426 0 False False 520 418 528 426 0 False False 50 432 550 432 0 True False 520 438 528 446 0 False False 520 438 528 446 0 False False 50 452 550 452 0 True False 50 472 550 472 0 True False 50 492 550 492 0 True False 50 512 550 512 0 True False 520 518 528 526 0 False False 520 518 528 526 0 False False 50 532 550 532 0 True False 50 552 550 552 0 True False"},{"location":"describe/","title":"Describe Functionality","text":"<p>The <code>describe()</code> and <code>inspect()</code> methods provide an easy way to understand the contents of your PDF elements without having to visualize them as images.</p>"},{"location":"describe/#basic-usage","title":"Basic Usage","text":"<p>Get a summary of an entire page:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.describe()\n</code></pre>"},{"location":"describe/#element-collection-summaries","title":"Element collection summaries","text":"<p>You can describe element collections on a page with <code>.describe()</code>.</p> <pre><code># Describe all elements on the page\npage.find_all('text').describe()\n</code></pre> <pre><code># Describe all elements on the page\npage.find_all('rect').describe()\n</code></pre>"},{"location":"describe/#inspecting-lists-of-elements","title":"Inspecting lists of elements","text":"<p>For more detail, you can view specific details of element collections with <code>inspect()</code>.</p> <pre><code>page.find_all('text').inspect()\n</code></pre> <pre><code>page.find_all('line').inspect()\n</code></pre>"},{"location":"document-qa/","title":"Document Question Answering","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n\n# Display the first page\npage = pdf.pages[0]\npage.show()\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")  # Display the first page page = pdf.pages[0] page.show() Out[1]: In\u00a0[2]: Copied! <pre># Ask a question about the entire document\npage.ask(\"How many votes did Harris and Waltz get?\")\n</pre> # Ask a question about the entire document page.ask(\"How many votes did Harris and Waltz get?\") <pre>Device set to use mps\n</pre> Out[2]: <pre>{'question': 'How many votes did Harris and Waltz get?',\n 'answer': '148',\n 'confidence': 0.9994352459907532,\n 'start': 21,\n 'end': 21,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre>page.ask(\"Who got the most votes for Attorney General?\")\n</pre> page.ask(\"Who got the most votes for Attorney General?\") Out[3]: <pre>{'question': 'Who got the most votes for Attorney General?',\n 'answer': 'DEM EUGENE DEPASQUALE',\n 'confidence': 0.9823383688926697,\n 'start': 64,\n 'end': 64,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre>page.ask(\"Who was the Republican candidate for Attorney General?\")\n</pre> page.ask(\"Who was the Republican candidate for Attorney General?\") Out[4]: <pre>{'question': 'Who was the Republican candidate for Attorney General?',\n 'answer': 'LIB ROBERT COWBURN',\n 'confidence': 0.34796959161758423,\n 'start': 68,\n 'end': 68,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[5]: Copied! <pre># Get a specific page\nregion = page.find('text:contains(\"Attorney General\")').below()\nregion.show()\n</pre> # Get a specific page region = page.find('text:contains(\"Attorney General\")').below() region.show() Out[5]: In\u00a0[6]: Copied! <pre>region.ask(\"How many write-in votes were cast?\")\n</pre> region.ask(\"How many write-in votes were cast?\") Out[6]: <pre>{'question': 'How many write-in votes were cast?',\n 'answer': '498',\n 'confidence': 0.9988918304443359,\n 'start': 17,\n 'end': 17,\n 'found': True,\n 'region': &lt;Region bbox=(0, 553.663, 612, 792)&gt;,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[7]: Copied! <pre>import pandas as pd\n\nquestions = [\n    \"How many votes did Harris and Walz get?\",\n    \"How many votes did Trump get?\",\n    \"How many votes did Natural PDF get?\",\n    \"What was the date of this form?\"\n]\n\n# You can actually do this but with multiple questions\n# in the model itself buuuut Natural PDF can'd do it yet\nresults = [page.ask(q) for q in questions]\n\ndf = pd.json_normalize(results)\ndf\n</pre> import pandas as pd  questions = [     \"How many votes did Harris and Walz get?\",     \"How many votes did Trump get?\",     \"How many votes did Natural PDF get?\",     \"What was the date of this form?\" ]  # You can actually do this but with multiple questions # in the model itself buuuut Natural PDF can'd do it yet results = [page.ask(q) for q in questions]  df = pd.json_normalize(results) df Out[7]: question answer confidence start end found page_num source_elements 0 How many votes did Harris and Walz get? 148 0.999612 21 21 True 0 [&lt;TextElement text='148' font='Helvetica' size... 1 How many votes did Trump get? 348 0.820931 23 23 True 0 [&lt;TextElement text='348' font='Helvetica' size... 2 How many votes did Natural PDF get? November 5, 2024 0.957622 4 4 True 0 [&lt;TextElement text='November 5...' font='Helve... 3 What was the date of this form? November 5, 2024 0.996624 4 4 True 0 [&lt;TextElement text='November 5...' font='Helve... In\u00a0[8]: Copied! <pre>result = page.ask(\"Who got the most votes for Attorney General?\")\n\n# See the answer\nprint(result.answer)  # \"John Smith\"\n\n# Show exactly where it found that answer\nresult.show()\n</pre> result = page.ask(\"Who got the most votes for Attorney General?\")  # See the answer print(result.answer)  # \"John Smith\"  # Show exactly where it found that answer result.show() <pre>DEM EUGENE DEPASQUALE\n</pre> Out[8]: <p>The <code>result.show()</code> method highlights the specific text elements the model used to answer your question - super helpful for debugging or when you need to double-check the results.</p> <p>You can also access result data like a normal dictionary or use dot notation if you prefer:</p> In\u00a0[9]: Copied! <pre># Both of these work the same way\nprint(result[\"confidence\"])  # 0.97\nprint(result.confidence)     # 0.97\n</pre> # Both of these work the same way print(result[\"confidence\"])  # 0.97 print(result.confidence)     # 0.97 <pre>0.9823383688926697\n0.9823383688926697\n</pre> <p>If the model couldn't find a confident answer, <code>result.found</code> will be <code>False</code> and calling <code>result.show()</code> will let you know there's nothing to visualize.</p>"},{"location":"document-qa/#document-question-answering","title":"Document Question Answering\u00b6","text":"<p>Natural PDF includes document QA functionality that allows you to ask natural language questions about your PDFs and get relevant answers. This feature uses LayoutLM models to understand both the text content and the visual layout of your documents.</p>"},{"location":"document-qa/#setup","title":"Setup\u00b6","text":"<p>Let's start by loading a sample PDF to experiment with question answering.</p>"},{"location":"document-qa/#basic-usage","title":"Basic Usage\u00b6","text":"<p>Here's how to ask questions to a PDF page:</p>"},{"location":"document-qa/#asking-questions-to-part-of-a-page-questions","title":"Asking questions to part of a page questions\u00b6","text":"<p>You can also ask questions to a specific region of a page*:</p>"},{"location":"document-qa/#asking-multiple-questions","title":"Asking multiple questions\u00b6","text":""},{"location":"document-qa/#visualizing-where-answers-come-from","title":"Visualizing where answers come from\u00b6","text":"<p>Sometimes you'll want to see exactly where the model found an answer in your document. Maybe you're checking if it grabbed the right table cell, or you want to verify it didn't confuse similar-looking sections.</p>"},{"location":"document-qa/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you've learned about document QA, explore:</p> <ul> <li>Element Selection: Find specific elements to focus your questions.</li> <li>Layout Analysis: Automatically detect document structure.</li> <li>Working with Regions: Define custom areas for targeted questioning.</li> <li>Text Extraction: Extract and preprocess text before QA.</li> </ul>"},{"location":"element-selection/","title":"Finding What You Need in PDFs","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() Out[1]: In\u00a0[2]: Copied! <pre># Find the first text element containing \"Summary\"\nsummary_text = page.find('text:contains(\"Summary\")')\nsummary_text\n</pre> # Find the first text element containing \"Summary\" summary_text = page.find('text:contains(\"Summary\")') summary_text Out[2]: <pre>&lt;TextElement text='Summary: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 144.07000000000005, 101.68, 154.07000000000005)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all text elements containing \"Inadequate\"\ncontains_inadequate = page.find_all('text:contains(\"Inadequate\")')\nlen(contains_inadequate)\n</pre> # Find all text elements containing \"Inadequate\" contains_inadequate = page.find_all('text:contains(\"Inadequate\")') len(contains_inadequate) Out[3]: <pre>2</pre> In\u00a0[4]: Copied! <pre># Let's see what we found\nwith page.highlights() as h:\n    h.add(summary_text, color='red', label='Summary')\n    h.add(contains_inadequate, color='blue', label='Inadequate')\n    h.show()\n</pre> # Let's see what we found with page.highlights() as h:     h.add(summary_text, color='red', label='Summary')     h.add(contains_inadequate, color='blue', label='Inadequate')     h.show() In\u00a0[5]: Copied! <pre># Find all text elements\nall_text = page.find_all('text')\nlen(all_text)\n</pre> # Find all text elements all_text = page.find_all('text') len(all_text) Out[5]: <pre>44</pre> In\u00a0[6]: Copied! <pre># Find all rectangle elements\nall_rects = page.find_all('rect')\nlen(all_rects)\n</pre> # Find all rectangle elements all_rects = page.find_all('rect') len(all_rects) Out[6]: <pre>8</pre> In\u00a0[7]: Copied! <pre># Find all line elements\nall_lines = page.find_all('line')\nlen(all_lines)\n</pre> # Find all line elements all_lines = page.find_all('line') len(all_lines) Out[7]: <pre>21</pre> In\u00a0[8]: Copied! <pre># Show where all the lines are\npage.find_all('line').show()\n</pre> # Show where all the lines are page.find_all('line').show() Out[8]: In\u00a0[9]: Copied! <pre># Find large text (probably headings)\npage.find_all('text[size&gt;=11]')\n</pre> # Find large text (probably headings) page.find_all('text[size&gt;=11]') Out[9]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[10]: Copied! <pre># Find text that uses Helvetica font\npage.find_all('text[fontname*=Helvetica]')\n</pre> # Find text that uses Helvetica font page.find_all('text[fontname*=Helvetica]') Out[10]: <pre>&lt;ElementCollection[TextElement](count=44)&gt;</pre> In\u00a0[11]: Copied! <pre># Find red text in this PDF\nred_text = page.find_all('text[color~=red]')\nred_text.show()\n</pre> # Find red text in this PDF red_text = page.find_all('text[color~=red]') red_text.show() Out[11]: In\u00a0[12]: Copied! <pre># Find thick lines (might be important borders)\npage.find_all('line[width&gt;=2]')\n</pre> # Find thick lines (might be important borders) page.find_all('line[width&gt;=2]') Out[12]: <pre>&lt;ElementCollection[LineElement](count=1)&gt;</pre> In\u00a0[13]: Copied! <pre># Find bold text (probably important)\npage.find_all('text:bold').show()\n</pre> # Find bold text (probably important) page.find_all('text:bold').show() Out[13]: In\u00a0[14]: Copied! <pre># Combine filters: large bold text (definitely headings)\npage.find_all('text[size&gt;=11]:bold')\n</pre> # Combine filters: large bold text (definitely headings) page.find_all('text[size&gt;=11]:bold') Out[14]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[15]: Copied! <pre># Find all text that's NOT bold\nnon_bold_text = page.find_all('text:not(:bold)')\n\n# Find all elements that are NOT tables\nnot_tables = page.find_all(':not(region[type=table])')\n\n# Find text that doesn't contain \"Total\"\nrelevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)\n\n# Find text that isn't empty\nnon_empty_text = page.find_all('text:not(:empty)')\n</pre> # Find all text that's NOT bold non_bold_text = page.find_all('text:not(:bold)')  # Find all elements that are NOT tables not_tables = page.find_all(':not(region[type=table])')  # Find text that doesn't contain \"Total\" relevant_text = page.find_all('text:not(:contains(\"Total\"))', case=False)  # Find text that isn't empty non_empty_text = page.find_all('text:not(:empty)') In\u00a0[16]: Copied! <pre># First, find a thick horizontal line\nref_line = page.find('line[width&gt;=2]')\n\n# Now find text that's above that line\ntext_above_line = page.find_all('text:above(\"line[width&gt;=2]\")')\ntext_above_line\n</pre> # First, find a thick horizontal line ref_line = page.find('line[width&gt;=2]')  # Now find text that's above that line text_above_line = page.find_all('text:above(\"line[width&gt;=2]\")') text_above_line Out[16]: <pre>&lt;ElementCollection[TextElement](count=17)&gt;</pre> In\u00a0[17]: Copied! <pre># Case-insensitive search\npage.find_all('text:contains(\"summary\")', case=False)\n</pre> # Case-insensitive search page.find_all('text:contains(\"summary\")', case=False) Out[17]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[18]: Copied! <pre># Regular expression search (for patterns like inspection IDs)\npage.find_all('text:contains(\"INS-\\\\w+\")', regex=True)\n</pre> # Regular expression search (for patterns like inspection IDs) page.find_all('text:contains(\"INS-\\\\w+\")', regex=True) Out[18]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[19]: Copied! <pre># Combine regex with case-insensitivity\npage.find_all('text:contains(\"jungle health\")', regex=True, case=False)\n</pre> # Combine regex with case-insensitivity page.find_all('text:contains(\"jungle health\")', regex=True, case=False) Out[19]: <pre>&lt;ElementCollection[TextElement](count=2)&gt;</pre> In\u00a0[20]: Copied! <pre># Get all headings (large, bold text)\nheadings = page.find_all('text[size&gt;=11]:bold')\nheadings\n</pre> # Get all headings (large, bold text) headings = page.find_all('text[size&gt;=11]:bold') headings Out[20]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[21]: Copied! <pre># Get the first and last heading in reading order\nfirst = headings.first\nlast = headings.last\n(first, last)\n</pre> # Get the first and last heading in reading order first = headings.first last = headings.last (first, last) Out[21]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold', 'strike'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold', 'strike'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[22]: Copied! <pre># Get the physically highest/lowest element\nhighest = headings.highest()\nlowest = headings.lowest()\n(highest, lowest)\n</pre> # Get the physically highest/lowest element highest = headings.highest() lowest = headings.lowest() (highest, lowest) Out[22]: <pre>(&lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold', 'strike'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;,\n &lt;TextElement text='Violations' font='Helvetica' size=12.0, style=['bold', 'strike'] bbox=(50.0, 372.484, 107.348, 384.484)&gt;)</pre> In\u00a0[23]: Copied! <pre># Filter the collection further\nservice_headings = headings.filter(lambda heading: 'Service' in heading.extract_text())\n</pre> # Filter the collection further service_headings = headings.filter(lambda heading: 'Service' in heading.extract_text()) In\u00a0[24]: Copied! <pre># Extract text from all elements at once\nheadings.extract_text()\n</pre> # Extract text from all elements at once headings.extract_text() Out[24]: <pre>'Violations'</pre> <p>Note: <code>.highest()</code>, <code>.lowest()</code>, etc. will complain if your collection spans multiple pages.</p> In\u00a0[25]: Copied! <pre># Find text with specific font variants (if they exist)\npage.find_all('text[font-variant=AAAAAB]')\n</pre> # Find text with specific font variants (if they exist) page.find_all('text[font-variant=AAAAAB]') Out[25]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre> In\u00a0[26]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/geometry.pdf\")\npage = pdf.pages[0]\n\nrect = page.find('rect')\nrect.show(width=500)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/geometry.pdf\") page = pdf.pages[0]  rect = page.find('rect') rect.show(width=500) Out[26]:"},{"location":"element-selection/#finding-what-you-need-in-pdfs","title":"Finding What You Need in PDFs\u00b6","text":"<p>Finding specific content in PDFs is like being a detective - you need the right tools to hunt down exactly what you're looking for. Natural PDF uses CSS-like selectors to help you find text, lines, images, and other elements in your documents. Think of it like using browser developer tools, but for PDFs.</p>"},{"location":"element-selection/#setup","title":"Setup\u00b6","text":"<p>Let's load up a sample PDF to experiment with. This one has various elements we can practice finding.</p>"},{"location":"element-selection/#the-basics-finding-elements","title":"The Basics: Finding Elements\u00b6","text":"<p>You have two main tools: <code>find()</code> (gets the first match) and <code>find_all()</code> (gets everything that matches). The basic pattern is <code>element_type[attribute_filter]:pseudo_class</code>.</p>"},{"location":"element-selection/#finding-text-by-what-it-says","title":"Finding Text by What It Says\u00b6","text":""},{"location":"element-selection/#finding-different-types-of-elements","title":"Finding Different Types of Elements\u00b6","text":"<p>PDFs contain more than just text - there are rectangles, lines, images, and other shapes.</p>"},{"location":"element-selection/#filtering-by-properties","title":"Filtering by Properties\u00b6","text":"<p>Use square brackets <code>[]</code> to filter elements by their characteristics - size, color, font, etc.</p>"},{"location":"element-selection/#common-properties-you-can-filter-on","title":"Common Properties You Can Filter On\u00b6","text":"Property Example Usage What It Does Notes <code>size</code> (text) <code>text[size&gt;=12]</code> Font size in points Use <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code> <code>fontname</code> <code>text[fontname*=Bold]</code> Font family name <code>*=</code> means \"contains\" <code>color</code> (text) <code>text[color~=red]</code> Text color <code>~=</code> for approximate match <code>width</code> (line) <code>line[width&gt;1]</code> Line thickness Useful for finding borders <code>source</code> <code>text[source=ocr]</code> Where text came from <code>pdf</code>, <code>ocr</code>, or <code>detected</code> <code>type</code> (region) <code>region[type=table]</code> Layout analysis result From layout detection models"},{"location":"element-selection/#using-special-conditions-pseudo-classes","title":"Using Special Conditions (Pseudo-Classes)\u00b6","text":"<p>These are powerful filters that let you find elements based on their content or relationship to other elements.</p>"},{"location":"element-selection/#common-pseudo-classes","title":"Common Pseudo-Classes\u00b6","text":"Pseudo-Class Example What It Finds <code>:contains('text')</code> <code>text:contains('Report')</code> Elements containing specific text <code>:bold</code> <code>text:bold</code> Bold text (detected automatically) <code>:italic</code> <code>text:italic</code> Italic text <code>:strike</code> <code>text:strike</code> Struck-through text <code>:underline</code> <code>text:underline</code> Underlined text <code>:below(selector)</code> <code>text:below('line[width&gt;=2]')</code> Elements below another element <code>:above(selector)</code> <code>text:above('text:contains(\"Summary\")')</code> Elements above another element <code>:near(selector)</code> <code>text:near('image')</code> Elements close to another element <p>Spatial pseudo-classes like <code>:below</code> and <code>:above</code> work based on the first element that matches the inner selector.</p>"},{"location":"element-selection/#excluding-things-with-not","title":"Excluding Things with <code>:not()</code>\u00b6","text":"<p>Sometimes it's easier to say what you don't want than what you do want.</p>"},{"location":"element-selection/#finding-things-relative-to-other-things","title":"Finding Things Relative to Other Things\u00b6","text":"<p>This is super useful when you know the structure of your document.</p>"},{"location":"element-selection/#advanced-text-searching","title":"Advanced Text Searching\u00b6","text":"<p>When you need more control over how text matching works:</p>"},{"location":"element-selection/#working-with-groups-of-elements","title":"Working with Groups of Elements\u00b6","text":"<p><code>find_all()</code> returns an <code>ElementCollection</code> - like a list, but with PDF-specific superpowers.</p>"},{"location":"element-selection/#dealing-with-weird-font-names","title":"Dealing with Weird Font Names\u00b6","text":"<p>PDFs sometimes have bizarre font names that don't look like normal fonts. Don't worry - they're usually normal fonts with weird internal names.</p>"},{"location":"element-selection/#testing-relationships-between-elements","title":"Testing Relationships Between Elements\u00b6","text":"<p>Want to see how elements relate to each other spatially? Let's try a different PDF:</p>"},{"location":"extracting-clean-text/","title":"Extract Clean Text Without Headers and Footers","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find and exclude the header (top 10% of page)\nheader_region = page.create_region(0, 0, page.width, page.height * 0.1)\npage.add_exclusion(header_region)\n\n# Find and exclude footer (bottom 10% of page)\nfooter_region = page.create_region(0, page.height * 0.9, page.width, page.height)\npage.add_exclusion(footer_region)\n\n# Now extract clean text\nclean_text = page.extract_text()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find and exclude the header (top 10% of page) header_region = page.create_region(0, 0, page.width, page.height * 0.1) page.add_exclusion(header_region)  # Find and exclude footer (bottom 10% of page) footer_region = page.create_region(0, page.height * 0.9, page.width, page.height) page.add_exclusion(footer_region)  # Now extract clean text clean_text = page.extract_text() In\u00a0[2]: Copied! <pre># Exclude anything containing \"CONFIDENTIAL\"\nconfidential = page.find('text:contains(\"CONFIDENTIAL\")')\nif confidential:\n    page.add_exclusion(confidential.above())  # Everything above it\n\n# Exclude page numbers (usually small text with just numbers)\npage_nums = page.find_all('text:contains(\"^\\\\d+$\")', regex=True)\nfor num in page_nums:\n    page.add_exclusion(num)\n\n# Exclude elements by position (like top-right logos)\ntop_right = page.create_region(page.width * 0.7, 0, page.width, page.height * 0.15)\npage.add_exclusion(top_right)\n</pre> # Exclude anything containing \"CONFIDENTIAL\" confidential = page.find('text:contains(\"CONFIDENTIAL\")') if confidential:     page.add_exclusion(confidential.above())  # Everything above it  # Exclude page numbers (usually small text with just numbers) page_nums = page.find_all('text:contains(\"^\\\\d+$\")', regex=True) for num in page_nums:     page.add_exclusion(num)  # Exclude elements by position (like top-right logos) top_right = page.create_region(page.width * 0.7, 0, page.width, page.height * 0.15) page.add_exclusion(top_right) Out[2]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[3]: Copied! <pre># Define exclusions that adapt to each page\ndef exclude_header(page):\n    # Top 50 points of every page\n    return page.create_region(0, 0, page.width, 50)\n\ndef exclude_footer(page):\n    # Bottom 30 points of every page\n    return page.create_region(0, page.height - 30, page.width, page.height)\n\ndef exclude_watermark(page):\n    # Find \"DRAFT\" watermark if it exists\n    draft = page.find('text:contains(\"DRAFT\")')\n    return draft.create_region() if draft else None\n\n# Apply to entire PDF\npdf.add_exclusion(exclude_header, label=\"Headers\")\npdf.add_exclusion(exclude_footer, label=\"Footers\")\npdf.add_exclusion(exclude_watermark, label=\"Watermarks\")\n\n# Extract clean text from any page\nclean_text = pdf.pages[0].extract_text()  # Headers/footers automatically excluded\n</pre> # Define exclusions that adapt to each page def exclude_header(page):     # Top 50 points of every page     return page.create_region(0, 0, page.width, 50)  def exclude_footer(page):     # Bottom 30 points of every page     return page.create_region(0, page.height - 30, page.width, page.height)  def exclude_watermark(page):     # Find \"DRAFT\" watermark if it exists     draft = page.find('text:contains(\"DRAFT\")')     return draft.create_region() if draft else None  # Apply to entire PDF pdf.add_exclusion(exclude_header, label=\"Headers\") pdf.add_exclusion(exclude_footer, label=\"Footers\") pdf.add_exclusion(exclude_watermark, label=\"Watermarks\")  # Extract clean text from any page clean_text = pdf.pages[0].extract_text()  # Headers/footers automatically excluded In\u00a0[4]: Copied! <pre># Apply OCR\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Only use high-confidence OCR text\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = reliable_text.extract_text()\n\n# Or exclude low-confidence noise\nnoisy_text = page.find_all('text[source=ocr][confidence&lt;0.5]')\nfor noise in noisy_text:\n    page.add_exclusion(noise)\n</pre> # Apply OCR page.apply_ocr(engine='easyocr', languages=['en'])  # Only use high-confidence OCR text reliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]') clean_text = reliable_text.extract_text()  # Or exclude low-confidence noise noisy_text = page.find_all('text[source=ocr][confidence&lt;0.5]') for noise in noisy_text:     page.add_exclusion(noise) <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> In\u00a0[5]: Copied! <pre># Extract just the main content column (avoiding sidebars)\nmain_column = page.create_region(\n    x0=page.width * 0.1,      # Start 10% from left\n    top=page.height * 0.15,   # Skip header area\n    x1=page.width * 0.7,      # End before sidebar\n    bottom=page.height * 0.9   # Stop before footer\n)\n\ncolumn_text = main_column.extract_text()\n</pre> # Extract just the main content column (avoiding sidebars) main_column = page.create_region(     x0=page.width * 0.1,      # Start 10% from left     top=page.height * 0.15,   # Skip header area     x1=page.width * 0.7,      # End before sidebar     bottom=page.height * 0.9   # Stop before footer )  column_text = main_column.extract_text() In\u00a0[6]: Copied! <pre># Highlight what you're about to exclude\nheader = page.create_region(0, 0, page.width, 50)\nfooter = page.create_region(0, page.height - 30, page.width, page.height)\n\n# Show the page to verify\nheader.show()\nfooter.show()\n\n# If it looks right, apply the exclusions\npage.add_exclusion(header)\npage.add_exclusion(footer)\n\n# Review what it looks like\npage.show(exclusions='red')\n</pre> # Highlight what you're about to exclude header = page.create_region(0, 0, page.width, 50) footer = page.create_region(0, page.height - 30, page.width, page.height)  # Show the page to verify header.show() footer.show()  # If it looks right, apply the exclusions page.add_exclusion(header) page.add_exclusion(footer)  # Review what it looks like page.show(exclusions='red') Out[6]: In\u00a0[7]: Copied! <pre># Extract with and without exclusions to see the difference\nfull_text = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text(use_exclusions=True)\n\nprint(f\"Original: {len(full_text)} characters\")\nprint(f\"Clean: {len(clean_text)} characters\")\nprint(f\"Removed: {len(full_text) - len(clean_text)} characters\")\n</pre> # Extract with and without exclusions to see the difference full_text = page.extract_text(use_exclusions=False) clean_text = page.extract_text(use_exclusions=True)  print(f\"Original: {len(full_text)} characters\") print(f\"Clean: {len(clean_text)} characters\") print(f\"Removed: {len(full_text) - len(clean_text)} characters\") <pre>Original: 2458 characters\nClean: 2247 characters\nRemoved: 211 characters\n</pre> In\u00a0[8]: Copied! <pre># Remove headers with logos and contact info\npage.add_exclusion(page.create_region(0, 0, page.width, 80))\n\n# Remove footers with page numbers and dates\npage.add_exclusion(page.create_region(0, page.height - 40, page.width, page.height))\n</pre> # Remove headers with logos and contact info page.add_exclusion(page.create_region(0, 0, page.width, 80))  # Remove footers with page numbers and dates page.add_exclusion(page.create_region(0, page.height - 40, page.width, page.height)) Out[8]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[9]: Copied! <pre># Remove running headers with paper title\nheader = page.find('text[size&lt;=10]').above() if page.find('text[size&lt;=10]') else None\nif header:\n    page.add_exclusion(header)\n\n# Remove footnotes (small text at bottom)\nfootnotes = page.find_all('text[size&lt;=8]')\nfor note in footnotes:\n    if note.top &gt; page.height * 0.8:  # Bottom 20% of page\n        page.add_exclusion(note)\n</pre> # Remove running headers with paper title header = page.find('text[size&lt;=10]').above() if page.find('text[size&lt;=10]') else None if header:     page.add_exclusion(header)  # Remove footnotes (small text at bottom) footnotes = page.find_all('text[size&lt;=8]') for note in footnotes:     if note.top &gt; page.height * 0.8:  # Bottom 20% of page         page.add_exclusion(note) In\u00a0[10]: Copied! <pre># Remove classification markings\nclassifications = page.find_all('text:contains(\"CONFIDENTIAL|SECRET|UNCLASSIFIED\")', regex=True)\nfor mark in classifications:\n    page.add_exclusion(mark)\n\n# Remove agency headers\nagency_header = page.find('text:contains(\"Department of|Agency|Office of\")', regex=True)\nif agency_header:\n    page.add_exclusion(agency_header.above())\n</pre> # Remove classification markings classifications = page.find_all('text:contains(\"CONFIDENTIAL|SECRET|UNCLASSIFIED\")', regex=True) for mark in classifications:     page.add_exclusion(mark)  # Remove agency headers agency_header = page.find('text:contains(\"Department of|Agency|Office of\")', regex=True) if agency_header:     page.add_exclusion(agency_header.above()) In\u00a0[11]: Copied! <pre># Arabic example \u2013 no special flags required\npage = pdf.pages[0]\nbody = page.extract_text()  # parentheses and numbers appear correctly\n\n# String queries work naturally\nrow = page.find(\"text:contains('\u0627\u0644\u062c\u0631\u064a\u062f\u0629 \u0627\u0644\u0631\u0633\u0645\u064a\u0629')\")\n\n# Disable the BiDi pass if you need raw PDF order\nraw = page.extract_text(bidi=False)\n</pre> # Arabic example \u2013 no special flags required page = pdf.pages[0] body = page.extract_text()  # parentheses and numbers appear correctly  # String queries work naturally row = page.find(\"text:contains('\u0627\u0644\u062c\u0631\u064a\u062f\u0629 \u0627\u0644\u0631\u0633\u0645\u064a\u0629')\")  # Disable the BiDi pass if you need raw PDF order raw = page.extract_text(bidi=False) <p>Tip: This RTL handling is line-aware, so mixed LTR/RTL documents (e.g. Arabic with English dates) still extract correctly without affecting Latin text on other pages.</p>"},{"location":"extracting-clean-text/#extract-clean-text-without-headers-and-footers","title":"Extract Clean Text Without Headers and Footers\u00b6","text":"<p>You've got a PDF where you need the main content, but every page has headers, footers, watermarks, or other junk that's messing up your text extraction. Here's how to get just the content you want.</p>"},{"location":"extracting-clean-text/#the-problem","title":"The Problem\u00b6","text":"<p>PDFs often have repeated content on every page that you don't want:</p> <ul> <li>Company headers with logos and contact info</li> <li>Page numbers and footers</li> <li>\"CONFIDENTIAL\" watermarks</li> <li>Navigation elements from web-to-PDF conversions</li> </ul> <p>When you extract text normally, all this noise gets mixed in with your actual content.</p>"},{"location":"extracting-clean-text/#quick-solution-exclude-by-pattern","title":"Quick Solution: Exclude by Pattern\u00b6","text":"<p>If the unwanted content is consistent across pages, you can exclude it once:</p>"},{"location":"extracting-clean-text/#exclude-specific-elements","title":"Exclude Specific Elements\u00b6","text":"<p>For more precision, exclude specific text or elements:</p>"},{"location":"extracting-clean-text/#apply-exclusions-to-all-pages","title":"Apply Exclusions to All Pages\u00b6","text":"<p>Set up exclusions that work across your entire document:</p>"},{"location":"extracting-clean-text/#remove-noise-from-scanned-documents","title":"Remove Noise from Scanned Documents\u00b6","text":"<p>For scanned PDFs, apply OCR first, then filter by confidence:</p>"},{"location":"extracting-clean-text/#handle-multi-column-layouts","title":"Handle Multi-Column Layouts\u00b6","text":"<p>Extract text from specific columns or sections:</p>"},{"location":"extracting-clean-text/#visual-debugging","title":"Visual Debugging\u00b6","text":"<p>See what you're excluding before committing:</p>"},{"location":"extracting-clean-text/#compare-before-and-after","title":"Compare Before and After\u00b6","text":""},{"location":"extracting-clean-text/#common-patterns","title":"Common Patterns\u00b6","text":""},{"location":"extracting-clean-text/#corporate-reports","title":"Corporate Reports\u00b6","text":""},{"location":"extracting-clean-text/#academic-papers","title":"Academic Papers\u00b6","text":""},{"location":"extracting-clean-text/#government-documents","title":"Government Documents\u00b6","text":""},{"location":"extracting-clean-text/#when-things-go-wrong","title":"When Things Go Wrong\u00b6","text":"<ul> <li>Problem: Headers vary between pages</li> <li>Solution: Use adaptive exclusions</li> </ul> <pre>def smart_header_exclusion(page):\n    # Look for common header patterns\n    logo = page.find('image')\n    company_name = page.find('text:contains(\"ACME Corp\")')\n\n    if logo:\n        return logo.above()\n    elif company_name and company_name.top &lt; page.height * 0.2:\n        return company_name.above()\n    else:\n        return page.create_region(0, 0, page.width, 60)  # Fallback\n\npdf.add_exclusion(smart_header_exclusion)\n</pre> <ul> <li>Problem: Need to preserve some header information</li> <li>Solution: Extract before excluding</li> </ul> <pre># Get the document title from the header first\ntitle = page.find('text[size&gt;=14]:bold')\ndocument_title = title.text if title else \"Unknown\"\n\n# Then exclude the header for clean body text\npage.add_exclusion(page.create_region(0, 0, page.width, 100))\nbody_text = page.extract_text()\n</pre>"},{"location":"extracting-clean-text/#handling-right-to-left-arabic-hebrew-text","title":"Handling Right-to-left (Arabic, Hebrew) Text\u00b6","text":"<p>Natural-PDF now automatically detects bidirectional (RTL) lines and applies the Unicode BiDi algorithm when you call <code>page.extract_text()</code>. This means the returned string is in logical reading order with brackets/parentheses correctly mirrored and Western digits left untouched.</p>"},{"location":"extracting-clean-text/","title":"Extract Clean Text Without Headers and Footers","text":"<p>You've got a PDF where you need the main content, but every page has headers, footers, watermarks, or other junk that's messing up your text extraction. Here's how to get just the content you want.</p>"},{"location":"extracting-clean-text/#the-problem","title":"The Problem","text":"<p>PDFs often have repeated content on every page that you don't want:</p> <ul> <li>Company headers with logos and contact info</li> <li>Page numbers and footers</li> <li>\"CONFIDENTIAL\" watermarks</li> <li>Navigation elements from web-to-PDF conversions</li> </ul> <p>When you extract text normally, all this noise gets mixed in with your actual content.</p>"},{"location":"extracting-clean-text/#quick-solution-exclude-by-pattern","title":"Quick Solution: Exclude by Pattern","text":"<p>If the unwanted content is consistent across pages, you can exclude it once:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find and exclude the header (top 10% of page)\nheader_region = page.create_region(0, 0, page.width, page.height * 0.1)\npage.add_exclusion(header_region)\n\n# Find and exclude footer (bottom 10% of page)\nfooter_region = page.create_region(0, page.height * 0.9, page.width, page.height)\npage.add_exclusion(footer_region)\n\n# Now extract clean text\nclean_text = page.extract_text()\n</code></pre>"},{"location":"extracting-clean-text/#exclude-specific-elements","title":"Exclude Specific Elements","text":"<p>For more precision, exclude specific text or elements:</p> <pre><code># Exclude anything containing \"CONFIDENTIAL\"\nconfidential = page.find('text:contains(\"CONFIDENTIAL\")')\nif confidential:\n    page.add_exclusion(confidential.above())  # Everything above it\n\n# Exclude page numbers (usually small text with just numbers)\npage_nums = page.find_all('text:contains(\"^\\\\d+$\")', regex=True)\nfor num in page_nums:\n    page.add_exclusion(num)\n\n# Exclude elements by position (like top-right logos)\ntop_right = page.create_region(page.width * 0.7, 0, page.width, page.height * 0.15)\npage.add_exclusion(top_right)\n</code></pre>"},{"location":"extracting-clean-text/#apply-exclusions-to-all-pages","title":"Apply Exclusions to All Pages","text":"<p>Set up exclusions that work across your entire document:</p> <pre><code># Define exclusions that adapt to each page\ndef exclude_header(page):\n    # Top 50 points of every page\n    return page.create_region(0, 0, page.width, 50)\n\ndef exclude_footer(page):\n    # Bottom 30 points of every page\n    return page.create_region(0, page.height - 30, page.width, page.height)\n\ndef exclude_watermark(page):\n    # Find \"DRAFT\" watermark if it exists\n    draft = page.find('text:contains(\"DRAFT\")')\n    return draft.create_region() if draft else None\n\n# Apply to entire PDF\npdf.add_exclusion(exclude_header, label=\"Headers\")\npdf.add_exclusion(exclude_footer, label=\"Footers\")\npdf.add_exclusion(exclude_watermark, label=\"Watermarks\")\n\n# Extract clean text from any page\nclean_text = pdf.pages[0].extract_text()  # Headers/footers automatically excluded\n</code></pre>"},{"location":"extracting-clean-text/#remove-noise-from-scanned-documents","title":"Remove Noise from Scanned Documents","text":"<p>For scanned PDFs, apply OCR first, then filter by confidence:</p> <pre><code># Apply OCR\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Only use high-confidence OCR text\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = reliable_text.extract_text()\n\n# Or exclude low-confidence noise\nnoisy_text = page.find_all('text[source=ocr][confidence&lt;0.5]')\nfor noise in noisy_text:\n    page.add_exclusion(noise)\n</code></pre>"},{"location":"extracting-clean-text/#handle-multi-column-layouts","title":"Handle Multi-Column Layouts","text":"<p>Extract text from specific columns or sections:</p> <pre><code># Extract just the main content column (avoiding sidebars)\nmain_column = page.create_region(\n    x0=page.width * 0.1,      # Start 10% from left\n    top=page.height * 0.15,   # Skip header area\n    x1=page.width * 0.7,      # End before sidebar\n    bottom=page.height * 0.9   # Stop before footer\n)\n\ncolumn_text = main_column.extract_text()\n</code></pre>"},{"location":"extracting-clean-text/#visual-debugging","title":"Visual Debugging","text":"<p>See what you're excluding before committing:</p> <pre><code># Highlight what you're about to exclude\nheader = page.create_region(0, 0, page.width, 50)\nfooter = page.create_region(0, page.height - 30, page.width, page.height)\n\n# Show the page to verify\nheader.show()\nfooter.show()\n\n# If it looks right, apply the exclusions\npage.add_exclusion(header)\npage.add_exclusion(footer)\n\n# Review what it looks like\npage.show(exclusions='red')\n</code></pre>"},{"location":"extracting-clean-text/#compare-before-and-after","title":"Compare Before and After","text":"<pre><code># Extract with and without exclusions to see the difference\nfull_text = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text(use_exclusions=True)\n\nprint(f\"Original: {len(full_text)} characters\")\nprint(f\"Clean: {len(clean_text)} characters\")\nprint(f\"Removed: {len(full_text) - len(clean_text)} characters\")\n</code></pre>"},{"location":"extracting-clean-text/#common-patterns","title":"Common Patterns","text":""},{"location":"extracting-clean-text/#corporate-reports","title":"Corporate Reports","text":"<pre><code># Remove headers with logos and contact info\npage.add_exclusion(page.create_region(0, 0, page.width, 80))\n\n# Remove footers with page numbers and dates\npage.add_exclusion(page.create_region(0, page.height - 40, page.width, page.height))\n</code></pre>"},{"location":"extracting-clean-text/#academic-papers","title":"Academic Papers","text":"<pre><code># Remove running headers with paper title\nheader = page.find('text[size&lt;=10]').above() if page.find('text[size&lt;=10]') else None\nif header:\n    page.add_exclusion(header)\n\n# Remove footnotes (small text at bottom)\nfootnotes = page.find_all('text[size&lt;=8]')\nfor note in footnotes:\n    if note.top &gt; page.height * 0.8:  # Bottom 20% of page\n        page.add_exclusion(note)\n</code></pre>"},{"location":"extracting-clean-text/#government-documents","title":"Government Documents","text":"<pre><code># Remove classification markings\nclassifications = page.find_all('text:contains(\"CONFIDENTIAL|SECRET|UNCLASSIFIED\")', regex=True)\nfor mark in classifications:\n    page.add_exclusion(mark)\n\n# Remove agency headers\nagency_header = page.find('text:contains(\"Department of|Agency|Office of\")', regex=True)\nif agency_header:\n    page.add_exclusion(agency_header.above())\n</code></pre>"},{"location":"extracting-clean-text/#when-things-go-wrong","title":"When Things Go Wrong","text":"<ul> <li>Problem: Headers vary between pages</li> <li>Solution: Use adaptive exclusions</li> </ul> <pre><code>def smart_header_exclusion(page):\n    # Look for common header patterns\n    logo = page.find('image')\n    company_name = page.find('text:contains(\"ACME Corp\")')\n\n    if logo:\n        return logo.above()\n    elif company_name and company_name.top &lt; page.height * 0.2:\n        return company_name.above()\n    else:\n        return page.create_region(0, 0, page.width, 60)  # Fallback\n\npdf.add_exclusion(smart_header_exclusion)\n</code></pre> <ul> <li>Problem: Need to preserve some header information</li> <li>Solution: Extract before excluding</li> </ul> <pre><code># Get the document title from the header first\ntitle = page.find('text[size&gt;=14]:bold')\ndocument_title = title.text if title else \"Unknown\"\n\n# Then exclude the header for clean body text\npage.add_exclusion(page.create_region(0, 0, page.width, 100))\nbody_text = page.extract_text()\n</code></pre>"},{"location":"extracting-clean-text/#handling-right-to-left-arabic-hebrew-text","title":"Handling Right-to-left (Arabic, Hebrew) Text","text":"<p>Natural-PDF now automatically detects bidirectional (RTL) lines and applies the Unicode BiDi algorithm when you call <code>page.extract_text()</code>. This means the returned string is in logical reading order with brackets/parentheses correctly mirrored and Western digits left untouched.</p> <pre><code># Arabic example \u2013 no special flags required\npage = pdf.pages[0]\nbody = page.extract_text()  # parentheses and numbers appear correctly\n\n# String queries work naturally\nrow = page.find(\"text:contains('\u0627\u0644\u062c\u0631\u064a\u062f\u0629 \u0627\u0644\u0631\u0633\u0645\u064a\u0629')\")\n\n# Disable the BiDi pass if you need raw PDF order\nraw = page.extract_text(bidi=False)\n</code></pre> <p>Tip: This RTL handling is line-aware, so mixed LTR/RTL documents (e.g. Arabic with English dates) still extract correctly without affecting Latin text on other pages.</p>"},{"location":"finetuning/","title":"OCR Fine-tuning","text":"<p>While the built-in OCR engines (EasyOCR, PaddleOCR, Surya) offer good general performance, you might encounter situations where their accuracy isn't sufficient for your specific needs. This is often the case with:</p> <ul> <li>Unique Fonts: Documents using unusual or stylized fonts.</li> <li>Specific Languages: Languages or scripts not perfectly covered by the default models.</li> <li>Low Quality Scans: Noisy or degraded document images.</li> <li>Specialized Layouts: Text within complex tables, forms, or unusual arrangements.</li> </ul> <p>Fine-tuning allows you to adapt a pre-trained OCR recognition model to your specific data, significantly improving its accuracy on documents similar to those used for training.</p>"},{"location":"finetuning/#why-fine-tune","title":"Why Fine-tune?","text":"<ul> <li>Higher Accuracy: Achieve better text extraction results on your specific document types.</li> <li>Adaptability: Train the model to recognize domain-specific terms, symbols, or layouts.</li> <li>Reduced Errors: Minimize downstream errors in data extraction and processing pipelines.</li> </ul>"},{"location":"finetuning/#strategy-detect-llm-correct-export","title":"Strategy: Detect + LLM Correct + Export","text":"<p>Training an OCR model requires accurate ground truth: images of text snippets paired with their correct transcriptions. Manually creating this data is tedious. A powerful alternative leverages the strengths of different models:</p> <ol> <li>Detect Text Regions: Use a robust local OCR engine (like Surya or PaddleOCR) primarily for its detection capabilities (<code>detect_only=True</code>). This identifies the locations of text on the page, even if the initial recognition isn't perfect. You can combine this with layout analysis or region selections (<code>.region()</code>, <code>.below()</code>, <code>.add_exclusion()</code>) to focus on the specific areas you care about.</li> <li>Correct with LLM: For each detected text region, send the image snippet to a powerful Large Language Model (LLM) with multimodal capabilities (like GPT-4o, Claude 3.5 Sonnet/Haiku) using the <code>direct_ocr_llm</code> utility. The LLM performs high-accuracy OCR on the snippet, providing a \"ground truth\" transcription.</li> <li>Export for Fine-tuning: Use the <code>PaddleOCRRecognitionExporter</code> to package the original image snippets (from step 1) along with their corresponding LLM-generated text labels (from step 2) into the specific format required by PaddleOCR for fine-tuning its recognition model.</li> </ol> <p>This approach combines the efficient spatial detection of local models with the superior text recognition of large generative models to create a high-quality fine-tuning dataset with minimal manual effort.</p>"},{"location":"finetuning/#example-fine-tuning-for-greek-spreadsheet-text","title":"Example: Fine-tuning for Greek Spreadsheet Text","text":"<p>Let's walk through an example of preparing data to fine-tune PaddleOCR for text from a scanned Greek spreadsheet, adapting the process described above.</p> <pre><code># --- 1. Setup and Load PDF ---\nfrom natural_pdf import PDF\nfrom natural_pdf.ocr.utils import direct_ocr_llm\nfrom natural_pdf.exporters import PaddleOCRRecognitionExporter\nimport openai # Or your preferred LLM client library\nimport os\n\n# Ensure your LLM API key is set (using environment variables is recommended)\n# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\" \n# os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\" \n\n# pdf_path = \"path/to/your/document.pdf\" \npdf_path = \"path/to/your/document.pdf\" \n# For demonstration we use a public sample PDF; replace with your own.\npdf_path = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\"\npdf = PDF(pdf_path)\n\n# --- 2. (Optional) Exclude Irrelevant Areas ---\n# If the document has consistent headers, footers, or margins you want to ignore\n# Use exclusions *before* detection\npdf.add_exclusion(lambda page: page.region(right=45)) # Exclude left margin/line numbers\npdf.add_exclusion(lambda page: page.region(left=500)) # Exclude right margin\n\n# --- 3. Detect Text Regions ---\n# Use a good detection engine. Surya is often robust for line detection.\n# We only want the bounding boxes, not the initial (potentially inaccurate) OCR text.\nprint(\"Detecting text regions...\")\n# Process only a subset of pages for demonstration if needed\nfor page in pdf.pages[:10]:\n    # Use a moderate resolution for detection; higher res used for LLM correction later\n    page.apply_ocr(engine='surya', resolution=120, detect_only=True) \nprint(f\"Detection complete for {num_pages_to_process} pages.\")\n\n# (Optional) Visualize detected boxes on a sample page\n# pdf.pages[9].find_all('text[source=ocr]').show() \n\n# --- 4. Correct with LLM ---\n# Configure your LLM client (example using OpenAI client, adaptable for others)\n# For Anthropic: client = openai.OpenAI(base_url=\"https://api.anthropic.com/v1/\", api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))\nclient = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\")) \n\n# Craft a clear prompt for the LLM\n# Be as specific as possible! If it's in a specific language, what kinds\n# of characters, etc.\nprompt = \"\"\"OCR this image patch. Return only the exact text content visible in the image. \nPreserve original spelling, capitalization, punctuation, and symbols. \nDo not add any explanatory text, translations, comments, or quotation marks around the result.\nThe text is likely from a Greek document, potentially a spreadsheet, containing Modern Greek words or numbers.\"\"\"\n\n# Define the correction function using direct_ocr_llm\ndef correct_text_region(region):\n    # Use a high resolution for the LLM call for best accuracy\n    return direct_ocr_llm(\n        region, \n        client, \n        prompt=prompt, \n        resolution=300, \n        # model=\"claude-3-5-sonnet-20240620\" # Example Anthropic model\n        model=\"gpt-4o-mini\" # Example OpenAI model\n    )\n\n# Apply the correction function to the detected text regions\nprint(\"Applying LLM correction to detected regions...\")\nfor page in pdf.pages[:num_pages_to_process]:\n    # This finds elements added by apply_ocr and passes their regions to 'correct_text_region'\n    # The returned text from the LLM replaces the original OCR text for these elements\n    # The source attribute is updated (e.g., to 'ocr-llm-corrected')\n    page.correct_ocr(correct_text_region) \nprint(\"LLM correction complete.\")\n\n# --- 5. Export for PaddleOCR Fine-tuning ---\nprint(\"Configuring exporter...\")\nexporter = PaddleOCRRecognitionExporter(\n    # Select all of the non-blank OCR text\n    # Hopefully it's all been LLM-corrected! \n    selector=\"text[source^=ocr][text!='']\", \n    resolution=300,     # Resolution for the exported image crops\n    padding=2,          # Add slight padding around text boxes\n    split_ratio=0.9,    # 90% for training, 10% for validation\n    random_seed=42,     # For reproducible train/val split\n    include_guide=True  # Include the Colab fine-tuning notebook\n)\n\n# Define the output directory\noutput_directory = \"./my_paddleocr_finetune_data\"\nprint(f\"Exporting data to {output_directory}...\")\n\n# Run the export process\nexporter.export(pdf, output_directory)\n\nprint(\"Export complete.\")\nprint(f\"Dataset ready for fine-tuning in: {output_directory}\")\nprint(f\"Next step: Upload '{os.path.join(output_directory, 'fine_tune_paddleocr.ipynb')}' and the rest of the contents to Google Colab.\")\n\n# --- Cleanup ---\npdf.close() \n</code></pre>"},{"location":"finetuning/#running-the-fine-tuning","title":"Running the Fine-tuning","text":"<p>The <code>PaddleOCRRecognitionExporter</code> automatically includes a Jupyter Notebook (<code>fine_tune_paddleocr.ipynb</code>) in the output directory. This notebook is pre-configured to guide you through the fine-tuning process on Google Colab (which offers free GPU access):</p> <ol> <li>Upload: Upload the entire output directory (e.g., <code>my_paddleocr_finetune_data</code>) to your Google Drive or directly to your Colab instance.</li> <li>Open Notebook: Open the <code>fine_tune_paddleocr.ipynb</code> notebook in Google Colab.</li> <li>Set Runtime: Ensure the Colab runtime is set to use a GPU (Runtime -&gt; Change runtime type -&gt; GPU).</li> <li>Run Cells: Execute the cells in the notebook sequentially. It will:<ul> <li>Install necessary libraries (PaddlePaddle, PaddleOCR).</li> <li>Point the training configuration to your uploaded dataset (<code>images/</code>, <code>train.txt</code>, <code>val.txt</code>, <code>dict.txt</code>).</li> <li>Download a pre-trained PaddleOCR model (usually a multilingual one).</li> <li>Start the fine-tuning process using your data.</li> <li>Save the fine-tuned model checkpoints.</li> <li>Export the best model into an \"inference format\" suitable for use with <code>natural-pdf</code>.</li> </ul> </li> <li>Download Model: Download the resulting <code>inference_model</code> directory from Colab.</li> </ol>"},{"location":"finetuning/#using-the-fine-tuned-model","title":"Using the Fine-tuned Model","text":"<p>Once you have the <code>inference_model</code> directory, you can instruct <code>natural-pdf</code> to use it for OCR:</p> <pre><code>from natural_pdf import PDF\nfrom natural_pdf.ocr import PaddleOCROptions\n\n# Path to the directory you downloaded from Colab\nfinetuned_model_dir = \"/path/to/your/downloaded/inference_model\" \n\n# Specify the path in PaddleOCROptions\npaddle_opts = PaddleOCROptions(\n    rec_model_dir=finetuned_model_dir,\n    rec_char_dict_path=os.path.join(finetuned_model_dir, 'your_dict.txt') # Or wherever your dict is\n    use_gpu=True # If using GPU locally\n)\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Apply OCR using your fine-tuned model\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# Extract text using the improved results\ntext = page.extract_text() \nprint(text)\n\npdf.close()\n</code></pre> <p>By following this process, you can significantly enhance OCR performance on your specific documents using the power of fine-tuning. </p>"},{"location":"fix-messy-tables/","title":"Fix Messy Table Extraction","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Detect table regions using AI\npage.analyze_layout(engine='yolo')\n\n# Find and extract detected tables\ntable_regions = page.find_all('region[type=table]')\nfor i, table in enumerate(table_regions):\n    data = table.extract_table()\n    print(f\"Table {i+1}: {len(data)} rows\")\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Detect table regions using AI page.analyze_layout(engine='yolo')  # Find and extract detected tables table_regions = page.find_all('region[type=table]') for i, table in enumerate(table_regions):     data = table.extract_table()     print(f\"Table {i+1}: {len(data)} rows\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpvfofocb4/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 999.2ms\n</pre> <pre>Speed: 6.1ms preprocess, 999.2ms inference, 9.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Table 1: 8 rows\n</pre> In\u00a0[2]: Copied! <pre># Use Table Transformer - understands table structure better\npage.clear_detected_layout_regions()  # Clear previous attempts\npage.analyze_layout(engine='tatr')\n\n# TATR understands rows, columns, and headers\ntable = page.find('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nheaders = page.find_all('region[type=table-column-header][model=tatr]')\n\nprint(f\"Found: {len(rows)} rows, {len(cols)} columns, {len(headers)} headers\")\n\n# Extract using the detected structure\ndata = table.extract_table(method='tatr')\n</pre> # Use Table Transformer - understands table structure better page.clear_detected_layout_regions()  # Clear previous attempts page.analyze_layout(engine='tatr')  # TATR understands rows, columns, and headers table = page.find('region[type=table][model=tatr]') rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') headers = page.find_all('region[type=table-column-header][model=tatr]')  print(f\"Found: {len(rows)} rows, {len(cols)} columns, {len(headers)} headers\")  # Extract using the detected structure data = table.extract_table(method='tatr') <pre>Found: 8 rows, 4 columns, 1 headers\n</pre> In\u00a0[3]: Copied! <pre># Visually inspect the page to find table boundaries\npage.show()\n\n# Manually define the table region\ntable_area = page.create_region(\n    x0=50,      # Left edge of table\n    top=200,    # Top edge of table\n    x1=550,     # Right edge of table\n    bottom=400  # Bottom edge of table\n)\n\n# Highlight to verify\ntable_area.show(color=\"blue\", label=\"Table area\")\n\n# Extract from the defined area\ndata = table_area.extract_table()\n</pre> # Visually inspect the page to find table boundaries page.show()  # Manually define the table region table_area = page.create_region(     x0=50,      # Left edge of table     top=200,    # Top edge of table     x1=550,     # Right edge of table     bottom=400  # Bottom edge of table )  # Highlight to verify table_area.show(color=\"blue\", label=\"Table area\")  # Extract from the defined area data = table_area.extract_table() Pro-tip: Instead of measuring 120\u00a0px below the header, try an   <code>until=...</code> anchor\u2014your script survives if the table grows or the scan   resolution changes.  <pre>header = page.find(text=\"Violations\":bold)\nbody   = header.below(\n    until=\"text:contains('Total violations')\",\n    include_endpoint=False\n)\ndata = body.extract_table()\n</pre> <p>In most cases that one extra <code>until=</code> makes the difference between a brittle coordinate hack and a reusable extractor.</p> In\u00a0[4]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find all header labels\nheaders = page.find_all(\n    'text:contains(\"Statute|Description|Level|Repeat\")',\n    regex=True,\n)\ncol_edges = headers.apply(lambda header: header.x0) + [page.width]\nprint(col_edges)\n\nheaders.show(crop=True)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find all header labels headers = page.find_all(     'text:contains(\"Statute|Description|Level|Repeat\")',     regex=True, ) col_edges = headers.apply(lambda header: header.x0) + [page.width] print(col_edges)  headers.show(crop=True) <pre>[55.0, 105.0, 455.0, 505.0, 612]\n</pre> Out[4]: In\u00a0[5]: Copied! <pre>table_area = (\n    headers[0]\n    .below(\n        until=\"text[size&lt;10]\",\n        include_endpoint=False\n    )\n)\ntable_area.show()\n</pre> table_area = (     headers[0]     .below(         until=\"text[size&lt;10]\",         include_endpoint=False     ) ) table_area.show() Out[5]: In\u00a0[6]: Copied! <pre>row_items = (\n    headers[0]\n    .below(width='element')\n    .find_all('text')\n)\n\nrow_borders = row_items.apply(lambda entry: entry.top) + [row_items[-1].bottom]\nprint(row_borders)\nrow_items.show()\n</pre> row_items = (     headers[0]     .below(width='element')     .find_all('text') )  row_borders = row_items.apply(lambda entry: entry.top) + [row_items[-1].bottom] print(row_borders) row_items.show() <pre>[418.07, 438.07, 458.07, 478.07, 498.07, 518.0699999999999, 538.0699999999999, 548.0699999999999]\n</pre> Out[6]: In\u00a0[7]: Copied! <pre>options = {\n    \"vertical_strategy\": \"explicit\",\n    \"horizontal_strategy\": \"explicit\",\n    \"explicit_vertical_lines\": col_edges,\n    \"explicit_horizontal_lines\": row_borders,\n}\ndata = table_area.extract_table('pdfplumber', table_settings=options)\ndata\n</pre> options = {     \"vertical_strategy\": \"explicit\",     \"horizontal_strategy\": \"explicit\",     \"explicit_vertical_lines\": col_edges,     \"explicit_horizontal_lines\": row_borders, } data = table_area.extract_table('pdfplumber', table_settings=options) data Out[7]: <pre>TableResult(rows=7\u2026)</pre> In\u00a0[8]: Copied! <pre># Customize pdfplumber settings for better alignment\ntable_settings = {\n    \"vertical_strategy\": \"text\",        # Use text alignment instead of lines\n    \"horizontal_strategy\": \"lines\",     # Still use lines for rows\n    \"intersection_x_tolerance\": 10,     # More forgiving column alignment\n    \"intersection_y_tolerance\": 5,      # More forgiving row alignment\n    \"edge_min_length\": 3,              # Shorter minimum line length\n}\n\ndata = table_area.extract_table(table_settings=table_settings)\n</pre> # Customize pdfplumber settings for better alignment table_settings = {     \"vertical_strategy\": \"text\",        # Use text alignment instead of lines     \"horizontal_strategy\": \"lines\",     # Still use lines for rows     \"intersection_x_tolerance\": 10,     # More forgiving column alignment     \"intersection_y_tolerance\": 5,      # More forgiving row alignment     \"edge_min_length\": 3,              # Shorter minimum line length }  data = table_area.extract_table(table_settings=table_settings) In\u00a0[9]: Copied! <pre># Detect all tables\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\nprint(f\"Found {len(tables)} tables\")\n\n# Extract each table separately\nall_data = []\nfor i, table in enumerate(tables):\n    data = table.extract_table()\n    all_data.append(data)\n\n# Show all tables together\nwith page.highlights() as h:\n    for i, table in enumerate(tables):\n        h.add(table, color=\"red\", label=f\"Table {i+1}\")\n    h.show()\n\n# Save each table separately\nimport pandas as pd\nfor i, tbl in enumerate(all_data):\n    df = tbl.to_df(header=\"first\")\n    df.to_csv(f\"table_{i+1}.csv\", index=False)\n</pre> # Detect all tables page.analyze_layout(engine='yolo') tables = page.find_all('region[type=table]')  print(f\"Found {len(tables)} tables\")  # Extract each table separately all_data = [] for i, table in enumerate(tables):     data = table.extract_table()     all_data.append(data)  # Show all tables together with page.highlights() as h:     for i, table in enumerate(tables):         h.add(table, color=\"red\", label=f\"Table {i+1}\")     h.show()  # Save each table separately import pandas as pd for i, tbl in enumerate(all_data):     df = tbl.to_df(header=\"first\")     df.to_csv(f\"table_{i+1}.csv\", index=False) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpa6halb0m/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 3045.2ms\n</pre> <pre>Speed: 9.9ms preprocess, 3045.2ms inference, 1.4ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Found 1 tables\n</pre> In\u00a0[10]: Copied! <pre># For rotated tables, you might need to work with coordinates differently\n# First, see if layout detection picks it up\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\n# If the table appears rotated in the highlights,\n# the extraction might still work normally\nfor table in tables:\n    table.show()  # Check if this looks right\n    data = table.extract_table()\n    if data and len(data) &gt; 1:  # If we got reasonable data\n        print(\"Table extracted successfully despite rotation\")\n</pre> # For rotated tables, you might need to work with coordinates differently # First, see if layout detection picks it up page.analyze_layout(engine='yolo') tables = page.find_all('region[type=table]')  # If the table appears rotated in the highlights, # the extraction might still work normally for table in tables:     table.show()  # Check if this looks right     data = table.extract_table()     if data and len(data) &gt; 1:  # If we got reasonable data         print(\"Table extracted successfully despite rotation\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpdaq7jkzh/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 2720.6ms\n</pre> <pre>Speed: 6.2ms preprocess, 2720.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Table extracted successfully despite rotation\n</pre> In\u00a0[11]: Copied! <pre># Apply OCR first to convert image text to searchable text\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Now try table detection on the OCR'd content\npage.analyze_layout(engine='tatr')\ntables = page.find_all('region[type=table]')\n\n# Extract using OCR text\nfor table in tables:\n    data = table.extract_table()\n</pre> # Apply OCR first to convert image text to searchable text page.apply_ocr(engine='easyocr', languages=['en'])  # Now try table detection on the OCR'd content page.analyze_layout(engine='tatr') tables = page.find_all('region[type=table]')  # Extract using OCR text for table in tables:     data = table.extract_table() <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> In\u00a0[12]: Copied! <pre># Step 1: See what elements exist in the table area\ntable_area = page.create_region(50, 200, 550, 400)\nelements = table_area.find_all('text')\n\nprint(f\"Found {len(elements)} text elements in table area\")\n\n# Step 2: See the actual text content\ntext_content = table_area.extract_text(layout=True)\nprint(\"Raw text content:\")\nprint(text_content)\n\n# Step 3: Check for lines that might define the table\nlines = table_area.find_all('line')\nprint(f\"Found {len(lines)} lines in table area\")\n\n# Step 4: Visualize everything\nwith page.highlights() as h:\n    h.add(elements, color=\"blue\", label=\"Text\")\n    h.add(lines, color=\"red\", label=\"Lines\")\n    h.show()\n</pre> # Step 1: See what elements exist in the table area table_area = page.create_region(50, 200, 550, 400) elements = table_area.find_all('text')  print(f\"Found {len(elements)} text elements in table area\")  # Step 2: See the actual text content text_content = table_area.extract_text(layout=True) print(\"Raw text content:\") print(text_content)  # Step 3: Check for lines that might define the table lines = table_area.find_all('line') print(f\"Found {len(lines)} lines in table area\")  # Step 4: Visualize everything with page.highlights() as h:     h.add(elements, color=\"blue\", label=\"Text\")     h.add(lines, color=\"red\", label=\"Lines\")     h.show() <pre>Found 9 text elements in table area\nRaw text content:\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fellsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and wheninto the vats; and whenthey they were fished out; there was never enough of them left to be worthwere fished out, there was never enough of them left to be worth\nexhibitingexhibiting - sometimes they would be overlooked forsometimes they would be overlooked fordays, days, till all but the bones of them had gone outtill all but the bones of them had gone out\nto the world as Durham's Pure Leaf Lardlto the world as Durham\u2019s Pure Leaf Lard!\nViolationsViolations                                                 \nStatuteStatute DescriptionDescription            LevelLevel Repeat?Repeat?\n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \n                                                                     \nFound 2 lines in table area\n</pre> In\u00a0[13]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Drop top 50 pt (header) and bottom 40 pt (footer) on *every* page\npdf.add_exclusion(lambda p: p.create_region(0, 0, p.width, 50))\npdf.add_exclusion(lambda p: p.create_region(0, p.height-40, p.width, p.height))\n\npdf.pages[0].extract_table().df\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Drop top 50 pt (header) and bottom 40 pt (footer) on *every* page pdf.add_exclusion(lambda p: p.create_region(0, 0, p.width, 50)) pdf.add_exclusion(lambda p: p.create_region(0, p.height-40, p.width, p.height))  pdf.pages[0].extract_table().df Out[13]: Statute Description Level Repeat? 0 4.12.7 Unsanitary Working Conditions. Critical 1 5.8.3 Inadequate Protective Equipment. Serious 2 6.3.9 Ineffective Injury Prevention. Serious 3 7.1.5 Failure to Properly Store Hazardous Materials. Critical 4 8.9.2 Lack of Adequate Fire Safety Measures. Serious 5 9.6.4 Inadequate Ventilation Systems. Serious 6 10.2.7 Insufficient Employee Training for Safe Work P... Serious"},{"location":"fix-messy-tables/#fix-messy-table-extraction","title":"Fix Messy Table Extraction\u00b6","text":"<p>Your PDF has tables, but when you try to extract them, you get garbled data, missing rows, or columns that don't line up. Here's how to fix the most common table extraction problems.</p>"},{"location":"fix-messy-tables/#the-problem","title":"The Problem\u00b6","text":"<p>PDF tables come in many flavors, and each one breaks in its own special way:</p> <ul> <li>No visible borders: Text that looks like a table but has no lines</li> <li>Merged cells: Headers that span multiple columns</li> <li>Inconsistent spacing: Columns that don't line up perfectly</li> <li>Multiple tables: Several tables crammed onto one page</li> <li>Rotated tables: Tables turned sideways or upside down</li> <li>Image tables: Tables that are actually pictures</li> </ul> <p>Simple extraction methods often fail on these cases.</p>"},{"location":"fix-messy-tables/#quick-fix-try-layout-detection-first","title":"Quick Fix: Try Layout Detection First\u00b6","text":"<p>Instead of extracting tables blind, detect them first:</p>"},{"location":"fix-messy-tables/#when-basic-detection-fails-use-tatr","title":"When Basic Detection Fails: Use TATR\u00b6","text":"<p>For tables without clear borders or with complex structure:</p>"},{"location":"fix-messy-tables/#manual-table-definition","title":"Manual Table Definition\u00b6","text":"<p>When detection fails completely, define the table area yourself:</p>"},{"location":"fix-messy-tables/#define-table-boundaries-using-header-labels","title":"Define Table Boundaries Using Header Labels\u00b6","text":"<p>Sometimes the easiest trick is to let the column headers tell you exactly where the table starts and ends. Here's a quick, self-contained approach that works great on the violations table inside <code>01-practice.pdf</code>:</p>"},{"location":"fix-messy-tables/#fix-alignment-issues","title":"Fix Alignment Issues\u00b6","text":"<p>When columns don't line up properly:</p>"},{"location":"fix-messy-tables/#handle-multiple-tables-on-one-page","title":"Handle Multiple Tables on One Page\u00b6","text":"<p>Separate and extract each table individually:</p>"},{"location":"fix-messy-tables/#deal-with-rotated-tables","title":"Deal with Rotated Tables\u00b6","text":"<p>Some PDFs have sideways tables:</p>"},{"location":"fix-messy-tables/#extract-tables-from-images","title":"Extract Tables from Images\u00b6","text":"<p>When tables are embedded as images:</p>"},{"location":"fix-messy-tables/#debug-table-extraction-step-by-step","title":"Debug Table Extraction Step by Step\u00b6","text":"<p>When nothing works, debug systematically:</p>"},{"location":"fix-messy-tables/#troubleshooting-checklist","title":"Troubleshooting Checklist\u00b6","text":"<p>No tables detected?</p> <ul> <li>Try different engines: <code>yolo</code>, <code>tatr</code>, <code>surya</code></li> <li>Apply OCR first if it's a scanned document</li> <li>Manually define the table area</li> </ul> <p>Columns misaligned?</p> <ul> <li>Adjust <code>intersection_x_tolerance</code> in table settings</li> <li>Try <code>vertical_strategy=\"text\"</code> instead of <code>\"lines\"</code></li> <li>Check if there are actual lines vs. just text alignment</li> </ul> <p>Missing rows?</p> <ul> <li>Increase <code>intersection_y_tolerance</code></li> <li>Try <code>horizontal_strategy=\"text\"</code></li> <li>Look for merged cells that might be confusing the detector</li> </ul> <p>Garbled data?</p> <ul> <li>Extract manually row by row</li> <li>Check OCR quality if text looks wrong</li> <li>Verify the table region boundaries</li> </ul> <p>Multiple tables mixed together?</p> <ul> <li>Use layout detection to separate them first</li> <li>Define manual regions for each table</li> <li>Process tables in reading order</li> </ul>"},{"location":"fix-messy-tables/#auto-ignore-headers-footers-with-exclusions","title":"Auto-ignore headers &amp; footers with exclusions\u00b6","text":"<p>Before you even think about column alignment, make sure repeated page furniture (running headers, footers, page numbers) is out of the way.  Two lines of code often fix \"shifted\" columns:</p>"},{"location":"fix-messy-tables/#rows-list-of-lists-wrap-in-tableresult-for-convenience","title":"rows list-of-lists; wrap in TableResult for convenience\u00b6","text":"<p>rows = ( pdf.pages .apply(lambda p: p.analyze_layout('tatr') or p) .apply(lambda p: p.find('table').extract_table()) .apply(lambda t: t[1:])     # skip header repetition .flatten() )</p> <p>from natural_pdf.tables import TableResult df = TableResult(rows).to_df(header=\"first\") print(df.head())</p>"},{"location":"fix-messy-tables/","title":"Fix Messy Table Extraction","text":"<p>Your PDF has tables, but when you try to extract them, you get garbled data, missing rows, or columns that don't line up. Here's how to fix the most common table extraction problems.</p>"},{"location":"fix-messy-tables/#the-problem","title":"The Problem","text":"<p>PDF tables come in many flavors, and each one breaks in its own special way:</p> <ul> <li>No visible borders: Text that looks like a table but has no lines</li> <li>Merged cells: Headers that span multiple columns</li> <li>Inconsistent spacing: Columns that don't line up perfectly</li> <li>Multiple tables: Several tables crammed onto one page</li> <li>Rotated tables: Tables turned sideways or upside down</li> <li>Image tables: Tables that are actually pictures</li> </ul> <p>Simple extraction methods often fail on these cases.</p>"},{"location":"fix-messy-tables/#quick-fix-try-layout-detection-first","title":"Quick Fix: Try Layout Detection First","text":"<p>Instead of extracting tables blind, detect them first:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Detect table regions using AI\npage.analyze_layout(engine='yolo')\n\n# Find and extract detected tables\ntable_regions = page.find_all('region[type=table]')\nfor i, table in enumerate(table_regions):\n    data = table.extract_table()\n    print(f\"Table {i+1}: {len(data)} rows\")\n</code></pre>"},{"location":"fix-messy-tables/#when-basic-detection-fails-use-tatr","title":"When Basic Detection Fails: Use TATR","text":"<p>For tables without clear borders or with complex structure:</p> <pre><code># Use Table Transformer - understands table structure better\npage.clear_detected_layout_regions()  # Clear previous attempts\npage.analyze_layout(engine='tatr')\n\n# TATR understands rows, columns, and headers\ntable = page.find('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nheaders = page.find_all('region[type=table-column-header][model=tatr]')\n\nprint(f\"Found: {len(rows)} rows, {len(cols)} columns, {len(headers)} headers\")\n\n# Extract using the detected structure\ndata = table.extract_table(method='tatr')\n</code></pre>"},{"location":"fix-messy-tables/#manual-table-definition","title":"Manual Table Definition","text":"<p>When detection fails completely, define the table area yourself:</p> <pre><code># Visually inspect the page to find table boundaries\npage.show()\n\n# Manually define the table region\ntable_area = page.create_region(\n    x0=50,      # Left edge of table\n    top=200,    # Top edge of table\n    x1=550,     # Right edge of table\n    bottom=400  # Bottom edge of table\n)\n\n# Highlight to verify\ntable_area.show(color=\"blue\", label=\"Table area\")\n\n# Extract from the defined area\ndata = table_area.extract_table()\n</code></pre> Pro-tip: Instead of measuring 120\u00a0px below the header, try an   <code>until=...</code> anchor\u2014your script survives if the table grows or the scan   resolution changes.    <pre><code>header = page.find(text=\"Violations\":bold)\nbody   = header.below(\n    until=\"text:contains('Total violations')\",\n    include_endpoint=False\n)\ndata = body.extract_table()\n</code></pre>    In most cases that one extra <code>until=</code> makes the difference between a   brittle coordinate hack and a reusable extractor."},{"location":"fix-messy-tables/#define-table-boundaries-using-header-labels","title":"Define Table Boundaries Using Header Labels","text":"<p>Sometimes the easiest trick is to let the column headers tell you exactly where the table starts and ends. Here's a quick, self-contained approach that works great on the violations table inside <code>01-practice.pdf</code>:</p> <pre><code>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find all header labels\nheaders = page.find_all(\n    'text:contains(\"Statute|Description|Level|Repeat\")',\n    regex=True,\n)\ncol_edges = headers.apply(lambda header: header.x0) + [page.width]\nprint(col_edges)\n\nheaders.show(crop=True)\n</code></pre> <pre><code>table_area = (\n    headers[0]\n    .below(\n        until=\"text[size&lt;10]\",\n        include_endpoint=False\n    )\n)\ntable_area.show()\n</code></pre> <pre><code>row_items = (\n    headers[0]\n    .below(width='element')\n    .find_all('text')\n)\n\nrow_borders = row_items.apply(lambda entry: entry.top) + [row_items[-1].bottom]\nprint(row_borders)\nrow_items.show()\n</code></pre> <pre><code>options = {\n    \"vertical_strategy\": \"explicit\",\n    \"horizontal_strategy\": \"explicit\",\n    \"explicit_vertical_lines\": col_edges,\n    \"explicit_horizontal_lines\": row_borders,\n}\ndata = table_area.extract_table('pdfplumber', table_settings=options)\ndata\n</code></pre>"},{"location":"fix-messy-tables/#fix-alignment-issues","title":"Fix Alignment Issues","text":"<p>When columns don't line up properly:</p> <pre><code># Customize pdfplumber settings for better alignment\ntable_settings = {\n    \"vertical_strategy\": \"text\",        # Use text alignment instead of lines\n    \"horizontal_strategy\": \"lines\",     # Still use lines for rows\n    \"intersection_x_tolerance\": 10,     # More forgiving column alignment\n    \"intersection_y_tolerance\": 5,      # More forgiving row alignment\n    \"edge_min_length\": 3,              # Shorter minimum line length\n}\n\ndata = table_area.extract_table(table_settings=table_settings)\n</code></pre>"},{"location":"fix-messy-tables/#handle-multiple-tables-on-one-page","title":"Handle Multiple Tables on One Page","text":"<p>Separate and extract each table individually:</p> <pre><code># Detect all tables\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\nprint(f\"Found {len(tables)} tables\")\n\n# Extract each table separately\nall_data = []\nfor i, table in enumerate(tables):\n    data = table.extract_table()\n    all_data.append(data)\n\n# Show all tables together\nwith page.highlights() as h:\n    for i, table in enumerate(tables):\n        h.add(table, color=\"red\", label=f\"Table {i+1}\")\n    h.show()\n\n# Save each table separately\nimport pandas as pd\nfor i, tbl in enumerate(all_data):\n    df = tbl.to_df(header=\"first\")\n    df.to_csv(f\"table_{i+1}.csv\", index=False)\n</code></pre>"},{"location":"fix-messy-tables/#deal-with-rotated-tables","title":"Deal with Rotated Tables","text":"<p>Some PDFs have sideways tables:</p> <pre><code># For rotated tables, you might need to work with coordinates differently\n# First, see if layout detection picks it up\npage.analyze_layout(engine='yolo')\ntables = page.find_all('region[type=table]')\n\n# If the table appears rotated in the highlights,\n# the extraction might still work normally\nfor table in tables:\n    table.show()  # Check if this looks right\n    data = table.extract_table()\n    if data and len(data) &gt; 1:  # If we got reasonable data\n        print(\"Table extracted successfully despite rotation\")\n</code></pre>"},{"location":"fix-messy-tables/#extract-tables-from-images","title":"Extract Tables from Images","text":"<p>When tables are embedded as images:</p> <pre><code># Apply OCR first to convert image text to searchable text\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Now try table detection on the OCR'd content\npage.analyze_layout(engine='tatr')\ntables = page.find_all('region[type=table]')\n\n# Extract using OCR text\nfor table in tables:\n    data = table.extract_table()\n</code></pre>"},{"location":"fix-messy-tables/#debug-table-extraction-step-by-step","title":"Debug Table Extraction Step by Step","text":"<p>When nothing works, debug systematically:</p> <pre><code># Step 1: See what elements exist in the table area\ntable_area = page.create_region(50, 200, 550, 400)\nelements = table_area.find_all('text')\n\nprint(f\"Found {len(elements)} text elements in table area\")\n\n# Step 2: See the actual text content\ntext_content = table_area.extract_text(layout=True)\nprint(\"Raw text content:\")\nprint(text_content)\n\n# Step 3: Check for lines that might define the table\nlines = table_area.find_all('line')\nprint(f\"Found {len(lines)} lines in table area\")\n\n# Step 4: Visualize everything\nwith page.highlights() as h:\n    h.add(elements, color=\"blue\", label=\"Text\")\n    h.add(lines, color=\"red\", label=\"Lines\")\n    h.show()\n</code></pre>"},{"location":"fix-messy-tables/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<p>No tables detected? - Try different engines: <code>yolo</code>, <code>tatr</code>, <code>surya</code> - Apply OCR first if it's a scanned document - Manually define the table area</p> <p>Columns misaligned? - Adjust <code>intersection_x_tolerance</code> in table settings - Try <code>vertical_strategy=\"text\"</code> instead of <code>\"lines\"</code> - Check if there are actual lines vs. just text alignment</p> <p>Missing rows? - Increase <code>intersection_y_tolerance</code> - Try <code>horizontal_strategy=\"text\"</code> - Look for merged cells that might be confusing the detector</p> <p>Garbled data? - Extract manually row by row - Check OCR quality if text looks wrong - Verify the table region boundaries</p> <p>Multiple tables mixed together? - Use layout detection to separate them first - Define manual regions for each table - Process tables in reading order</p>"},{"location":"fix-messy-tables/#auto-ignore-headers-footers-with-exclusions","title":"Auto-ignore headers &amp; footers with exclusions","text":"<p>Before you even think about column alignment, make sure repeated page furniture (running headers, footers, page numbers) is out of the way.  Two lines of code often fix \"shifted\" columns:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Drop top 50 pt (header) and bottom 40 pt (footer) on *every* page\npdf.add_exclusion(lambda p: p.create_region(0, 0, p.width, 50))\npdf.add_exclusion(lambda p: p.create_region(0, p.height-40, p.width, p.height))\n\npdf.pages[0].extract_table().df\n</code></pre>"},{"location":"fix-messy-tables/#rows-list-of-lists-wrap-in-tableresult-for-convenience","title":"rows list-of-lists; wrap in TableResult for convenience","text":"<p>rows = (     pdf.pages        .apply(lambda p: p.analyze_layout('tatr') or p)        .apply(lambda p: p.find('table').extract_table())        .apply(lambda t: t[1:])     # skip header repetition        .flatten() )</p> <p>from natural_pdf.tables import TableResult df = TableResult(rows).to_df(header=\"first\") print(df.head())</p>"},{"location":"installation/","title":"Getting Started with Natural PDF","text":"<p>Let's get Natural PDF installed and run your first extraction.</p>"},{"location":"installation/#installation","title":"Installation","text":"<p>The base installation includes the core library which will allow you to select, extract, and use spatial navigation.</p> <pre><code>pip install natural-pdf\n</code></pre> <p>But! If you want to recognize text, do page layout analysis, document q-and-a or other things, you can install optional dependencies.</p> <p>Natural PDF has modular dependencies for different features. Install them based on your needs:</p> <pre><code># Full ML / QA / semantic-search stack\npip install natural-pdf[ai]\n\n# Deskewing\npip install natural-pdf[deskew]\n\n# Semantic search\npip install natural-pdf[search]\n</code></pre> <p>Other OCR and layout analysis engines like <code>surya</code>, <code>easyocr</code>, <code>paddle</code>, <code>doctr</code>, and <code>docling</code> can be installed via <code>pip</code> as needed. The library will provide you with an error message and installation command if you try to use an engine that isn't installed.</p> <p>After the core install you have two ways to add optional engines:</p>"},{"location":"installation/#1-helper-cli-recommended","title":"1 \u2013 Helper CLI (recommended)","text":"<pre><code># list optional groups and their install-status\nnpdf list\n\n# everything for classification, QA, semantic search, etc.\nnpdf install ai\n\n# install PaddleOCR stack\nnpdf install paddle\n\n# install Surya OCR + YOLO layout detector\nnpdf install surya yolo\n</code></pre> <p>The CLI runs each wheel in its own resolver pass, so it avoids strict version pins like <code>paddleocr \u2192 paddlex==3.0.1</code> while still upgrading to <code>paddlex 3.0.2</code>.</p>"},{"location":"installation/#2-classic-extras-for-the-light-stuff","title":"2 \u2013 Classic extras (for the light stuff)","text":"<pre><code># Full AI/ML stack\npip install \"natural-pdf[ai]\"\n\n# Deskewing\npip install \"natural-pdf[deskew]\"\n\n# Semantic search service\npip install \"natural-pdf[search]\"\n</code></pre> <p>If you attempt to use an engine that is missing, the library will raise an error that tells you which <code>npdf install \u2026</code> command to run.</p>"},{"location":"installation/#your-first-pdf-extraction","title":"Your First PDF Extraction","text":"<p>Here's a quick example to make sure everything is working:</p> <pre><code>from natural_pdf import PDF\n\n# Open a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Extract all text\ntext = page.extract_text()\nprint(text)\n\n# Find something specific\ntitle = page.find('text:bold')\nprint(f\"Found title: {title.text}\")\n</code></pre>"},{"location":"installation/#whats-next","title":"What's Next?","text":"<p>Now that you have Natural PDF installed, you can:</p> <ul> <li>Learn to navigate PDFs</li> <li>Explore how to select elements</li> <li>See how to extract text</li> </ul>"},{"location":"interactive-widget/","title":"Interactive widget","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.viewer()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[1]:"},{"location":"interactive-widget/#interactive-widget","title":"Interactive widget\u00b6","text":"<p>This is the best possible way, in all of history, to explore a PDF.</p>"},{"location":"layout-analysis/","title":"Document Layout Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\npage.show(width=700)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  page.show(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Analyze the layout using the default engine (YOLO)\n# This adds 'region' elements to the page\npage.analyze_layout()\n</pre> # Analyze the layout using the default engine (YOLO) # This adds 'region' elements to the page page.analyze_layout() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpwgtplbgp/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 2463.5ms\n</pre> <pre>Speed: 5.7ms preprocess, 2463.5ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[2]: <pre>&lt;ElementCollection[Region](count=7)&gt;</pre> In\u00a0[3]: Copied! <pre># Find all detected regions\nregions = page.find_all('region')\nlen(regions) # Show how many regions were detected\n</pre> # Find all detected regions regions = page.find_all('region') len(regions) # Show how many regions were detected Out[3]: <pre>7</pre> In\u00a0[4]: Copied! <pre>first_region = regions[0]\nf\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\"\n</pre> first_region = regions[0] f\"First region: type='{first_region.type}', confidence={first_region.confidence:.2f}\" Out[4]: <pre>\"First region: type='abandon', confidence=0.81\"</pre> In\u00a0[5]: Copied! <pre># Highlight all detected regions, colored by type\nregions.show(group_by='type', width=700)\n</pre> # Highlight all detected regions, colored by type regions.show(group_by='type', width=700) Out[5]: In\u00a0[6]: Copied! <pre># Find all detected titles\ntitles = page.find_all('region[type=title]')\ntitles\n</pre> # Find all detected titles titles = page.find_all('region[type=title]') titles Out[6]: <pre>&lt;ElementCollection[Region](count=1)&gt;</pre> In\u00a0[7]: Copied! <pre>titles.show()\n</pre> titles.show() Out[7]: In\u00a0[8]: Copied! <pre>page.find_all('region[type=table]').show()\n</pre> page.find_all('region[type=table]').show() Out[8]: In\u00a0[9]: Copied! <pre>page.find('region[type=table]').extract_text(layout=True)\n</pre> page.find('region[type=table]').extract_text(layout=True) Out[9]: <pre>'Statute Description                              Level  Repeat?      \\n4.12.7 Unsanitary Working Conditions.            Critical            \\n5.8.3 Inadequate Protective Equipment.           Serious             \\n6.3.9 Ineffective Injury Prevention.             Serious             \\n7.1.5 Failure to Properly Store Hazardous Materials. Critical        \\n8.9.2 Lack of Adequate Fire Safety Measures.     Serious             \\n9.6.4 Inadequate Ventilation Systems.            Serious             \\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\\n                                                                     \\n                                                                     \\n                                                                     \\n                                                                     '</pre> In\u00a0[10]: Copied! <pre>page.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"paddle\")\npage.find_all('region[model=paddle]').show(group_by='region_type', width=700)\n</pre> page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"paddle\") page.find_all('region[model=paddle]').show(group_by='region_type', width=700) <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>[2025-07-09 08:12:17,897] [ WARNING] render_spec.py:198 - ElementCollection.show() generated no render specs\n</pre> In\u00a0[11]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').show(group_by='region_type', width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').show(group_by='region_type', width=700) Out[11]: In\u00a0[12]: Copied! <pre># Analyze using Docling\n# https://docling-project.github.io/docling/\n\n# Docling has been weird, it's not included at the moment\n\n# page.clear_detected_layout_regions()\n# page.clear_highlights()\n\n# page.analyze_layout(engine=\"docling\")\n# page.find_all('region[model=docling]').show(group_by='region_type')\n# page.to_image(width=700)\n</pre> # Analyze using Docling # https://docling-project.github.io/docling/  # Docling has been weird, it's not included at the moment  # page.clear_detected_layout_regions() # page.clear_highlights()  # page.analyze_layout(engine=\"docling\") # page.find_all('region[model=docling]').show(group_by='region_type') # page.to_image(width=700) In\u00a0[13]: Copied! <pre># Analyze using Table Transformer (TATR) - specialized for tables\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"surya\")\npage.find_all('region[model=surya]').show(group_by='region_type', width=700)\n</pre> # Analyze using Table Transformer (TATR) - specialized for tables page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"surya\") page.find_all('region[model=surya]').show(group_by='region_type', width=700) <pre>\rRecognizing layout:   0%|                                                                           | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.34s/it]</pre> <pre>\rRecognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.34s/it]</pre> <pre>\n</pre> <pre>\rRecognizing tables:   0%|                                                                           | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing tables: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.37s/it]</pre> <pre>\rRecognizing tables: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.37s/it]</pre> <pre>\n</pre> Out[13]: <p>Note: Calling <code>analyze_layout</code> multiple times (even with the same engine) can add duplicate regions. You might want to use <code>page.clear_detected_layout_regions()</code> first, or filter by model using <code>region[model=yolo]</code>.</p> In\u00a0[14]: Copied! <pre># Re-run YOLO analysis (clearing previous results might be good practice)\npage.clear_detected_layout_regions()\npage.analyze_layout(engine=\"yolo\")\n\n# Find only high-confidence regions (e.g., &gt;= 0.8)\nhigh_conf_regions = page.find_all('region[confidence&gt;=0.8]')\nlen(high_conf_regions)\n</pre> # Re-run YOLO analysis (clearing previous results might be good practice) page.clear_detected_layout_regions() page.analyze_layout(engine=\"yolo\")  # Find only high-confidence regions (e.g., &gt;= 0.8) high_conf_regions = page.find_all('region[confidence&gt;=0.8]') len(high_conf_regions) <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp2h0gntaf/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 717.1ms\n</pre> <pre>Speed: 3.6ms preprocess, 717.1ms inference, 0.6ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[14]: <pre>5</pre> In\u00a0[15]: Copied! <pre># Ensure TATR analysis has been run\npage.clear_detected_layout_regions()\npage.clear_highlights()\n\npage.analyze_layout(engine=\"tatr\")\npage.find_all('region[model=tatr]').show(group_by='region_type', width=700)\n</pre> # Ensure TATR analysis has been run page.clear_detected_layout_regions() page.clear_highlights()  page.analyze_layout(engine=\"tatr\") page.find_all('region[model=tatr]').show(group_by='region_type', width=700) Out[15]: In\u00a0[16]: Copied! <pre># Find different structural elements from TATR\ntables = page.find_all('region[type=table][model=tatr]')\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\n\nf\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\"\n</pre> # Find different structural elements from TATR tables = page.find_all('region[type=table][model=tatr]') rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]')  f\"Found: {len(tables)} tables, {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers (from TATR)\" Out[16]: <pre>'Found: 2 tables, 8 rows, 4 columns, 1 headers (from TATR)'</pre> In\u00a0[17]: Copied! <pre># Find the TATR table region again\ntatr_table = page.find('region[type=table][model=tatr]')\n\n# This extraction uses the detected rows/columns\ntatr_table.extract_table()\n</pre> # Find the TATR table region again tatr_table = page.find('region[type=table][model=tatr]')  # This extraction uses the detected rows/columns tatr_table.extract_table() Out[17]: <pre>TableResult(rows=9\u2026)</pre> <p>if you'd like the normal approach instead of the \"intelligent\" one, you can ask for pdfplumber.</p> In\u00a0[18]: Copied! <pre># This extraction uses the detected rows/columns\ntatr_table.extract_table(method='pdfplumber')\n</pre> # This extraction uses the detected rows/columns tatr_table.extract_table(method='pdfplumber') Out[18]: <pre>TableResult(rows=6\u2026)</pre>"},{"location":"layout-analysis/#document-layout-analysis","title":"Document Layout Analysis\u00b6","text":"<p>Natural PDF can automatically detect the structure of a document (titles, paragraphs, tables, figures) using layout analysis models. This guide shows how to use this feature.</p>"},{"location":"layout-analysis/#setup","title":"Setup\u00b6","text":"<p>We'll use a sample PDF that includes various layout elements.</p>"},{"location":"layout-analysis/#running-basic-layout-analysis","title":"Running Basic Layout Analysis\u00b6","text":"<p>Use the <code>analyze_layout()</code> method. By default, it uses the YOLO model.</p>"},{"location":"layout-analysis/#visualizing-detected-layout","title":"Visualizing Detected Layout\u00b6","text":"<p>Use <code>show()</code> on the detected regions.</p>"},{"location":"layout-analysis/#finding-specific-region-types","title":"Finding Specific Region Types\u00b6","text":"<p>Use attribute selectors to find regions of a specific type.</p>"},{"location":"layout-analysis/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>Detected regions are like any other <code>Region</code> object. You can extract text, find elements within them, etc.</p>"},{"location":"layout-analysis/#using-different-layout-models","title":"Using Different Layout Models\u00b6","text":"<p>Natural PDF supports multiple engines (<code>yolo</code>, <code>paddle</code>, <code>tatr</code>). Specify the engine when calling <code>analyze_layout</code>.</p> <p>Note: Using different engines requires installing the corresponding extras (e.g., <code>natural-pdf[layout_paddle]</code>). <code>yolo</code> is the default.</p>"},{"location":"layout-analysis/#controlling-confidence-threshold","title":"Controlling Confidence Threshold\u00b6","text":"<p>Filter detections by their confidence score.</p>"},{"location":"layout-analysis/#table-structure-with-tatr","title":"Table Structure with TATR\u00b6","text":"<p>The TATR engine provides detailed table structure elements (<code>table</code>, <code>table-row</code>, <code>table-column</code>, <code>table-column-header</code>). This is very useful for precise table extraction.</p>"},{"location":"layout-analysis/#enhanced-table-extraction-with-tatr","title":"Enhanced Table Extraction with TATR\u00b6","text":"<p>When a <code>region[type=table]</code> comes from the TATR model, <code>extract_table()</code> can use the underlying row/column structure for more robust extraction.</p>"},{"location":"layout-analysis/#using-gemini-for-layout-analysis-advanced","title":"Using Gemini for Layout Analysis (Advanced)\u00b6","text":"<p>Natural PDF supports layout analysis using Google's Gemini models via an OpenAI-compatible API. This is an advanced feature and requires you to provide your own OpenAI client, API key, and endpoint.</p> <p>Example usage:</p> <pre>from openai import OpenAI\nfrom natural_pdf import PDF\nfrom natural_pdf.analyzers.layout.layout_options import GeminiLayoutOptions\n\n# Create a compatible OpenAI client for Gemini\nclient = OpenAI(\n    api_key=\"YOUR_GOOGLE_API_KEY\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\noptions = GeminiLayoutOptions(\n    model_name=\"gemini-2.0-flash\",\n    client=client,\n    classes=[\"text\", \"title\"]\n)\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\nregions = page.analyze_layout(engine=\"gemini\", options=options)\n</pre> <ul> <li>You must provide your own API key and endpoint for Gemini.</li> <li>The client must be compatible with the OpenAI API (see the <code>openai</code> Python package).</li> <li>This feature is intended for advanced users who need LLM-based layout analysis.</li> </ul>"},{"location":"layout-analysis/#next-steps","title":"Next Steps\u00b6","text":"<p>Layout analysis provides regions that you can use for:</p> <ul> <li>Table Extraction: Especially powerful with TATR regions.</li> <li>Text Extraction: Extract text only from specific region types (e.g., paragraphs).</li> <li>Document QA: Focus question answering on specific detected regions.</li> </ul>"},{"location":"loops-and-groups/","title":"Loops, groups and repetitive tasks","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Path to sample PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Display the first page\npage = pdf.pages[0]\npage.show(width=500)\n</pre> from natural_pdf import PDF  # Path to sample PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Display the first page page = pdf.pages[0] page.show(width=500) Out[1]: <p>We can find all of the book titles by finding (Removed: on the page...</p> In\u00a0[2]: Copied! <pre>page.find_all('text:contains(\"(Removed:\")').show()\n</pre> page.find_all('text:contains(\"(Removed:\")').show() Out[2]: <p>...but it's repeated on each following page, too!</p> In\u00a0[3]: Copied! <pre>pdf.pages[1].find_all('text:contains(\"(Removed:\")').show()\n</pre> pdf.pages[1].find_all('text:contains(\"(Removed:\")').show() Out[3]: <p>No problem, you can use <code>pdf.find_all</code> the same way to do with a single page - you just can't highlight them with <code>.show()</code> the same way.</p> In\u00a0[4]: Copied! <pre>pdf.find_all('text:contains(\"(Removed:\")')\n</pre> pdf.find_all('text:contains(\"(Removed:\")') Out[4]: <pre>&lt;ElementCollection[TextElement](count=37)&gt;</pre> <p>You can see there are 37 across the entire PDF.</p> In\u00a0[5]: Copied! <pre>titles = pdf.find_all('text:contains(\"(Removed:\")')\n\ntitles.extract_each_text()\n</pre> titles = pdf.find_all('text:contains(\"(Removed:\")')  titles.extract_each_text() Out[5]: <pre>['Tristan Strong punches a hole in the sky (Removed: 1)',\n 'Upside down in the middle of nowhere (Removed: 1)',\n 'Buddhism (Removed: 1)',\n 'Voodoo (Removed: 1)',\n 'The Abenaki (Removed: 1)',\n 'Afghanistan (Removed: 1)',\n 'Alexander the Great rocks the world (Removed: 1)',\n 'The Anasazi (Removed: 1)',\n 'And then what happened, Paul Revere? (Removed: 1)',\n 'The assassination of Martin Luther King Jr (Removed: 1)',\n 'Barbara Jordan. (Removed: 1)',\n 'Bedtime for Batman (Removed: 1)',\n 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskegee Airmen leader (Removed: 1)',\n 'Bigfoot Wallace (Removed: 1)',\n 'The blaze engulfs : January 1939 to December 1941 (Removed: 1)',\n 'The boys who challenged Hitler : Knud Pedersen and the Churchill Club (Removed: 1)',\n 'Brown v. Board of Education (Removed: 1)',\n 'The Cahuilla (Removed: 1)',\n 'Cambodia (Removed: 1)',\n 'Celebrate China (Removed: 1)',\n 'Cesar Chavez : a photo-illustrated biography (Removed: 1)',\n 'The Cherokee Indians (Removed: 1)',\n 'Children of the Philippines (Removed: 1)',\n 'The Chinook people (Removed: 1)',\n 'The Choctaw (Removed: 1)',\n 'Christopher Columbus (Removed: 1)',\n 'The Comanche Indians (Removed: 1)',\n 'Dare to dream : Coretta Scott King and the civil rights movement (Removed: 1)',\n 'A day in the life of a Native American (Removed: 1)',\n 'Dear Benjamin Banneker (Removed: 1)',\n 'Dolley Madison (Removed: 1)',\n 'Dreams from my father : a story of race and inheritance (Removed: 1)',\n 'Eleanor Roosevelt : a life of discovery (Removed: 1)',\n 'Elie Wiesel : bearing witness (Removed: 1)',\n 'Elizabeth Cady Stanton : a photo-illustrated biography (Removed: 1)',\n 'Family dinner (Removed: 1)',\n 'A firestorm unleashed : January 1942 - June 1943 (Removed: 1)']</pre> <p>You can also loop through them like a normal list...</p> In\u00a0[6]: Copied! <pre>for title in titles[:10]:\n    print(title.extract_text(), title.page.number)\n</pre> for title in titles[:10]:     print(title.extract_text(), title.page.number) <pre>Tristan Strong punches a hole in the sky (Removed: 1) 1\nUpside down in the middle of nowhere (Removed: 1) 1\nBuddhism (Removed: 1) 1\nVoodoo (Removed: 1) 1\nThe Abenaki (Removed: 1) 1\nAfghanistan (Removed: 1) 1\nAlexander the Great rocks the world (Removed: 1) 1\nThe Anasazi (Removed: 1) 2\nAnd then what happened, Paul Revere? (Removed: 1) 2\nThe assassination of Martin Luther King Jr (Removed: 1) 2\n</pre> <p>...but you can also use <code>.apply</code> for a little functional-programming flavor.</p> In\u00a0[7]: Copied! <pre>titles.apply(lambda title: {\n    'title': title.extract_text(),\n    'page': title.page.number\n})\n</pre> titles.apply(lambda title: {     'title': title.extract_text(),     'page': title.page.number }) Out[7]: <pre>[{'title': 'Tristan Strong punches a hole in the sky (Removed: 1)', 'page': 1},\n {'title': 'Upside down in the middle of nowhere (Removed: 1)', 'page': 1},\n {'title': 'Buddhism (Removed: 1)', 'page': 1},\n {'title': 'Voodoo (Removed: 1)', 'page': 1},\n {'title': 'The Abenaki (Removed: 1)', 'page': 1},\n {'title': 'Afghanistan (Removed: 1)', 'page': 1},\n {'title': 'Alexander the Great rocks the world (Removed: 1)', 'page': 1},\n {'title': 'The Anasazi (Removed: 1)', 'page': 2},\n {'title': 'And then what happened, Paul Revere? (Removed: 1)', 'page': 2},\n {'title': 'The assassination of Martin Luther King Jr (Removed: 1)',\n  'page': 2},\n {'title': 'Barbara Jordan. (Removed: 1)', 'page': 2},\n {'title': 'Bedtime for Batman (Removed: 1)', 'page': 2},\n {'title': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskegee Airmen leader (Removed: 1)',\n  'page': 2},\n {'title': 'Bigfoot Wallace (Removed: 1)', 'page': 2},\n {'title': 'The blaze engulfs : January 1939 to December 1941 (Removed: 1)',\n  'page': 2},\n {'title': 'The boys who challenged Hitler : Knud Pedersen and the Churchill Club (Removed: 1)',\n  'page': 3},\n {'title': 'Brown v. Board of Education (Removed: 1)', 'page': 3},\n {'title': 'The Cahuilla (Removed: 1)', 'page': 3},\n {'title': 'Cambodia (Removed: 1)', 'page': 3},\n {'title': 'Celebrate China (Removed: 1)', 'page': 3},\n {'title': 'Cesar Chavez : a photo-illustrated biography (Removed: 1)',\n  'page': 3},\n {'title': 'The Cherokee Indians (Removed: 1)', 'page': 3},\n {'title': 'Children of the Philippines (Removed: 1)', 'page': 4},\n {'title': 'The Chinook people (Removed: 1)', 'page': 4},\n {'title': 'The Choctaw (Removed: 1)', 'page': 4},\n {'title': 'Christopher Columbus (Removed: 1)', 'page': 4},\n {'title': 'The Comanche Indians (Removed: 1)', 'page': 4},\n {'title': 'Dare to dream : Coretta Scott King and the civil rights movement (Removed: 1)',\n  'page': 4},\n {'title': 'A day in the life of a Native American (Removed: 1)', 'page': 4},\n {'title': 'Dear Benjamin Banneker (Removed: 1)', 'page': 4},\n {'title': 'Dolley Madison (Removed: 1)', 'page': 5},\n {'title': 'Dreams from my father : a story of race and inheritance (Removed: 1)',\n  'page': 5},\n {'title': 'Eleanor Roosevelt : a life of discovery (Removed: 1)', 'page': 5},\n {'title': 'Elie Wiesel : bearing witness (Removed: 1)', 'page': 5},\n {'title': 'Elizabeth Cady Stanton : a photo-illustrated biography (Removed: 1)',\n  'page': 5},\n {'title': 'Family dinner (Removed: 1)', 'page': 5},\n {'title': 'A firestorm unleashed : January 1942 - June 1943 (Removed: 1)',\n  'page': 5}]</pre> <p>I think <code>.map</code> also works on that front?</p> In\u00a0[8]: Copied! <pre>elements = page.find_all('text:contains(\"Removed:\")')\nelements.show()\n</pre> elements = page.find_all('text:contains(\"Removed:\")') elements.show() Out[8]: <p>We can filter for the ones that don't say \"Copies Removed\"</p> In\u00a0[9]: Copied! <pre>titles = elements.filter(\n    lambda element: 'Copies Removed' not in element.extract_text()\n)\ntitles.show()\n</pre> titles = elements.filter(     lambda element: 'Copies Removed' not in element.extract_text() ) titles.show() Out[9]:"},{"location":"loops-and-groups/#loops-groups-and-repetitive-tasks","title":"Loops, groups and repetitive tasks\u00b6","text":"<p>Sometimes you need to do things again and again.</p>"},{"location":"loops-and-groups/#selecting-things","title":"Selecting things\u00b6","text":"<p>Let's say we have a lot of pages that all look like this:</p>"},{"location":"loops-and-groups/#extracting-data-from-elements","title":"Extracting data from elements\u00b6","text":"<p>If you just want the text, <code>.extract_text()</code> will smush it all together, but you can also get it as a list.</p>"},{"location":"loops-and-groups/#filtering","title":"Filtering\u00b6","text":"<p>You can also filter if you only want some of them. For example, maybe we weren't sure how to pick between the different Removed: text blocks.</p>"},{"location":"ocr/","title":"Getting Text from Scanned Documents","text":"<p>Got a PDF that's actually just a bunch of scanned images? Or maybe a PDF where the text got mangled somehow? OCR (Optical Character Recognition) is your friend. Natural PDF can extract text from image-based PDFs using several different OCR engines.</p>"},{"location":"ocr/#which-ocr-engine-should-you-use","title":"Which OCR Engine Should You Use?","text":"<p>Natural PDF supports multiple OCR engines, each with different strengths:</p> <ul> <li>EasyOCR</li> <li>PaddleOCR</li> <li>Surya</li> <li>DocTR</li> </ul> <p>What are those strengths??? It honestly doesn't even matter, it's so easy to try each of them you can just see what works best for you.</p> <p>If you try to use an engine that isn't installed, Natural PDF will tell you exactly what to install.</p>"},{"location":"ocr/#basic-ocr-just-make-it-work","title":"Basic OCR: Just Make It Work","text":"<p>The simplest approach - just apply OCR to a page and get the text:</p> <pre><code>from natural_pdf import PDF\n\n# Load a PDF that needs OCR\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Apply OCR using the default engine\nocr_elements = page.apply_ocr()\n\n# Extract the text (uses OCR results automatically)\ntext = page.extract_text()\nprint(text)\n</code></pre>"},{"location":"ocr/#choosing-your-ocr-engine","title":"Choosing Your OCR Engine","text":"<p>Pick the engine that fits your needs:</p> <pre><code># Use PaddleOCR for Chinese and English documents\nocr_elements = page.apply_ocr(engine='paddle', languages=['zh-cn', 'en'])\n\n# Use EasyOCR with looser confidence requirements\nocr_elements = page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.3)\n</code></pre>"},{"location":"ocr/#fine-tuning-ocr-settings","title":"Fine-Tuning OCR Settings","text":"<p>For more control, use the engine-specific options classes:</p> <pre><code>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\n\n# Configure PaddleOCR with custom settings\npaddle_opts = PaddleOCROptions(\n    # Check the documentation for all available options\n)\nocr_elements = page.apply_ocr(engine='paddle', options=paddle_opts)\n\n# Configure EasyOCR for better paragraph detection\neasy_opts = EasyOCROptions(\n    languages=['en', 'fr'],\n    gpu=True,            # Use GPU if available\n    paragraph=True,      # Group text into paragraphs\n    text_threshold=0.6,  # How confident to be about text detection\n    batch_size=8         # Process multiple regions at once\n)\nocr_elements = page.apply_ocr(engine='easyocr', options=easy_opts)\n</code></pre>"},{"location":"ocr/#ocring-regions","title":"OCRing regions","text":"<p>Don't want to apply OCR to an entire page? You don't need to!</p> <pre><code># Grab the top half of the page\nregion = page.region(0, 0, height=page.height/2, width=page.width)\nregion.apply_ocr(engine='paddle')\n</code></pre> <p>Note: Running OCR again on the same area will replace the previous OCR results.</p>"},{"location":"ocr/#working-with-ocr-results","title":"Working with OCR Results","text":"<p>Once you've run OCR, the text elements work just like regular PDF text:</p> <pre><code># Find all OCR-generated text\nocr_text = page.find_all('text[source=ocr]')\n\n# Find only high-confidence OCR text\nhigh_conf = page.find_all('text[source=ocr][confidence&gt;=0.8]')\n\n# Extract just the OCR text\nocr_content = page.find_all('text[source=ocr]').extract_text()\n\n# Search within OCR results\nnames = page.find_all('text[source=ocr]:contains(\"Smith\")', case=False)\n</code></pre>"},{"location":"ocr/#debugging-ocr-results","title":"Debugging OCR Results","text":"<p>See what the OCR engine actually found:</p> <pre><code># Run OCR first\nocr_elements = page.apply_ocr()\n\n# Color-code by confidence level\nwith page.highlights() as h:\n    for element in ocr_elements:\n        if element.confidence &gt;= 0.8:\n            color = \"green\"     # High confidence\n        elif element.confidence &gt;= 0.5:\n            color = \"yellow\"    # Medium confidence\n        else:\n            color = \"red\"       # Low confidence\n\n        h.add(element, color=color, label=f\"OCR ({element.confidence:.2f})\")\n    h.show()\n</code></pre>"},{"location":"ocr/#visualizing-ocr-confidence-with-gradient-colors","title":"Visualizing OCR Confidence with Gradient Colors","text":"<p>Natural PDF can automatically detect when you're working with quantitative data (like confidence scores) and use gradient colors instead of categorical colors:</p> <pre><code># Get OCR elements\nocr_elements = page.apply_ocr()\n\n# Visualize confidence scores with gradient colors\nocr_elements.show(group_by='confidence')\n\n# Try different colormaps for better contrast\nocr_elements.show(group_by='confidence', color='plasma')    # Purple to yellow\nocr_elements.show(group_by='confidence', color='viridis')   # Blue to yellow\nocr_elements.show(group_by='confidence', color='coolwarm')  # Blue to red\n\n# Focus on problematic areas (low confidence)\nocr_elements.show(group_by='confidence', bins=[0, 0.5])     # Only show 0-0.5 range\n\n# Create meaningful confidence bands\nocr_elements.show(group_by='confidence', bins=[0, 0.3, 0.7, 1.0])  # Poor/OK/Good\n</code></pre> <p>This makes it much easier to spot problematic OCR results at a glance compared to manual color coding. You'll automatically get a color scale showing the confidence range instead of a discrete legend.</p>"},{"location":"ocr/#advanced-local-detection-llm-cleanup","title":"Advanced: Local Detection + LLM Cleanup","text":"<p>For really tricky documents, you can use a local model to find text regions, then send those specific regions to a language model for cleanup:</p> <pre><code>from natural_pdf.ocr.utils import direct_ocr_llm\nimport openai\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Step 1: Find text regions locally (fast, no API calls)\npage.apply_ocr('paddle', resolution=120, detect_only=True)\n\n# Step 2: Set up LLM for cleanup\nclient = openai.OpenAI(\n    base_url=\"https://api.anthropic.com/v1/\",\n    api_key='your-api-key-here'\n)\n\nprompt = \"\"\"OCR this image. Return only the exact text from the image.\nInclude misspellings, punctuation, etc. Do not add quotes or comments.\nThe text is from a Greek spreadsheet, so expect Modern Greek or numbers.\"\"\"\n\n# Step 3: Define cleanup function\ndef correct(region):\n    return direct_ocr_llm(\n        region,\n        client,\n        prompt=prompt,\n        resolution=300,\n        model=\"claude-3-5-haiku-20241022\"\n    )\n\n# Step 4: Apply cleanup to each detected region\npage.correct_ocr(correct)\n\n# You're done! The page now has cleaned-up text\n</code></pre>"},{"location":"ocr/#interactive-ocr-correction","title":"Interactive OCR Correction","text":"<p>Natural PDF includes a web app for reviewing and correcting OCR results:</p> <ol> <li> <p>Package your PDF data: <pre><code>from natural_pdf.utils.packaging import create_correction_task_package\n\n# After running OCR on your PDF\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</code></pre></p> </li> <li> <p>Visit the live OCR tool and upload your zip file.</p> </li> </ol> <p>If you're a crazy person, alternatively you can do it locally like this:</p> <pre><code># Find where Natural PDF is installed\nNATURAL_PDF_PATH=$(python -c \"import site; print(site.getsitepackages()[0])\")/natural_pdf\n\n# Start the web server\ncd $NATURAL_PDF_PATH/templates/spa\npython -m http.server 8000\n\n# Open http://localhost:8000 in your browser\n</code></pre>"},{"location":"ocr/#next-steps","title":"Next Steps","text":"<p>Once you've got OCR working:</p> <ul> <li>Layout Analysis: Automatically detect document structure</li> <li>Document QA: Ask questions about your newly-readable documents</li> </ul>"},{"location":"pdf-navigation/","title":"PDF Navigation","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Open a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\")\n</pre> from natural_pdf import PDF  # Open a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42001.pdf\") In\u00a0[2]: Copied! <pre># Get the total number of pages\nnum_pages = len(pdf)\nprint(f\"This PDF has {num_pages} pages\")\n\n# Get a specific page (0-indexed)\nfirst_page = pdf.pages[0]\nlast_page = pdf.pages[-1]\n\n# Iterate through the first 20 pages\nfor page in pdf.pages[:20]:\n    print(f\"Page {page.number} has {len(page.extract_text())} characters\")\n</pre> # Get the total number of pages num_pages = len(pdf) print(f\"This PDF has {num_pages} pages\")  # Get a specific page (0-indexed) first_page = pdf.pages[0] last_page = pdf.pages[-1]  # Iterate through the first 20 pages for page in pdf.pages[:20]:     print(f\"Page {page.number} has {len(page.extract_text())} characters\") <pre>This PDF has 153 pages\nPage 1 has 985 characters\nPage 2 has 778 characters\nPage 3 has 522 characters\nPage 4 has 984 characters\nPage 5 has 778 characters\nPage 6 has 523 characters\nPage 7 has 982 characters\nPage 8 has 772 characters\nPage 9 has 522 characters\nPage 10 has 1008 characters\nPage 11 has 796 characters\nPage 12 has 532 characters\nPage 13 has 986 characters\nPage 14 has 780 characters\nPage 15 has 523 characters\nPage 16 has 990 characters\nPage 17 has 782 characters\nPage 18 has 520 characters\nPage 19 has 1006 characters\nPage 20 has 795 characters\n</pre> In\u00a0[3]: Copied! <pre># Page dimensions in points (1/72 inch)\nprint(page.width, page.height)\n\n# Page number (1-indexed as shown in PDF viewers)\nprint(page.number)\n\n# Page index (0-indexed position in the PDF)\nprint(page.index)\n</pre> # Page dimensions in points (1/72 inch) print(page.width, page.height)  # Page number (1-indexed as shown in PDF viewers) print(page.number)  # Page index (0-indexed position in the PDF) print(page.index) <pre>612 792\n20\n19\n</pre> In\u00a0[4]: Copied! <pre># Extract text from all pages\nall_text = pdf.extract_text()\n\n# Find elements across all pages\nall_headings = pdf.find_all('text[size&gt;=14]:bold')\n\n# Add exclusion zones to all pages (like headers/footers)\npdf.add_exclusion(\n    lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,\n    label=\"header\"\n)\n</pre> # Extract text from all pages all_text = pdf.extract_text()  # Find elements across all pages all_headings = pdf.find_all('text[size&gt;=14]:bold')  # Add exclusion zones to all pages (like headers/footers) pdf.add_exclusion(     lambda page: page.find('text:contains(\"CONFIDENTIAL\")').above() if page.find('text:contains(\"CONFIDENTIAL\")') else None,     label=\"header\" ) Out[4]: <pre>&lt;natural_pdf.core.pdf.PDF at 0x109bee9b0&gt;</pre> In\u00a0[5]: Copied! <pre># Extract text from specific pages\ntext = pdf.pages[2:5].extract_text()\n\n# Find elements across specific pages\nelements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")')\n</pre> # Extract text from specific pages text = pdf.pages[2:5].extract_text()  # Find elements across specific pages elements = pdf.pages[2:5].find_all('text:contains(\"Annual Report\")') <pre>2025-05-06T15:29:28.620225Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,620] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> <pre>2025-05-06T15:29:28.631200Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,631] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> <pre>2025-05-06T15:29:28.640121Z [warning  ] Ignoring unsupported layout keyword argument: 'apply_exclusions' lineno=64 module=natural_pdf.utils.text_extraction\n</pre> <pre>[2025-05-06 11:29:28,640] [ WARNING] text_extraction.py:64 - Ignoring unsupported layout keyword argument: 'apply_exclusions'\n</pre> In\u00a0[6]: Copied! <pre># Get sections with headings as section starts\nsections = pdf.pages.get_sections(\n    start_elements='text[size&gt;=14]:bold',\n    new_section_on_page_break=False\n)\n</pre> # Get sections with headings as section starts sections = pdf.pages.get_sections(     start_elements='text[size&gt;=14]:bold',     new_section_on_page_break=False )"},{"location":"pdf-navigation/#pdf-navigation","title":"PDF Navigation\u00b6","text":"<p>This guide covers the basics of working with PDFs in Natural PDF - opening documents, accessing pages, and navigating through content.</p>"},{"location":"pdf-navigation/#opening-a-pdf","title":"Opening a PDF\u00b6","text":"<p>The main entry point to Natural PDF is the <code>PDF</code> class:</p>"},{"location":"pdf-navigation/#accessing-pages","title":"Accessing Pages\u00b6","text":"<p>Once you have a PDF object, you can access its pages:</p>"},{"location":"pdf-navigation/#page-properties","title":"Page Properties\u00b6","text":"<p>Each <code>Page</code> object has useful properties:</p>"},{"location":"pdf-navigation/#working-across-pages","title":"Working Across Pages\u00b6","text":"<p>Natural PDF makes it easy to work with content across multiple pages:</p>"},{"location":"pdf-navigation/#the-page-collection","title":"The Page Collection\u00b6","text":"<p>The <code>pdf.pages</code> object is a <code>PageCollection</code> that allows batch operations on pages:</p>"},{"location":"pdf-navigation/#document-sections-across-pages","title":"Document Sections Across Pages\u00b6","text":"<p>You can extract sections that span across multiple pages:</p>"},{"location":"pdf-navigation/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to navigate PDFs, you can:</p> <ul> <li>Find elements using selectors</li> <li>Extract text from your documents</li> <li>Work with specific regions</li> </ul>"},{"location":"process-forms-and-invoices/","title":"Extract Data from Forms and Invoices","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Extract data using a simple list that matches the inspection report columns\ndata = page.extract(schema=[\"site\", \"violation count\", \"date\", \"inspection number\", \"summary\"]).extracted()\n\n# Access the extracted information\nprint(f\"Site: {data.site}\")\nprint(f\"Violations: {data.violation_count}\")\nprint(f\"Date: {data.date}\")\nprint(f\"Inspection #: {data.inspection_number}\")\n\n# Check confidence levels\nprint(f\"Confidence \u2013 Site: {data.site_confidence:.2f}\")\nprint(f\"Confidence \u2013 Violations: {data.violation_count_confidence:.2f}\")\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Extract data using a simple list that matches the inspection report columns data = page.extract(schema=[\"site\", \"violation count\", \"date\", \"inspection number\", \"summary\"]).extracted()  # Access the extracted information print(f\"Site: {data.site}\") print(f\"Violations: {data.violation_count}\") print(f\"Date: {data.date}\") print(f\"Inspection #: {data.inspection_number}\")  # Check confidence levels print(f\"Confidence \u2013 Site: {data.site_confidence:.2f}\") print(f\"Confidence \u2013 Violations: {data.violation_count_confidence:.2f}\") <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> <pre>Device set to use mps:0\n</pre> <pre>Site: Durham\u2019s Meatpacking  \nViolations: 7\nDate: February 3, 1905\nInspection #: None\nConfidence \u2013 Site: 0.58\nConfidence \u2013 Violations: 0.41\n</pre> <p>This works completely offline using document question-answering models.</p> In\u00a0[2]: Copied! <pre>from pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Define exactly what you want to extract for the inspection report\nclass InspectionReport(BaseModel):\n    site_name: str = Field(description=\"Name of the inspection site\")\n    violation_count: int = Field(description=\"Number of violations found\")\n    inspection_date: str = Field(description=\"Inspection date in any format\")\n    inspection_number: str = Field(description=\"Inspection reference ID\")\n    summary: str = Field(description=\"Inspection summary paragraph\")\n\n# Set up LLM client (using Anthropic here)\nclient = OpenAI(\n    api_key=\"your-api-key\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\n# Extract structured data\npage.extract(schema=InspectionReport, client=client, model=\"gemini-2.0-flash\")\n\ntry:\n    report_data = page.extracted()\n    print(f\"Site: {report_data.site_name}\")\n    print(f\"Violations: {report_data.violation_count}\")\n    print(f\"Inspection #: {report_data.inspection_number}\")\nexcept Exception as e:\n    print(\"Extraction failed with error\", e)\n</pre> from pydantic import BaseModel, Field from openai import OpenAI  # Define exactly what you want to extract for the inspection report class InspectionReport(BaseModel):     site_name: str = Field(description=\"Name of the inspection site\")     violation_count: int = Field(description=\"Number of violations found\")     inspection_date: str = Field(description=\"Inspection date in any format\")     inspection_number: str = Field(description=\"Inspection reference ID\")     summary: str = Field(description=\"Inspection summary paragraph\")  # Set up LLM client (using Anthropic here) client = OpenAI(     api_key=\"your-api-key\",     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\" )  # Extract structured data page.extract(schema=InspectionReport, client=client, model=\"gemini-2.0-flash\")  try:     report_data = page.extracted()     print(f\"Site: {report_data.site_name}\")     print(f\"Violations: {report_data.violation_count}\")     print(f\"Inspection #: {report_data.inspection_number}\") except Exception as e:     print(\"Extraction failed with error\", e) <pre>Extraction failed with error Stored result for 'structured' indicates a failed extraction attempt. Error: Error code: 400 - [{'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}]\n</pre> In\u00a0[3]: Copied! <pre># Sometimes data is in specific areas of the page\nheader_region = page.create_region(0, 0, page.width, page.height * 0.3)\nfooter_region = page.create_region(0, page.height * 0.7, page.width, page.height)\n\n# Extract company info from header\ncompany_data = header_region.extract(\n    schema=[\"company name\", \"address\", \"phone\"]\n).extracted()\n\n# Extract totals from footer  \ntotals_data = footer_region.extract(\n    schema=[\"subtotal\", \"tax\", \"total\"]\n).extracted()\n</pre> # Sometimes data is in specific areas of the page header_region = page.create_region(0, 0, page.width, page.height * 0.3) footer_region = page.create_region(0, page.height * 0.7, page.width, page.height)  # Extract company info from header company_data = header_region.extract(     schema=[\"company name\", \"address\", \"phone\"] ).extracted()  # Extract totals from footer   totals_data = footer_region.extract(     schema=[\"subtotal\", \"tax\", \"total\"] ).extracted() In\u00a0[4]: Copied! <pre>import os\nfrom pathlib import Path\n\n# Define your extraction schema\nclass FormData(BaseModel):\n    applicant_name: str\n    application_date: str  \n    reference_number: str\n    status: str = Field(default=\"unknown\")\n\n# Process all PDFs in a folder\nform_results = []\npdf_folder = Path(\"forms/\")\n\nfor pdf_file in pdf_folder.glob(\"*.pdf\"):\n    print(f\"Processing {pdf_file.name}...\")\n    \n    pdf = PDF(str(pdf_file))\n    page = pdf.pages[0]  # Assuming single-page forms\n    \n    # Extract data\n    page.extract(schema=FormData, client=client)\n    data = page.extracted()\n    \n    # Add filename for tracking\n    result = {\n        \"filename\": pdf_file.name,\n        \"applicant_name\": data.applicant_name,\n        \"application_date\": data.application_date,\n        \"reference_number\": data.reference_number,\n        \"status\": data.status\n    }\n    form_results.append(result)\n    \n    pdf.close()  # Clean up\n\n# Save results to CSV\nimport pandas as pd\ndf = pd.DataFrame(form_results)\ndf.to_csv(\"extracted_form_data.csv\", index=False)\nprint(f\"Processed {len(form_results)} forms\")\n</pre> import os from pathlib import Path  # Define your extraction schema class FormData(BaseModel):     applicant_name: str     application_date: str       reference_number: str     status: str = Field(default=\"unknown\")  # Process all PDFs in a folder form_results = [] pdf_folder = Path(\"forms/\")  for pdf_file in pdf_folder.glob(\"*.pdf\"):     print(f\"Processing {pdf_file.name}...\")          pdf = PDF(str(pdf_file))     page = pdf.pages[0]  # Assuming single-page forms          # Extract data     page.extract(schema=FormData, client=client)     data = page.extracted()          # Add filename for tracking     result = {         \"filename\": pdf_file.name,         \"applicant_name\": data.applicant_name,         \"application_date\": data.application_date,         \"reference_number\": data.reference_number,         \"status\": data.status     }     form_results.append(result)          pdf.close()  # Clean up  # Save results to CSV import pandas as pd df = pd.DataFrame(form_results) df.to_csv(\"extracted_form_data.csv\", index=False) print(f\"Processed {len(form_results)} forms\") <pre>Processed 0 forms\n</pre> In\u00a0[5]: Copied! <pre># Apply OCR before extraction\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Filter out low-confidence OCR text to avoid noise\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nprint(f\"Using {len(reliable_text)} high-confidence OCR elements\")\n\n# Now extract data (works on OCR'd text)\ndata = page.extract(schema=[\"invoice number\", \"total\", \"date\"]).extracted()\n</pre> # Apply OCR before extraction page.apply_ocr(engine='easyocr', languages=['en'])  # Filter out low-confidence OCR text to avoid noise reliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]') print(f\"Using {len(reliable_text)} high-confidence OCR elements\")  # Now extract data (works on OCR'd text) data = page.extract(schema=[\"invoice number\", \"total\", \"date\"]).extracted() <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> <pre>Using 29 high-confidence OCR elements\n</pre>"},{"location":"process-forms-and-invoices/#extract-data-from-forms-and-invoices","title":"Extract Data from Forms and Invoices\u00b6","text":"<p>You have a stack of invoices, forms, or structured documents where you need to pull out specific pieces of information - invoice numbers, totals, dates, names, etc. Here's how to automate that extraction.</p>"},{"location":"process-forms-and-invoices/#the-problem","title":"The Problem\u00b6","text":"<p>Manual data entry from PDFs is slow and error-prone. You need to:</p> <ul> <li>Extract the same fields from hundreds of similar documents</li> <li>Handle slight variations in layout between documents</li> <li>Get structured data you can actually work with</li> <li>Maintain accuracy while processing quickly</li> </ul>"},{"location":"process-forms-and-invoices/#quick-solution-list-the-fields-you-want","title":"Quick Solution: List the Fields You Want\u00b6","text":"<p>Don't overthink it - just tell Natural PDF what information you're looking for:</p>"},{"location":"process-forms-and-invoices/#for-complex-data-use-pydantic-schemas","title":"For Complex Data: Use Pydantic Schemas\u00b6","text":"<p>When you need more control over data types and validation:</p>"},{"location":"process-forms-and-invoices/#handle-different-document-layouts","title":"Handle Different Document Layouts\u00b6","text":"<p>For documents that vary in structure, use spatial hints:</p>"},{"location":"process-forms-and-invoices/#process-multiple-documents","title":"Process Multiple Documents\u00b6","text":"<p>Batch process a folder of similar documents:</p>"},{"location":"process-forms-and-invoices/#handle-scanned-documents","title":"Handle Scanned Documents\u00b6","text":"<p>For image-based PDFs, apply OCR first:</p>"},{"location":"process-forms-and-invoices/#common-form-patterns","title":"Common Form Patterns\u00b6","text":""},{"location":"process-forms-and-invoices/#validation-and-error-handling","title":"Validation and Error Handling\u00b6","text":"<p>Check your extracted data for common issues:</p> <pre>def validate_invoice_data(data):\n    issues = []\n    \n    # Check for missing required fields\n    if not data.invoice_number or data.invoice_number.strip() == \"\":\n        issues.append(\"Missing invoice number\")\n    \n    # Validate amounts\n    if data.total_amount &lt;= 0:\n        issues.append(\"Invalid total amount\")\n    \n    # Check date format\n    try:\n        from datetime import datetime\n        datetime.strptime(data.invoice_date, \"%Y-%m-%d\")\n    except ValueError:\n        # Try common date formats\n        common_formats = [\"%m/%d/%Y\", \"%d/%m/%Y\", \"%B %d, %Y\"]\n        date_valid = False\n        for fmt in common_formats:\n            try:\n                datetime.strptime(data.invoice_date, fmt)\n                date_valid = True\n                break\n            except ValueError:\n                continue\n        if not date_valid:\n            issues.append(f\"Invalid date format: {data.invoice_date}\")\n    \n    return issues\n\n# Validate extracted data\nvalidation_issues = validate_invoice_data(invoice_data)\nif validation_issues:\n    print(\"Data quality issues found:\")\n    for issue in validation_issues:\n        print(f\"- {issue}\")\nelse:\n    print(\"Data validation passed!\")\n</pre>"},{"location":"process-forms-and-invoices/#improve-accuracy-with-context","title":"Improve Accuracy with Context\u00b6","text":"<p>Give the AI more context for better extraction:</p> <pre># Add context about the document type\nextraction_prompt = \"\"\"\nThis is a medical insurance claim form. \nExtract the following information, paying attention to:\n- Policy numbers are usually 10-12 digits\n- Claim amounts should be in dollars\n- Dates should be in MM/DD/YYYY format\n- Provider names are usually at the top of the form\n\"\"\"\n\nclass InsuranceClaim(BaseModel):\n    policy_number: str = Field(description=\"Insurance policy number (10-12 digits)\")\n    claim_amount: float = Field(description=\"Total claim amount in USD\")\n    service_date: str = Field(description=\"Date of service in MM/DD/YYYY format\")\n    provider_name: str = Field(description=\"Healthcare provider name\")\n    patient_name: str = Field(description=\"Patient full name\")\n\n# Use custom prompt for better results\npage.extract(\n    schema=InsuranceClaim, \n    client=client,\n    prompt=extraction_prompt\n)\n</pre>"},{"location":"process-forms-and-invoices/#debug-extraction-issues","title":"Debug Extraction Issues\u00b6","text":"<p>When extraction isn't working well:</p> <pre># 1. Check what text the AI can actually see\nextracted_text = page.extract_text()\nprint(\"Available text:\")\nprint(extracted_text[:500])  # First 500 characters\n\n# 2. Try extracting with lower confidence threshold\ndata = page.extract(\n    schema=[\"invoice number\", \"total\"], \n    min_confidence=0.5  # Lower threshold\n).extracted()\n\n# 3. Check confidence scores for each field\nfor field_name in data.__fields__:\n    confidence_field = f\"{field_name}_confidence\"\n    if hasattr(data, confidence_field):\n        confidence = getattr(data, confidence_field)\n        value = getattr(data, field_name)\n        print(f\"{field_name}: '{value}' (confidence: {confidence:.2f})\")\n\n# 4. Try vision mode if text mode fails\nif any(getattr(data, f\"{field}_confidence\", 0) &lt; 0.7 for field in [\"invoice_number\", \"total\"]):\n    print(\"Low confidence detected, trying vision mode...\")\n    page.extract(schema=[\"invoice number\", \"total\"], client=client, using='vision')\n    data = page.extracted()\n</pre>"},{"location":"process-forms-and-invoices/","title":"Extract Data from Forms and Invoices","text":"<p>You have a stack of invoices, forms, or structured documents where you need to pull out specific pieces of information - invoice numbers, totals, dates, names, etc. Here's how to automate that extraction.</p>"},{"location":"process-forms-and-invoices/#the-problem","title":"The Problem","text":"<p>Manual data entry from PDFs is slow and error-prone. You need to: - Extract the same fields from hundreds of similar documents - Handle slight variations in layout between documents - Get structured data you can actually work with - Maintain accuracy while processing quickly</p>"},{"location":"process-forms-and-invoices/#quick-solution-list-the-fields-you-want","title":"Quick Solution: List the Fields You Want","text":"<p>Don't overthink it - just tell Natural PDF what information you're looking for:</p> <pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Extract data using a simple list that matches the inspection report columns\ndata = page.extract(schema=[\"site\", \"violation count\", \"date\", \"inspection number\", \"summary\"]).extracted()\n\n# Access the extracted information\nprint(f\"Site: {data.site}\")\nprint(f\"Violations: {data.violation_count}\")\nprint(f\"Date: {data.date}\")\nprint(f\"Inspection #: {data.inspection_number}\")\n\n# Check confidence levels\nprint(f\"Confidence \u2013 Site: {data.site_confidence:.2f}\")\nprint(f\"Confidence \u2013 Violations: {data.violation_count_confidence:.2f}\")\n</code></pre> <p>This works completely offline using document question-answering models.</p>"},{"location":"process-forms-and-invoices/#for-complex-data-use-pydantic-schemas","title":"For Complex Data: Use Pydantic Schemas","text":"<p>When you need more control over data types and validation:</p> <pre><code>from pydantic import BaseModel, Field\nfrom openai import OpenAI\n\n# Define exactly what you want to extract for the inspection report\nclass InspectionReport(BaseModel):\n    site_name: str = Field(description=\"Name of the inspection site\")\n    violation_count: int = Field(description=\"Number of violations found\")\n    inspection_date: str = Field(description=\"Inspection date in any format\")\n    inspection_number: str = Field(description=\"Inspection reference ID\")\n    summary: str = Field(description=\"Inspection summary paragraph\")\n\n# Set up LLM client (using Anthropic here)\nclient = OpenAI(\n    api_key=\"your-api-key\",\n    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n)\n\n# Extract structured data\npage.extract(schema=InspectionReport, client=client, model=\"gemini-2.0-flash\")\n\ntry:\n    report_data = page.extracted()\n    print(f\"Site: {report_data.site_name}\")\n    print(f\"Violations: {report_data.violation_count}\")\n    print(f\"Inspection #: {report_data.inspection_number}\")\nexcept Exception as e:\n    print(\"Extraction failed with error\", e)\n</code></pre>"},{"location":"process-forms-and-invoices/#handle-different-document-layouts","title":"Handle Different Document Layouts","text":"<p>For documents that vary in structure, use spatial hints:</p> <pre><code># Sometimes data is in specific areas of the page\nheader_region = page.create_region(0, 0, page.width, page.height * 0.3)\nfooter_region = page.create_region(0, page.height * 0.7, page.width, page.height)\n\n# Extract company info from header\ncompany_data = header_region.extract(\n    schema=[\"company name\", \"address\", \"phone\"]\n).extracted()\n\n# Extract totals from footer  \ntotals_data = footer_region.extract(\n    schema=[\"subtotal\", \"tax\", \"total\"]\n).extracted()\n</code></pre>"},{"location":"process-forms-and-invoices/#process-multiple-documents","title":"Process Multiple Documents","text":"<p>Batch process a folder of similar documents:</p> <pre><code>import os\nfrom pathlib import Path\n\n# Define your extraction schema\nclass FormData(BaseModel):\n    applicant_name: str\n    application_date: str  \n    reference_number: str\n    status: str = Field(default=\"unknown\")\n\n# Process all PDFs in a folder\nform_results = []\npdf_folder = Path(\"forms/\")\n\nfor pdf_file in pdf_folder.glob(\"*.pdf\"):\n    print(f\"Processing {pdf_file.name}...\")\n\n    pdf = PDF(str(pdf_file))\n    page = pdf.pages[0]  # Assuming single-page forms\n\n    # Extract data\n    page.extract(schema=FormData, client=client)\n    data = page.extracted()\n\n    # Add filename for tracking\n    result = {\n        \"filename\": pdf_file.name,\n        \"applicant_name\": data.applicant_name,\n        \"application_date\": data.application_date,\n        \"reference_number\": data.reference_number,\n        \"status\": data.status\n    }\n    form_results.append(result)\n\n    pdf.close()  # Clean up\n\n# Save results to CSV\nimport pandas as pd\ndf = pd.DataFrame(form_results)\ndf.to_csv(\"extracted_form_data.csv\", index=False)\nprint(f\"Processed {len(form_results)} forms\")\n</code></pre>"},{"location":"process-forms-and-invoices/#handle-scanned-documents","title":"Handle Scanned Documents","text":"<p>For image-based PDFs, apply OCR first:</p> <pre><code># Apply OCR before extraction\npage.apply_ocr(engine='easyocr', languages=['en'])\n\n# Filter out low-confidence OCR text to avoid noise\nreliable_text = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nprint(f\"Using {len(reliable_text)} high-confidence OCR elements\")\n\n# Now extract data (works on OCR'd text)\ndata = page.extract(schema=[\"invoice number\", \"total\", \"date\"]).extracted()\n</code></pre>"},{"location":"process-forms-and-invoices/#common-form-patterns","title":"Common Form Patterns","text":""},{"location":"process-forms-and-invoices/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>Check your extracted data for common issues:</p> <pre><code>def validate_invoice_data(data):\n    issues = []\n\n    # Check for missing required fields\n    if not data.invoice_number or data.invoice_number.strip() == \"\":\n        issues.append(\"Missing invoice number\")\n\n    # Validate amounts\n    if data.total_amount &lt;= 0:\n        issues.append(\"Invalid total amount\")\n\n    # Check date format\n    try:\n        from datetime import datetime\n        datetime.strptime(data.invoice_date, \"%Y-%m-%d\")\n    except ValueError:\n        # Try common date formats\n        common_formats = [\"%m/%d/%Y\", \"%d/%m/%Y\", \"%B %d, %Y\"]\n        date_valid = False\n        for fmt in common_formats:\n            try:\n                datetime.strptime(data.invoice_date, fmt)\n                date_valid = True\n                break\n            except ValueError:\n                continue\n        if not date_valid:\n            issues.append(f\"Invalid date format: {data.invoice_date}\")\n\n    return issues\n\n# Validate extracted data\nvalidation_issues = validate_invoice_data(invoice_data)\nif validation_issues:\n    print(\"Data quality issues found:\")\n    for issue in validation_issues:\n        print(f\"- {issue}\")\nelse:\n    print(\"Data validation passed!\")\n</code></pre>"},{"location":"process-forms-and-invoices/#improve-accuracy-with-context","title":"Improve Accuracy with Context","text":"<p>Give the AI more context for better extraction:</p> <pre><code># Add context about the document type\nextraction_prompt = \"\"\"\nThis is a medical insurance claim form. \nExtract the following information, paying attention to:\n- Policy numbers are usually 10-12 digits\n- Claim amounts should be in dollars\n- Dates should be in MM/DD/YYYY format\n- Provider names are usually at the top of the form\n\"\"\"\n\nclass InsuranceClaim(BaseModel):\n    policy_number: str = Field(description=\"Insurance policy number (10-12 digits)\")\n    claim_amount: float = Field(description=\"Total claim amount in USD\")\n    service_date: str = Field(description=\"Date of service in MM/DD/YYYY format\")\n    provider_name: str = Field(description=\"Healthcare provider name\")\n    patient_name: str = Field(description=\"Patient full name\")\n\n# Use custom prompt for better results\npage.extract(\n    schema=InsuranceClaim, \n    client=client,\n    prompt=extraction_prompt\n)\n</code></pre>"},{"location":"process-forms-and-invoices/#debug-extraction-issues","title":"Debug Extraction Issues","text":"<p>When extraction isn't working well:</p> <pre><code># 1. Check what text the AI can actually see\nextracted_text = page.extract_text()\nprint(\"Available text:\")\nprint(extracted_text[:500])  # First 500 characters\n\n# 2. Try extracting with lower confidence threshold\ndata = page.extract(\n    schema=[\"invoice number\", \"total\"], \n    min_confidence=0.5  # Lower threshold\n).extracted()\n\n# 3. Check confidence scores for each field\nfor field_name in data.__fields__:\n    confidence_field = f\"{field_name}_confidence\"\n    if hasattr(data, confidence_field):\n        confidence = getattr(data, confidence_field)\n        value = getattr(data, field_name)\n        print(f\"{field_name}: '{value}' (confidence: {confidence:.2f})\")\n\n# 4. Try vision mode if text mode fails\nif any(getattr(data, f\"{field}_confidence\", 0) &lt; 0.7 for field in [\"invoice_number\", \"total\"]):\n    print(\"Low confidence detected, trying vision mode...\")\n    page.extract(schema=[\"invoice number\", \"total\"], client=client, using='vision')\n    data = page.extracted()\n</code></pre>"},{"location":"quick-reference/","title":"Quick Reference","text":""},{"location":"quick-reference/#quick-reference","title":"Quick Reference\u00b6","text":""},{"location":"quick-reference/#essential-workflows","title":"Essential Workflows\u00b6","text":""},{"location":"quick-reference/#basic-text-extraction","title":"Basic Text Extraction\u00b6","text":"<pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\ntext = page.extract_text()\n</pre>"},{"location":"quick-reference/#find-extract-pattern","title":"Find \u2192 Extract Pattern\u00b6","text":"<pre># Find specific elements, then extract\nheading = page.find('text:contains(\"Summary\"):bold')\ncontent = heading.below().extract_text()\n</pre>"},{"location":"quick-reference/#ocr-for-scanned-documents","title":"OCR for Scanned Documents\u00b6","text":"<pre># Apply OCR first, then extract\npage.apply_ocr(engine='easyocr', languages=['en'])\ntext = page.extract_text()\n</pre>"},{"location":"quick-reference/#layout-analysis-table-extraction","title":"Layout Analysis \u2192 Table Extraction\u00b6","text":"<pre># Detect layout, then extract tables\npage.analyze_layout(engine='yolo')\ntable_region = page.find('region[type=table]')\ndata = table_region.extract_table()\n</pre>"},{"location":"quick-reference/#common-selectors","title":"Common Selectors\u00b6","text":""},{"location":"quick-reference/#text-content","title":"Text Content\u00b6","text":"<pre>page.find('text:contains(\"Invoice\")')           # Contains text\npage.find('text:contains(\"total\")', case=False) # Case insensitive\npage.find('text:contains(\"\\\\d+\")', regex=True)  # Regex pattern\n</pre>"},{"location":"quick-reference/#text-formatting","title":"Text Formatting\u00b6","text":"<pre>page.find_all('text:bold')                      # Bold text\npage.find_all('text:italic')                    # Italic text\npage.find_all('text:strike')                    # Struck-through text\npage.find_all('text:underline')                 # Underlined text\npage.find_all('text[size&gt;=12]')                 # Large text\npage.find_all('text[fontname*=Arial]')          # Specific font\n</pre>"},{"location":"quick-reference/#spatial-relationships","title":"Spatial Relationships\u00b6","text":"<pre>page.find('text:above(\"line[width&gt;=2]\")')       # Above thick line\npage.find('text:below(\"text:contains(\"Title\")\")')  # Below title\npage.find('text:near(\"image\")')                 # Near images\n</pre>"},{"location":"quick-reference/#layout-elements","title":"Layout Elements\u00b6","text":"<pre>page.find_all('line:horizontal')                # Horizontal lines\npage.find_all('rect')                           # Rectangles\npage.find_all('region[type=table]')             # Detected tables\npage.find_all('region[type=title]')             # Detected titles\n</pre>"},{"location":"quick-reference/#ocr-and-sources","title":"OCR and Sources\u00b6","text":"<pre>page.find_all('text[source=ocr]')               # OCR-generated text\npage.find_all('text[source=pdf]')               # Original PDF text\npage.find_all('text[confidence&gt;=0.8]')          # High-confidence OCR\n</pre>"},{"location":"quick-reference/#essential-methods","title":"Essential Methods\u00b6","text":""},{"location":"quick-reference/#finding-elements","title":"Finding Elements\u00b6","text":"<pre>page.find(selector)                             # First match\npage.find_all(selector)                         # All matches\nelement.next()                                  # Next element in reading order\nelement.previous()                              # Previous element\n</pre>"},{"location":"quick-reference/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<pre>element.above(height=100)                       # Region above element\nelement.below(until='line:horizontal')          # Below until boundary\nelement.left(width=200)                         # Region to the left\nelement.right()                                 # Region to the right\n</pre>"},{"location":"quick-reference/#text-extraction","title":"Text Extraction\u00b6","text":"<pre>page.extract_text()                             # All text from page\npage.extract_text(layout=True)                  # Preserve layout\nelement.extract_text()                          # Text from specific element\nregion.extract_text()                           # Text from region\n</pre>"},{"location":"quick-reference/#table-extraction","title":"Table Extraction\u00b6","text":"<pre>page.extract_table()                            # First table on page\nregion.extract_table()                          # Table from region\nregion.extract_table(method='tatr')             # Force TATR method\nregion.extract_table(method='pdfplumber')       # Force pdfplumber method\n</pre>"},{"location":"quick-reference/#ocr","title":"OCR\u00b6","text":"<pre>page.apply_ocr()                                # Default OCR\npage.apply_ocr(engine='paddle', languages=['en', 'zh-cn'])\npage.apply_ocr(engine='easyocr', min_confidence=0.8)\nregion.apply_ocr()                              # OCR specific region\n</pre>"},{"location":"quick-reference/#layout-analysis","title":"Layout Analysis\u00b6","text":"<pre>page.analyze_layout()                           # Default YOLO\npage.analyze_layout(engine='tatr')              # Table-focused\npage.analyze_layout(engine='surya')             # High accuracy\npage.clear_detected_layout_regions()           # Clear previous results\n</pre>"},{"location":"quick-reference/#document-qa","title":"Document QA\u00b6","text":"<pre>result = page.ask(\"What is the total amount?\")\nprint(result.answer)                            # The answer\nprint(result.confidence)                        # Confidence score\nresult.show()                                   # Highlight answer location\n</pre>"},{"location":"quick-reference/#structured-data-extraction","title":"Structured Data Extraction\u00b6","text":"<pre># Simple approach\ndata = page.extract(schema=[\"company\", \"date\", \"total\"]).extracted()\n\n# With Pydantic schema\nfrom pydantic import BaseModel\nclass Invoice(BaseModel):\n    company: str\n    total: float\n    date: str\n\ndata = page.extract(schema=Invoice, client=client).extracted()\n</pre>"},{"location":"quick-reference/#visualization-debugging","title":"Visualization &amp; Debugging\u00b6","text":""},{"location":"quick-reference/#highlighting","title":"Highlighting\u00b6","text":"<pre># Simple visualization\nelements.show(color=\"red\")                      # Single collection\nelements.show(color=\"blue\", label=\"Headers\")    # With label\nelements.show(group_by='type')                  # Color by type\n\n# Multiple collections together\nwith page.highlights() as h:\n    h.add(elements1, color=\"red\", label=\"Type 1\")\n    h.add(elements2, color=\"blue\", label=\"Type 2\")\n    h.show()\n</pre>"},{"location":"quick-reference/#viewing","title":"Viewing\u00b6","text":"<pre>page.show()                                     # Show page with highlights\nelement.show()                                  # Show specific element\npage.show(width=700)                        # Generate image\nregion.show(crop=True)                 # Crop to region only\n</pre>"},{"location":"quick-reference/#interactive-viewer","title":"Interactive Viewer\u00b6","text":"<pre>page.viewer()                                   # Launch interactive viewer (Jupyter)\n</pre>"},{"location":"quick-reference/#exclusion-zones","title":"Exclusion Zones\u00b6","text":""},{"location":"quick-reference/#page-level-exclusions","title":"Page-Level Exclusions\u00b6","text":"<pre>header = page.find('text:contains(\"CONFIDENTIAL\")').above()\npage.add_exclusion(header)                      # Exclude from extraction\npage.clear_exclusions()                         # Remove exclusions\ntext = page.extract_text(use_exclusions=False)  # Ignore exclusions\n</pre>"},{"location":"quick-reference/#pdf-level-exclusions","title":"PDF-Level Exclusions\u00b6","text":"<pre># Exclude headers from all pages\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.1),\n    label=\"Header\"\n)\n</pre>"},{"location":"quick-reference/#configuration-options","title":"Configuration Options\u00b6","text":""},{"location":"quick-reference/#ocr-engines","title":"OCR Engines\u00b6","text":"<pre>from natural_pdf.ocr import EasyOCROptions, PaddleOCROptions\n\neasy_opts = EasyOCROptions(gpu=True, paragraph=True)\npaddle_opts = PaddleOCROptions(lang='en')\n</pre>"},{"location":"quick-reference/#layout-analysis-options","title":"Layout Analysis Options\u00b6","text":"<pre>from natural_pdf.analyzers.layout import YOLOOptions\n\nyolo_opts = YOLOOptions(confidence_threshold=0.5)\npage.analyze_layout(engine='yolo', options=yolo_opts)\n</pre>"},{"location":"quick-reference/#common-patterns","title":"Common Patterns\u00b6","text":""},{"location":"quick-reference/#extract-inspection-report-data","title":"Extract Inspection Report Data\u00b6","text":"<pre># Find violation count\nviolations = page.find('text:contains(\"Violation Count\"):right(width=100)')\n\n# Get inspection number from the header box (regex search)\ninspection_num = page.find('text:contains(\"INS-[A-Z0-9]+\")', regex=True)\n\n# Extract inspection date\ninspection_date = page.find('text:contains(\"Date:\"):right(width=150)')\n\n# Get site name (text to the right of \"Site:\")\nsite_name = page.find('text:contains(\"Site:\"):right(width=300)').extract_text()\n</pre>"},{"location":"quick-reference/#process-forms","title":"Process Forms\u00b6","text":"<pre># Exclude header/footer\npage.add_exclusion(page.create_region(0, 0, page.width, 50))\npage.add_exclusion(page.create_region(0, page.height-50, page.width, page.height))\n\n# Extract form fields\nfields = page.find_all('text:bold')\nvalues = [field.right(width=300).extract_text() for field in fields]\n</pre>"},{"location":"quick-reference/#handle-scanned-documents","title":"Handle Scanned Documents\u00b6","text":"<pre># Apply OCR with high accuracy\npage.apply_ocr(engine='surya', languages=['en'])\n\n# Extract with confidence filtering\ntext_elements = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = text_elements.extract_text()\n</pre>"},{"location":"quick-reference/#troubleshooting","title":"Troubleshooting\u00b6","text":"Problem Solution No text found Try <code>page.apply_ocr()</code> first Wrong elements selected Use <code>elements.show()</code> to debug selectors Poor table extraction Try <code>page.analyze_layout(engine='tatr')</code> first Text extraction includes headers Use <code>page.add_exclusion()</code> Low OCR accuracy Try different engine or increase resolution Elements overlap multiple pages Use page-specific searches"},{"location":"quick-reference/#file-formats","title":"File Formats\u00b6","text":""},{"location":"quick-reference/#saving-results","title":"Saving Results\u00b6","text":"<pre># Save as image\npage.save_image(\"output.png\", width=700)\n\n# Save table as CSV\nimport pandas as pd\ndf = table_data.to_df(header=\"first\")\ndf.to_csv(\"table.csv\")\n\n# Export searchable PDF\nfrom natural_pdf.exporters import SearchablePDFExporter\nexporter = SearchablePDFExporter()\nexporter.export(pdf, \"searchable.pdf\")\n</pre>"},{"location":"quick-reference/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>New to Natural PDF? \u2192 Start with Installation</li> <li>Learning the basics? \u2192 Follow the Tutorials</li> <li>Solving specific problems? \u2192 Check the how-to guides</li> <li>Need detailed info? \u2192 See the API Reference</li> </ul>"},{"location":"quick-reference/","title":"Quick Reference","text":""},{"location":"quick-reference/#essential-workflows","title":"Essential Workflows","text":""},{"location":"quick-reference/#basic-text-extraction","title":"Basic Text Extraction","text":"<pre><code>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\ntext = page.extract_text()\n</code></pre>"},{"location":"quick-reference/#find-extract-pattern","title":"Find \u2192 Extract Pattern","text":"<pre><code># Find specific elements, then extract\nheading = page.find('text:contains(\"Summary\"):bold')\ncontent = heading.below().extract_text()\n</code></pre>"},{"location":"quick-reference/#ocr-for-scanned-documents","title":"OCR for Scanned Documents","text":"<pre><code># Apply OCR first, then extract\npage.apply_ocr(engine='easyocr', languages=['en'])\ntext = page.extract_text()\n</code></pre>"},{"location":"quick-reference/#layout-analysis-table-extraction","title":"Layout Analysis \u2192 Table Extraction","text":"<pre><code># Detect layout, then extract tables\npage.analyze_layout(engine='yolo')\ntable_region = page.find('region[type=table]')\ndata = table_region.extract_table()\n</code></pre>"},{"location":"quick-reference/#common-selectors","title":"Common Selectors","text":""},{"location":"quick-reference/#text-content","title":"Text Content","text":"<pre><code>page.find('text:contains(\"Invoice\")')           # Contains text\npage.find('text:contains(\"total\")', case=False) # Case insensitive\npage.find('text:contains(\"\\\\d+\")', regex=True)  # Regex pattern\n</code></pre>"},{"location":"quick-reference/#text-formatting","title":"Text Formatting","text":"<pre><code>page.find_all('text:bold')                      # Bold text\npage.find_all('text:italic')                    # Italic text\npage.find_all('text:strike')                    # Struck-through text\npage.find_all('text:underline')                 # Underlined text\npage.find_all('text[size&gt;=12]')                 # Large text\npage.find_all('text[fontname*=Arial]')          # Specific font\n</code></pre>"},{"location":"quick-reference/#spatial-relationships","title":"Spatial Relationships","text":"<pre><code>page.find('text:above(\"line[width&gt;=2]\")')       # Above thick line\npage.find('text:below(\"text:contains(\"Title\")\")')  # Below title\npage.find('text:near(\"image\")')                 # Near images\n</code></pre>"},{"location":"quick-reference/#layout-elements","title":"Layout Elements","text":"<pre><code>page.find_all('line:horizontal')                # Horizontal lines\npage.find_all('rect')                           # Rectangles\npage.find_all('region[type=table]')             # Detected tables\npage.find_all('region[type=title]')             # Detected titles\n</code></pre>"},{"location":"quick-reference/#ocr-and-sources","title":"OCR and Sources","text":"<pre><code>page.find_all('text[source=ocr]')               # OCR-generated text\npage.find_all('text[source=pdf]')               # Original PDF text\npage.find_all('text[confidence&gt;=0.8]')          # High-confidence OCR\n</code></pre>"},{"location":"quick-reference/#essential-methods","title":"Essential Methods","text":""},{"location":"quick-reference/#finding-elements","title":"Finding Elements","text":"<pre><code>page.find(selector)                             # First match\npage.find_all(selector)                         # All matches\nelement.next()                                  # Next element in reading order\nelement.previous()                              # Previous element\n</code></pre>"},{"location":"quick-reference/#spatial-navigation","title":"Spatial Navigation","text":"<pre><code>element.above(height=100)                       # Region above element\nelement.below(until='line:horizontal')          # Below until boundary\nelement.left(width=200)                         # Region to the left\nelement.right()                                 # Region to the right\n</code></pre>"},{"location":"quick-reference/#text-extraction","title":"Text Extraction","text":"<pre><code>page.extract_text()                             # All text from page\npage.extract_text(layout=True)                  # Preserve layout\nelement.extract_text()                          # Text from specific element\nregion.extract_text()                           # Text from region\n</code></pre>"},{"location":"quick-reference/#table-extraction","title":"Table Extraction","text":"<pre><code>page.extract_table()                            # First table on page\nregion.extract_table()                          # Table from region\nregion.extract_table(method='tatr')             # Force TATR method\nregion.extract_table(method='pdfplumber')       # Force pdfplumber method\n</code></pre>"},{"location":"quick-reference/#ocr","title":"OCR","text":"<pre><code>page.apply_ocr()                                # Default OCR\npage.apply_ocr(engine='paddle', languages=['en', 'zh-cn'])\npage.apply_ocr(engine='easyocr', min_confidence=0.8)\nregion.apply_ocr()                              # OCR specific region\n</code></pre>"},{"location":"quick-reference/#layout-analysis","title":"Layout Analysis","text":"<pre><code>page.analyze_layout()                           # Default YOLO\npage.analyze_layout(engine='tatr')              # Table-focused\npage.analyze_layout(engine='surya')             # High accuracy\npage.clear_detected_layout_regions()           # Clear previous results\n</code></pre>"},{"location":"quick-reference/#document-qa","title":"Document QA","text":"<pre><code>result = page.ask(\"What is the total amount?\")\nprint(result.answer)                            # The answer\nprint(result.confidence)                        # Confidence score\nresult.show()                                   # Highlight answer location\n</code></pre>"},{"location":"quick-reference/#structured-data-extraction","title":"Structured Data Extraction","text":"<pre><code># Simple approach\ndata = page.extract(schema=[\"company\", \"date\", \"total\"]).extracted()\n\n# With Pydantic schema\nfrom pydantic import BaseModel\nclass Invoice(BaseModel):\n    company: str\n    total: float\n    date: str\n\ndata = page.extract(schema=Invoice, client=client).extracted()\n</code></pre>"},{"location":"quick-reference/#visualization-debugging","title":"Visualization &amp; Debugging","text":""},{"location":"quick-reference/#highlighting","title":"Highlighting","text":"<pre><code># Simple visualization\nelements.show(color=\"red\")                      # Single collection\nelements.show(color=\"blue\", label=\"Headers\")    # With label\nelements.show(group_by='type')                  # Color by type\n\n# Multiple collections together\nwith page.highlights() as h:\n    h.add(elements1, color=\"red\", label=\"Type 1\")\n    h.add(elements2, color=\"blue\", label=\"Type 2\")\n    h.show()\n</code></pre>"},{"location":"quick-reference/#viewing","title":"Viewing","text":"<pre><code>page.show()                                     # Show page with highlights\nelement.show()                                  # Show specific element\npage.show(width=700)                        # Generate image\nregion.show(crop=True)                 # Crop to region only\n</code></pre>"},{"location":"quick-reference/#interactive-viewer","title":"Interactive Viewer","text":"<pre><code>page.viewer()                                   # Launch interactive viewer (Jupyter)\n</code></pre>"},{"location":"quick-reference/#exclusion-zones","title":"Exclusion Zones","text":""},{"location":"quick-reference/#page-level-exclusions","title":"Page-Level Exclusions","text":"<pre><code>header = page.find('text:contains(\"CONFIDENTIAL\")').above()\npage.add_exclusion(header)                      # Exclude from extraction\npage.clear_exclusions()                         # Remove exclusions\ntext = page.extract_text(use_exclusions=False)  # Ignore exclusions\n</code></pre>"},{"location":"quick-reference/#pdf-level-exclusions","title":"PDF-Level Exclusions","text":"<pre><code># Exclude headers from all pages\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.1),\n    label=\"Header\"\n)\n</code></pre>"},{"location":"quick-reference/#configuration-options","title":"Configuration Options","text":""},{"location":"quick-reference/#ocr-engines","title":"OCR Engines","text":"<pre><code>from natural_pdf.ocr import EasyOCROptions, PaddleOCROptions\n\neasy_opts = EasyOCROptions(gpu=True, paragraph=True)\npaddle_opts = PaddleOCROptions(lang='en')\n</code></pre>"},{"location":"quick-reference/#layout-analysis-options","title":"Layout Analysis Options","text":"<pre><code>from natural_pdf.analyzers.layout import YOLOOptions\n\nyolo_opts = YOLOOptions(confidence_threshold=0.5)\npage.analyze_layout(engine='yolo', options=yolo_opts)\n</code></pre>"},{"location":"quick-reference/#common-patterns","title":"Common Patterns","text":""},{"location":"quick-reference/#extract-inspection-report-data","title":"Extract Inspection Report Data","text":"<pre><code># Find violation count\nviolations = page.find('text:contains(\"Violation Count\"):right(width=100)')\n\n# Get inspection number from the header box (regex search)\ninspection_num = page.find('text:contains(\"INS-[A-Z0-9]+\")', regex=True)\n\n# Extract inspection date\ninspection_date = page.find('text:contains(\"Date:\"):right(width=150)')\n\n# Get site name (text to the right of \"Site:\")\nsite_name = page.find('text:contains(\"Site:\"):right(width=300)').extract_text()\n</code></pre>"},{"location":"quick-reference/#process-forms","title":"Process Forms","text":"<pre><code># Exclude header/footer\npage.add_exclusion(page.create_region(0, 0, page.width, 50))\npage.add_exclusion(page.create_region(0, page.height-50, page.width, page.height))\n\n# Extract form fields\nfields = page.find_all('text:bold')\nvalues = [field.right(width=300).extract_text() for field in fields]\n</code></pre>"},{"location":"quick-reference/#handle-scanned-documents","title":"Handle Scanned Documents","text":"<pre><code># Apply OCR with high accuracy\npage.apply_ocr(engine='surya', languages=['en'])\n\n# Extract with confidence filtering\ntext_elements = page.find_all('text[source=ocr][confidence&gt;=0.8]')\nclean_text = text_elements.extract_text()\n</code></pre>"},{"location":"quick-reference/#troubleshooting","title":"Troubleshooting","text":"Problem Solution No text found Try <code>page.apply_ocr()</code> first Wrong elements selected Use <code>elements.show()</code> to debug selectors Poor table extraction Try <code>page.analyze_layout(engine='tatr')</code> first Text extraction includes headers Use <code>page.add_exclusion()</code> Low OCR accuracy Try different engine or increase resolution Elements overlap multiple pages Use page-specific searches"},{"location":"quick-reference/#file-formats","title":"File Formats","text":""},{"location":"quick-reference/#saving-results","title":"Saving Results","text":"<pre><code># Save as image\npage.save_image(\"output.png\", width=700)\n\n# Save table as CSV\nimport pandas as pd\ndf = table_data.to_df(header=\"first\")\ndf.to_csv(\"table.csv\")\n\n# Export searchable PDF\nfrom natural_pdf.exporters import SearchablePDFExporter\nexporter = SearchablePDFExporter()\nexporter.export(pdf, \"searchable.pdf\")\n</code></pre>"},{"location":"quick-reference/#next-steps","title":"Next Steps","text":"<ul> <li>New to Natural PDF? \u2192 Start with Installation</li> <li>Learning the basics? \u2192 Follow the Tutorials</li> <li>Solving specific problems? \u2192 Check the how-to guides</li> <li>Need detailed info? \u2192 See the API Reference</li> </ul>"},{"location":"reflowing-pages/","title":"Restructuring page content","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\nfrom natural_pdf.flows import Flow\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/multicolumn.pdf\")\npage = pdf.pages[0]\npage.show(width=500)\n</pre> from natural_pdf import PDF from natural_pdf.flows import Flow  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/multicolumn.pdf\") page = pdf.pages[0] page.show(width=500) Out[1]: <p>We can grab individual columns from it.</p> In\u00a0[2]: Copied! <pre>left = page.region(right=page.width/3)\nmid = page.region(left=page.width/3, right=page.width/3*2)\nright = page.region(left=page.width/3*2)\n\nmid.show(width=500)\n</pre> left = page.region(right=page.width/3) mid = page.region(left=page.width/3, right=page.width/3*2) right = page.region(left=page.width/3*2)  mid.show(width=500) Out[2]: In\u00a0[3]: Copied! <pre>stacked = [left, mid, right]\nflow = Flow(segments=stacked, arrangement=\"vertical\")\n</pre> stacked = [left, mid, right] flow = Flow(segments=stacked, arrangement=\"vertical\") <p>As a result, I can find text in the first column and ask it to grab what's \"below\" until it hits content in the second column.</p> In\u00a0[4]: Copied! <pre>region = (\n    flow\n    .find('text:contains(\"Table one\")')\n    .below(\n        until='text:contains(\"Table two\")',\n        include_endpoint=False\n    )\n)\nregion.show()\n</pre> region = (     flow     .find('text:contains(\"Table one\")')     .below(         until='text:contains(\"Table two\")',         include_endpoint=False     ) ) region.show() Out[4]: <p>While you can't easily extract tables yet, you can at least extract text!</p> In\u00a0[5]: Copied! <pre>print(region.extract_text())\n</pre> print(region.extract_text()) <pre>index number\n1 123\n2 456\n3 789\n4 1122\n5 1455\n6 1788\n7 2121\n8 2454\n9 2787\n10 3120\n11 3453\n12 3786\n13 4119\n14 4452\n15 4785\n16 5118\n17 5451\n18 5784\n19 6117\n20 6450\n21 6783\n22 7116\n23 7449\n24 7782\n25 8115\n26 8448\n27 8781\n28 9114\n29 9447\n30 9780\n31 10113\n32 10446\n33 10779\n34 11112\n35 11445\n36 11778\n37 12111\n38 12444\n39 12777\n</pre> In\u00a0[6]: Copied! <pre>(\n    flow\n    .find_all('text[width&gt;10]:bold')\n    .show()\n)\n</pre> (     flow     .find_all('text[width&gt;10]:bold')     .show() ) Out[6]: <p>...it's easy to extract each table that's betwen them.</p> In\u00a0[7]: Copied! <pre>regions = (\n    flow\n    .find_all('text[width&gt;10]:bold')\n    .below(\n        until='text[width&gt;10]:bold|text:contains(\"Here is a bit\")',\n        include_endpoint=False\n    )\n)\nregions.show()\n</pre> regions = (     flow     .find_all('text[width&gt;10]:bold')     .below(         until='text[width&gt;10]:bold|text:contains(\"Here is a bit\")',         include_endpoint=False     ) ) regions.show() Out[7]:"},{"location":"reflowing-pages/#restructuring-page-content","title":"Restructuring page content\u00b6","text":"<p>Flows are a way to restructure pages that are not in normal one-page reading order. This might be columnal data, tables than span pages, etc.</p>"},{"location":"reflowing-pages/#a-multi-column-pdf","title":"A multi-column PDF\u00b6","text":"<p>Here is a multi column PDF.</p>"},{"location":"reflowing-pages/#restructuring","title":"Restructuring\u00b6","text":"<p>We can use Flows to stack the three columns on top of each other.</p>"},{"location":"reflowing-pages/#find_all-and-reflows","title":"find_all and reflows\u00b6","text":"<p>Let's say we have a few headers...</p>"},{"location":"reflowing-pages/#merging-tables-that-span-pages","title":"Merging tables that span pages\u00b6","text":"<p>TK</p>"},{"location":"regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page page = pdf.pages[0]  # Display the page page.show(width=700) Out[1]: In\u00a0[2]: Copied! <pre># Create a region by specifying (x0, top, x1, bottom) coordinates\n# Let's create a region in the middle of the page\nmid_region = page.create_region(\n    x0=100,         # Left edge\n    top=200,        # Top edge\n    x1=500,         # Right edge\n    bottom=400      # Bottom edge\n)\n\n# Highlight the region to see it\nmid_region.show(color=\"blue\")\n</pre> # Create a region by specifying (x0, top, x1, bottom) coordinates # Let's create a region in the middle of the page mid_region = page.create_region(     x0=100,         # Left edge     top=200,        # Top edge     x1=500,         # Right edge     bottom=400      # Bottom edge )  # Highlight the region to see it mid_region.show(color=\"blue\") Out[2]: In\u00a0[3]: Copied! <pre># Find a heading-like element\nheading = page.find('text[size&gt;=12]:bold')\n\n# Create a region below this heading element\nif heading:\n    region_below = heading.below()\n\n    # Highlight the heading and the region below it\n    with page.highlights() as h:\n        h.add(heading, color=\"red\")\n        h.add(region_below, color=\"blue\")\n        h.show()\n</pre> # Find a heading-like element heading = page.find('text[size&gt;=12]:bold')  # Create a region below this heading element if heading:     region_below = heading.below()      # Highlight the heading and the region below it     with page.highlights() as h:         h.add(heading, color=\"red\")         h.add(region_below, color=\"blue\")         h.show() In\u00a0[4]: Copied! <pre># Create a region with height limit\nif heading:\n    # Only include 100px below the heading\n    small_region_below = heading.below(height=100)\n\n    with page.highlights() as h:\n        h.add(heading, color=\"red\")\n        h.add(small_region_below, color=\"green\")\n        h.show()\n</pre> # Create a region with height limit if heading:     # Only include 100px below the heading     small_region_below = heading.below(height=100)      with page.highlights() as h:         h.add(heading, color=\"red\")         h.add(small_region_below, color=\"green\")         h.show() In\u00a0[5]: Copied! <pre># Find a line or other element to create a region above\nline = page.find('line')\nif line:\n    # Create a region above the line\n    region_above = line.above()\n\n    with page.highlights() as h:\n        h.add(line, color=\"black\")\n        h.add(region_above, color=\"purple\")\n        h.show()\n</pre> # Find a line or other element to create a region above line = page.find('line') if line:     # Create a region above the line     region_above = line.above()      with page.highlights() as h:         h.add(line, color=\"black\")         h.add(region_above, color=\"purple\")         h.show() In\u00a0[6]: Copied! <pre># Find two elements to use as boundaries\nfirst_heading = page.find('text[size&gt;=11]:bold')\nnext_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None\n\nif first_heading and next_heading:\n    # Create a region from the first heading until the next heading\n    section = first_heading.below(until=next_heading, include_endpoint=False)\n\n    # Highlight both elements and the region between them\n    with page.highlights() as h:\n        h.add(first_heading, color=\"red\")\n        h.add(next_heading, color=\"red\")\n        h.add(section, color=\"yellow\")\n        h.show()\n</pre> # Find two elements to use as boundaries first_heading = page.find('text[size&gt;=11]:bold') next_heading = first_heading.next('text[size&gt;=11]:bold') if first_heading else None  if first_heading and next_heading:     # Create a region from the first heading until the next heading     section = first_heading.below(until=next_heading, include_endpoint=False)      # Highlight both elements and the region between them     with page.highlights() as h:         h.add(first_heading, color=\"red\")         h.add(next_heading, color=\"red\")         h.add(section, color=\"yellow\")         h.show() In\u00a0[7]: Copied! <pre># After running a layout model\npage.analyze_layout('tatr')\n\ntext  = page.find(text=\"Hazardous Materials\")\nrow       = text.parent('region[type=table-row]')\nrow.show(crop=True)\n</pre> # After running a layout model page.analyze_layout('tatr')  text  = page.find(text=\"Hazardous Materials\") row       = text.parent('region[type=table-row]') row.show(crop=True) Out[7]: <p>Parameters</p> <pre><code>parent(selector=None, *, mode=\"contains\")\n\nmode = \"contains\"  # candidate fully covers the element (default)\n     | \"center\"    # candidate contains element's centroid\n     | \"overlap\"   # any intersection &gt; 0pt\u00b2\n</code></pre> <p>If no enclosing object matches the selector (or exists at all) it returns <code>None</code>, so chaining is safe.</p> In\u00a0[8]: Copied! <pre># Find a region to work with (e.g., from a title to the next bold text)\ntitle = page.find('text:contains(\"Site\")')  # Adjust if needed\nif title:\n    # Create a region from title down to the next bold text\n    content_region = title.below(until='line:horizontal', include_endpoint=False)\n\n    # Extract text from just this region\n    region_text = content_region.extract_text()\n\n    # Show the region and the extracted text\n    content_region.show(color=\"green\")\n\n    # Displaying the text (first 300 chars if long)\n    print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text)\n</pre> # Find a region to work with (e.g., from a title to the next bold text) title = page.find('text:contains(\"Site\")')  # Adjust if needed if title:     # Create a region from title down to the next bold text     content_region = title.below(until='line:horizontal', include_endpoint=False)      # Extract text from just this region     region_text = content_region.extract_text()      # Show the region and the extracted text     content_region.show(color=\"green\")      # Displaying the text (first 300 chars if long)     print(region_text[:300] + \"...\" if len(region_text) &gt; 300 else region_text) <pre>Date: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other me...\n</pre> In\u00a0[9]: Copied! <pre># Create a region in an interesting part of the page\ntest_region = page.create_region(\n    x0=page.width * 0.1,\n    top=page.height * 0.25,\n    x1=page.width * 0.9,\n    bottom=page.height * 0.75\n)\n\n# Find all text elements ONLY within this region\ntext_in_region = test_region.find_all('text')\n\n# Display result\nwith page.highlights() as h:\n    h.add(test_region, color=\"blue\")\n    h.add(text_in_region, color=\"red\")\n    h.show()\n\nlen(text_in_region)  # Number of text elements found in region\n</pre> # Create a region in an interesting part of the page test_region = page.create_region(     x0=page.width * 0.1,     top=page.height * 0.25,     x1=page.width * 0.9,     bottom=page.height * 0.75 )  # Find all text elements ONLY within this region text_in_region = test_region.find_all('text')  # Display result with page.highlights() as h:     h.add(test_region, color=\"blue\")     h.add(text_in_region, color=\"red\")     h.show()  len(text_in_region)  # Number of text elements found in region Out[9]: <pre>17</pre> In\u00a0[10]: Copied! <pre># Find a specific region to capture\n# (Could be a table, figure, or any significant area)\nregion_for_image = page.create_region(\n    x0=100,\n    top=150,\n    x1=page.width - 100,\n    bottom=300\n)\n\n# Generate an image of just this region\nregion_for_image.show(crop=True)  # Shows just the region\n</pre> # Find a specific region to capture # (Could be a table, figure, or any significant area) region_for_image = page.create_region(     x0=100,     top=150,     x1=page.width - 100,     bottom=300 )  # Generate an image of just this region region_for_image.show(crop=True)  # Shows just the region Out[10]: In\u00a0[11]: Copied! <pre># Take an existing region and expand it\nregion_a = page.create_region(200, 200, 400, 400)\n\n# Expand by a certain number of points in each direction\nexpanded = region_a.expand(left=20, right=20, top=20, bottom=20)\n\n# Visualize original and expanded regions\nwith page.highlights() as h:\n    h.add(region_a, color=\"blue\", label=\"Original\")\n    h.add(expanded, color=\"red\", label=\"Expanded\")\n    h.show()\n</pre> # Take an existing region and expand it region_a = page.create_region(200, 200, 400, 400)  # Expand by a certain number of points in each direction expanded = region_a.expand(left=20, right=20, top=20, bottom=20)  # Visualize original and expanded regions with page.highlights() as h:     h.add(region_a, color=\"blue\", label=\"Original\")     h.add(expanded, color=\"red\", label=\"Expanded\")     h.show() In\u00a0[12]: Copied! <pre># Create a region for the whole page\nfull_page_region = page.create_region(0, 0, page.width, page.height)\n\n# Extract text without exclusions as baseline\nfull_text = full_page_region.extract_text()\nprint(f\"Full page text length: {len(full_text)} characters\")\n</pre> # Create a region for the whole page full_page_region = page.create_region(0, 0, page.width, page.height)  # Extract text without exclusions as baseline full_text = full_page_region.extract_text() print(f\"Full page text length: {len(full_text)} characters\") <pre>Full page text length: 1255 characters\n</pre> In\u00a0[13]: Copied! <pre># Define an area we want to exclude (like a header)\n# Let's exclude the top 10% of the page\nheader_zone = page.create_region(0, 0, page.width, page.height * 0.1)\n\n# Add this as an exclusion for the page\npage.add_exclusion(header_zone)\n\n# Visualize the exclusion\nheader_zone.show(color=\"red\", label=\"Excluded\")\n</pre> # Define an area we want to exclude (like a header) # Let's exclude the top 10% of the page header_zone = page.create_region(0, 0, page.width, page.height * 0.1)  # Add this as an exclusion for the page page.add_exclusion(header_zone)  # Visualize the exclusion header_zone.show(color=\"red\", label=\"Excluded\") Out[13]: In\u00a0[14]: Copied! <pre># Now extract text again - the header should be excluded\ntext_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default\n\n# Compare text lengths\nprint(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\")\nprint(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\")\n</pre> # Now extract text again - the header should be excluded text_with_exclusion = full_page_region.extract_text() # Uses apply_exclusions=True by default  # Compare text lengths print(f\"Original text: {len(full_text)} chars\\nText with exclusion: {len(text_with_exclusion)} chars\") print(f\"Difference: {len(full_text) - len(text_with_exclusion)} chars excluded\") <pre>Original text: 1255 chars\nText with exclusion: 1193 chars\nDifference: 62 chars excluded\n</pre> In\u00a0[15]: Copied! <pre># When done with this page, clear exclusions\npage.clear_exclusions()\n</pre> # When done with this page, clear exclusions page.clear_exclusions() Out[15]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[16]: Copied! <pre># Define a PDF-level exclusion for headers\n# This will exclude the top 30% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, 0, p.width, p.height * 0.3),\n    label=\"Header zone\"\n)\n\n# Define a PDF-level exclusion for footers\n# This will exclude the bottom 20% of every page\npdf.add_exclusion(\n    lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),\n    label=\"Footer zone\"\n)\n\n# PDF-level exclusions are used whenever you extract text\n# Let's try on the first three pages\nfor page in pdf.pages[:3]:\n    text = page.extract_text()\n    text_original = page.extract_text(use_exclusions=False)\n    print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\")\n</pre> # Define a PDF-level exclusion for headers # This will exclude the top 30% of every page pdf.add_exclusion(     lambda p: p.create_region(0, 0, p.width, p.height * 0.3),     label=\"Header zone\" )  # Define a PDF-level exclusion for footers # This will exclude the bottom 20% of every page pdf.add_exclusion(     lambda p: p.create_region(0, p.height * 0.8, p.width, p.height),     label=\"Footer zone\" )  # PDF-level exclusions are used whenever you extract text # Let's try on the first three pages for page in pdf.pages[:3]:     text = page.extract_text()     text_original = page.extract_text(use_exclusions=False)     print(f\"Page {page.number} \u2013 Before: {len(text_original)} After: {len(text)}\") <pre>Page 1 \u2013 Before: 1255 After: 456\n</pre> In\u00a0[17]: Copied! <pre># Clear PDF-level exclusions when done\npdf.clear_exclusions()\nprint(\"Cleared all PDF-level exclusions\")\n</pre> # Clear PDF-level exclusions when done pdf.clear_exclusions() print(\"Cleared all PDF-level exclusions\") <pre>Cleared all PDF-level exclusions\n</pre> In\u00a0[18]: Copied! <pre># First, run layout analysis to detect regions\npage.analyze_layout()  # Uses 'yolo' engine by default\n\n# Find all detected regions\ndetected_regions = page.find_all('region')\nprint(f\"Found {len(detected_regions)} layout regions\")\n</pre> # First, run layout analysis to detect regions page.analyze_layout()  # Uses 'yolo' engine by default  # Find all detected regions detected_regions = page.find_all('region') print(f\"Found {len(detected_regions)} layout regions\") <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpkzml2cdb/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 2833.0ms\n</pre> <pre>Speed: 7.2ms preprocess, 2833.0ms inference, 14.8ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> <pre>Found 1 layout regions\n</pre> In\u00a0[19]: Copied! <pre># Highlight all detected regions by type\ndetected_regions.show(group_by='region_type')\n</pre> # Highlight all detected regions by type detected_regions.show(group_by='region_type') Out[19]: In\u00a0[20]: Copied! <pre># Extract text from a specific region type (e.g., title)\ntitle_regions = page.find_all('region[type=title]')\nif title_regions:\n    titles_text = title_regions.extract_text()\n    print(f\"Title text: {titles_text}\")\n</pre> # Extract text from a specific region type (e.g., title) title_regions = page.find_all('region[type=title]') if title_regions:     titles_text = title_regions.extract_text()     print(f\"Title text: {titles_text}\")"},{"location":"regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that define boundaries for operations like text extraction, element finding, or visualization. They're one of Natural PDF's most powerful features for working with specific parts of a document.</p>"},{"location":"regions/#setup","title":"Setup\u00b6","text":"<p>Let's set up a PDF to experiment with regions.</p>"},{"location":"regions/#creating-regions","title":"Creating Regions\u00b6","text":"<p>There are several ways to create regions in Natural PDF.</p>"},{"location":"regions/#using-create_region-with-coordinates","title":"Using <code>create_region()</code> with Coordinates\u00b6","text":"<p>This is the most direct method - provide the coordinates directly.</p>"},{"location":"regions/#using-element-methods-above-below-left-right","title":"Using Element Methods: <code>above()</code>, <code>below()</code>, <code>left()</code>, <code>right()</code>\u00b6","text":"<p>You can create regions relative to existing elements.</p>"},{"location":"regions/#creating-a-region-between-elements-with-until","title":"Creating a Region Between Elements with <code>until()</code>\u00b6","text":""},{"location":"regions/#jump-to-the-enclosing-area-with-parent","title":"Jump to the enclosing area with <code>parent()</code>\u00b6","text":"<p>Need to know which table, figure or coloured panel an element sits inside?</p> <p>The new <code>parent()</code> helper finds the smallest element or detected region that spatially encloses the current one.</p>"},{"location":"regions/#using-regions","title":"Using Regions\u00b6","text":"<p>Once you have a region, here's what you can do with it.</p>"},{"location":"regions/#extract-text-from-a-region","title":"Extract Text from a Region\u00b6","text":""},{"location":"regions/#find-elements-within-a-region","title":"Find Elements Within a Region\u00b6","text":"<p>You can use a region as a \"filter\" to only find elements within its boundaries.</p>"},{"location":"regions/#generate-an-image-of-a-region","title":"Generate an Image of a Region\u00b6","text":""},{"location":"regions/#adjust-and-expand-regions","title":"Adjust and Expand Regions\u00b6","text":""},{"location":"regions/#using-exclusion-zones-with-regions","title":"Using Exclusion Zones with Regions\u00b6","text":"<p>Exclusion zones are regions that you want to ignore during operations like text extraction.</p>"},{"location":"regions/#document-level-exclusions","title":"Document-Level Exclusions\u00b6","text":"<p>PDF-level exclusions apply to all pages and use functions to adapt to each page.</p>"},{"location":"regions/#working-with-layout-analysis-regions","title":"Working with Layout Analysis Regions\u00b6","text":"<p>When you run layout analysis, the detected regions (tables, titles, etc.) are also Region objects.</p>"},{"location":"regions/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you understand regions, you can:</p> <ul> <li>Extract tables from table regions</li> <li>Ask questions about specific regions</li> <li>Exclude content from extraction</li> </ul>"},{"location":"tables/","title":"Getting Tables Out of PDFs","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\n# Display the page\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  # Display the page page.show() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Try to extract the first table found on the page\n# This uses pdfplumber behind the scenes\ntable_data = page.extract_table() # Returns a list of lists\ntable_data\n</pre> # Try to extract the first table found on the page # This uses pdfplumber behind the scenes table_data = page.extract_table() # Returns a list of lists table_data Out[2]: <pre>TableResult(rows=8\u2026)</pre> <p>This might work great, or it might give you garbage. Tables are tricky.</p> In\u00a0[3]: Copied! <pre># Use YOLO to find table regions\npage.analyze_layout(engine='yolo')\n\n# Find what it thinks are tables\ntable_regions_yolo = page.find_all('region[type=table][model=yolo]')\ntable_regions_yolo.show()\n</pre> # Use YOLO to find table regions page.analyze_layout(engine='yolo')  # Find what it thinks are tables table_regions_yolo = page.find_all('region[type=table][model=yolo]') table_regions_yolo.show() <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmps28iifoq/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 695.2ms\n</pre> <pre>Speed: 5.1ms preprocess, 695.2ms inference, 1.5ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[3]: In\u00a0[4]: Copied! <pre># Extract data from the detected table\ntable_regions_yolo[0].extract_table()\n</pre> # Extract data from the detected table table_regions_yolo[0].extract_table() Out[4]: <pre>TableResult(rows=8\u2026)</pre> In\u00a0[5]: Copied! <pre># Clear previous results and try TATR\npage.clear_detected_layout_regions() \npage.analyze_layout(engine='tatr')\n</pre> # Clear previous results and try TATR page.clear_detected_layout_regions()  page.analyze_layout(engine='tatr') Out[5]: <pre>&lt;ElementCollection[Region](count=15)&gt;</pre> In\u00a0[6]: Copied! <pre># Find the table that TATR detected\ntatr_table = page.find('region[type=table][model=tatr]')\ntatr_table.show()\n</pre> # Find the table that TATR detected tatr_table = page.find('region[type=table][model=tatr]') tatr_table.show() Out[6]: In\u00a0[7]: Copied! <pre># TATR finds the internal structure too\nrows = page.find_all('region[type=table-row][model=tatr]')\ncols = page.find_all('region[type=table-column][model=tatr]')\nhdrs = page.find_all('region[type=table-column-header][model=tatr]')\nf\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\"\n</pre> # TATR finds the internal structure too rows = page.find_all('region[type=table-row][model=tatr]') cols = page.find_all('region[type=table-column][model=tatr]') hdrs = page.find_all('region[type=table-column-header][model=tatr]') f\"TATR found: {len(rows)} rows, {len(cols)} columns, {len(hdrs)} headers\" Out[7]: <pre>'TATR found: 8 rows, 4 columns, 1 headers'</pre> In\u00a0[8]: Copied! <pre>tatr_table = page.find('region[type=table][model=tatr]')\n# Use TATR's smart extraction\ntatr_table.extract_table(method='tatr')\n</pre> tatr_table = page.find('region[type=table][model=tatr]') # Use TATR's smart extraction tatr_table.extract_table(method='tatr') Out[8]: <pre>TableResult(rows=9\u2026)</pre> In\u00a0[9]: Copied! <pre># Or force it to use pdfplumber instead (maybe for comparison)\ntatr_table.extract_table(method='pdfplumber')\n</pre> # Or force it to use pdfplumber instead (maybe for comparison) tatr_table.extract_table(method='pdfplumber') Out[9]: <pre>TableResult(rows=6\u2026)</pre> In\u00a0[10]: Copied! <pre># Custom settings for tricky tables\ntable_settings = {\n    \"vertical_strategy\": \"text\",      # Use text alignment instead of lines\n    \"horizontal_strategy\": \"lines\",   # Still use lines for rows\n    \"intersection_x_tolerance\": 5,    # Be more forgiving about line intersections\n}\n\nresults = page.extract_table(table_settings=table_settings)\n</pre> # Custom settings for tricky tables table_settings = {     \"vertical_strategy\": \"text\",      # Use text alignment instead of lines     \"horizontal_strategy\": \"lines\",   # Still use lines for rows     \"intersection_x_tolerance\": 5,    # Be more forgiving about line intersections }  results = page.extract_table(table_settings=table_settings) In\u00a0[11]: Copied! <pre>from natural_pdf import PDF\nfrom natural_pdf.analyzers import Guides\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n\nregion = (\n    page\n    .find('text:contains(Violations)')\n    .below(\n        until='text[size&lt;10]',\n        include_endpoint=False\n    )\n)\n\n# Create guides for a page\nguides = Guides(region)\n\n# Create horizontal guides between rows\nguides.vertical.from_content(\n    markers=region.find_all(\"text[size=10]:bold\"),  # Use CSS selectors to find row content\n    align='between'\n)\n\n# Create horizontal guides by finding visual lines\nguides.horizontal.from_lines()\n\nguides.show()\n</pre> from natural_pdf import PDF from natural_pdf.analyzers import Guides  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0]  region = (     page     .find('text:contains(Violations)')     .below(         until='text[size&lt;10]',         include_endpoint=False     ) )  # Create guides for a page guides = Guides(region)  # Create horizontal guides between rows guides.vertical.from_content(     markers=region.find_all(\"text[size=10]:bold\"),  # Use CSS selectors to find row content     align='between' )  # Create horizontal guides by finding visual lines guides.horizontal.from_lines()  guides.show() <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[11]: <p>But it doesn't line up well!! So we can adjust it to snap to the whitespace between the text.</p> In\u00a0[12]: Copied! <pre>guides.vertical.snap_to_whitespace(detection_method='text')\n</pre> guides.vertical.snap_to_whitespace(detection_method='text') Out[12]: <pre>Guides(verticals=5, horizontals=9, cells=32)</pre> In\u00a0[13]: Copied! <pre># Create vertical guides between columns by finding content gaps\nguides.vertical.from_content(\n    markers=['Statute', 'Description', 'Level' ,'Repeat'],  # Look for column headers\n    align='between'  # Place guides between columns, not on them\n)\n\n# You can also use a single selector or ElementCollection\nguides.vertical.from_content(\n    markers=\"text[size=10]:bold\",  # Single selector\n    align='after'\n)\n\n# Or pass an ElementCollection directly\nheaders = page.find_all('text:bold')\nguides.horizontal.from_content(\n    markers=headers,  # ElementCollection of elements\n    align='center'\n)\n</pre> # Create vertical guides between columns by finding content gaps guides.vertical.from_content(     markers=['Statute', 'Description', 'Level' ,'Repeat'],  # Look for column headers     align='between'  # Place guides between columns, not on them )  # You can also use a single selector or ElementCollection guides.vertical.from_content(     markers=\"text[size=10]:bold\",  # Single selector     align='after' )  # Or pass an ElementCollection directly headers = page.find_all('text:bold') guides.horizontal.from_content(     markers=headers,  # ElementCollection of elements     align='center' ) Out[13]: <pre>Guides(verticals=2, horizontals=3, cells=2)</pre> In\u00a0[14]: Copied! <pre># Place guides at specific coordinates\nguides.vertical.add([150, 300, 450])  # x-coordinates (list)\nguides.horizontal.add([200, 250, 300, 350])  # y-coordinates (list)\n\n# Or add them one at a time\nguides.vertical.add(500)  # single x-coordinate\nguides.horizontal.add(400)  # single y-coordinate\n\nguides.show()\n</pre> # Place guides at specific coordinates guides.vertical.add([150, 300, 450])  # x-coordinates (list) guides.horizontal.add([200, 250, 300, 350])  # y-coordinates (list)  # Or add them one at a time guides.vertical.add(500)  # single x-coordinate guides.horizontal.add(400)  # single y-coordinate  guides.show() Out[14]: In\u00a0[15]: Copied! <pre># Snap guides to whitespace gaps for cleaner alignment\nguides.vertical.snap_to_whitespace()\nguides.horizontal.snap_to_whitespace()\n\n# Snap guides to content boundaries\nguides.vertical.snap_to_content()\n\n# Manually adjust specific guides\nguides.vertical.shift(index=1, offset=10)  # Move second guide 10 points right\n\nguides.show()\n</pre> # Snap guides to whitespace gaps for cleaner alignment guides.vertical.snap_to_whitespace() guides.horizontal.snap_to_whitespace()  # Snap guides to content boundaries guides.vertical.snap_to_content()  # Manually adjust specific guides guides.vertical.shift(index=1, offset=10)  # Move second guide 10 points right  guides.show() Out[15]: In\u00a0[16]: Copied! <pre># Create guides for a page\nguides = Guides(region)\n\n# Create horizontal guides between rows\nguides.vertical.from_content(\n    markers=region.find_all(\"text[size=10]:bold\"),  # Use CSS selectors to find row content\n    align='between'\n)\n\n# Create horizontal guides by finding visual lines\nguides.horizontal.from_lines()\n\n# Create table, row, column, and cell regions\ngrid_info = guides.build_grid(source='manual')\n\n# See what was created\nprint(f\"Created: {grid_info}\")\n# Output: {'table': 1, 'rows': 3, 'columns': 4, 'cells': 12}\n</pre> # Create guides for a page guides = Guides(region)  # Create horizontal guides between rows guides.vertical.from_content(     markers=region.find_all(\"text[size=10]:bold\"),  # Use CSS selectors to find row content     align='between' )  # Create horizontal guides by finding visual lines guides.horizontal.from_lines()  # Create table, row, column, and cell regions grid_info = guides.build_grid(source='manual')  # See what was created print(f\"Created: {grid_info}\") # Output: {'table': 1, 'rows': 3, 'columns': 4, 'cells': 12} <pre>Created: {'counts': {'table': 1, 'rows': 8, 'columns': 4, 'cells': 32}, 'regions': {'table': &lt;Region type='table' source='manual' bbox=(0.0, 392.0, 612.0, 552.0)&gt;, 'rows': [&lt;Region type='table_row' source='manual' bbox=(0.0, 392.0, 612.0, 412.0)&gt;, &lt;Region type='table_row' source='manual' bbox=(0.0, 412.0, 612.0, 432.0)&gt;, &lt;Region type='table_row' source='manual' bbox=(0.0, 432.0, 612.0, 452.0)&gt;, &lt;Region type='table_row' source='manual' bbox=(0.0, 452.0, 612.0, 472.0)&gt;, &lt;Region type='table_row' source='manual' bbox=(0.0, 472.0, 612.0, 492.0)&gt;, &lt;Region type='table_row' source='manual' bbox=(0.0, 492.0, 612.0, 512.0)&gt;, &lt;Region type='table_row' source='manual' bbox=(0.0, 512.0, 612.0, 532.0)&gt;, &lt;Region type='table_row' source='manual' bbox=(0.0, 532.0, 612.0, 552.0)&gt;], 'columns': [&lt;Region type='table_column' source='manual' bbox=(0.0, 392.0, 96.945, 552.0)&gt;, &lt;Region type='table_column' source='manual' bbox=(96.945, 392.0, 307.505, 552.0)&gt;, &lt;Region type='table_column' source='manual' bbox=(307.505, 392.0, 492.78499999999997, 552.0)&gt;, &lt;Region type='table_column' source='manual' bbox=(492.78499999999997, 392.0, 612.0, 552.0)&gt;], 'cells': [&lt;Region type='table_cell' source='manual' bbox=(0.5, 392.5, 96.445, 411.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 392.5, 307.005, 411.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 392.5, 492.28499999999997, 411.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 392.5, 611.5, 411.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(0.5, 412.5, 96.445, 431.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 412.5, 307.005, 431.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 412.5, 492.28499999999997, 431.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 412.5, 611.5, 431.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(0.5, 432.5, 96.445, 451.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 432.5, 307.005, 451.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 432.5, 492.28499999999997, 451.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 432.5, 611.5, 451.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(0.5, 452.5, 96.445, 471.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 452.5, 307.005, 471.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 452.5, 492.28499999999997, 471.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 452.5, 611.5, 471.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(0.5, 472.5, 96.445, 491.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 472.5, 307.005, 491.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 472.5, 492.28499999999997, 491.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 472.5, 611.5, 491.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(0.5, 492.5, 96.445, 511.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 492.5, 307.005, 511.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 492.5, 492.28499999999997, 511.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 492.5, 611.5, 511.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(0.5, 512.5, 96.445, 531.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 512.5, 307.005, 531.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 512.5, 492.28499999999997, 531.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 512.5, 611.5, 531.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(0.5, 532.5, 96.445, 551.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(97.445, 532.5, 307.005, 551.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(308.005, 532.5, 492.28499999999997, 551.5)&gt;, &lt;Region type='table_cell' source='manual' bbox=(493.28499999999997, 532.5, 611.5, 551.5)&gt;]}}\n</pre> In\u00a0[17]: Copied! <pre># Find the table you just created\ntable = page.find('table[source=manual]')\n\n# Extract data - this uses the cell regions you built!\ntable_data = table.extract_table()\ntable_data.to_df()\n</pre> # Find the table you just created table = page.find('table[source=manual]')  # Extract data - this uses the cell regions you built! table_data = table.extract_table() table_data.to_df() Out[17]: Statute Description Level Repeat? 0 4.12.7 Unsanitary Working Conditions. Critical None 1 5.8.3 Inadequate Protective Equipment. Serious None 2 6.3.9 Ineffective Injury Prevention. Serious None 3 7.1.5 Failure to Properly Store Hazardous Materials . Critical None 4 8.9.2 Lack of Adequate Fire Safety Measures. Serious None 5 9.6.4 Inadequate Ventilation Systems. Serious None 6 10.2.7 Insufficient Employee Training for Safe Work P Practices. Serious None In\u00a0[18]: Copied! <pre># Method 1: Use existing vector lines in the PDF\npage.detect_lines(source_label='my_lines')  # First detect lines if not already done\nguides.vertical.from_lines(source='my_lines')\nguides.horizontal.from_lines(source='my_lines')\n\n# Method 2: Detect lines directly from pixels (no pre-detection needed!)\nguides.vertical.from_lines(detection_method='pixels', max_lines=5)\nguides.horizontal.from_lines(detection_method='pixels', threshold=0.3)\n\n# Method 3: Create all guides at once with pixel detection\nguides = Guides.from_lines(\n    page,\n    detection_method='pixels',\n    threshold='auto',  # Automatically find threshold\n    max_lines_h=4,     # Find up to 4 horizontal lines\n    max_lines_v=3      # Find up to 3 vertical lines\n)\n\n# Additional parameters for pixel detection\nguides = Guides.from_lines(\n    page,\n    detection_method='pixels',\n    resolution=192,              # DPI for rasterization\n    min_gap_h=10,               # Minimum gap between horizontal lines\n    min_gap_v=15,               # Minimum gap between vertical lines  \n    binarization_method='adaptive',  # or 'otsu'\n    method='projection'          # or 'lsd' (requires opencv)\n)\n</pre> # Method 1: Use existing vector lines in the PDF page.detect_lines(source_label='my_lines')  # First detect lines if not already done guides.vertical.from_lines(source='my_lines') guides.horizontal.from_lines(source='my_lines')  # Method 2: Detect lines directly from pixels (no pre-detection needed!) guides.vertical.from_lines(detection_method='pixels', max_lines=5) guides.horizontal.from_lines(detection_method='pixels', threshold=0.3)  # Method 3: Create all guides at once with pixel detection guides = Guides.from_lines(     page,     detection_method='pixels',     threshold='auto',  # Automatically find threshold     max_lines_h=4,     # Find up to 4 horizontal lines     max_lines_v=3      # Find up to 3 vertical lines )  # Additional parameters for pixel detection guides = Guides.from_lines(     page,     detection_method='pixels',     resolution=192,              # DPI for rasterization     min_gap_h=10,               # Minimum gap between horizontal lines     min_gap_v=15,               # Minimum gap between vertical lines       binarization_method='adaptive',  # or 'otsu'     method='projection'          # or 'lsd' (requires opencv) ) In\u00a0[19]: Copied! <pre>tbl = page.extract_table()\n\n# Grab it as a dataframe\n# Shortcut for .to_df() because it's so many letters\ntbl.df\n</pre> tbl = page.extract_table()  # Grab it as a dataframe # Shortcut for .to_df() because it's so many letters tbl.df Out[19]: Statute Description Level Repeat? 0 4.12.7 Unsanitary Working Conditions. Critical None 1 5.8.3 Inadequate Protective Equipment. Serious None 2 6.3.9 Ineffective Injury Prevention. Serious None 3 7.1.5 Failure to Properly Store Hazardous Materials . Critical None 4 8.9.2 Lack of Adequate Fire Safety Measures. Serious None 5 9.6.4 Inadequate Ventilation Systems. Serious None 6 10.2.7 Insufficient Employee Training for Safe Work P Practices. Serious None In\u00a0[20]: Copied! <pre># First row is NOT headers\ntbl.to_df(header=None)\n</pre> # First row is NOT headers tbl.to_df(header=None) Out[20]: 0 1 2 3 0 Statute Description Level Repeat? 1 4.12.7 Unsanitary Working Conditions. Critical None 2 5.8.3 Inadequate Protective Equipment. Serious None 3 6.3.9 Ineffective Injury Prevention. Serious None 4 7.1.5 Failure to Properly Store Hazardous Materials . Critical None 5 8.9.2 Lack of Adequate Fire Safety Measures. Serious None 6 9.6.4 Inadequate Ventilation Systems. Serious None 7 10.2.7 Insufficient Employee Training for Safe Work P Practices. Serious None"},{"location":"tables/#getting-tables-out-of-pdfs","title":"Getting Tables Out of PDFs\u00b6","text":"<p>Tables in PDFs can be a real pain. Sometimes they're perfectly formatted with nice lines, other times they're just text floating around that vaguely looks like a table. Natural PDF gives you several different approaches to tackle whatever table nightmare you're dealing with.</p>"},{"location":"tables/#setup","title":"Setup\u00b6","text":"<p>Let's start with a PDF that has some tables to work with.</p>"},{"location":"tables/#the-quick-and-dirty-approach","title":"The Quick and Dirty Approach\u00b6","text":"<p>If you know there's a table somewhere and just want to try extracting it, start simple:</p>"},{"location":"tables/#the-smart-way-detect-first-then-extract","title":"The Smart Way: Detect First, Then Extract\u00b6","text":"<p>A better approach is to first find where the tables actually are, then extract them properly.</p>"},{"location":"tables/#finding-tables-with-yolo-fast-and-pretty-good","title":"Finding Tables with YOLO (Fast and Pretty Good)\u00b6","text":"<p>The YOLO model is good at spotting table-shaped areas on a page.</p>"},{"location":"tables/#finding-tables-with-tatr-slow-but-very-smart","title":"Finding Tables with TATR (Slow but Very Smart)\u00b6","text":"<p>The TATR model actually understands table structure - it can tell you where rows, columns, and headers are.</p>"},{"location":"tables/#choosing-your-extraction-method","title":"Choosing Your Extraction Method\u00b6","text":"<p>When you call <code>extract_table()</code> on a detected region, Natural PDF picks the extraction method automatically:</p> <ul> <li>YOLO-detected regions \u2192 uses <code>pdfplumber</code> (looks for lines and text alignment)</li> <li>TATR-detected regions \u2192 uses the smart <code>tatr</code> method (uses the detected structure)</li> </ul> <p>You can override this if needed:</p>"},{"location":"tables/#when-to-use-which","title":"When to Use Which?\u00b6","text":"<ul> <li><code>pdfplumber</code>: Great for clean tables with visible grid lines. Fast and reliable.</li> <li><code>tatr</code>: Better for messy tables, tables without lines, or tables with merged cells. Slower but smarter.</li> </ul>"},{"location":"tables/#when-tables-dont-cooperate","title":"When Tables Don't Cooperate\u00b6","text":"<p>Sometimes the automatic detection doesn't work well. You can tweak pdfplumber's settings:</p>"},{"location":"tables/#manual-table-structure-with-guides","title":"Manual Table Structure with Guides\u00b6","text":"<p>When automatic detection fails completely, you can manually define table structure using the Guides API. This is perfect for scanned documents, forms, or when you need precise control over table boundaries.</p>"},{"location":"tables/#what-are-guides","title":"What are Guides?\u00b6","text":"<p>Guides are visual alignment lines - think of them like the rulers in a design program. You can place them manually or detect them automatically, then use them to build perfect table grids.</p>"},{"location":"tables/#creating-guides-from-content","title":"Creating Guides from Content\u00b6","text":"<p>The smartest way is to create guides based on the actual content on the page:</p>"},{"location":"tables/#manual-guide-placement","title":"Manual Guide Placement\u00b6","text":"<p>For maximum control, place guides manually:</p>"},{"location":"tables/#fine-tuning-guide-positions","title":"Fine-tuning Guide Positions\u00b6","text":"<p>Guides have smart methods for adjusting positions:</p>"},{"location":"tables/#building-tables-from-guides","title":"Building Tables from Guides\u00b6","text":"<p>Once your guides are positioned correctly, build the table structure:</p>"},{"location":"tables/#extracting-data-from-guide-based-tables","title":"Extracting Data from Guide-Based Tables\u00b6","text":"<p>The magic happens here - <code>extract_table()</code> automatically detects your guide-based table structure:</p>"},{"location":"tables/#pro-tips-for-guides","title":"Pro Tips for Guides\u00b6","text":"<ul> <li>Preview everything: Use <code>guides.show()</code> liberally to check your work</li> <li>Use content-based creation: <code>from_content()</code> is usually smarter than manual placement</li> <li>Snap to clean up: The <code>snap_to_*</code> methods fix small alignment issues</li> <li>Label your work: Use descriptive <code>source</code> labels to keep track of different table attempts</li> </ul>"},{"location":"tables/#creating-guides-from-lines","title":"Creating Guides from Lines\u00b6","text":"<p>The most accurate method is to use existing lines in the PDF (if they exist) or detect them from the image:</p>"},{"location":"tables/#working-with-the-result-tableresult","title":"Working with the result: <code>TableResult</code>\u00b6","text":"<p><code>extract_table()</code> returns a <code>TableResult</code> object \u2013 it behaves like a regular list of rows and offers two convenience shortcuts:</p>"},{"location":"tables/#stitching-rows-across-page-breaks-multi-page-tables","title":"Stitching rows across page breaks (multi-page tables)\u00b6","text":"<p>If your table spans multiple pages you might find that the first row on a continuation page actually belongs to the last row on the previous page (for example when the bottom border of the table is repeated but the first column on the new page is blank).  <code>extract_table()</code> has a simple fix for this:</p> <pre># Create a Flow from all pages that make up the table\nflow = pdf.pages[1:4].as_flow(arrangement=\"vertical\")\n\n# Provide a rule (prev_row, cur_row, row_idx, segment) -&gt; bool\nmerge_if_first_cell_empty = lambda prev, cur, i, seg: i == 0 and not cur[0]\n\ndata = flow.extract_table(stitch_rows=merge_if_first_cell_empty)\n</pre> <p><code>stitch_rows</code> is optional \u2013 if you leave it out, extraction behaves exactly as before.  Pass a callable predicate to decide, row-by-row, whether a row should be spliced into the previous one.  The same parameter is available on <code>FlowRegion.extract_table()</code>.</p>"},{"location":"tables/#next-steps","title":"Next Steps\u00b6","text":"<p>Tables are just one part of document structure. Once you've got table extraction working:</p> <ul> <li>Layout Analysis: See how table detection fits into understanding the whole document</li> <li>Working with Regions: Manually define table areas when automatic detection fails</li> </ul>"},{"location":"text-analysis/","title":"Text Analysis","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page page = pdf.pages[0] In\u00a0[2]: Copied! <pre># Find the first word element\nword = page.find('word')\n\nprint(f\"Text:\", word.text)\nprint(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name\nprint(f\"Size:\", word.size)\nprint(f\"Color:\", word.color) # Non-stroking color\nprint(f\"Is Bold:\", word.bold)\nprint(f\"Is Italic:\", word.italic)\n</pre> # Find the first word element word = page.find('word')  print(f\"Text:\", word.text) print(f\"Font Name:\", word.fontname) # Font reference (e.g., F1) or name print(f\"Size:\", word.size) print(f\"Color:\", word.color) # Non-stroking color print(f\"Is Bold:\", word.bold) print(f\"Is Italic:\", word.italic) <pre>Text: Jungle Health and Safety Inspection Service\nFont Name: Helvetica\nSize: 8.0\nColor: (0, 0, 0)\nIs Bold: False\nIs Italic: False\n</pre> <ul> <li><code>fontname</code>: Often an internal reference (like 'F1', 'F2') or a basic name.</li> <li><code>size</code>: Font size in points.</li> <li><code>color</code>: The non-stroking color, typically a tuple representing RGB or Grayscale values (e.g., <code>(0.0, 0.0, 0.0)</code> for black).</li> <li><code>bold</code>, <code>italic</code>: Boolean flags indicating if the font style is bold or italic (heuristically determined based on font name conventions).</li> </ul> In\u00a0[3]: Copied! <pre># Find all bold text elements\nbold_text = page.find_all('text:bold')\n\n# Find all italic text elements\nitalic_text = page.find_all('text:italic')\n\n# Find text that is both bold and larger than 12pt\nbold_headings = page.find_all('text:bold[size&gt;=12]')\n\nprint(f\"Found {len(bold_text)} bold elements.\")\nprint(f\"Found {len(italic_text)} italic elements.\")\nprint(f\"Found {len(bold_headings)} bold headings.\")\n</pre> # Find all bold text elements bold_text = page.find_all('text:bold')  # Find all italic text elements italic_text = page.find_all('text:italic')  # Find text that is both bold and larger than 12pt bold_headings = page.find_all('text:bold[size&gt;=12]')  print(f\"Found {len(bold_text)} bold elements.\") print(f\"Found {len(italic_text)} italic elements.\") print(f\"Found {len(bold_headings)} bold headings.\") <pre>Found 9 bold elements.\nFound 0 italic elements.\nFound 1 bold headings.\n</pre> In\u00a0[4]: Copied! <pre>page.analyze_text_styles()\npage.text_style_labels\n</pre> page.analyze_text_styles() page.text_style_labels Out[4]: <pre>['10.0pt Bold Helvetica (medium)',\n '10.0pt Helvetica (medium)',\n '12.0pt Bold Helvetica (large)',\n '8.0pt Helvetica (small)']</pre> <p>One they're assigned, you can filter based on <code>style_label</code> instead of going bit-by-bit.</p> In\u00a0[5]: Copied! <pre>page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]')\n</pre> page.find_all('text[style_label=\"10.0pt Bold Helvetica\"]') Out[5]: <pre>&lt;ElementCollection[Mixed](count=0)&gt;</pre> In\u00a0[6]: Copied! <pre>page.find_all('text').show(group_by='style_label', width=700)\n</pre> page.find_all('text').show(group_by='style_label', width=700) Out[6]: <p>This allows you to quickly see patterns in font usage across the page layout.</p> In\u00a0[7]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\n\n# Select the first page\npage = pdf.pages[0]\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")  # Select the first page page = pdf.pages[0] page.show(width=700) Out[7]: <p>Look!</p> In\u00a0[8]: Copied! <pre>page.find_all('text')[0].fontname\n</pre> page.find_all('text')[0].fontname Out[8]: <pre>'AAAAAB+font000000002a8d158a'</pre> <p>The part before the <code>+</code> is the variant \u2013 bold, italic, etc \u2013 while the part after it is the \"real\" font name.</p>"},{"location":"text-analysis/#text-analysis","title":"Text Analysis\u00b6","text":"<p>Analyzing the properties of text elements, such as their font, size, style, and color, can be crucial for understanding document structure and extracting specific information. Natural PDF provides tools to access and analyze these properties.</p>"},{"location":"text-analysis/#introduction","title":"Introduction\u00b6","text":"<p>Beyond just the sequence of characters, the style of text carries significant meaning. Headings are often larger and bolder, important terms might be italicized, and different sections might use distinct fonts. This page covers how to access and utilize this stylistic information.</p>"},{"location":"text-analysis/#accessing-font-information","title":"Accessing Font Information\u00b6","text":"<p>Every <code>TextElement</code> (representing characters or words) holds information about its font properties.</p>"},{"location":"text-analysis/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>You can directly select text based on its style using pseudo-classes in selectors:</p>"},{"location":"text-analysis/#analyzing-fonts-on-a-page","title":"Analyzing Fonts on a Page\u00b6","text":"<p>You can use <code>analyze_text_styles</code> to assign labels to text based on font sizes, bold/italic and font names.</p>"},{"location":"text-analysis/#visualizing-text-properties","title":"Visualizing Text Properties\u00b6","text":"<p>Use highlighting to visually inspect text properties. Grouping by attributes like <code>fontname</code> or <code>size</code> can be very insightful. In the example below we go right to grouping by the <code>style_label</code>, which combines font name, size and variant.</p>"},{"location":"text-analysis/#weird-font-names","title":"Weird font names\u00b6","text":"<p>Oftentimes font names aren't what you're used to \u2013 Arial, Helvetica, etc \u2013 the PDF has given them weird, weird names. Relax, it's okay, they're normal fonts.</p>"},{"location":"text-extraction/","title":"Text Extraction Guide","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Select the first page for initial examples\npage = pdf.pages[0]\n\n# Display the first page\npage.show(width=700)\n</pre> from natural_pdf import PDF  # Load the PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Select the first page for initial examples page = pdf.pages[0]  # Display the first page page.show(width=700) <pre>CropBox missing from /Page, defaulting to MediaBox\n</pre> Out[1]: In\u00a0[2]: Copied! <pre># Extract all text from the first page\n# Displaying first 500 characters\nprint(page.extract_text()[:500])\n</pre> # Extract all text from the first page # Displaying first 500 characters print(page.extract_text()[:500]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the lev\n</pre> <p>You can also preserve layout with <code>layout=True</code>.</p> In\u00a0[3]: Copied! <pre># Extract text from the entire document (may take time)\n# Uncomment to run:\nprint(page.extract_text(layout=True)[:2000])\n</pre> # Extract text from the entire document (may take time) # Uncomment to run: print(page.extract_text(layout=True)[:2000]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[4]: Copied! <pre># Find a single element, e.g., a title containing \"Summary\"\n# Adjust selector as needed\ndate_element = page.find('text:contains(\"Site\")')\ndate_element # Display the found element object\n</pre> # Find a single element, e.g., a title containing \"Summary\" # Adjust selector as needed date_element = page.find('text:contains(\"Site\")') date_element # Display the found element object Out[4]: <pre>&lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;</pre> In\u00a0[5]: Copied! <pre>date_element.show()\n</pre> date_element.show() Out[5]: In\u00a0[6]: Copied! <pre>date_element.text\n</pre> date_element.text Out[6]: <pre>'Site: '</pre> In\u00a0[7]: Copied! <pre># Find multiple elements, e.g., bold headings (size &gt;= 8)\nheading_elements = page.find_all('text[size&gt;=8]:bold')\nheading_elements \n</pre> # Find multiple elements, e.g., bold headings (size &gt;= 8) heading_elements = page.find_all('text[size&gt;=8]:bold') heading_elements  Out[7]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[8]: Copied! <pre>page.find_all('text[size&gt;=8]:bold').show()\n</pre> page.find_all('text[size&gt;=8]:bold').show() Out[8]: In\u00a0[9]: Copied! <pre># Pull out all of their text (why? I don't know!)\nprint(heading_elements.extract_text())\n</pre> # Pull out all of their text (why? I don't know!) print(heading_elements.extract_text()) <pre>Site: Date:  Violation Count: Summary: ViolationsStatuteDescriptionLevelRepeat?\n</pre> In\u00a0[10]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"Hazardous Materials\")').text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"Hazardous Materials\")').text Out[10]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[11]: Copied! <pre># Exact phrase (case-sensitive)\npage.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text\n</pre> # Exact phrase (case-sensitive) page.find('text:contains(\"HAZARDOUS MATERIALS\")', case=False).text Out[11]: <pre>'Failure to Properly Store Hazardous Materials.'</pre> In\u00a0[12]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\nregex = \"\\d+, \\d{4}\"\npage.find(f'text:contains(\"{regex}\")', regex=True)\n</pre> # Regular expression (e.g., \"YYYY Report\") regex = \"\\d+, \\d{4}\" page.find(f'text:contains(\"{regex}\")', regex=True) Out[12]: <pre>&lt;TextElement text='February 3...' font='Helvetica' size=10.0 bbox=(80.56, 104.07000000000005, 156.71000000000004, 114.07000000000005)&gt;</pre> In\u00a0[13]: Copied! <pre># Regular expression (e.g., \"YYYY Report\")\npage.find_all('text[fontname=\"Helvetica\"][size=10]')\n</pre> # Regular expression (e.g., \"YYYY Report\") page.find_all('text[fontname=\"Helvetica\"][size=10]') Out[13]: <pre>&lt;ElementCollection[TextElement](count=32)&gt;</pre> In\u00a0[14]: Copied! <pre># Region below an element (e.g., below \"Introduction\")\n# Adjust selector as needed\npage.find('text:contains(\"Summary\")').below(include_source=True).show()\n</pre> # Region below an element (e.g., below \"Introduction\") # Adjust selector as needed page.find('text:contains(\"Summary\")').below(include_source=True).show() Out[14]: In\u00a0[15]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_source=True)\n    .extract_text()\n    [:500]\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_source=True)     .extract_text()     [:500] ) Out[15]: <pre>'Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\ninto the vats; and when they were fished out, there was never enough of them left to b'</pre> In\u00a0[16]: Copied! <pre>(\n    page\n    .find('text:contains(\"Summary\")')\n    .below(include_source=True, until='line:horizontal')\n    .show()\n)\n</pre> (     page     .find('text:contains(\"Summary\")')     .below(include_source=True, until='line:horizontal')     .show() ) Out[16]: In\u00a0[17]: Copied! <pre># Manually defined region via coordinates (x0, top, x1, bottom)\nmanual_region = page.create_region(30, 60, 600, 300)\nmanual_region.show()\n</pre> # Manually defined region via coordinates (x0, top, x1, bottom) manual_region = page.create_region(30, 60, 600, 300) manual_region.show() Out[17]: In\u00a0[18]: Copied! <pre># Extract text from the manual region\nmanual_region.extract_text()[:500]\n</pre> # Extract text from the manual region manual_region.extract_text()[:500] Out[18]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\nint'</pre> In\u00a0[19]: Copied! <pre>header_content = page.find('rect')\nfooter_content = page.find_all('line')[-1].below()\n\nheader_content.highlight()\nfooter_content.highlight()\npage.show()\n</pre> header_content = page.find('rect') footer_content = page.find_all('line')[-1].below()  header_content.highlight() footer_content.highlight() page.show() Out[19]: In\u00a0[20]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[20]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the lev'</pre> In\u00a0[21]: Copied! <pre>page.add_exclusion(header_content)\npage.add_exclusion(footer_content)\n</pre> page.add_exclusion(header_content) page.add_exclusion(footer_content) Out[21]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[22]: Copied! <pre>page.extract_text()[:500]\n</pre> page.extract_text()[:500] Out[22]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\nint'</pre> In\u00a0[23]: Copied! <pre>full_text_no_exclusions = page.extract_text(use_exclusions=False)\nclean_text = page.extract_text()\nf\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\"\n</pre> full_text_no_exclusions = page.extract_text(use_exclusions=False) clean_text = page.extract_text() f\"Original length: {len(full_text_no_exclusions)}, Excluded length: {len(clean_text)}\" Out[23]: <pre>'Original length: 1149, Excluded length: 1149'</pre> In\u00a0[24]: Copied! <pre>page.clear_exclusions()\n</pre> page.clear_exclusions() Out[24]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>Exclusions can also be defined globally at the PDF level using <code>pdf.add_exclusion()</code> with a function.</p> In\u00a0[25]: Copied! <pre>print(page.extract_text())\n</pre> print(page.extract_text()) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[26]: Copied! <pre>print(page.extract_text(use_exclusions=False, layout=True))\n</pre> print(page.extract_text(use_exclusions=False, layout=True)) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\ninto the vats; and when they were fished out, there was never enough of them left to be worth\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\nto the world as Durham\u2019s Pure Leaf Lard!\nViolations\nStatute Description Level Repeat?\n4.12.7 Unsanitary Working Conditions. Critical\n5.8.3 Inadequate Protective Equipment. Serious\n6.3.9 Ineffective Injury Prevention. Serious\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\n9.6.4 Inadequate Ventilation Systems. Serious\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\nJungle Health and Safety Inspection Service\n</pre> In\u00a0[27]: Copied! <pre># Find the first text element on the page\nfirst_text = page.find_all('text')[1]\nfirst_text # Display basic info\n</pre> # Find the first text element on the page first_text = page.find_all('text')[1] first_text # Display basic info Out[27]: <pre>&lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;</pre> In\u00a0[28]: Copied! <pre># Highlight the first text element\nfirst_text.show()\n</pre> # Highlight the first text element first_text.show() Out[28]: In\u00a0[29]: Copied! <pre># Get detailed font properties dictionary\nfirst_text.font_info()\n</pre> # Get detailed font properties dictionary first_text.font_info() Out[29]: <pre>{'text': 'INS-UP70N51NCL41R',\n 'fontname': 'Helvetica',\n 'font_family': 'Helvetica',\n 'font_variant': '',\n 'size': 8.0,\n 'bold': False,\n 'italic': False,\n 'color': (1, 0, 0)}</pre> In\u00a0[30]: Copied! <pre># Check specific style properties directly\nf\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\"\n</pre> # Check specific style properties directly f\"Is Bold: {first_text.bold}, Is Italic: {first_text.italic}, Font: {first_text.fontname}, Size: {first_text.size}\" Out[30]: <pre>'Is Bold: False, Is Italic: False, Font: Helvetica, Size: 8.0'</pre> In\u00a0[31]: Copied! <pre># Find elements by font attributes (adjust selectors)\n# Example: Find Arial fonts\narial_text = page.find_all('text[fontname*=Helvetica]')\narial_text # Display list of found elements\n</pre> # Find elements by font attributes (adjust selectors) # Example: Find Arial fonts arial_text = page.find_all('text[fontname*=Helvetica]') arial_text # Display list of found elements Out[31]: <pre>&lt;ElementCollection[TextElement](count=44)&gt;</pre> In\u00a0[32]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nlarge_text = page.find_all('text[size&gt;=12]')\nlarge_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) large_text = page.find_all('text[size&gt;=12]') large_text Out[32]: <pre>&lt;ElementCollection[TextElement](count=1)&gt;</pre> In\u00a0[33]: Copied! <pre># Example: Find large text (e.g., size &gt;= 16)\nbold_text = page.find_all('text:bold')\nbold_text\n</pre> # Example: Find large text (e.g., size &gt;= 16) bold_text = page.find_all('text:bold') bold_text Out[33]: <pre>&lt;ElementCollection[TextElement](count=9)&gt;</pre> In\u00a0[34]: Copied! <pre># Analyze styles on the page\n# This returns a dictionary mapping style names to ElementList objects\npage.analyze_text_styles()\npage.text_style_labels\n</pre> # Analyze styles on the page # This returns a dictionary mapping style names to ElementList objects page.analyze_text_styles() page.text_style_labels Out[34]: <pre>['10.0pt Bold Helvetica (medium)',\n '10.0pt Helvetica (medium)',\n '12.0pt Bold Helvetica (large)',\n '8.0pt Helvetica (small)']</pre> In\u00a0[35]: Copied! <pre>page.find_all('text').show(group_by='style_label')\n</pre> page.find_all('text').show(group_by='style_label') Out[35]: In\u00a0[36]: Copied! <pre>page.find_all('text[style_label=\"8.0pt Helvetica (small)\"]')\n</pre> page.find_all('text[style_label=\"8.0pt Helvetica (small)\"]') Out[36]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> In\u00a0[37]: Copied! <pre>page.find_all('text[fontname=\"Helvetica\"][size=8]')\n</pre> page.find_all('text[fontname=\"Helvetica\"][size=8]') Out[37]: <pre>&lt;ElementCollection[TextElement](count=3)&gt;</pre> <p>Font variants (e.g., <code>AAAAAB+FontName</code>) are also accessible via the <code>font-variant</code> attribute selector: <code>page.find_all('text[font-variant=\"AAAAAB\"]')</code>.</p> In\u00a0[38]: Copied! <pre># Get first 5 text elements in reading order\nelements_in_order = page.find_all('text')\nelements_in_order[:5]\n</pre> # Get first 5 text elements in reading order elements_in_order = page.find_all('text') elements_in_order[:5] Out[38]: <pre>[&lt;TextElement text='Jungle Hea...' font='Helvetica' size=8.0 bbox=(385.0, 35.65599999999995, 541.9680000000001, 43.65599999999995)&gt;,\n &lt;TextElement text='INS-UP70N5...' font='Helvetica' size=8.0 bbox=(385.0, 45.65599999999995, 466.35200000000003, 53.65599999999995)&gt;,\n &lt;TextElement text='Site: ' font='Helvetica' size=10.0, style=['bold'] bbox=(50.0, 84.07000000000005, 74.45, 94.07000000000005)&gt;,\n &lt;TextElement text='Durham\u2019s M...' font='Helvetica' size=10.0 bbox=(74.45, 84.07000000000005, 182.26000000000002, 94.07000000000005)&gt;,\n &lt;TextElement text='Chicago, I...' font='Helvetica' size=10.0 bbox=(182.26000000000002, 84.07000000000005, 234.50000000000003, 94.07000000000005)&gt;]</pre> In\u00a0[39]: Copied! <pre># Text extracted via page.extract_text() respects this order automatically\n# (Result already shown in Basic Text Extraction section)\npage.extract_text()[:100]\n</pre> # Text extracted via page.extract_text() respects this order automatically # (Result already shown in Basic Text Extraction section) page.extract_text()[:100] Out[39]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Il'</pre> In\u00a0[40]: Copied! <pre>page.clear_highlights()\n\nstart = page.find('text:contains(\"Date\")')\nstart.highlight(label='Date label')\nstart.next().highlight(label='Maybe the date', color='green')\nstart.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')\n\npage.show()\n</pre> page.clear_highlights()  start = page.find('text:contains(\"Date\")') start.highlight(label='Date label') start.next().highlight(label='Maybe the date', color='green') start.next('text:contains(\"\\d\")', regex=True).highlight(label='Probably the date')  page.show() Out[40]:"},{"location":"text-extraction/#text-extraction-guide","title":"Text Extraction Guide\u00b6","text":"<p>This guide demonstrates various ways to extract text from PDFs using Natural PDF, from simple page dumps to targeted extraction based on elements, regions, and styles.</p>"},{"location":"text-extraction/#setup","title":"Setup\u00b6","text":"<p>First, let's import necessary libraries and load a sample PDF. We'll use <code>example.pdf</code> from the tutorials' <code>pdfs</code> directory. Adjust the path if your setup differs.</p>"},{"location":"text-extraction/#basic-text-extraction","title":"Basic Text Extraction\u00b6","text":"<p>Get all text from a page or the entire document.</p>"},{"location":"text-extraction/#extracting-text-from-specific-elements","title":"Extracting Text from Specific Elements\u00b6","text":"<p>Use selectors with <code>find()</code> or <code>find_all()</code> to target specific elements. Selectors like <code>:contains(\"Summary\")</code> are examples; adapt them to your PDF.</p>"},{"location":"text-extraction/#advanced-text-searches","title":"Advanced text searches\u00b6","text":""},{"location":"text-extraction/#regions","title":"Regions\u00b6","text":""},{"location":"text-extraction/#filtering-out-headers-and-footers","title":"Filtering Out Headers and Footers\u00b6","text":"<p>Use Exclusion Zones to remove unwanted content before extraction. Adjust selectors for typical header/footer content.</p>"},{"location":"text-extraction/#controlling-whitespace","title":"Controlling Whitespace\u00b6","text":"<p>Manage how spaces and blank lines are handled during extraction using <code>layout</code>.</p>"},{"location":"text-extraction/#font-information-access","title":"Font Information Access\u00b6","text":"<p>Inspect font details of text elements.</p>"},{"location":"text-extraction/#working-with-font-styles","title":"Working with Font Styles\u00b6","text":"<p>Analyze and group text elements by their computed font style, which combines attributes like font name, size, boldness, etc., into logical groups.</p>"},{"location":"text-extraction/#reading-order","title":"Reading Order\u00b6","text":"<p>Text extraction respects a pathetic attempt at natural reading order (top-to-bottom, left-to-right by default). <code>page.find_all('text')</code> returns elements already sorted this way.</p>"},{"location":"text-extraction/#element-navigation","title":"Element Navigation\u00b6","text":"<p>Move between elements sequentially based on reading order using <code>.next()</code> and <code>.previous()</code>.</p>"},{"location":"text-extraction/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to extract text, you might want to explore:</p> <ul> <li>Working with regions for more precise extraction</li> <li>OCR capabilities for scanned documents</li> <li>Document layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"},{"location":"tutorials/01-loading-and-extraction/","title":"Loading and Basic Text Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" <p>In this tutorial, we'll learn how to:</p> <ol> <li>Load a PDF document</li> <li>Extract text from pages</li> <li>Extract specific elements</li> </ol> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\nimport os\n\n# Load a PDF file\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Basic info about the document\n{\n    \"Filename\": os.path.basename(pdf.path),\n    \"Pages\": len(pdf.pages),\n    \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),\n    \"Author\": pdf.metadata.get(\"Author\", \"N/A\")\n}\n</pre> from natural_pdf import PDF import os  # Load a PDF file pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Basic info about the document {     \"Filename\": os.path.basename(pdf.path),     \"Pages\": len(pdf.pages),     \"Title\": pdf.metadata.get(\"Title\", \"N/A\"),     \"Author\": pdf.metadata.get(\"Author\", \"N/A\") } Out[2]: <pre>{'Filename': '01-practice.pdf', 'Pages': 1, 'Title': 'N/A', 'Author': 'N/A'}</pre> In\u00a0[3]: Copied! <pre># Get the first page\npage = pdf.pages[0]\n\n# Extract text from the page\ntext = page.extract_text()\n\n# Show the first 200 characters of the text\nprint(text[:200])\n</pre> # Get the first page page = pdf.pages[0]  # Extract text from the page text = page.extract_text()  # Show the first 200 characters of the text print(text[:200]) <pre>Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men\n</pre> In\u00a0[4]: Copied! <pre># Find text elements containing specific words\nelements = page.find_all('text:contains(\"Inadequate\")')\n\n# Show these elements on the page\nelements.show()\n</pre> # Find text elements containing specific words elements = page.find_all('text:contains(\"Inadequate\")')  # Show these elements on the page elements.show() Out[4]: In\u00a0[5]: Copied! <pre># Analyze the page layout\npage.analyze_layout(engine='yolo')\n\n# Find and highlight all detected regions\npage.find_all('region').show(group_by='type')\n</pre> # Analyze the page layout page.analyze_layout(engine='yolo')  # Find and highlight all detected regions page.find_all('region').show(group_by='type') <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmpucuv2sxu/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 976.1ms\n</pre> <pre>Speed: 5.3ms preprocess, 976.1ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> Out[5]: In\u00a0[6]: Copied! <pre># Process all pages\nfor page in pdf.pages:\n    page_text = page.extract_text()\n    print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page\n</pre> # Process all pages for page in pdf.pages:     page_text = page.extract_text()     print(f\"Page {page.number}\", page_text[:100])  # First 100 chars of each page <pre>Page 1 Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Il\n</pre> <p>This tutorial covered the basics of loading PDFs and extracting text. In the next tutorials, we'll explore more advanced features like searching for specific elements, extracting structured content, and working with tables.</p>"},{"location":"tutorials/01-loading-and-extraction/#loading-and-basic-text-extraction","title":"Loading and Basic Text Extraction\u00b6","text":""},{"location":"tutorials/01-loading-and-extraction/#loading-a-pdf","title":"Loading a PDF\u00b6","text":"<p>Let's start by loading a PDF file:</p>"},{"location":"tutorials/01-loading-and-extraction/#extracting-text","title":"Extracting Text\u00b6","text":"<p>Now that we have loaded the PDF, let's extract the text from the first page:</p>"},{"location":"tutorials/01-loading-and-extraction/#finding-and-extracting-specific-elements","title":"Finding and Extracting Specific Elements\u00b6","text":"<p>We can find specific elements using spatial queries and text content:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-layout-regions","title":"Working with Layout Regions\u00b6","text":"<p>We can analyze the layout of the page to identify different regions:</p>"},{"location":"tutorials/01-loading-and-extraction/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>You can also work with multiple pages:</p>"},{"location":"tutorials/02-finding-elements/","title":"Finding Specific Elements","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\n\n# Get the first page (index 0)\npage = pdf.pages[0]\n\n# Find the text element containing \"Site:\"\n# The ':contains()' pseudo-class looks for text content.\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Find the text element containing \"Date:\"\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Access the text content directly\n{\n    \"Site Label\": site_label.text,\n    \"Date Label\": date_label.text\n}\n\n# Visualize the found elements\nwith page.highlights() as h:\n    h.add(site_label, color=\"red\", label=\"Site\")\n    h.add(date_label, color=\"blue\", label=\"Date\")\n    h.show()\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")  # Get the first page (index 0) page = pdf.pages[0]  # Find the text element containing \"Site:\" # The ':contains()' pseudo-class looks for text content. site_label = page.find('text:contains(\"Site:\")')  # Find the text element containing \"Date:\" date_label = page.find('text:contains(\"Date:\")')  # Access the text content directly {     \"Site Label\": site_label.text,     \"Date Label\": date_label.text }  # Visualize the found elements with page.highlights() as h:     h.add(site_label, color=\"red\", label=\"Site\")     h.add(date_label, color=\"blue\", label=\"Date\")     h.show() In\u00a0[3]: Copied! <pre># Find text elements that are red\nred_text = page.find('text[color~=red]')\nprint(f\"Found red text: {red_text.text}\")\nred_text.show()\n\n# Find elements with specific RGB colors\nblue_text = page.find('text[color=rgb(0,0,255)]')\n</pre> # Find text elements that are red red_text = page.find('text[color~=red]') print(f\"Found red text: {red_text.text}\") red_text.show()  # Find elements with specific RGB colors blue_text = page.find('text[color=rgb(0,0,255)]') <pre>Found red text: INS-UP70N51NCL41R\n</pre> In\u00a0[4]: Copied! <pre># Find horizontal lines\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Find thick lines (width &gt;= 2)\nthick_lines = page.find_all('line[width&gt;=2]')\n\n# Find rectangles\nrectangles = page.find_all('rect')\n\n# Visualize what we found\nwith page.highlights() as h:\n    h.add(horizontal_lines, color=\"blue\", label=\"Horizontal\")\n    h.add(thick_lines, color=\"red\", label=\"Thick\")\n    h.add(rectangles, color=\"green\", label=\"Rectangles\")\n    h.show()\n</pre> # Find horizontal lines horizontal_lines = page.find_all('line:horizontal')  # Find thick lines (width &gt;= 2) thick_lines = page.find_all('line[width&gt;=2]')  # Find rectangles rectangles = page.find_all('rect')  # Visualize what we found with page.highlights() as h:     h.add(horizontal_lines, color=\"blue\", label=\"Horizontal\")     h.add(thick_lines, color=\"red\", label=\"Thick\")     h.add(rectangles, color=\"green\", label=\"Rectangles\")     h.show() In\u00a0[5]: Copied! <pre># Find text with specific font properties\nbold_text = page.find_all('text:bold')\nlarge_text = page.find_all('text[size&gt;=12]')\n\n# Find text with specific font names\nhelvetica_text = page.find_all('text[fontname=Helvetica]')\n</pre> # Find text with specific font properties bold_text = page.find_all('text:bold') large_text = page.find_all('text[size&gt;=12]')  # Find text with specific font names helvetica_text = page.find_all('text[fontname=Helvetica]') In\u00a0[6]: Copied! <pre># Find text above a specific element\nabove_text = page.find('line[width=2]').above().extract_text()\n\n# Find text below a specific element\nbelow_text = page.find('text:contains(\"Summary\")').below().extract_text()\n\n# Find text to the right of a specific element\nnearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text()\n</pre> # Find text above a specific element above_text = page.find('line[width=2]').above().extract_text()  # Find text below a specific element below_text = page.find('text:contains(\"Summary\")').below().extract_text()  # Find text to the right of a specific element nearby_text = page.find('text:contains(\"Site\")').right(width=200).extract_text() In\u00a0[7]: Copied! <pre># Find large, bold text that contains specific words\nimportant_text = page.find_all('text[size&gt;=12]:bold:contains(\"Critical\")')\n\n# Find red text inside a rectangle\nhighlighted_text = page.find('rect').find_all('text[color~=red]')\n</pre> # Find large, bold text that contains specific words important_text = page.find_all('text[size&gt;=12]:bold:contains(\"Critical\")')  # Find red text inside a rectangle highlighted_text = page.find('rect').find_all('text[color~=red]') <p>Handling Missing Elements</p> <pre><code>In these examples, we know certain elements exist in the PDF. In real-world scenarios, `page.find()` might not find a match and would return `None`. Production code should check for this:\n\n```py\nsite_label = page.find('text:contains(\"Site:\")')\nif site_label:\n    # Found it! Proceed...\n    print(site_label.extract_text())\nelse:\n    # Didn't find it, handle appropriately...\n    \"Warning: 'Site:' label not found.\"\n```</code></pre> <p>Visual Debugging</p> <pre><code>When working with complex selectors, it's helpful to visualize what you're finding:\n\n```py\nelements = page.find_all('text[color~=red]')\nelements.show()\n```</code></pre>"},{"location":"tutorials/02-finding-elements/#finding-specific-elements","title":"Finding Specific Elements\u00b6","text":"<p>Extracting all the text is useful, but often you need specific pieces of information. <code>natural-pdf</code> lets you find elements using selectors, similar to CSS.</p> <p>Let's find the \"Site\" and \"Date\" information from our <code>01-practice.pdf</code>:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-color","title":"Finding Elements by Color\u00b6","text":"<p>You can find elements based on their color:</p>"},{"location":"tutorials/02-finding-elements/#finding-lines-and-shapes","title":"Finding Lines and Shapes\u00b6","text":"<p>Find lines and rectangles based on their properties:</p>"},{"location":"tutorials/02-finding-elements/#finding-elements-by-font-properties","title":"Finding Elements by Font Properties\u00b6","text":""},{"location":"tutorials/02-finding-elements/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>You can find elements based on their position relative to other elements:</p>"},{"location":"tutorials/02-finding-elements/#combining-selectors","title":"Combining Selectors\u00b6","text":"<p>You can combine multiple conditions to find exactly what you need:</p>"},{"location":"tutorials/03-extracting-blocks/","title":"Extracting Text Blocks","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the starting element (\"Summary:\")\nstart_marker = page.find('text:contains(\"Summary:\")')\n\n# Select elements below the start_marker, stopping *before*\n# the thick horizontal line (a line with height &gt; 1).\nsummary_elements = start_marker.below(\n    include_source=True, # Include the \"Summary:\" text itself\n    until=\"line[height &gt; 1]\"\n)\n\n# Extract and display the text from the collection of summary elements\nsummary_elements.extract_text()\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the starting element (\"Summary:\") start_marker = page.find('text:contains(\"Summary:\")')  # Select elements below the start_marker, stopping *before* # the thick horizontal line (a line with height &gt; 1). summary_elements = start_marker.below(     include_source=True, # Include the \"Summary:\" text itself     until=\"line[height &gt; 1]\" )  # Extract and display the text from the collection of summary elements summary_elements.extract_text()  Out[2]: <pre>'Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell\\ninto the vats; and when they were fished out, there was never enough of them left to be worth\\nexhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out\\nto the world as Durham\u2019s Pure Leaf Lard!\\nViolations\\nStatute Description Level Repeat?\\n4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious\\n6.3.9 Ineffective Injury Prevention. Serious\\n7.1.5 Failure to Properly Store Hazardous Materials. Critical\\n8.9.2 Lack of Adequate Fire Safety Measures. Serious\\n9.6.4 Inadequate Ventilation Systems. Serious\\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious'</pre> In\u00a0[3]: Copied! <pre># Visualize the elements found in this block\nsummary_elements.show(color=\"lightgreen\", label=\"Summary Block\")\n</pre> # Visualize the elements found in this block summary_elements.show(color=\"lightgreen\", label=\"Summary Block\") Out[3]: <p>This selects the elements using <code>.below(until=...)</code> and extracts their text. The second code block displays the page image with the visualized section.</p> <p>Selector Specificity</p> <pre><code>We used `line[height &gt; 1]` to find the thick horizontal line. You might need to adjust selectors based on the specific PDF structure. Inspecting element properties can help you find reliable start and end markers.</code></pre>"},{"location":"tutorials/03-extracting-blocks/#extracting-text-blocks","title":"Extracting Text Blocks\u00b6","text":"<p>Often, you need a specific section, like a paragraph between two headings. You can find a starting element and select everything below it until an ending element.</p> <p>Let's extract the \"Summary\" section from <code>01-practice.pdf</code>. It starts after \"Summary:\" and ends before the thick horizontal line.</p>"},{"location":"tutorials/04-table-extraction/","title":"Basic Table Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf  # core install already includes pdfplumber\n</pre> #%pip install natural-pdf  # core install already includes pdfplumber In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# For a single table, extract_table returns list-of-lists\ntable = page.extract_table(method=\"pdfplumber\")\ntable  # List-of-lists of cell text\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # For a single table, extract_table returns list-of-lists table = page.extract_table(method=\"pdfplumber\") table  # List-of-lists of cell text Out[2]: <pre>TableResult(rows=8\u2026)</pre> <p><code>extract_table()</code> defaults to the plumber backend, so the explicit <code>method</code> is optional\u2014but it clarifies what's happening.</p> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Detect layout with Table Transformer\npage.analyze_layout(engine=\"tatr\")\n\n# Grab the first detected table region\ntable_region = page.find('region[type=table]')\n\ntable_region.show(label=\"TATR Table\", color=\"purple\")\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Detect layout with Table Transformer page.analyze_layout(engine=\"tatr\")  # Grab the first detected table region table_region = page.find('region[type=table]')  table_region.show(label=\"TATR Table\", color=\"purple\") Out[3]: In\u00a0[4]: Copied! <pre>tatr_rows = table_region.extract_table()  # Uses TATR backend implicitly\n</pre> tatr_rows = table_region.extract_table()  # Uses TATR backend implicitly In\u00a0[5]: Copied! <pre>page.clear_detected_layout_regions()\npage.analyze_layout(engine=\"paddle\", confidence=0.3)\n\npaddle_table = page.find('region[type=table]')\nif paddle_table:\n    paddle_table.show(color=\"green\", label=\"Paddle Table\")\n    paddle_rows = paddle_table.extract_table(method=\"pdfplumber\")  # fall back to ruling-line extraction inside the region\n</pre> page.clear_detected_layout_regions() page.analyze_layout(engine=\"paddle\", confidence=0.3)  paddle_table = page.find('region[type=table]') if paddle_table:     paddle_table.show(color=\"green\", label=\"Paddle Table\")     paddle_rows = paddle_table.extract_table(method=\"pdfplumber\")  # fall back to ruling-line extraction inside the region <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre>"},{"location":"tutorials/04-table-extraction/#basic-table-extraction","title":"Basic Table Extraction\u00b6","text":"<p>PDFs often contain tables, and <code>natural-pdf</code> provides methods to extract their data. The key is to first triangulate where your table is on the page, then use powerful extraction tools on that specific region.</p> <p>Let's extract the \"Violations\" table from our practice PDF.</p>"},{"location":"tutorials/04-table-extraction/#method-1-pdfplumber-default","title":"Method 1 \u2013 pdfplumber (default)\u00b6","text":""},{"location":"tutorials/04-table-extraction/#method-2-tatr-based-extraction","title":"Method 2 \u2013 TATR-based extraction\u00b6","text":"<p>When you do a TATR layout analysis, it detects tables, rows and cells with a LayoutLM model. Once a region has <code>source=\"detected\"</code> and <code>type=\"table\"</code>, calling <code>extract_table()</code> on that region uses the tatr backend automatically.</p>"},{"location":"tutorials/04-table-extraction/#method-3-paddleocr-layout","title":"Method 3 \u2013 PaddleOCR Layout\u00b6","text":"<p>You can also try PaddleOCR's layout detector to locate tables:</p>"},{"location":"tutorials/04-table-extraction/#choosing-the-right-backend","title":"Choosing the right backend\u00b6","text":"<ul> <li>plumber \u2013 fastest; needs rule lines or tidy whitespace.</li> <li>tatr \u2013 robust to missing lines; slower; requires AI extra.</li> <li>text \u2013 whitespace clustering; fallback when lines + models fail.</li> </ul> <p>You can call <code>page.extract_table(method=\"text\")</code> or on a <code>Region</code> as well.</p> <p>The general workflow is: try different layout analyzers to locate your table, then extract from the specific region. Keep trying options until one works for your particular PDF!</p> <p>For complex grids where even models struggle, see Tutorial 11 (enhanced table processing) for a lines-first workflow.</p>"},{"location":"tutorials/04-table-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Compare accuracy/time of the three methods on the sample PDF.</li> <li>Show how to call <code>page.extract_table(method=\"text\")</code> as a no-dependency fallback.</li> <li>Add snippet exporting <code>rows</code> to pandas DataFrame.</li> <li>Demonstrate cell post-processing (strip %, cast numbers).</li> </ul>"},{"location":"tutorials/05-excluding-content/","title":"Excluding Content (Headers/Footers)","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\n\n# Load the PDF\npdf = PDF(pdf_url)\npage = pdf.pages[0]\n\n# Let's see the bottom part of the text WITHOUT exclusions\n# It likely contains page numbers or other footer info.\nfull_text_unfiltered = page.extract_text()\n\n# Show the last 200 characters (likely containing footer text)\nfull_text_unfiltered[-200:]\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"  # Load the PDF pdf = PDF(pdf_url) page = pdf.pages[0]  # Let's see the bottom part of the text WITHOUT exclusions # It likely contains page numbers or other footer info. full_text_unfiltered = page.extract_text()  # Show the last 200 characters (likely containing footer text) full_text_unfiltered[-200:] Out[2]: <pre>' C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0\\nWrite-In Totals 0 0 0 0\\nPrecinct Summary - 11/06/2024 12:22 AM Page 1 of 387\\nReport generated with Electionware Copyright \u00a9 2007-2020'</pre> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\n\n# Define the exclusion region on every page using a lambda function\nfooter_height = 200\npdf.add_exclusion(\n    lambda page: page.region(top=page.height - footer_height),\n    label=\"Bottom 200pt Footer\"\n)\n\n# Now extract text from the first page again, exclusions are active by default\npage = pdf.pages[0]\n\n# Visualize the excluded area\nfooter_region_viz = page.region(top=page.height - footer_height)\nfooter_region_viz.show(label=\"Excluded Footer Area\")\npage.show()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url)  # Define the exclusion region on every page using a lambda function footer_height = 200 pdf.add_exclusion(     lambda page: page.region(top=page.height - footer_height),     label=\"Bottom 200pt Footer\" )  # Now extract text from the first page again, exclusions are active by default page = pdf.pages[0]  # Visualize the excluded area footer_region_viz = page.region(top=page.height - footer_height) footer_region_viz.show(label=\"Excluded Footer Area\") page.show() Out[3]: In\u00a0[4]: Copied! <pre>filtered_text = page.extract_text() # use_exclusions=True is default\n\n# Show the last 200 chars with footer area excluded\nfiltered_text[-200:]\n</pre> filtered_text = page.extract_text() # use_exclusions=True is default  # Show the last 200 chars with footer area excluded filtered_text[-200:] Out[4]: <pre>'TOR\\nVote For 1\\nElection Provisional\\nTOTAL Mail Votes\\nDay Votes\\nDEM ROBERT P CASEY JR 99 70 29 0\\nREP DAVE MCCORMICK 79 69 10 0\\nLIB JOHN C THOMAS 2 2 0 0\\nGRN LEILA HAZOU 2 1 1 0\\nCST MARTY SELKER 2 2 0 0'</pre> <p>This method is simple but might cut off content if the footer height varies or content extends lower on some pages.</p> In\u00a0[5]: Copied! <pre>from natural_pdf import PDF\n\npdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\"\npdf = PDF(pdf_url)\npage = pdf.pages[0] # Get page for finding elements\n\n# Find the last horizontal line on the first page\n# We'll use this logic to define our exclusion for all pages\nlast_line = page.find_all('line')[-1]\n\n# Define the exclusion function using a lambda\n# This finds the last line on *each* page and excludes below it\npdf.add_exclusion(\n    lambda p: p.find_all('line')[-1].below(),\n    label=\"Element-Based Footer\"\n)\n\n# Extract text again, with the element-based exclusion active\nfiltered_text_element = page.extract_text()\n\n# Show the last 200 chars with element-based footer exclusion\n\"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]\n\n# Visualize the element-based exclusion area\npage.clear_highlights()\n# Need to find the region again for visualization\nfooter_boundary = page.find_all('line')[-1]\nfooter_region_element = footer_boundary.below()\nfooter_region_element.show(label=\"Excluded Footer Area (Element)\")\npage.show()\n</pre> from natural_pdf import PDF  pdf_url = \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\" pdf = PDF(pdf_url) page = pdf.pages[0] # Get page for finding elements  # Find the last horizontal line on the first page # We'll use this logic to define our exclusion for all pages last_line = page.find_all('line')[-1]  # Define the exclusion function using a lambda # This finds the last line on *each* page and excludes below it pdf.add_exclusion(     lambda p: p.find_all('line')[-1].below(),     label=\"Element-Based Footer\" )  # Extract text again, with the element-based exclusion active filtered_text_element = page.extract_text()  # Show the last 200 chars with element-based footer exclusion \"Element-Based Excluded (last 200 chars): \" + filtered_text_element[-200:]  # Visualize the element-based exclusion area page.clear_highlights() # Need to find the region again for visualization footer_boundary = page.find_all('line')[-1] footer_region_element = footer_boundary.below() footer_region_element.show(label=\"Excluded Footer Area (Element)\") page.show() Out[5]: <p>This element-based approach is usually more reliable as it adapts to the content's position, but it depends on finding consistent boundary elements (like lines or specific text markers).</p>"},{"location":"tutorials/05-excluding-content/#excluding-content-headersfooters","title":"Excluding Content (Headers/Footers)\u00b6","text":"<p>Often, PDFs have repeating headers or footers on every page that you want to ignore when extracting the main content. <code>natural-pdf</code> allows you to define exclusion regions.</p> <p>We'll use a different PDF for this example, which has a distinct header and footer section: <code>0500000US42007.pdf</code>.</p>"},{"location":"tutorials/05-excluding-content/#approach-1-excluding-a-fixed-area","title":"Approach 1: Excluding a Fixed Area\u00b6","text":"<p>A simple way to exclude headers or footers is to define a fixed region based on page coordinates. Let's exclude the bottom 200 pixels of the page.</p>"},{"location":"tutorials/05-excluding-content/#approach-2-excluding-based-on-elements","title":"Approach 2: Excluding Based on Elements\u00b6","text":"<p>A more robust way is to find specific elements that reliably mark the start of the footer (or end of the header) and exclude everything below (or above) them. In <code>Examples.md</code>, the footer was defined as everything below the last horizontal line.</p>"},{"location":"tutorials/05-excluding-content/#todo","title":"TODO\u00b6","text":"<ul> <li>Show a text-based exclusion: <code>pdf.add_exclusion(lambda p: p.find('text:contains(\"Page \")').below())</code> for dynamic page numbers.</li> <li>Demonstrate stacking multiple exclusions (e.g., header + footer) and the order they are applied.</li> <li>Provide an example disabling exclusions temporarily with <code>extract_text(use_exclusions=False)</code>.</li> <li>Include a multi-page preview that outlines exclusions on every page.</li> </ul> <p>Applying Exclusions</p> <pre><code>*   `pdf.add_exclusion(func)` applies the exclusion function (which takes a page and returns a region) to *all* pages in the PDF.\n*   `page.add_exclusion(region)` adds an exclusion region only to that specific page.\n*   `extract_text(use_exclusions=False)` can be used to temporarily disable exclusions.</code></pre>"},{"location":"tutorials/06-document-qa/","title":"Document Question Answering (QA)","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"  # DocumentQA relies on torch + transformers\n</pre> #%pip install \"natural-pdf[ai]\"  # DocumentQA relies on torch + transformers In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Ask about the date\nquestion_1 = \"What is the inspection date?\"\nanswer_1 = page.ask(question_1)\n\n# The result dictionary always contains:\n#   question    - original question\n#   answer      \u2013 extracted span (string, may be empty)\n#   confidence  \u2013 model score 0\u20131\n#   start / end \u2013 indices into page.words\n#   found       \u2013 False if confidence &lt; min_confidence\nanswer_1\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Ask about the date question_1 = \"What is the inspection date?\" answer_1 = page.ask(question_1)  # The result dictionary always contains: #   question    - original question #   answer      \u2013 extracted span (string, may be empty) #   confidence  \u2013 model score 0\u20131 #   start / end \u2013 indices into page.words #   found       \u2013 False if confidence &lt; min_confidence answer_1 <pre>Device set to use mps\n</pre> Out[2]: <pre>{'question': 'What is the inspection date?',\n 'answer': 'February 3, 1905',\n 'confidence': 0.9979940056800842,\n 'start': 6,\n 'end': 6,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[3]: Copied! <pre>page.ask(\"What company was inspected?\")\n</pre> page.ask(\"What company was inspected?\") Out[3]: <pre>{'question': 'What company was inspected?',\n 'answer': 'Jungle Health and Safety Inspection Service',\n 'confidence': 0.9988948106765747,\n 'start': 0,\n 'end': 0,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[4]: Copied! <pre>page.ask( \"What is statute 5.8.3 about?\")\n</pre> page.ask( \"What is statute 5.8.3 about?\") Out[4]: <pre>{'question': 'What is statute 5.8.3 about?',\n 'answer': 'Inadequate Protective Equipment.',\n 'confidence': 0.9997999668121338,\n 'start': 26,\n 'end': 26,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> <p>The results include the extracted <code>answer</code>, a <code>confidence</code> score (useful for filtering uncertain answers), the <code>page_num</code>, and the <code>source_elements</code>.</p> In\u00a0[5]: Copied! <pre>answer = page.ask(\"What is the inspection ID?\")\nanswer.show()\n</pre> answer = page.ask(\"What is the inspection ID?\") answer.show() Out[5]: In\u00a0[6]: Copied! <pre>pdf.ask(\"What company was inspected?\")\n</pre> pdf.ask(\"What company was inspected?\") Out[6]: <pre>{'answer': 'Jungle Health and Safety Inspection Service',\n 'confidence': 0.9988948106765747,\n 'found': True,\n 'page_num': 1,\n 'source_elements': [],\n 'start': 0,\n 'end': 0}</pre> <p>Notice that it collects the page number for later investigation.</p> In\u00a0[7]: Copied! <pre>from natural_pdf import PDF\nimport pandas as pd\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\nquestions = [\n    \"What is the inspection date?\",\n    \"What company was inspected?\",\n    \"What is statute 5.8.3 about?\",\n    \"How many violations were there in total?\"\n]\n\nanswers = page.ask(questions, min_confidence=0.2)\n\ndf = pd.json_normalize(answers)\ndf\n</pre> from natural_pdf import PDF import pandas as pd  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  questions = [     \"What is the inspection date?\",     \"What company was inspected?\",     \"What is statute 5.8.3 about?\",     \"How many violations were there in total?\" ]  answers = page.ask(questions, min_confidence=0.2)  df = pd.json_normalize(answers) df Out[7]: question answer confidence start end found page_num source_elements 0 What is the inspection date? February 3, 1905 0.997994 6 6 True 0 [&lt;TextElement text='February 3...' font='Helve... 1 What company was inspected? Jungle Health and Safety Inspection Service 0.998895 0 0 True 0 [&lt;TextElement text='Jungle Hea...' font='Helve... 2 What is statute 5.8.3 about? Inadequate Protective Equipment. 0.999800 26 26 True 0 [&lt;TextElement text='Inadequate...' font='Helve... 3 How many violations were there in total? 4.12.7 0.662560 22 22 True 0 [&lt;TextElement text='4.12.7' font='Helvetica' s... <p><code>pd.json_normalize</code> flattens the list of answer dictionaries straight into a DataFrame, making it easy to inspect the questions, their extracted answers, and associated confidence scores.</p>"},{"location":"tutorials/06-document-qa/#document-question-answering-qa","title":"Document Question Answering (QA)\u00b6","text":"<p>Sometimes, instead of searching for specific text patterns, you just want to ask the document a question directly. <code>natural-pdf</code> includes an extractive Question Answering feature.</p> <p>\"Extractive\" means it finds the literal answer text within the document, rather than generating a new answer or summarizing.</p> <p>Let's ask our <code>01-practice.pdf</code> a few questions.</p>"},{"location":"tutorials/06-document-qa/#visualising-where-the-answer-came-from","title":"Visualising Where the Answer Came From\u00b6","text":"<p>You can manually access results sources through <code>answer['source_elements']</code> but it's much more fun to just use <code>.show()</code>.</p>"},{"location":"tutorials/06-document-qa/#asking-an-entire-pdf","title":"Asking an entire PDF\u00b6","text":"<p>You don't need to select a single page to use <code>.ask</code>! It also works for entire PDFs, regions, anything.</p>"},{"location":"tutorials/06-document-qa/#collecting-results-into-a-dataframe","title":"Collecting Results into a DataFrame\u00b6","text":"<p>If you're asking multiple questions, it's often useful to collect the results into a pandas DataFrame. <code>page.ask</code> supports passing a list of questions directly. This is far faster than looping because the underlying model is invoked only once.</p>"},{"location":"tutorials/06-document-qa/#todo","title":"TODO\u00b6","text":"<ul> <li>Demonstrate passing <code>model=\"impira/layoutlm-document-qa\"</code> to switch models.</li> </ul>"},{"location":"tutorials/06-document-qa/#qa-model-and-limitations","title":"QA Model and Limitations\u00b6","text":"<ul> <li>The QA system relies on underlying transformer models. Performance and confidence scores vary.</li> <li>It works best for questions where the answer is explicitly stated. It cannot synthesize information or perform calculations (e.g., counting items might fail or return text containing a number rather than the count itself).</li> <li>You can potentially specify different QA models via the <code>model=</code> argument in <code>page.ask()</code> if others are configured.</li> </ul>"},{"location":"tutorials/07-layout-analysis/","title":"Layout Analysis","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF and get the page\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Analyze the layout using the default model\n# This adds 'detected' Region objects to the page\n# It returns an ElementCollection of the detected regions\npage.analyze_layout()\ndetected_regions = page.find_all('region[source=\"detected\"]')\n</pre> from natural_pdf import PDF  # Load the PDF and get the page pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Analyze the layout using the default model # This adds 'detected' Region objects to the page # It returns an ElementCollection of the detected regions page.analyze_layout() detected_regions = page.find_all('region[source=\"detected\"]') <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp2w6y7meq/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 2984.3ms\n</pre> <pre>Speed: 6.1ms preprocess, 2984.3ms inference, 3.6ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[3]: Copied! <pre># Visualize all detected regions, using default colors based on type\ndetected_regions.show(group_by='type', annotate=['confidence'])\n</pre> # Visualize all detected regions, using default colors based on type detected_regions.show(group_by='type', annotate=['confidence']) Out[3]: In\u00a0[4]: Copied! <pre># Find and visualize only the detected table region(s)\ntables = page.find_all('region[type=table]')\ntables.show(color='lightgreen', label='Detected Table')\n</pre> # Find and visualize only the detected table region(s) tables = page.find_all('region[type=table]') tables.show(color='lightgreen', label='Detected Table') Out[4]: In\u00a0[5]: Copied! <pre># Extract text specifically from the detected table region\ntable_region = tables.first # Assuming only one table was detected\n# Extract text preserving layout\ntable_text_layout = table_region.extract_text(layout=True)\ntable_text_layout\n</pre> # Extract text specifically from the detected table region table_region = tables.first # Assuming only one table was detected # Extract text preserving layout table_text_layout = table_region.extract_text(layout=True) table_text_layout Out[5]: <pre>'Statute Description                              Level  Repeat?      \\n4.12.7 Unsanitary Working Conditions.            Critical            \\n5.8.3 Inadequate Protective Equipment.           Serious             \\n6.3.9 Ineffective Injury Prevention.             Serious             \\n7.1.5 Failure to Properly Store Hazardous Materials. Critical        \\n8.9.2 Lack of Adequate Fire Safety Measures.     Serious             \\n9.6.4 Inadequate Ventilation Systems.            Serious             \\n10.2.7 Insufficient Employee Training for Safe Work Practices. Serious\\n                                                                     \\n                                                                     \\n                                                                     \\n                                                                     '</pre> In\u00a0[6]: Copied! <pre># Layout-detected regions can also be used for table extraction\n# This can be more robust than the basic page.extract_tables()\n# especially for tables without clear lines.\ntable_data = table_region.extract_table()\ntable_data\n</pre> # Layout-detected regions can also be used for table extraction # This can be more robust than the basic page.extract_tables() # especially for tables without clear lines. table_data = table_region.extract_table() table_data Out[6]: <pre>TableResult(rows=8\u2026)</pre> In\u00a0[7]: Copied! <pre># Re-run layout with PaddleOCR detector\npage.clear_detected_layout_regions()\n\npaddle_regions = page.analyze_layout(engine=\"paddle\", confidence=0.3)\n#paddle_regions.show(group_by=\"type\")\n\n# Only keep detections the model tagged as \"table\" or \"figure\"\ntables_and_figs = paddle_regions.filter(lambda r: r.normalized_type in {\"table\", \"figure\"})\n#tables_and_figs.show(label_format=\"{normalized_type} ({confidence:.2f})\")\n</pre> # Re-run layout with PaddleOCR detector page.clear_detected_layout_regions()  paddle_regions = page.analyze_layout(engine=\"paddle\", confidence=0.3) #paddle_regions.show(group_by=\"type\")  # Only keep detections the model tagged as \"table\" or \"figure\" tables_and_figs = paddle_regions.filter(lambda r: r.normalized_type in {\"table\", \"figure\"}) #tables_and_figs.show(label_format=\"{normalized_type} ({confidence:.2f})\") <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('UVDoc', None)\n</pre> <pre>Using official model (UVDoc), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocBlockLayout', None)\n</pre> <pre>Using official model (PP-DocBlockLayout), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-DocLayout_plus-L', None)\n</pre> <pre>Using official model (PP-DocLayout_plus-L), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_table_cls', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_table_cls), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANeXt_wired', None)\n</pre> <pre>Using official model (SLANeXt_wired), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('SLANet_plus', None)\n</pre> <pre>Using official model (SLANet_plus), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wired_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wired_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('RT-DETR-L_wireless_table_cell_det', None)\n</pre> <pre>Using official model (RT-DETR-L_wireless_table_cell_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-Chart2Table', None)\n</pre> <pre>Using official model (PP-Chart2Table), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/config.json\n</pre> <pre>Loading weights file /Users/soma/.paddlex/official_models/PP-Chart2Table/model_state.pdparams\n</pre> <pre>Loaded weights file from disk, setting weights to model.\n</pre> <pre>All model checkpoint weights were used when initializing PPChart2TableInference.\n\n</pre> <pre>All the weights of PPChart2TableInference were initialized from the model checkpoint at /Users/soma/.paddlex/official_models/PP-Chart2Table.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use PPChart2TableInference for predictions without further training.\n</pre> <pre>Loading configuration file /Users/soma/.paddlex/official_models/PP-Chart2Table/generation_config.json\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_doc_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_doc_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-LCNet_x1_0_textline_ori', None)\n</pre> <pre>Using official model (PP-LCNet_x1_0_textline_ori), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <p>The helper accepts these common kwargs (see <code>LayoutOptions</code> subclasses for full list):</p> <ul> <li><code>confidence</code> \u2013 minimum score for retaining a prediction.</li> <li><code>classes</code> / <code>exclude_classes</code> \u2013 whitelist or blacklist region types.</li> <li><code>device</code> \u2013 \"cuda\" or \"cpu\"; defaults to GPU if available.</li> </ul> <p>Each engine also exposes its own options class (e.g., <code>YOLOLayoutOptions</code>) for fine control over NMS thresholds, model sizes, etc. Pass an instance via the <code>options=</code> param.</p> <p>Layout analysis provides structured <code>Region</code> objects. You can filter these regions by their predicted <code>type</code> and then perform actions like visualization or extracting text/tables specifically from those regions.</p>"},{"location":"tutorials/07-layout-analysis/#layout-analysis","title":"Layout Analysis\u00b6","text":"<p>Beyond simple text and lines, <code>natural-pdf</code> can use layout analysis models (like YOLO or DETR) to identify semantic regions within a page, such as paragraphs, tables, figures, headers, etc. This provides a higher-level understanding of the document structure.</p>"},{"location":"tutorials/07-layout-analysis/#available-layout-engines","title":"Available Layout Engines\u00b6","text":"<ul> <li>yolo \u2013 YOLOv5 model trained on DocLayNet; fast and good at classic page objects (paragraph, table, figure, heading).  Install via <code>npdf install yolo</code>.</li> <li>tatr \u2013 Microsoft's Table Transformer (LayoutLM) specialised in tables; already included in the ai extra.</li> <li>paddle \u2013 PaddleOCR`s layout detector; lightweight and CPU-friendly.</li> <li>surya \u2013 Surya Layout Parser (DETR backbone) tuned for invoices and forms.</li> <li>docling \u2013 YOLOX model published by DocLING researchers; performs well on historical documents.</li> <li>gemini \u2013 Calls Google's Vision Gemini API (experimental, requires <code>OPENAI_API_KEY</code>).</li> </ul> <p><code>page.analyze_layout()</code> defaults to the first available engine (search order <code>yolo \u2192 paddle \u2192 tatr</code>), but you can pick one explicitly with <code>engine=\"...\"</code>.</p> <p>Let's analyze the layout of our <code>01-practice.pdf</code>.</p>"},{"location":"tutorials/07-layout-analysis/#switching-engines-and-tuning-thresholds","title":"Switching Engines and Tuning Thresholds\u00b6","text":""},{"location":"tutorials/07-layout-analysis/#todo","title":"TODO\u00b6","text":"<ul> <li>Add a speed/accuracy comparison snippet looping over all installed engines.</li> <li>Demonstrate multi-page batch: <code>pdf.pages[::2].analyze_layout(engine=\"yolo\")</code>.</li> <li>Show <code>page.get_sections(start_elements=page.find_all('region[type=heading]'))</code> to split by detected headings.</li> <li>Include an example of exporting regions to COCO JSON for custom model fine-tuning.</li> <li>Document how to override the model path via <code>model_name</code> and how to plug a remote inference client (<code>client=</code>).</li> </ul>"},{"location":"tutorials/07-layout-analysis/#wish-list-future-enhancements","title":"Wish List (Future Enhancements)\u00b6","text":"<ul> <li>Confidence palette \u2013 Allow <code>show(color_by=\"confidence\")</code> to auto-map scores to a red\u2013green gradient.</li> <li><code>ElementCollection.to_json()</code> \u2013 one-liner export of detected regions (and optionally <code>to_df()</code>).</li> <li>Model cache override \u2013 honor an env variable like <code>NATPDF_MODEL_DIR</code> so enterprises can redirect weight downloads.</li> <li>Remote inference support \u2013 make the <code>client=</code> hook forward images to a custom REST or gRPC service.</li> </ul>"},{"location":"tutorials/07-working-with-regions/","title":"Working with Regions","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Create a region in the top portion of the page\ntop_region = page.create_region(\n    50,          # x0 (left)\n    100,          # y0 (top)\n    page.width - 50,  # x1 (right)\n    200          # y1 (bottom)\n)\n\n# Visualize the region\ntop_region.show(color=\"blue\")\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Create a region in the top portion of the page top_region = page.create_region(     50,          # x0 (left)     100,          # y0 (top)     page.width - 50,  # x1 (right)     200          # y1 (bottom) )  # Visualize the region top_region.show(color=\"blue\") Out[2]: In\u00a0[3]: Copied! <pre># Extract text from this region\ntop_region.extract_text()\n</pre> # Extract text from this region top_region.extract_text() Out[3]: <pre>'Date: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'</pre> In\u00a0[4]: Copied! <pre># Find an element to create regions around\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Create regions relative to this element\nbelow_title = title.below(height=100)\nright_of_title = title.right(width=200)\nabove_title = title.above(height=50)\n\n# Visualize these regions\nwith page.highlights() as h:\n    h.add(below_title, color=\"green\", label=\"Below\")\n    h.add(right_of_title, color=\"red\", label=\"Right\")\n    h.add(above_title, color=\"orange\", label=\"Above\")\n    h.show()\n</pre> # Find an element to create regions around title = page.find('text:contains(\"Jungle Health\")')  # Create regions relative to this element below_title = title.below(height=100) right_of_title = title.right(width=200) above_title = title.above(height=50)  # Visualize these regions with page.highlights() as h:     h.add(below_title, color=\"green\", label=\"Below\")     h.add(right_of_title, color=\"red\", label=\"Right\")     h.add(above_title, color=\"orange\", label=\"Above\")     h.show() In\u00a0[5]: Copied! <pre># Extract text from the region below the title\nbelow_title.extract_text()\n</pre> # Extract text from the region below the title below_title.extract_text() Out[5]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[6]: Copied! <pre># Create a region for a specific document section\nform_region = page.create_region(50, 100, page.width - 50, 300)\n\n# Find elements only within this region\nlabels = form_region.find_all('text:contains(\":\")')\n\n# Visualize the region and the elements found\nform_region.show(\n    color=(0, 0, 1, 0.2),\n    label=\"Form Region\"\n)\nlabels.show(color=\"purple\", label=\"Labels\")\n</pre> # Create a region for a specific document section form_region = page.create_region(50, 100, page.width - 50, 300)  # Find elements only within this region labels = form_region.find_all('text:contains(\":\")')  # Visualize the region and the elements found form_region.show(     color=(0, 0, 1, 0.2),     label=\"Form Region\" ) labels.show(color=\"purple\", label=\"Labels\") Out[6]: In\u00a0[7]: Copied! <pre># Find an element to work with\nelement = page.find('text:contains(\"Summary:\")')\n\n# Create a tight region around the element\ntight_region = element.expand(0, 0, 0, 0)\n\n# Expand it to include surrounding content\nexpanded_region = tight_region.expand(\n    left=10,       # Expand 10 points to the left\n    right=200,     # Expand 200 points to the right\n    top=5,  # Expand 5 points above\n    bottom=100  # Expand 100 points below\n)\n\n# Visualize both regions\nwith page.highlights() as h:\n    h.add(tight_region, color=\"red\", label=\"Original\")\n    h.add(expanded_region, color=\"blue\", label=\"Expanded\")\n    h.show()\n</pre> # Find an element to work with element = page.find('text:contains(\"Summary:\")')  # Create a tight region around the element tight_region = element.expand(0, 0, 0, 0)  # Expand it to include surrounding content expanded_region = tight_region.expand(     left=10,       # Expand 10 points to the left     right=200,     # Expand 200 points to the right     top=5,  # Expand 5 points above     bottom=100  # Expand 100 points below )  # Visualize both regions with page.highlights() as h:     h.add(tight_region, color=\"red\", label=\"Original\")     h.add(expanded_region, color=\"blue\", label=\"Expanded\")     h.show() In\u00a0[8]: Copied! <pre># Find two elements to serve as boundaries\nstart_elem = page.find('text:contains(\"Summary:\")')\nend_elem = page.find('text:contains(\"Violations\")')\n\n# Create a region from start to end element\nbounded_region = start_elem.until(end_elem)\n\n# Visualize the bounded region\nbounded_region.show(color=\"green\", label=\"Bounded Region\")\n\n# Extract text from this bounded region\nbounded_region.extract_text()[:200] + \"...\"\n</pre> # Find two elements to serve as boundaries start_elem = page.find('text:contains(\"Summary:\")') end_elem = page.find('text:contains(\"Violations\")')  # Create a region from start to end element bounded_region = start_elem.until(end_elem)  # Visualize the bounded region bounded_region.show(color=\"green\", label=\"Bounded Region\")  # Extract text from this bounded region bounded_region.extract_text()[:200] + \"...\" Out[8]: <pre>'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men...'</pre> In\u00a0[9]: Copied! <pre># Define multiple regions to extract different parts of the document\nheader_region = page.create_region(0, 0, page.width, 100)\nmain_region = page.create_region(100, 100, page.width - 100, page.height - 150)\nfooter_region = page.create_region(0, page.height - 50, page.width, page.height)\n\n# Visualize all regions\nheader_region.show(color=\"blue\", label=\"Header\")\nmain_region.show(color=\"green\", label=\"Main Content\")\nfooter_region.show(color=\"red\", label=\"Footer\")\n\n# Extract content from each region\ndocument_parts = {\n    \"header\": header_region.extract_text(),\n    \"main\": main_region.extract_text()[:100] + \"...\",\n    \"footer\": footer_region.extract_text()\n}\n\n# Show what we extracted\ndocument_parts\n</pre> # Define multiple regions to extract different parts of the document header_region = page.create_region(0, 0, page.width, 100) main_region = page.create_region(100, 100, page.width - 100, page.height - 150) footer_region = page.create_region(0, page.height - 50, page.width, page.height)  # Visualize all regions header_region.show(color=\"blue\", label=\"Header\") main_region.show(color=\"green\", label=\"Main Content\") footer_region.show(color=\"red\", label=\"Footer\")  # Extract content from each region document_parts = {     \"header\": header_region.extract_text(),     \"main\": main_region.extract_text()[:100] + \"...\",     \"footer\": footer_region.extract_text() }  # Show what we extracted document_parts Out[9]: <pre>{'header': 'Jungle Health and Safety Inspection Service\\nINS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.',\n 'main': 'ruary 3, 1905\\nCount: 7\\nWorst of any, however, were the fertilizer men, and those who served in the c...',\n 'footer': 'Jungle Health and Safety Inspection Service'}</pre> In\u00a0[10]: Copied! <pre># Find a region of interest\ntable_header = page.find('text:contains(\"Statute\")')\ntable_region = table_header.below(height=100)\n\n# Visualize the region\ntable_region.show(color=\"purple\", label=\"Table Region\")\n\n# Create an image of just this region\ntable_region.show(resolution=150)\n</pre> # Find a region of interest table_header = page.find('text:contains(\"Statute\")') table_region = table_header.below(height=100)  # Visualize the region table_region.show(color=\"purple\", label=\"Table Region\")  # Create an image of just this region table_region.show(resolution=150) Out[10]: <p>Regions allow you to precisely target specific parts of a document for extraction and analysis. They're essential for handling complex document layouts and isolating the exact content you need.</p>"},{"location":"tutorials/07-working-with-regions/#working-with-regions","title":"Working with Regions\u00b6","text":"<p>Regions are rectangular areas on a page that let you focus on specific parts of a document. They're perfect for extracting text from defined areas, finding elements within certain boundaries, and working with document sections.</p>"},{"location":"tutorials/07-working-with-regions/#creating-regions-from-elements","title":"Creating Regions from Elements\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#finding-elements-within-regions","title":"Finding Elements Within Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#expanding-and-adjusting-regions","title":"Expanding and Adjusting Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-bounded-regions","title":"Creating Bounded Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#working-with-multiple-regions","title":"Working with Multiple Regions\u00b6","text":""},{"location":"tutorials/07-working-with-regions/#creating-an-image-of-a-region","title":"Creating an Image of a Region\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/","title":"Spatial Navigation","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find the title of the document\ntitle = page.find('text:contains(\"Jungle Health\")')\n\n# Visualize our starting point\ntitle.show(color=\"red\", label=\"Document Title\")\n\n# Display the title text\ntitle.text\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find the title of the document title = page.find('text:contains(\"Jungle Health\")')  # Visualize our starting point title.show(color=\"red\", label=\"Document Title\")  # Display the title text title.text Out[2]: <pre>'Jungle Health and Safety Inspection Service'</pre> In\u00a0[3]: Copied! <pre># Create a region below the title\nregion_below = title.below(height=100)\n\n# Visualize the region\nregion_below.show(color=\"blue\", label=\"Below Title\")\n\n# Find and extract text from this region\ntext_below = region_below.extract_text()\ntext_below\n</pre> # Create a region below the title region_below = title.below(height=100)  # Visualize the region region_below.show(color=\"blue\", label=\"Below Title\")  # Find and extract text from this region text_below = region_below.extract_text() text_below Out[3]: <pre>'INS-UP70N51NCL41R\\nSite: Durham\u2019s Meatpacking Chicago, Ill.\\nDate: February 3, 1905\\nViolation Count: 7'</pre> In\u00a0[4]: Copied! <pre># Find two labels to serve as boundaries\nsite_label = page.find('text:contains(\"Site:\")')\ndate_label = page.find('text:contains(\"Date:\")')\n\n# Get the region between these labels\nbetween_region = site_label.below(\n    include_source=True,     # Include starting element\n    until='text:contains(\"Date:\")',  # Stop at this element\n    include_endpoint=False    # Don't include ending element\n)\n\n# Visualize the region between labels\nbetween_region.show(color=\"green\", label=\"Between\")\n\n# Extract text from this bounded area\nbetween_region.extract_text()\n</pre> # Find two labels to serve as boundaries site_label = page.find('text:contains(\"Site:\")') date_label = page.find('text:contains(\"Date:\")')  # Get the region between these labels between_region = site_label.below(     include_source=True,     # Include starting element     until='text:contains(\"Date:\")',  # Stop at this element     include_endpoint=False    # Don't include ending element )  # Visualize the region between labels between_region.show(color=\"green\", label=\"Between\")  # Extract text from this bounded area between_region.extract_text() Out[4]: <pre>'Site: Durham\u2019s Meatpacking Chicago, Ill.'</pre> In\u00a0[5]: Copied! <pre># Find a field label\nsite_label = page.find('text:contains(\"Site:\")')\n\n# Get the content to the right (the field value)\nvalue_region = site_label.right(width=200)\n\n# Visualize the label and value regions\nsite_label.show(color=\"red\", label=\"Label\")\nvalue_region.show(color=\"blue\", label=\"Value\")\n\n# Extract just the value text\nvalue_region.extract_text()\n</pre> # Find a field label site_label = page.find('text:contains(\"Site:\")')  # Get the content to the right (the field value) value_region = site_label.right(width=200)  # Visualize the label and value regions site_label.show(color=\"red\", label=\"Label\") value_region.show(color=\"blue\", label=\"Value\")  # Extract just the value text value_region.extract_text() Out[5]: <pre>'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt'</pre> In\u00a0[6]: Copied! <pre># Start with a label element\nlabel = page.find('text:contains(\"Site:\")')\n\n# Find the next and previous elements in reading order\nnext_elem = label.next()\nprev_elem = label.prev()\n\n# Visualize all three elements\nlabel.show(color=\"red\", label=\"Current\")\nnext_elem.show(color=\"green\", label=\"Next\") if next_elem else None\nprev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None\n\n# Show the text of adjacent elements\n{\n    \"current\": label.text,\n    \"next\": next_elem.text if next_elem else \"None\",\n    \"previous\": prev_elem.text if prev_elem else \"None\"\n}\n</pre> # Start with a label element label = page.find('text:contains(\"Site:\")')  # Find the next and previous elements in reading order next_elem = label.next() prev_elem = label.prev()  # Visualize all three elements label.show(color=\"red\", label=\"Current\") next_elem.show(color=\"green\", label=\"Next\") if next_elem else None prev_elem.show(color=\"blue\", label=\"Previous\") if prev_elem else None  # Show the text of adjacent elements {     \"current\": label.text,     \"next\": next_elem.text if next_elem else \"None\",     \"previous\": prev_elem.text if prev_elem else \"None\" } Out[6]: <pre>{'current': 'Site: ', 'next': 'i', 'previous': 'S'}</pre> In\u00a0[7]: Copied! <pre># Find a section label\nsummary = page.find('text:contains(\"Summary:\")')\n\n# Find the next bold text element\nnext_bold = summary.next('text:bold', limit=20)\n\n# Find the nearest line element\nnearest_line = summary.nearest('line')\n\n# Visualize what we found\nsummary.show(color=\"red\", label=\"Summary\")\nnext_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None\nnearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None\n\n# Show the content we found\n{\n    \"summary\": summary.text,\n    \"next_bold\": next_bold.text if next_bold else \"None found\",\n    \"nearest_line\": nearest_line if nearest_line else \"None found\"\n}\n</pre> # Find a section label summary = page.find('text:contains(\"Summary:\")')  # Find the next bold text element next_bold = summary.next('text:bold', limit=20)  # Find the nearest line element nearest_line = summary.nearest('line')  # Visualize what we found summary.show(color=\"red\", label=\"Summary\") next_bold.show(color=\"blue\", label=\"Next Bold\") if next_bold else None nearest_line.show(color=\"green\", label=\"Nearest Line\") if nearest_line else None  # Show the content we found {     \"summary\": summary.text,     \"next_bold\": next_bold.text if next_bold else \"None found\",     \"nearest_line\": nearest_line if nearest_line else \"None found\" } Out[7]: <pre>{'summary': 'Summary: ',\n 'next_bold': 'u',\n 'nearest_line': &lt;LineElement type=horizontal width=2.0 bbox=(50.0, 352.0, 550.0, 352.0)&gt;}</pre> In\u00a0[8]: Copied! <pre># Find a table heading\ntable_heading = page.find('text:contains(\"Statute\")')\ntable_heading.show(color=\"purple\", label=\"Table Header\")\n\n# Extract table rows using spatial navigation\nrows = []\ncurrent = table_heading\n\n# Get the next 4 rows\nfor i in range(4):\n    # Find the next row below the current one\n    next_row = current.below(height=15)\n\n    if next_row:\n        rows.append(next_row)\n        current = next_row  # Move to the next row\n    else:\n        break\n\n# Visualize all found rows\nwith page.highlights() as h:\n    for i, row in enumerate(rows):\n        h.add(row, label=f\"Row {i+1}\")\n    h.show()\n</pre> # Find a table heading table_heading = page.find('text:contains(\"Statute\")') table_heading.show(color=\"purple\", label=\"Table Header\")  # Extract table rows using spatial navigation rows = [] current = table_heading  # Get the next 4 rows for i in range(4):     # Find the next row below the current one     next_row = current.below(height=15)      if next_row:         rows.append(next_row)         current = next_row  # Move to the next row     else:         break  # Visualize all found rows with page.highlights() as h:     for i, row in enumerate(rows):         h.add(row, label=f\"Row {i+1}\")     h.show() In\u00a0[9]: Copied! <pre># Extract text from each row\n[row.extract_text() for row in rows]\n</pre> # Extract text from each row [row.extract_text() for row in rows] Out[9]: <pre>['4.12.7 Unsanitary Working Conditions. Critical',\n '4.12.7 Unsanitary Working Conditions. Critical\\n5.8.3 Inadequate Protective Equipment. Serious',\n '5.8.3 Inadequate Protective Equipment. Serious',\n '6.3.9 Ineffective Injury Prevention. Serious']</pre> In\u00a0[10]: Copied! <pre># Find all potential field labels (text with a colon)\nlabels = page.find_all('text:contains(\":\")')\n\n# Visualize the labels\nlabels.show(color=\"blue\", label=\"Labels\")\n\n# Extract key-value pairs\nfield_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    key = label.text.strip().rstrip(':')\n\n    # Skip if not a proper label\n    if not key:\n        continue\n\n    # Get the value to the right\n    value = label.right(width=200).extract_text().strip()\n\n    # Add to our collection\n    field_data[key] = value\n\n# Show the extracted data\nfield_data\n</pre> # Find all potential field labels (text with a colon) labels = page.find_all('text:contains(\":\")')  # Visualize the labels labels.show(color=\"blue\", label=\"Labels\")  # Extract key-value pairs field_data = {}  for label in labels:     # Clean up the label text     key = label.text.strip().rstrip(':')      # Skip if not a proper label     if not key:         continue      # Get the value to the right     value = label.right(width=200).extract_text().strip()      # Add to our collection     field_data[key] = value  # Show the extracted data field_data Out[10]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> <p>Spatial navigation mimics how humans read documents, letting you navigate content based on physical relationships between elements. It's especially useful for extracting structured data from forms, tables, and formatted documents.</p> In\u00a0[11]: Copied! <pre># Step 1 \u2013 find the heading text\nheading = page.find('text:contains(\"Summary:\")')\n\n# Step 2 \u2013 get the first bold word after that heading (skip up to 30 elements)\nvalue_label = heading.next('text:bold', limit=30)\n\n# Step 3 \u2013 grab the value region to the right of that bold word\nvalue_region = value_label.right(until='line')  # Extend until the boundary line\n\nvalue_region.show(color=\"orange\", label=\"Summary Value\")\nvalue_region.extract_text()\n</pre> # Step 1 \u2013 find the heading text heading = page.find('text:contains(\"Summary:\")')  # Step 2 \u2013 get the first bold word after that heading (skip up to 30 elements) value_label = heading.next('text:bold', limit=30)  # Step 3 \u2013 grab the value region to the right of that bold word value_region = value_label.right(until='line')  # Extend until the boundary line  value_region.show(color=\"orange\", label=\"Summary Value\") value_region.extract_text() Out[11]: <pre>'e: Durha\\nte: Febr\\nolation C\\nmmary:\\nese peop\\nitor at a h\\nme of wh\\no the vats\\nhibiting -\\nthe world\\nolation\\ntatute\\n12.7\\n8.3\\n3.9\\n1.5\\n9.2\\n6.4\\n0.2.7'</pre> In\u00a0[12]: Copied! <pre>inspection_date_value = (\n    page.find('text:startswith(\"Date:\")')\n        .right(width=500, height='element')            # Move right to get the date value region\n        .find('text')                # Narrow to text elements only\n)\n</pre> inspection_date_value = (     page.find('text:startswith(\"Date:\")')         .right(width=500, height='element')            # Move right to get the date value region         .find('text')                # Narrow to text elements only ) <p>Because each call returns an element, you never lose the spatial context \u2013 you can always add another <code>.below()</code> or <code>.nearest()</code> later.</p>"},{"location":"tutorials/08-spatial-navigation/#spatial-navigation","title":"Spatial Navigation\u00b6","text":"<p>Spatial navigation lets you work with PDF content based on the physical layout of elements on the page. It's perfect for finding elements relative to each other and extracting information in context.</p>"},{"location":"tutorials/08-spatial-navigation/#finding-elements-above-and-below","title":"Finding Elements Above and Below\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-content-between-elements","title":"Finding Content Between Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#navigating-left-and-right","title":"Navigating Left and Right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#finding-adjacent-elements","title":"Finding Adjacent Elements\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#combining-with-element-selectors","title":"Combining with Element Selectors\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-table-rows-with-spatial-navigation","title":"Extracting Table Rows with Spatial Navigation\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#extracting-key-value-pairs","title":"Extracting Key-Value Pairs\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#todo","title":"TODO\u00b6","text":"<ul> <li>Add examples for navigating across multiple pages using <code>pdf.pages</code> slicing and <code>below(..., until=...)</code> that spans pages.</li> <li>Show how to chain selectors, e.g., <code>page.find('text:bold').below().right()</code> for complex paths.</li> <li>Include a sidebar on performance when many spatial calls are chained and how to cache intermediate regions.</li> <li>Add examples using <code>.until()</code> for one-liner \"from here until X\" extractions.</li> <li>Show using <code>width=\"element\"</code> vs <code>\"full\"</code> in <code>.below()</code> and <code>.above()</code> to restrict horizontal span.</li> <li>Demonstrate attribute selectors (e.g., <code>line[width&gt;2]</code>) and <code>:not()</code> pseudo-class for exclusion in spatial chains.</li> <li>Briefly introduce <code>.expand()</code> for fine-tuning region size after spatial selection.</li> </ul>"},{"location":"tutorials/08-spatial-navigation/#chaining-spatial-calls","title":"Chaining Spatial Calls\u00b6","text":"<p>Spatial helpers like <code>.below()</code>, <code>.right()</code>, <code>.nearest()</code> and friends return Element or Region objects, so you can keep chaining operations just like you would with jQuery or BeautifulSoup.</p> <ol> <li>Start with a selector (string or Element).</li> <li>Apply a spatial function.</li> <li>Optionally, add another selector to narrow the result.</li> <li>Repeat!</li> </ol>"},{"location":"tutorials/08-spatial-navigation/#example-1-heading-next-bold-word-value-to-its-right","title":"Example 1 \u2013 Heading \u2192 next bold word \u2192 value to its right\u00b6","text":""},{"location":"tutorials/08-spatial-navigation/#example-2-find-a-label-anywhere-on-the-document-and-walk-to-its-value-in-one-chain","title":"Example 2 \u2013 Find a label anywhere on the document and walk to its value in one chain\u00b6","text":""},{"location":"tutorials/09-section-extraction/","title":"Section Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install natural-pdf\n</pre> #%pip install natural-pdf In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load the PDF using the relative path\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\n# Identify horizontal rules that look like section dividers\nhorizontal_lines = page.find_all('line:horizontal')\n\n# Visualize the potential section boundaries (single element type \u279c use .show())\nhorizontal_lines.show(color=\"red\", label=\"Section Boundaries\")\npage.show()\n</pre> from natural_pdf import PDF  # Load the PDF using the relative path pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  # Identify horizontal rules that look like section dividers horizontal_lines = page.find_all('line:horizontal')  # Visualize the potential section boundaries (single element type \u279c use .show()) horizontal_lines.show(color=\"red\", label=\"Section Boundaries\") page.show() Out[2]: In\u00a0[3]: Copied! <pre># Count what we found\nlen(horizontal_lines)\n</pre> # Count what we found len(horizontal_lines) Out[3]: <pre>9</pre> In\u00a0[4]: Copied! <pre># Extract sections based on horizontal lines\n# Each section starts at a horizontal line and ends at the next one\nbook_sections = page.get_sections(\n    start_elements=horizontal_lines,\n    include_boundaries='start'  # Include the boundary in the section\n)\n\n# Visualize each section\npage.clear_highlights()\nfor section in book_sections:\n    section.show()\npage.show()\n</pre> # Extract sections based on horizontal lines # Each section starts at a horizontal line and ends at the next one book_sections = page.get_sections(     start_elements=horizontal_lines,     include_boundaries='start'  # Include the boundary in the section )  # Visualize each section page.clear_highlights() for section in book_sections:     section.show() page.show() Out[4]: In\u00a0[5]: Copied! <pre># Display section count and preview the first section\n{\n    \"total_sections\": len(book_sections),\n    \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\"\n}\n</pre> # Display section count and preview the first section {     \"total_sections\": len(book_sections),     \"first_section_text\": book_sections[0].extract_text()[:100] + \"...\" if book_sections else \"No sections found\" } Out[5]: <pre>{'total_sections': 9,\n 'first_section_text': '6/12/2023 - Copies Removed: 2\\nTristan Strong punches a hole in the sky (Removed: 1)\\nAuthor: Mbalia, ...'}</pre> In\u00a0[6]: Copied! <pre># Extract and display content from the first few book entries\nbook_entries = []\n\nfor i, section in enumerate(book_sections[:5]):\n    # Extract the section text\n    text = section.extract_text().strip()\n\n    # Try to parse book information\n    title = \"\"\n    author = \"\"\n    isbn = \"\"\n\n    # Extract title (typically the first line)\n    title_match = section.find('text:contains(\"Title:\")')\n    if title_match:\n        title_value = title_match.right(width=400).extract_text()\n        title = title_value.strip()\n\n    # Extract author\n    author_match = section.find('text:contains(\"Author:\")')\n    if author_match:\n        author_value = author_match.right(width=400).extract_text()\n        author = author_value.strip()\n\n    # Extract ISBN\n    isbn_match = section.find('text:contains(\"ISBN:\")')\n    if isbn_match:\n        isbn_value = isbn_match.right(width=400).extract_text()\n        isbn = isbn_value.strip()\n\n    # Add to our collection\n    book_entries.append({\n        \"number\": i + 1,\n        \"title\": title,\n        \"author\": author,\n        \"isbn\": isbn,\n        \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text\n    })\n\n# Display the structured book entries\nimport pandas as pd\npd.DataFrame(book_entries)\n</pre> # Extract and display content from the first few book entries book_entries = []  for i, section in enumerate(book_sections[:5]):     # Extract the section text     text = section.extract_text().strip()      # Try to parse book information     title = \"\"     author = \"\"     isbn = \"\"      # Extract title (typically the first line)     title_match = section.find('text:contains(\"Title:\")')     if title_match:         title_value = title_match.right(width=400).extract_text()         title = title_value.strip()      # Extract author     author_match = section.find('text:contains(\"Author:\")')     if author_match:         author_value = author_match.right(width=400).extract_text()         author = author_value.strip()      # Extract ISBN     isbn_match = section.find('text:contains(\"ISBN:\")')     if isbn_match:         isbn_value = isbn_match.right(width=400).extract_text()         isbn = isbn_value.strip()      # Add to our collection     book_entries.append({         \"number\": i + 1,         \"title\": title,         \"author\": author,         \"isbn\": isbn,         \"preview\": text[:50] + \"...\" if len(text) &gt; 50 else text     })  # Display the structured book entries import pandas as pd pd.DataFrame(book_entries) Out[6]: number title author isbn preview 0 1 Log Atlanta Public S\\n023\\nemoved: 2\\na hole i... Atlanta Public Schools\\nPublished: 2019\\nAcqui... 6/12/2023 - Copies Removed: 2\\nTristan Strong ... 1 2 6/7/2023 - Copies Removed: 2 2 3 Atlanta Public School\\nved: 2\\nin the sky (Rem... Atlanta Public Schools\\n93-2 Published: 2019\\n... Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Ba... 3 4 Atlanta Public Schools\\nd: 2\\nn the sky (Remov... Atlanta Public Schools\\nPublished: 2019\\nAcqui... Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book ... 4 5 6/6/2023 - Copies Removed: 130 In\u00a0[7]: Copied! <pre>page.viewer()\n</pre> page.viewer() Zoom In (+) Zoom Out (-) Reset Element Info <pre></pre> Out[7]: <pre>InteractiveViewerWidget()</pre> In\u00a0[8]: Copied! <pre># Find title elements with specific selectors\ntitle_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\ntitle_elements.show()\n</pre> # Find title elements with specific selectors title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]') title_elements.show() Out[8]: In\u00a0[9]: Copied! <pre># Extract sections starting from each title\n# This now directly returns an ElementCollection\ntitle_sections = page.get_sections(\n    start_elements=title_elements,\n    include_boundaries='start'\n)\n\n# Show the title-based sections\npage.clear_highlights()\ntitle_sections.show()\npage.show()\n</pre> # Extract sections starting from each title # This now directly returns an ElementCollection title_sections = page.get_sections(     start_elements=title_elements,     include_boundaries='start' )  # Show the title-based sections page.clear_highlights() title_sections.show() page.show() Out[9]: In\u00a0[10]: Copied! <pre># Count the sections found\nlen(title_sections)\n</pre> # Count the sections found len(title_sections) Out[10]: <pre>7</pre> In\u00a0[11]: Copied! <pre># Use horizontal line elements as section dividers\ndividers = page.find_all('line:horizontal')\n\n# Compare the different boundary inclusion options\ninclusion_options = {\n    'none': page.get_sections(start_elements=dividers, include_boundaries='none'),\n    'start': page.get_sections(start_elements=dividers, include_boundaries='start'),\n    'end': page.get_sections(start_elements=dividers, include_boundaries='end'),\n    'both': page.get_sections(start_elements=dividers, include_boundaries='both')\n}\n\n# Count sections with each option\nsection_counts = {option: len(sections) for option, sections in inclusion_options.items()}\nsection_counts\n</pre> # Use horizontal line elements as section dividers dividers = page.find_all('line:horizontal')  # Compare the different boundary inclusion options inclusion_options = {     'none': page.get_sections(start_elements=dividers, include_boundaries='none'),     'start': page.get_sections(start_elements=dividers, include_boundaries='start'),     'end': page.get_sections(start_elements=dividers, include_boundaries='end'),     'both': page.get_sections(start_elements=dividers, include_boundaries='both') }  # Count sections with each option section_counts = {option: len(sections) for option, sections in inclusion_options.items()} section_counts Out[11]: <pre>{'none': 9, 'start': 9, 'end': 9, 'both': 9}</pre> In\u00a0[12]: Copied! <pre># Define specific start and end points - let's extract just one book entry\n# We'll look for the first and second horizontal lines\npage.clear_highlights()\n\nstart_point = title_elements[0]\nend_point = title_elements[1]\n\n# Extract the section between these points\nsingle_book_entry = page.get_sections(\n    start_elements=[start_point],\n    end_elements=[end_point],\n    include_boundaries='start'  # Include the start but not the end\n)\n\n# Visualize the custom section\nsingle_book_entry.show(color=\"green\", label=\"Single Book Entry\")\n\nprint(single_book_entry[0].extract_text())\n\npage.show()\n</pre> # Define specific start and end points - let's extract just one book entry # We'll look for the first and second horizontal lines page.clear_highlights()  start_point = title_elements[0] end_point = title_elements[1]  # Extract the section between these points single_book_entry = page.get_sections(     start_elements=[start_point],     end_elements=[end_point],     include_boundaries='start'  # Include the start but not the end )  # Visualize the custom section single_book_entry.show(color=\"green\", label=\"Single Book Entry\")  print(single_book_entry[0].extract_text())  page.show() <pre>Tristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-2 Published: 2019\nSite Barcode Price Acquired Removed By\nJoseph Humphries 32441014018707 6/11/2021 113396-42441\nElementary School\nWas Available -- Weeded\nUpside down in the middle of nowhere (Removed: 1)\n</pre> Out[12]: In\u00a0[13]: Copied! <pre># Get sections across the first two pages\nmulti_page_sections = [] # Initialize as a list\n\nfor page_num in range(min(2, len(pdf.pages))):\n    page = pdf.pages[page_num]\n\n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n\n    # Get sections for this page (returns ElementCollection)\n    page_sections = page.get_sections(\n        start_elements=title_elements,\n        include_boundaries='start'\n    )\n\n    # Add elements from the collection to our list\n    multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection\n\n# Display info about each section (showing first 3)\n[{\n    \"page\": section.page.number + 1,  # 1-indexed page number for display\n    \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text()\n} for section in multi_page_sections]\n</pre> # Get sections across the first two pages multi_page_sections = [] # Initialize as a list  for page_num in range(min(2, len(pdf.pages))):     page = pdf.pages[page_num]      # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')      # Get sections for this page (returns ElementCollection)     page_sections = page.get_sections(         start_elements=title_elements,         include_boundaries='start'     )      # Add elements from the collection to our list     multi_page_sections.extend(page_sections) # list.extend works with iterables like ElementCollection  # Display info about each section (showing first 3) [{     \"page\": section.page.number + 1,  # 1-indexed page number for display     \"text\": section.extract_text()[:50] + \"...\" if len(section.extract_text()) &gt; 50 else section.extract_text() } for section in multi_page_sections] Out[13]: <pre>[{'page': 2, 'text': 'Tristan Strong punches a hole in the sky (Removed:...'},\n {'page': 2, 'text': 'Upside down in the middle of nowhere (Removed: 1)\\n...'},\n {'page': 2, 'text': 'Buddhism (Removed: 1)\\nAuthor: Wangu, Madhu Bazaz. ...'},\n {'page': 2, 'text': 'Voodoo (Removed: 1)\\nAuthor: Kelly Wand, book edito...'},\n {'page': 2, 'text': 'The Abenaki (Removed: 1)\\nAuthor: Landau, Elaine. I...'},\n {'page': 2, 'text': 'Afghanistan (Removed: 1)\\nAuthor: Milivojevic, Jova...'},\n {'page': 2, 'text': 'Alexander the Great rocks the world (Removed: 1)\\nA...'},\n {'page': 3, 'text': 'The Anasazi (Removed: 1)\\nAuthor: Petersen, David. ...'},\n {'page': 3, 'text': 'And then what happened, Paul Revere? (Removed: 1)\\n...'},\n {'page': 3, 'text': 'The assassination of Martin Luther King Jr (Remove...'},\n {'page': 3, 'text': 'Barbara Jordan. (Removed: 1)\\nAuthor: Wexler, Diane...'},\n {'page': 3, 'text': 'Bedtime for Batman (Removed: 1)\\nAuthor: Dahl, Mich...'},\n {'page': 3, 'text': 'Benjamin O. Davis, Jr : Air Force general &amp; Tuskeg...'},\n {'page': 3, 'text': 'Bigfoot Wallace (Removed: 1)\\nAuthor: Harper,Jo. IS...'},\n {'page': 3, 'text': 'The blaze engulfs : January 1939 to December 1941 ...'}]</pre> In\u00a0[14]: Copied! <pre># Extract all book entries across multiple pages\nbook_database = []\n\n# Process first 3 pages (or fewer if the document is shorter)\nfor page_num in range(min(3, len(pdf.pages))):\n    page = pdf.pages[page_num]\n\n    # Find horizontal lines on this page\n    title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')\n\n    # Get sections for this page\n    book_sections = page.get_sections(\n        start_elements=title_elements,\n        include_boundaries='start'\n    )\n\n    # Process each book section\n    for section in book_sections:\n        # Skip sections that are too short (might be headers/footers)\n        if len(section.extract_text()) &lt; 50:\n            continue\n\n        # Extract book information\n        book_info = {\"page\": page_num + 1}\n\n        for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.strip(':').lower()\n                field_value = field_element.extract_text().replace(field, '').strip()\n                book_info[field_name] = field_value\n\n        # Below the field name\n        for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:\n            field_element = section.find(f'text:contains(\"{field}\")')\n            if field_element:\n                field_name = field.lower()\n                field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()\n                book_info[field_name] = field_value\n\n        book_database.append(book_info)\n\n# Display sample entries (first 3)\nimport pandas as pd\n\ndf = pd.json_normalize(book_database)\ndf.head()\n</pre> # Extract all book entries across multiple pages book_database = []  # Process first 3 pages (or fewer if the document is shorter) for page_num in range(min(3, len(pdf.pages))):     page = pdf.pages[page_num]      # Find horizontal lines on this page     title_elements = page.find('line[width=2]').below().find_all('text[fontname=\"AAAAAB+font000000002a8d158a\"][size=10]')      # Get sections for this page     book_sections = page.get_sections(         start_elements=title_elements,         include_boundaries='start'     )      # Process each book section     for section in book_sections:         # Skip sections that are too short (might be headers/footers)         if len(section.extract_text()) &lt; 50:             continue          # Extract book information         book_info = {\"page\": page_num + 1}          for field in [\"Title:\", \"Author:\", \"ISBN:\", \"Publisher:\", \"Copyright:\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.strip(':').lower()                 field_value = field_element.extract_text().replace(field, '').strip()                 book_info[field_name] = field_value          # Below the field name         for field in [\"Price\", \"Acquired\", \"Barcode\", \"Removed By\"]:             field_element = section.find(f'text:contains(\"{field}\")')             if field_element:                 field_name = field.lower()                 field_value = field_element.below(height=10, width='element').expand(right=50).extract_text().strip()                 book_info[field_name] = field_value          book_database.append(book_info)  # Display sample entries (first 3) import pandas as pd  df = pd.json_normalize(book_database) df.head() Out[14]: page author isbn price acquired barcode removed by 0 1 Mbalia, Kwame. 978-1-36803993-2 6/11/2021 11 32441014018707 113396-42441 1 1 Lamana, Julie T. 978-1-45212456-8 (alk. $15.00 6/12/2023 11 32441012580849 113396-42441 2 1 Wangu, Madhu Bazaz. 0-8160-2442-1 $10.00 4/19/2018 ch 33343000017835 christen.mcclain 3 1 Kelly Wand, book editor. 0-7377-1314-3 (lib.) $19.95 3/21/2006 ch *3431000028742 christen.mcclain 4 1 Landau, Elaine. 0-531-20227-5 $16.50 2/21/2000 33 33170000506628 33554-43170 <p>Section extraction lets you break down documents into logical parts, making it easier to generate summaries, extract specific content, and create structured data from semi-structured documents. In this example, we've shown how to convert a PDF library catalog into a structured book database.</p>"},{"location":"tutorials/09-section-extraction/#section-extraction","title":"Section Extraction\u00b6","text":"<p>Documents are often organized into logical sections like chapters, articles, or content blocks. This tutorial shows how to extract these sections using natural-pdf, using a library weeding log as an example.</p>"},{"location":"tutorials/09-section-extraction/#basic-section-extraction","title":"Basic Section Extraction\u00b6","text":""},{"location":"tutorials/09-section-extraction/#working-with-section-content","title":"Working with Section Content\u00b6","text":""},{"location":"tutorials/09-section-extraction/#using-different-section-boundaries","title":"Using Different Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#section-boundary-inclusion-options","title":"Section Boundary Inclusion Options\u00b6","text":""},{"location":"tutorials/09-section-extraction/#custom-section-boundaries","title":"Custom Section Boundaries\u00b6","text":""},{"location":"tutorials/09-section-extraction/#multi-page-sections","title":"Multi-page Sections\u00b6","text":""},{"location":"tutorials/09-section-extraction/#building-a-book-database","title":"Building a Book Database\u00b6","text":""},{"location":"tutorials/09-section-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Demonstrate using <code>page.init_search()</code> to pre-index all pages and retrieve section headings quickly.</li> <li>Add an example that merges multi-page sections by passing <code>new_section_on_page_break=False</code>.</li> <li>Include tips for detecting numbered headings (\"1.\", \"2.\") when ruling lines are absent.</li> <li>Provide a performance note on large PDFs and how to stream through pages lazily.</li> </ul>"},{"location":"tutorials/10-form-field-extraction/","title":"Form Field Extraction","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"\n</pre> #%pip install \"natural-pdf[ai]\" <p>If you already have the core library, simply run <code>npdf install ai</code> to add the extra ML packages.</p> In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find fields with labels ending in colon\nlabels = page.find_all('text:contains(\":\")')\n\n# Visualize the found labels\nlabels.show(color=\"blue\", label=\"Field Labels\")\n\n# Count how many potential fields we found\nlen(labels)\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find fields with labels ending in colon labels = page.find_all('text:contains(\":\")')  # Visualize the found labels labels.show(color=\"blue\", label=\"Field Labels\")  # Count how many potential fields we found len(labels) Out[2]: <pre>4</pre> In\u00a0[3]: Copied! <pre># Extract the value for each field label\nform_data = {}\n\nfor label in labels:\n    # Clean up the label text\n    field_name = label.text.strip().rstrip(':')\n\n    # Find the value to the right of the label\n    value_region = label.right(width=200)\n    value = value_region.extract_text().strip()\n\n    # Store in our dictionary\n    form_data[field_name] = value\n\n# Display the extracted data\nform_data\n</pre> # Extract the value for each field label form_data = {}  for label in labels:     # Clean up the label text     field_name = label.text.strip().rstrip(':')      # Find the value to the right of the label     value_region = label.right(width=200)     value = value_region.extract_text().strip()      # Store in our dictionary     form_data[field_name] = value  # Display the extracted data form_data Out[3]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> In\u00a0[4]: Copied! <pre># Clear previous highlights\npage.clear_highlights()\n\n# Highlight both labels and their values\nfor label in labels:\n    # Highlight the label in red\n    label.show(color=\"red\", label=\"Label\")\n\n    # Highlight the value area in blue\n    label.right(width=200).show(color=\"blue\", label=\"Value\")\n\n# Show the page image with highlighted elements\npage.show()\n</pre> # Clear previous highlights page.clear_highlights()  # Highlight both labels and their values for label in labels:     # Highlight the label in red     label.show(color=\"red\", label=\"Label\")      # Highlight the value area in blue     label.right(width=200).show(color=\"blue\", label=\"Value\")  # Show the page image with highlighted elements page.show() Out[4]: In\u00a0[5]: Copied! <pre># Extract values that might span multiple lines\nmulti_line_data = {}\n\nfor label in labels:\n    # Get the field name\n    field_name = label.text.strip().rstrip(':')\n\n    # Look both to the right and below\n    right_value = label.right(width=200).extract_text().strip()\n    below_value = label.below(height=50).extract_text().strip()\n\n    # Combine the values if they're different\n    if right_value in below_value:\n        value = below_value\n    else:\n        value = f\"{right_value} {below_value}\".strip()\n\n    # Add to results\n    multi_line_data[field_name] = value\n\n# Show fields with potential multi-line values\nmulti_line_data\n</pre> # Extract values that might span multiple lines multi_line_data = {}  for label in labels:     # Get the field name     field_name = label.text.strip().rstrip(':')      # Look both to the right and below     right_value = label.right(width=200).extract_text().strip()     below_value = label.below(height=50).extract_text().strip()      # Combine the values if they're different     if right_value in below_value:         value = below_value     else:         value = f\"{right_value} {below_value}\".strip()      # Add to results     multi_line_data[field_name] = value  # Show fields with potential multi-line values multi_line_data Out[5]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt Date: February 3, 1905\\nViolation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health Violation Count: 7\\nSummary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins Summary: Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.\\nThese people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary\\nvisitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in\\nsome of which there were open vats near the level of the floor, their peculiar trouble was that they fell'}</pre> In\u00a0[6]: Copied! <pre>import re\n\n# Find dates in the format July 31, YYY\ndate_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'\n\n# Search all text elements for dates\ntext_elements = page.find_all('text')\nprint([elem.text for elem in text_elements])\ndates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))\n\n# Visualize the date fields\ndates.show(color=\"green\", label=\"Date\")\n\n# Extract just the date values\ndate_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates]\ndate_texts\n</pre> import re  # Find dates in the format July 31, YYY date_pattern = r'\\b\\w+ \\d+, \\d\\d\\d\\d\\b'  # Search all text elements for dates text_elements = page.find_all('text') print([elem.text for elem in text_elements]) dates = text_elements.filter(lambda elem: re.search(date_pattern, elem.text))  # Visualize the date fields dates.show(color=\"green\", label=\"Date\")  # Extract just the date values date_texts = [re.search(date_pattern, elem.text).group(0) for elem in dates] date_texts <pre>['Jungle Health and Safety Inspection Service', 'INS-UP70N51NCL41R', 'Site: ', 'Durham\u2019s Meatpacking  ', 'Chicago, Ill.', 'Date:  ', 'February 3, 1905', 'Violation Count: ', '7', 'Summary: ', 'Worst of any, however, were the fertilizer men, and those who served in the cooking rooms.', 'These people could not be shown to the visitor - for the odor of a fertilizer man would scare any ordinary ', 'visitor at a hundred yards, and as for the other men, who worked in tank rooms full of steam, and in ', 'some of which there were open vats near the level of the floor, their peculiar trouble was that they fell', 'into the vats; and when they were fished out, there was never enough of them left to be worth ', 'exhibiting - sometimes they would be overlooked for days, till all but the bones of them had gone out', 'to the world as Durham\u2019s Pure Leaf Lard!', 'Violations', 'Statute', 'Description', 'Level', 'Repeat?', '4.12.7', 'Unsanitary Working Conditions.', 'Critical', '5.8.3', 'Inadequate Protective Equipment.', 'Serious', '6.3.9', 'Ineffective Injury Prevention.', 'Serious', '7.1.5', 'Failure to Properly Store Hazardous Materials.', 'Critical', '8.9.2', 'Lack of Adequate Fire Safety Measures.', 'Serious', '9.6.4', 'Inadequate Ventilation Systems.', 'Serious', '10.2.7', 'Insufficient Employee Training for Safe Work Practices.', 'Serious', 'Jungle Health and Safety Inspection Service']\n</pre> Out[6]: <pre>['February 3, 1905']</pre> In\u00a0[7]: Copied! <pre># Run layout analysis to find table structures\npage.analyze_layout()\n\n# Find possible form tables\ntables = page.find_all('region[type=table]')\n\nif tables:\n    # Visualize the tables\n    tables.show(color=\"purple\", label=\"Form Table\")\n\n    # Extract data from the first table\n    first_table = tables[0]\n    table_data = first_table.extract_table()\n    table_data\nelse:\n    # Try to find form-like structure using text alignment\n    # Create a region where a form might be\n    form_region = page.create_region(50, 200, page.width - 50, 500)\n\n    # Group text by vertical position\n    rows = {}\n    text_elements = form_region.find_all('text')\n\n    for elem in text_elements:\n        # Round y-position to group elements in the same row\n        row_pos = round(elem.top / 5) * 5\n        if row_pos not in rows:\n            rows[row_pos] = []\n        rows[row_pos].append(elem)\n\n    # Extract data from rows (first 5 rows)\n    row_data = []\n    for y in sorted(rows.keys())[:5]:\n        # Sort elements by x-position (left to right)\n        elements = sorted(rows[y], key=lambda e: e.x0)\n\n        # Show the row\n        row_box = form_region.create_region(\n            min(e.x0 for e in elements),\n            min(e.top for e in elements),\n            max(e.x1 for e in elements),\n            max(e.bottom for e in elements)\n        )\n        row_box.show(color=None, use_color_cycling=True)\n\n        # Extract text from row\n        row_text = [e.text for e in elements]\n        row_data.append(row_text)\n\n    # Show the extracted rows\n    row_data\n</pre> # Run layout analysis to find table structures page.analyze_layout()  # Find possible form tables tables = page.find_all('region[type=table]')  if tables:     # Visualize the tables     tables.show(color=\"purple\", label=\"Form Table\")      # Extract data from the first table     first_table = tables[0]     table_data = first_table.extract_table()     table_data else:     # Try to find form-like structure using text alignment     # Create a region where a form might be     form_region = page.create_region(50, 200, page.width - 50, 500)      # Group text by vertical position     rows = {}     text_elements = form_region.find_all('text')      for elem in text_elements:         # Round y-position to group elements in the same row         row_pos = round(elem.top / 5) * 5         if row_pos not in rows:             rows[row_pos] = []         rows[row_pos].append(elem)      # Extract data from rows (first 5 rows)     row_data = []     for y in sorted(rows.keys())[:5]:         # Sort elements by x-position (left to right)         elements = sorted(rows[y], key=lambda e: e.x0)          # Show the row         row_box = form_region.create_region(             min(e.x0 for e in elements),             min(e.top for e in elements),             max(e.x1 for e in elements),             max(e.bottom for e in elements)         )         row_box.show(color=None, use_color_cycling=True)          # Extract text from row         row_text = [e.text for e in elements]         row_data.append(row_text)      # Show the extracted rows     row_data <pre>\n</pre> <pre>image 1/1 /var/folders/25/h3prywj14qb0mlkl2s8bxq5m0000gn/T/tmp7ns9pqit/temp_layout_image.png: 1024x800 1 title, 3 plain texts, 2 abandons, 1 table, 1856.1ms\n</pre> <pre>Speed: 6.8ms preprocess, 1856.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 800)\n</pre> In\u00a0[8]: Copied! <pre># Combine label-based and pattern-based extraction\nall_fields = {}\n\n# 1. First get fields with explicit labels\nfor label in labels:\n    field_name = label.text.strip().rstrip(':')\n    value = label.right(width=200).extract_text().strip()\n    all_fields[field_name] = value\n\n# 2. Add date fields that we found with pattern matching\nfor date_elem in dates:\n    # Find the nearest label\n    nearby_label = date_elem.nearest('text:contains(\":\")')\n\n    if nearby_label:\n        # Extract the label text\n        label_text = nearby_label.text.strip().rstrip(':')\n\n        # Get the date value\n        date_value = re.search(date_pattern, date_elem.text).group(0)\n\n        # Add to our results if not already present\n        if label_text not in all_fields:\n            all_fields[label_text] = date_value\n\n# Show all extracted fields\nall_fields\n</pre> # Combine label-based and pattern-based extraction all_fields = {}  # 1. First get fields with explicit labels for label in labels:     field_name = label.text.strip().rstrip(':')     value = label.right(width=200).extract_text().strip()     all_fields[field_name] = value  # 2. Add date fields that we found with pattern matching for date_elem in dates:     # Find the nearest label     nearby_label = date_elem.nearest('text:contains(\":\")')      if nearby_label:         # Extract the label text         label_text = nearby_label.text.strip().rstrip(':')          # Get the date value         date_value = re.search(date_pattern, date_elem.text).group(0)          # Add to our results if not already present         if label_text not in all_fields:             all_fields[label_text] = date_value  # Show all extracted fields all_fields Out[8]: <pre>{'Site': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\ntion Count: 7\\nmary: Worst of any, however, were the fertilize\\ne people could not be shown to the visitor - for\\nr at a hundred yards, and as for the other men\\nof which there were open vats near the level\\nhe vats; and when they were fished out, there\\niting - sometimes they would be overlooked fo\\nworld as Durham\u2019s Pure Leaf Lard!\\nations\\nute Description\\n.7 Unsanitary Working Conditions.\\n3 Inadequate Protective Equipment.\\n9 Ineffective Injury Prevention.\\n5 Failure to Properly Store Hazardous M\\n2 Lack of Adequate Fire Safety Measure\\n4 Inadequate Ventilation Systems.\\n.7 Insufficient Employee Training for Safe\\nJungle Healt',\n 'Date': 'Durham\u2019s Meatpacking Chicago, Ill.\\nFebruary 3, 1905\\non Count: 7\\nary: Worst of any, however, were the fertilizer\\npeople could not be shown to the visitor - for t\\nat a hundred yards, and as for the other men,\\nof which there were open vats near the level o\\ne vats; and when they were fished out, there w\\nng - sometimes they would be overlooked for\\nworld as Durham\u2019s Pure Leaf Lard!\\ntions\\nte Description\\n7 Unsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Ma\\nLack of Adequate Fire Safety Measures\\nInadequate Ventilation Systems.\\n7 Insufficient Employee Training for Safe W\\nJungle Health',\n 'Violation Count': 'eatpacking Chicago, Ill.\\n, 1905\\n7\\nof any, however, were the fertilizer men, and\\nld not be shown to the visitor - for the odor of\\nd yards, and as for the other men, who worke\\nre were open vats near the level of the floor, t\\nwhen they were fished out, there was never e\\nmes they would be overlooked for days, till all\\nrham\u2019s Pure Leaf Lard!\\nription\\nnitary Working Conditions.\\nquate Protective Equipment.\\nctive Injury Prevention.\\ne to Properly Store Hazardous Materials.\\nof Adequate Fire Safety Measures.\\nquate Ventilation Systems.\\nicient Employee Training for Safe Work Practi\\nJungle Health and Safety Ins',\n 'Summary': 'm\u2019s Meatpacking Chicago, Ill.\\nuary 3, 1905\\nount: 7\\nWorst of any, however, were the fertilizer men\\nple could not be shown to the visitor - for the o\\nhundred yards, and as for the other men, who\\nich there were open vats near the level of the\\ns; and when they were fished out, there was n\\nsometimes they would be overlooked for days\\nas Durham\u2019s Pure Leaf Lard!\\ns\\nDescription\\nUnsanitary Working Conditions.\\nInadequate Protective Equipment.\\nIneffective Injury Prevention.\\nFailure to Properly Store Hazardous Material\\nLack of Adequate Fire Safety Measures.\\nInadequate Ventilation Systems.\\nInsufficient Employee Training for Safe Work\\nJungle Health and S'}</pre> In\u00a0[9]: Copied! <pre>answer = page.ask(\"What is the invoice total?\")\n</pre> answer = page.ask(\"What is the invoice total?\") <pre>Device set to use mps\n</pre> <p><code>answer['answer']</code> is the literal text found on the page.</p> <p>For a deep dive into Question Answering\u2014including confidence tuning, batching, and answer-span highlighting\u2014see Tutorial 06: Document Question Answering.</p> <p>Form field extraction enables you to automate data entry and document processing. By combining different techniques like label detection, spatial navigation, and pattern matching, you can handle a wide variety of form layouts.</p>"},{"location":"tutorials/10-form-field-extraction/#form-field-extraction","title":"Form Field Extraction\u00b6","text":"<p>Extracting key-value pairs from documents can be tackled in two complementary ways:</p> <ul> <li>Rule-based / spatial heuristics \u2013 look for label text, navigate rightward or downward, group elements into rows, etc.</li> <li>Extractive Document QA \u2013 feed the page image and its words to a fine-tuned LayoutLM model and ask natural-language questions such as \"What is the invoice total?\". The model returns the answer span exactly as it appears in the document along with a confidence score.</li> </ul> <p>This tutorial starts with classical heuristics and then upgrades to the LayoutLM-based DocumentQA engine built into <code>natural-pdf</code>. Because DocumentQA relies on <code>torch</code>, <code>transformers</code>, and <code>vision</code> extras, install the AI optional dependencies first:</p>"},{"location":"tutorials/10-form-field-extraction/#extracting-field-values","title":"Extracting Field Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#visualizing-labels-and-values","title":"Visualizing Labels and Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#handling-multi-line-values","title":"Handling Multi-line Values\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#finding-pattern-based-fields","title":"Finding Pattern-Based Fields\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#working-with-form-tables","title":"Working with Form Tables\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#combining-different-extraction-techniques","title":"Combining Different Extraction Techniques\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#asking-questions-with-layoutlm-documentqa","title":"Asking Questions with LayoutLM (DocumentQA)\u00b6","text":""},{"location":"tutorials/10-form-field-extraction/#optional-one-liner-qa","title":"Optional one-liner QA\u00b6","text":"<p>Need a single field but can't locate the right label?  You can fall back to <code>page.ask()</code> which runs the LayoutLM extractive QA model:</p>"},{"location":"tutorials/10-form-field-extraction/#todo","title":"TODO\u00b6","text":"<ul> <li>Showcase the new <code>init_search</code> workflow for quickly locating form labels across multi-page documents.</li> <li>Compare heuristics for multi-col forms (e.g., left/right alignment vs. table structures) and when to switch strategies.</li> <li>Demonstrate embedding page classification (e.g., \"invoice\" vs \"purchase order\") before field extraction to route documents to the correct template.</li> <li>Provide an end-to-end example saving the extracted dictionary to JSON and a searchable PDF via <code>pdf.save_searchable()</code>.</li> <li>Add a sidebar contrasting extractive QA with generative LLM approaches and notes on when to choose each.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/","title":"Enhanced Table Processing","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Optional fine-tuning for pdfplumber.  Typical tweaks are vertical/horizontal strategies.\nsettings = {\n    \"vertical_strategy\": \"lines\",\n    \"horizontal_strategy\": \"lines\",\n    \"intersection_tolerance\": 3,\n}\n\nrows = page.extract_table(method=\"pdfplumber\", table_settings=settings)\nrows  # \u25b6\ufe0e returns a list of lists\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Optional fine-tuning for pdfplumber.  Typical tweaks are vertical/horizontal strategies. settings = {     \"vertical_strategy\": \"lines\",     \"horizontal_strategy\": \"lines\",     \"intersection_tolerance\": 3, }  rows = page.extract_table(method=\"pdfplumber\", table_settings=settings) rows  # \u25b6\ufe0e returns a list of lists Out[1]: <pre>TableResult(rows=8\u2026)</pre> <p>Expected output: a small list of rows containing the text exactly as it appears in the digital table.</p> In\u00a0[2]: Copied! <pre>settings_text = {\n    \"vertical_strategy\": \"text\",   # look for whitespace gutters\n    \"horizontal_strategy\": \"text\", # group into rows by vertical gaps\n    \"text_x_tolerance\": 2,          # tune for narrow columns\n    \"text_y_tolerance\": 2,\n}\n\nrows_text = page.extract_table(method=\"pdfplumber\", table_settings=settings_text)\n</pre> settings_text = {     \"vertical_strategy\": \"text\",   # look for whitespace gutters     \"horizontal_strategy\": \"text\", # group into rows by vertical gaps     \"text_x_tolerance\": 2,          # tune for narrow columns     \"text_y_tolerance\": 2, }  rows_text = page.extract_table(method=\"pdfplumber\", table_settings=settings_text) <p>Compare <code>rows_text</code> with the earlier <code>rows</code> list\u2014if your PDF omits the grid, the whitespace strategy will usually outperform line-based detection.</p> In\u00a0[3]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# If the page is scanned, run OCR first so each cell has text\npage.apply_ocr(engine=\"easyocr\", languages=[\"en\"], resolution=200)\n\n# Table Transformer needs the layout model; specify device if you have GPU\nrows = page.extract_table(method=\"tatr\")\nrows\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # If the page is scanned, run OCR first so each cell has text page.apply_ocr(engine=\"easyocr\", languages=[\"en\"], resolution=200)  # Table Transformer needs the layout model; specify device if you have GPU rows = page.extract_table(method=\"tatr\") rows <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> Out[3]: <pre>TableResult(rows=0)</pre> <p>Expected output: the table rows\u2014even when the grid is just implied\u2014arrive with text already OCR-corrected.</p> In\u00a0[4]: Copied! <pre># from natural_pdf import PDF\n\n# pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/whitespace-table.pdf\")\n# page = pdf.pages[0]\n\n# rows = page.extract_table(method=\"text\", table_settings={\"min_words_horizontal\": 2})\n# for row in rows:\n#     print(row)\n</pre> # from natural_pdf import PDF  # pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/whitespace-table.pdf\") # page = pdf.pages[0]  # rows = page.extract_table(method=\"text\", table_settings={\"min_words_horizontal\": 2}) # for row in rows: #     print(row) <p>Expected output: printed rows that roughly match the visual columns; best effort on ragged layouts.</p> In\u00a0[5]: Copied! <pre>from natural_pdf.analyzers import Guides\n\npage.detect_lines(resolution=200, source_label=\"detected\", horizontal=True, vertical=True)\n\n# (Optional) visual check\npage.find_all(\"line[source=detected]\").show(group_by=\"orientation\")\n\n# Convert lines \u2192 regions using Guides\nguides = Guides.from_lines(page, source_label=\"detected\")\nguides.build_grid(source=\"detected\", cell_padding=0.5)\n\ntable = page.find(\"region[type='table']\")\n</pre> from natural_pdf.analyzers import Guides  page.detect_lines(resolution=200, source_label=\"detected\", horizontal=True, vertical=True)  # (Optional) visual check page.find_all(\"line[source=detected]\").show(group_by=\"orientation\")  # Convert lines \u2192 regions using Guides guides = Guides.from_lines(page, source_label=\"detected\") guides.build_grid(source=\"detected\", cell_padding=0.5)  table = page.find(\"region[type='table']\")"},{"location":"tutorials/11-enhanced-table-processing/#enhanced-table-processing","title":"Enhanced Table Processing\u00b6","text":"<p>Tables can appear in PDFs in wildly different ways\u2014cleanly tagged in the PDF structure, drawn with ruling lines, or simply implied by visual spacing.  <code>natural-pdf</code> exposes several back-ends under the single method <code>extract_table()</code> so you can choose the strategy that matches your document.</p> <p>Below we walk through the three main options, when to reach for each one, and sample code you can adapt (replace the example PDF URLs with your own files).</p>"},{"location":"tutorials/11-enhanced-table-processing/#1-methodpdfplumber-default","title":"1. <code>method=\"pdfplumber\"</code>  (default)\u00b6","text":"<ul> <li>How it works \u2013 delegates to pdfplumber's ruling-line heuristics; looks for vertical/horizontal lines and whitespace gutters.</li> <li>Best for \u2013 digitally-born PDFs where the table grid is drawn or where columns have consistent whitespace.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example-a-grid-based-line-detection","title":"Example A \u2013 Grid-based (line) detection\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#example-b-whitespace-driven-detection","title":"Example B \u2013 Whitespace-driven detection\u00b6","text":"<p>Sometimes a table is drawn without ruling lines (or the PDF stores them as thick rectangles so the line detector ignores them).  In that case you can switch both strategies to <code>\"text\"</code> so pdfplumber clusters by the gaps between words rather than relying on graphics commands:</p>"},{"location":"tutorials/11-enhanced-table-processing/#2-methodtatr-table-transformer","title":"2. <code>method=\"tatr\"</code>  (Table Transformer)\u00b6","text":"<ul> <li>How it works \u2013 runs Microsoft's Table Transformer (LayoutLM-based) to detect tables, rows and cells visually, then reads the text inside each cell.</li> <li>Best for \u2013 scanned or camera-based documents, or born-digital files where ruling lines are missing/irregular.</li> <li>Dependencies \u2013 requires the AI extra (<code>pip install \"natural-pdf[ai]\"</code>) because it needs <code>torch</code>, <code>transformers</code>, and <code>torchvision</code>.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example","title":"Example\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#3-methodtext-whitespace-heuristic","title":"3. <code>method=\"text\"</code>  (Whitespace heuristic)\u00b6","text":"<ul> <li>How it works \u2013 groups words into lines, then uses whitespace clustering (Jenks breaks) to infer columns; no layout model.</li> <li>Best for \u2013 simple, left-aligned tables with consistent columns but no ruling lines; fastest option.</li> </ul>"},{"location":"tutorials/11-enhanced-table-processing/#example","title":"Example\u00b6","text":""},{"location":"tutorials/11-enhanced-table-processing/#4-lines-first-workflow-when-pdfplumber-misses-rowscols","title":"4. Lines-first workflow (when pdfplumber misses rows/cols)\u00b6","text":"<p>If <code>method=\"pdfplumber\"</code> cannot find the grid, detect lines explicitly and build the table structure yourself.</p>"},{"location":"tutorials/11-enhanced-table-processing/#todo","title":"TODO\u00b6","text":"<ul> <li>Provide a benchmark matrix of speed vs. accuracy for the three methods.</li> <li>Add a snippet showing how to export cell regions directly to a pandas DataFrame.</li> <li>Document edge-cases: rotated tables, merged cells, or header repetition across pages.</li> <li>Include guidance on mixing methods\u2014e.g., run <code>detect_lines</code> first, fall back to <code>text</code> for cells lacking grid.</li> </ul>"},{"location":"tutorials/12-ocr-integration/","title":"OCR Integration for Scanned Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[all]\"\n</pre> #%pip install \"natural-pdf[all]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\n# Load a PDF\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npage = pdf.pages[0]\n\n# Try extracting text without OCR\ntext_without_ocr = page.extract_text()\nf\"Without OCR: {len(text_without_ocr)} characters extracted\"\n</pre> from natural_pdf import PDF  # Load a PDF pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") page = pdf.pages[0]  # Try extracting text without OCR text_without_ocr = page.extract_text() f\"Without OCR: {len(text_without_ocr)} characters extracted\" Out[2]: <pre>'Without OCR: 0 characters extracted'</pre> In\u00a0[3]: Copied! <pre># Apply OCR using the default engine (EasyOCR) for English\npage.apply_ocr(languages=['en'])\n\n# Select all text pieces found by OCR\ntext_elements = page.find_all('text[source=ocr]')\nprint(f\"Found {len(text_elements)} text elements using default OCR\")\n\n# Visualize the elements\ntext_elements.show()\n</pre> # Apply OCR using the default engine (EasyOCR) for English page.apply_ocr(languages=['en'])  # Select all text pieces found by OCR text_elements = page.find_all('text[source=ocr]') print(f\"Found {len(text_elements)} text elements using default OCR\")  # Visualize the elements text_elements.show() <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> <pre>Found 45 text elements using default OCR\n</pre> Out[3]: In\u00a0[4]: Copied! <pre># Visualize confidence scores with gradient colors (auto-detected as quantitative)\ntext_elements.show(group_by='confidence')\n\n# Use different colormaps for better visualization\ntext_elements.show(group_by='confidence', color='viridis')  # Blue to yellow\ntext_elements.show(group_by='confidence', color='plasma')   # Purple to yellow\ntext_elements.show(group_by='confidence', color='RdYlBu')   # Red-yellow-blue\n\n# Focus on a specific confidence range\ntext_elements.show(group_by='confidence', bins=[0.3, 0.8])  # Only show 0.3-0.8 range\n\n# Create custom bins for confidence levels\ntext_elements.show(group_by='confidence', bins=[0, 0.5, 0.8, 1.0])  # Low/medium/high\n</pre> # Visualize confidence scores with gradient colors (auto-detected as quantitative) text_elements.show(group_by='confidence')  # Use different colormaps for better visualization text_elements.show(group_by='confidence', color='viridis')  # Blue to yellow text_elements.show(group_by='confidence', color='plasma')   # Purple to yellow text_elements.show(group_by='confidence', color='RdYlBu')   # Red-yellow-blue  # Focus on a specific confidence range text_elements.show(group_by='confidence', bins=[0.3, 0.8])  # Only show 0.3-0.8 range  # Create custom bins for confidence levels text_elements.show(group_by='confidence', bins=[0, 0.5, 0.8, 1.0])  # Low/medium/high Out[4]: <p>This makes it easy to spot low-confidence OCR results that might need manual review or correction. You'll automatically get a color scale showing the confidence range instead of a discrete legend.</p> In\u00a0[5]: Copied! <pre># Apply OCR using PaddleOCR for English\npage.apply_ocr(engine='paddle', languages=['en'])\nprint(f\"Found {len(page.find_all('text[source=ocr]'))} elements after English OCR.\")\n\n# Apply OCR using PaddleOCR for Chinese\npage.apply_ocr(engine='paddle', languages=['ch'])\nprint(f\"Found {len(page.find_all('text[source=ocr]'))} elements after Chinese OCR.\")\n\ntext_with_ocr = page.extract_text()\nprint(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\")\n</pre> # Apply OCR using PaddleOCR for English page.apply_ocr(engine='paddle', languages=['en']) print(f\"Found {len(page.find_all('text[source=ocr]'))} elements after English OCR.\")  # Apply OCR using PaddleOCR for Chinese page.apply_ocr(engine='paddle', languages=['ch']) print(f\"Found {len(page.find_all('text[source=ocr]'))} elements after Chinese OCR.\")  text_with_ocr = page.extract_text() print(f\"\\nExtracted text after OCR:\\n{text_with_ocr[:150]}...\") <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:715: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n  warnings.warn(warning_message)\n</pre> <pre>Creating model: ('PP-OCRv5_server_det', None)\n</pre> <pre>Using official model (PP-OCRv5_server_det), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Creating model: ('PP-OCRv5_server_rec', None)\n</pre> <pre>Using official model (PP-OCRv5_server_rec), the model files will be automatically downloaded and saved in /Users/soma/.paddlex/official_models.\n</pre> <pre>Found 43 elements after English OCR.\n</pre> <pre>Found 43 elements after Chinese OCR.\n\nExtracted text after OCR:\nJungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\n Site: Durham's Meatpacking Chicago, III.\nDate: February 3, 1905\nViolation Count: 7\nSumm...\n</pre> <p>You can also use <code>.describe()</code> to see a summary of the OCR outcome...</p> In\u00a0[6]: Copied! <pre>page.describe()\n</pre> page.describe() Out[6]: Page 1 Summary <p>Elements:</p> <ul> <li>text: 43 elements</li> <li>image: 1 elements</li> </ul> <p>Text Analysis:</p> <ul> <li>typography:</li> <li>fonts:<ul> <li>OCR: 43</li> </ul> </li> <li>sizes:<ul> <li>29.0pt: 9</li> <li>34.0pt: 9</li> <li>23.0pt: 8</li> <li>32.0pt: 8</li> <li>39.0pt: 3</li> <li>22.0pt: 2</li> <li>31.0pt: 2</li> <li>27.0pt: 1</li> <li>16.0pt: 1</li> </ul> </li> <li>styles: 43 highlight</li> <li>ocr quality:</li> <li>confidence stats:<ul> <li>mean: 0.98</li> <li>min: 0.87</li> <li>max: 1.00</li> </ul> </li> <li>quality distribution:<ul> <li>99%+ (20/43) 47%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591</code></li> <li>95%+ (39/43) 91%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591</code></li> <li>90%+ (42/43) 98%: <code>\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591</code></li> </ul> </li> <li>lowest scoring:<ul> <li> 1: 0.87: \u25a1 </li> <li> 2: 0.91: \u25a1 </li> <li> 3: 0.91: Date: February 3, 1905 </li> <li> 4: 0.93: \u25a1 </li> <li> 5: 0.95: These people could not be shown to the visitor-for the odor ... </li> <li> 6: 0.95: Unsanitary Working Conditions. </li> <li> 7: 0.96: exhibiting - sometimes they would be overlooked for days, ti... </li> <li> 8: 0.96: into the vats; and when they were fished out, there was neve... </li> <li> 9: 0.96: \u25a1 </li> <li> 10: 0.96: Summary: Worst of any, however, were the fertilizer men, and... </li> </ul> </li> </ul> <p>...or <code>.inspect()</code> on the text elements for individual details.</p> In\u00a0[7]: Copied! <pre>page.find_all('text').inspect()\n</pre> page.find_all('text').inspect() Out[7]: Collection Inspection (43 elements) Word Elements text x0 top x1 bottom font_family font_variant size bold italic strike underline highlight source confidence color Jungle Health and Safety Inspection Service 797 70 1131 92 OCR 23 False False False False True ocr 0.98 #000000 INS-UP70N51NCL41R 797 90 972 112 OCR 23 False False False False True ocr 1.00 #000000 Site: Durham's Meatpacking Chicago, III. 97 168 489 197 OCR 29 False False False False True ocr 0.96 #000000 Date: February 3, 1905 97 211 328 240 OCR 29 False False False False True ocr 0.91 #000000 Violation Count: 7 97 251 286 280 OCR 29 False False False False True ocr 0.97 #000000 Summary: Worst of any, however, were the fertilize... 100 296 1051 319 OCR 23 False False False False True ocr 0.96 #000000 These people could not be shown to the visitor-for... 97 325 1061 354 OCR 29 False False False False True ocr 0.95 #000000 visitor at a hundred yards, and as for the other m... 100 363 1016 386 OCR 23 False False False False True ocr 0.97 #000000 some of which there were open vats near the level ... 100 397 1034 419 OCR 23 False False False False True ocr 0.98 #000000 into the vats; and when they were fished out, ther... 100 431 963 453 OCR 23 False False False False True ocr 0.96 #000000 exhibiting - sometimes they would be overlooked fo... 100 465 1027 487 OCR 22 False False False False True ocr 0.96 #000000 to the world as Durham's Pure Leaf Lard! 100 496 483 518 OCR 23 False False False False True ocr 0.97 #000000 Violations 97 765 226 803 OCR 39 False False False False True ocr 1.00 #000000 Description 210 818 337 857 OCR 39 False False False False True ocr 1.00 #000000 Statute 106 821 191 855 OCR 34 False False False False True ocr 1.00 #000000 Level 939 821 1007 855 OCR 34 False False False False True ocr 1.00 #000000 Repeat? 1045 821 1138 855 OCR 34 False False False False True ocr 1.00 #000000 4.12.7 106 863 177 895 OCR 32 False False False False True ocr 1.00 #000000 Critical 941 863 1016 895 OCR 32 False False False False True ocr 1.00 #000000 Unsanitary Working Conditions. 214 866 512 895 OCR 29 False False False False True ocr 0.95 #000000 Serious 938 901 1024 941 OCR 39 False False False False True ocr 1.00 #000000 5.8.3 106 904 166 938 OCR 34 False False False False True ocr 1.00 #000000 Inadequate Protective Equipment. 213 906 534 935 OCR 29 False False False False True ocr 0.97 #000000 \u25a1 1073 945 1107 978 OCR 34 False False False False True ocr 0.91 #000000 6.3.9 106 946 166 980 OCR 34 False False False False True ocr 1.00 #000000 Serious 941 946 1023 978 OCR 32 False False False False True ocr 1.00 #000000 Ineffective Injury Prevention. 213 949 483 978 OCR 29 False False False False True ocr 0.98 #000000 7.1.5 106 987 168 1021 OCR 34 False False False False True ocr 1.00 #000000 \u25a1 1073 987 1107 1021 OCR 34 False False False False True ocr 0.87 #000000 Critical 941 989 1016 1021 OCR 32 False False False False True ocr 1.00 #000000 Showing 30 of 43 elements (pass limit= to see more) In\u00a0[8]: Copied! <pre>import natural_pdf as npdf\n\n# Set global OCR defaults\nnpdf.options.ocr.engine = 'surya'          # Default OCR engine\nnpdf.options.ocr.min_confidence = 0.7      # Default confidence threshold\n\n# Now all OCR calls use these defaults\npdf = npdf.PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\")\npdf.pages[0].apply_ocr()  # Uses: engine='surya', languages=['en', 'es'], min_confidence=0.7\n\n# You can still override defaults for specific calls\npdf.pages[0].apply_ocr(engine='easyocr', languages=['fr'])  # Override engine and languages\n</pre> import natural_pdf as npdf  # Set global OCR defaults npdf.options.ocr.engine = 'surya'          # Default OCR engine npdf.options.ocr.min_confidence = 0.7      # Default confidence threshold  # Now all OCR calls use these defaults pdf = npdf.PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/needs-ocr.pdf\") pdf.pages[0].apply_ocr()  # Uses: engine='surya', languages=['en', 'es'], min_confidence=0.7  # You can still override defaults for specific calls pdf.pages[0].apply_ocr(engine='easyocr', languages=['fr'])  # Override engine and languages <pre>\rDetecting bboxes:   0%|                               | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.09s/it]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.09s/it]</pre> <pre>\n</pre> <pre>\rRecognizing Text:   0%|                              | 0/46 [00:00&lt;?, ?it/s]</pre> <pre>\rRecognizing Text:   2%|\u258d                     | 1/46 [00:09&lt;06:48,  9.08s/it]</pre> <pre>\rRecognizing Text:  13%|\u2588\u2588\u258a                   | 6/46 [00:09&lt;00:45,  1.14s/it]</pre> <pre>\rRecognizing Text:  26%|\u2588\u2588\u2588\u2588\u2588\u258d               | 12/46 [00:09&lt;00:16,  2.11it/s]</pre> <pre>\rRecognizing Text:  43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f           | 20/46 [00:09&lt;00:06,  4.32it/s]</pre> <pre>\rRecognizing Text:  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589          | 24/46 [00:09&lt;00:04,  5.25it/s]</pre> <pre>\rRecognizing Text:  59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e        | 27/46 [00:11&lt;00:04,  4.21it/s]</pre> <pre>\rRecognizing Text:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f       | 29/46 [00:11&lt;00:04,  3.84it/s]</pre> <pre>\rRecognizing Text:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f      | 31/46 [00:13&lt;00:04,  3.03it/s]</pre> <pre>\rRecognizing Text:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c      | 32/46 [00:13&lt;00:04,  3.26it/s]</pre> <pre>\rRecognizing Text:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      | 33/46 [00:13&lt;00:03,  3.51it/s]</pre> <pre>\rRecognizing Text:  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c     | 34/46 [00:14&lt;00:04,  2.57it/s]</pre> <pre>\rRecognizing Text:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589     | 35/46 [00:14&lt;00:04,  2.74it/s]</pre> <pre>\rRecognizing Text:  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d    | 36/46 [00:14&lt;00:03,  2.92it/s]</pre> <pre>\rRecognizing Text:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589    | 37/46 [00:14&lt;00:02,  3.36it/s]</pre> <pre>\rRecognizing Text:  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a   | 39/46 [00:15&lt;00:01,  3.84it/s]</pre> <pre>\rRecognizing Text:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e  | 40/46 [00:16&lt;00:03,  1.80it/s]</pre> <pre>\rRecognizing Text:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 41/46 [00:23&lt;00:10,  2.12s/it]</pre> <pre>\rRecognizing Text:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 42/46 [00:25&lt;00:07,  1.91s/it]</pre> <pre>\rRecognizing Text:  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 43/46 [00:25&lt;00:04,  1.45s/it]</pre> <pre>\rRecognizing Text:  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 45/46 [00:26&lt;00:00,  1.02it/s]</pre> <pre>\rRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46/46 [00:26&lt;00:00,  1.26it/s]</pre> <pre>\rRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46/46 [00:26&lt;00:00,  1.75it/s]</pre> <pre>\n</pre> <pre>[2025-07-04 08:09:24,417] [ WARNING] easyocr.py:71 - Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> Out[8]: <pre>&lt;Page number=1 index=0&gt;</pre> <p>This is especially useful when processing many documents with the same OCR settings, as you don't need to specify the parameters repeatedly.</p> In\u00a0[9]: Copied! <pre>from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions\n\n# Re-apply OCR using EasyOCR with specific options\neasy_opts = EasyOCROptions(\n    paragraph=False,\n)\npage.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)\n\npaddle_opts = PaddleOCROptions()\npage.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)\n\nsurya_opts = SuryaOCROptions()\npage.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts)\n</pre> from natural_pdf.ocr import PaddleOCROptions, EasyOCROptions, SuryaOCROptions  # Re-apply OCR using EasyOCR with specific options easy_opts = EasyOCROptions(     paragraph=False, ) page.apply_ocr(engine='easyocr', languages=['en'], min_confidence=0.1, options=easy_opts)  paddle_opts = PaddleOCROptions() page.apply_ocr(engine='paddle', languages=['en'], options=paddle_opts)  surya_opts = SuryaOCROptions() page.apply_ocr(engine='surya', languages=['en'], min_confidence=0.5, detect_only=True, options=surya_opts) <pre>\rDetecting bboxes:   0%|                               | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.10it/s]</pre> <pre>\rDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.10it/s]</pre> <pre>\n</pre> Out[9]: <pre>&lt;Page number=1 index=0&gt;</pre> In\u00a0[10]: Copied! <pre># Process all pages in the document\n\n# Apply OCR to all pages (example using EasyOCR)\npdf.apply_ocr(engine='easyocr', languages=['en'])\nprint(f\"Applied OCR to {len(pdf.pages)} pages.\")\n\n# Or apply layout analysis to all pages (example using Paddle)\n# pdf.apply_layout(engine='paddle')\n# print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")\n\n# Extract text from all pages (uses OCR results if available)\nall_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")\n\nprint(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\")\n</pre> # Process all pages in the document  # Apply OCR to all pages (example using EasyOCR) pdf.apply_ocr(engine='easyocr', languages=['en']) print(f\"Applied OCR to {len(pdf.pages)} pages.\")  # Or apply layout analysis to all pages (example using Paddle) # pdf.apply_layout(engine='paddle') # print(f\"Applied Layout Analysis to {len(pdf.pages)} pages.\")  # Extract text from all pages (uses OCR results if available) all_text_content = pdf.extract_text(page_separator=\"\\\\n\\\\n---\\\\n\\\\n\")  print(f\"\\nCombined text from all pages:\\n{all_text_content[:500]}...\") <pre>[2025-07-04 08:10:26,802] [ WARNING] text_extraction.py:65 - Ignoring unsupported layout keyword argument: 'page_separator'\n</pre> <pre>Applied OCR to 1 pages.\n\nCombined text from all pages:\nJungle Health and Safety Inspection Service\nViolation Count: 7\nThese people could not be shown to the visitor for the odor of a fertilizer man would scare any ordinary\nsome of which there were open vats near the level of the floor; their peculiar trouble was thattheyfell\ninto the vats; and whentheywere fished out; there was never enough of them left to be worth\nexhibiting sometimestheywould be overlooked for days, till all but the bones of them had gone out\nViolations\nStatute Description Level\n4...\n</pre>"},{"location":"tutorials/12-ocr-integration/#ocr-integration-for-scanned-documents","title":"OCR Integration for Scanned Documents\u00b6","text":"<p>Optical Character Recognition (OCR) allows you to extract text from scanned documents where the text isn't embedded in the PDF. This tutorial demonstrates how to work with scanned documents.</p>"},{"location":"tutorials/12-ocr-integration/#applying-ocr-and-finding-elements","title":"Applying OCR and Finding Elements\u00b6","text":"<p>The core method is <code>page.apply_ocr()</code>. This runs the OCR process and adds <code>TextElement</code> objects to the page. You can specify the engine and languages.</p> <p>Note: Re-applying OCR to the same page or region will automatically remove any previously generated OCR elements for that area before adding the new ones.</p>"},{"location":"tutorials/12-ocr-integration/#visualizing-ocr-confidence-scores","title":"Visualizing OCR Confidence Scores\u00b6","text":"<p>OCR engines provide confidence scores for each detected text element. You can visualize these scores using gradient colors to quickly identify areas that may need attention:</p>"},{"location":"tutorials/12-ocr-integration/#setting-default-ocr-options","title":"Setting Default OCR Options\u00b6","text":"<p>You can set global default OCR options using <code>natural_pdf.options</code>. These defaults will be used automatically when you call <code>apply_ocr()</code> without specifying parameters.</p>"},{"location":"tutorials/12-ocr-integration/#advanced-ocr-configuration","title":"Advanced OCR Configuration\u00b6","text":"<p>For more control, import and use the specific <code>Options</code> class for your chosen engine within the <code>apply_ocr</code> call.</p>"},{"location":"tutorials/12-ocr-integration/#interactive-ocr-correction-debugging","title":"Interactive OCR Correction / Debugging\u00b6","text":"<p>If OCR results aren't perfect, you can use the bundled interactive web application (SPA) to review and correct them.</p> <ol> <li><p>Package the data: After running <code>apply_ocr</code> (or <code>apply_layout</code>), use <code>create_correction_task_package</code> to create a zip file containing the PDF images and detected elements.</p> <pre>from natural_pdf.utils.packaging import create_correction_task_package\n\npage.apply_ocr()\n\ncreate_correction_task_package(pdf, \"correction_package.zip\", overwrite=True)\n</pre> </li> <li><p>Run the SPA: Navigate to the SPA directory within the installed <code>natural_pdf</code> library in your terminal and start a simple web server.</p> </li> <li><p>Use the SPA: Open <code>http://localhost:8000</code> in your browser. Drag the <code>correction_package.zip</code> file onto the page to load the document. You can then click on text elements to correct the OCR results.</p> </li> </ol>"},{"location":"tutorials/12-ocr-integration/#working-with-multiple-pages","title":"Working with Multiple Pages\u00b6","text":"<p>Apply OCR or layout analysis to all pages using the <code>PDF</code> object.</p>"},{"location":"tutorials/12-ocr-integration/#saving-pdfs-with-searchable-text","title":"Saving PDFs with Searchable Text\u00b6","text":"<p>After applying OCR to a PDF, you can save a new version of the PDF where the recognized text is embedded as an invisible layer. This makes the text searchable and copyable in standard PDF viewers.</p> <p>Use the <code>save_searchable()</code> method on the <code>PDF</code></p>"},{"location":"tutorials/12-ocr-integration/#todo","title":"TODO\u00b6","text":"<ul> <li>Add guidance on installing only the OCR engines you need (e.g. <code>pip install \"natural-pdf[ai] easyocr\"</code>) instead of the heavy <code>[all]</code> extra.</li> <li>Show how to use <code>detect_only=True</code> to combine OCR detection with external recognition for higher accuracy (ties into fine-tuning tutorial).</li> <li>Include an example of saving a searchable PDF via <code>pdf.save_searchable(\"output.pdf\")</code> after OCR.</li> <li>Mention <code>resolution</code> parameter trade-offs (speed vs accuracy) when calling <code>apply_ocr</code>.</li> <li>Provide a quick snippet demonstrating <code>.viewer()</code> for interactive visual QC of OCR results.</li> </ul>"},{"location":"tutorials/13-semantic-search/","title":"Semantic Search Across Multiple Documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[search]\"\n</pre> #%pip install \"natural-pdf[search]\" In\u00a0[2]: Copied! <pre>import natural_pdf\n\n# Define the paths to your PDF files\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n]\n\n# Or use glob patterns\n# collection = natural_pdf.PDFCollection(\"pdfs/*.pdf\")\n\n# Create a PDFCollection\ncollection = natural_pdf.PDFCollection(pdf_paths)\nprint(f\"Created collection with {len(collection.pdfs)} PDFs.\")\n</pre> import natural_pdf  # Define the paths to your PDF files pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\" ]  # Or use glob patterns # collection = natural_pdf.PDFCollection(\"pdfs/*.pdf\")  # Create a PDFCollection collection = natural_pdf.PDFCollection(pdf_paths) print(f\"Created collection with {len(collection.pdfs)} PDFs.\") <pre>Created collection with 2 PDFs.\n</pre> In\u00a0[3]: Copied! <pre># Initialize search.\n# index=True will build the serachable database immediately\n# persist=True will save it so you don't need to do it every time\ncollection.init_search(index=True)\nprint(\"Search index initialized.\")\n</pre> # Initialize search. # index=True will build the serachable database immediately # persist=True will save it so you don't need to do it every time collection.init_search(index=True) print(\"Search index initialized.\") <pre>Search index initialized.\n</pre> In\u00a0[4]: Copied! <pre># Perform a search query\nquery = \"american president\"\nresults = collection.find_relevant(query)\n\nprint(f\"Found {len(results)} results for '{query}':\")\n</pre> # Perform a search query query = \"american president\" results = collection.find_relevant(query)  print(f\"Found {len(results)} results for '{query}':\") <pre>Found 6 results for 'american president':\n</pre> In\u00a0[5]: Copied! <pre># Process and display the results\nif results:\n    for i, result in enumerate(results):\n        print(f\"  {i+1}. PDF: {result['pdf_path']}\")\n        print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")\n        # Display a snippet of the content\n        snippet = result.get('content_snippet', '')\n        print(f\"     Snippet: {snippet}...\") \nelse:\n    print(\"  No relevant results found.\")\n\n# You can access the full content if needed via the result object, \n# though 'content_snippet' is usually sufficient for display.\n</pre> # Process and display the results if results:     for i, result in enumerate(results):         print(f\"  {i+1}. PDF: {result['pdf_path']}\")         print(f\"     Page: {result['page_number']} (Score: {result['score']:.4f})\")         # Display a snippet of the content         snippet = result.get('content_snippet', '')         print(f\"     Snippet: {snippet}...\")  else:     print(\"  No relevant results found.\")  # You can access the full content if needed via the result object,  # though 'content_snippet' is usually sufficient for display. <pre>  1. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 2 (Score: -0.8584)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nThe Anasazi (Removed: 1)\nAuthor: Petersen, David. ISBN: 0-516-01121-9 (trade) Published: 1991\nSit...\n  2. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 5 (Score: -0.8661)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000562167 $13.10 11/5/1999 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  3. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\n     Page: 1 (Score: -1.0080)\n     Snippet: Jungle Health and Safety Inspection Service\nINS-UP70N51NCL41R\nSite: Durham\u2019s Meatpacking Chicago, Ill.\nDate: February 3, 1905\nViolation Count: 7\nSummary: Worst of any, however, were the fertilizer men...\n  4. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 4 (Score: -1.0489)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nChildren of the Philippines (Removed: 1)\nAuthor: Kinkade, Sheila, 1962- ISBN: 0-87614-993-X Publi...\n  5. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 3 (Score: -1.0890)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/6/2023 - Copies Removed: 130\nCentennial Place 33170000507600 $19.45 2/21/2000 33554-43170\nAcademy (Charter)\nWas Available -- W...\n  6. PDF: https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\n     Page: 1 (Score: -1.0946)\n     Snippet: Library Weeding Log Atlanta Public Schools\nFrom: 8/1/2017 To: 6/30/2023\n6/12/2023 - Copies Removed: 2\nTristan Strong punches a hole in the sky (Removed: 1)\nAuthor: Mbalia, Kwame. ISBN: 978-1-36803993-...\n</pre> <p>Semantic search allows you to efficiently query large sets of documents to find the most relevant information without needing exact keyword matches, leveraging the meaning and context of your query.</p>"},{"location":"tutorials/13-semantic-search/#semantic-search-across-multiple-documents","title":"Semantic Search Across Multiple Documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to find information relevant to a specific query across all documents, not just within a single one. This tutorial demonstrates how to perform semantic search over a <code>PDFCollection</code>.</p> <p>You can do semantic search with the default install, but for increased performance with LanceDB I recommend installing the search extension.</p>"},{"location":"tutorials/13-semantic-search/#initializing-the-search-index","title":"Initializing the Search Index\u00b6","text":"<p>Before performing a search, you need to initialize the search capabilities for the collection. This involves processing the documents and building an index.</p>"},{"location":"tutorials/13-semantic-search/#performing-a-semantic-search","title":"Performing a Semantic Search\u00b6","text":"<p>Once the index is ready, you can use the <code>find_relevant()</code> method to search for content semantically related to your query.</p>"},{"location":"tutorials/13-semantic-search/#understanding-search-results","title":"Understanding Search Results\u00b6","text":"<p>The <code>find_relevant()</code> method returns a list of dictionaries, each representing a relevant text chunk found in one of the PDFs. Each result includes:</p> <ul> <li><code>pdf_path</code>: The path to the PDF document where the result was found.</li> <li><code>page_number</code>: The page number within the PDF.</li> <li><code>score</code>: A relevance score (higher means more relevant).</li> <li><code>content_snippet</code>: A snippet of the text chunk that matched the query.</li> </ul> <p>In the future we should be able to easily look at the PDF!</p>"},{"location":"tutorials/13-semantic-search/#todo","title":"TODO\u00b6","text":"<ul> <li>Add example for using <code>persist=True</code> and <code>collection_name</code> in <code>init_search</code> to create a persistent on-disk index.</li> <li>Show how to override the embedding model (e.g. <code>embedding_model=\"all-MiniLM-L12-v2\"</code>).</li> <li>Mention <code>top_k</code> and filtering options available through <code>SearchOptions</code> when calling <code>find_relevant</code>.</li> <li>Provide a short snippet on visualising matched pages/elements once highlighting support lands (future feature).</li> <li>Clarify that installing the AI stack (<code>natural-pdf[ai]</code>) also pulls in <code>sentence-transformers</code>, which is needed for in-memory NumPy fallback.</li> </ul>"},{"location":"tutorials/14-categorizing-documents/","title":"Categorizing documents","text":"In\u00a0[1]: Copied! <pre>#%pip install \"natural-pdf[ai]\"\n</pre> #%pip install \"natural-pdf[ai]\" In\u00a0[2]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/cia-doc.pdf\")\npdf.pages.show(columns=6)\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/cia-doc.pdf\") pdf.pages.show(columns=6) <pre>Could get FontBBox from font descriptor because None cannot be parsed as 4 floats\n</pre> Out[2]: In\u00a0[3]: Copied! <pre>pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='vision')\n\nfor page in pdf.pages:\n    print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\")\n</pre> pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='vision')  for page in pdf.pages:     print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\") <pre>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n</pre> <pre>Device set to use mps:0\n</pre> <pre>Page 1 is text - 0.633\nPage 2 is text - 0.957\nPage 3 is text - 0.921\nPage 4 is diagram - 0.895\nPage 5 is diagram - 0.891\nPage 6 is invoice - 0.919\nPage 7 is text - 0.834\nPage 8 is invoice - 0.594\nPage 9 is invoice - 0.971\nPage 10 is invoice - 0.987\nPage 11 is invoice - 0.994\nPage 12 is invoice - 0.992\nPage 13 is text - 0.822\nPage 14 is text - 0.936\nPage 15 is diagram - 0.913\nPage 16 is text - 0.617\nPage 17 is invoice - 0.868\n</pre> <p>How did it do?</p> In\u00a0[4]: Copied! <pre>(\n    pdf.pages\n    .filter(lambda page: page.category == 'diagram')\n    .show(show_category=True)\n)\n</pre> (     pdf.pages     .filter(lambda page: page.category == 'diagram')     .show(show_category=True) ) Out[4]: <p>Looks great! Note that I had to play around with the categories a bit before I got something that worked. Using \"blank\" doesn't ever show up, \"invoice\" did a lot better than \"form,\" etc etc. It's pretty quick and easy to sanity check so you shouldn't have to suffer too much.</p> <p>I can also save just those pages into a new PDF document.</p> In\u00a0[\u00a0]: skip-execution Copied! <pre>(\n    pdf.pages\n    .filter(lambda page: page.category == 'diagram')\n    .save_pdf(\"output.pdf\", original=True)\n)\n</pre> (     pdf.pages     .filter(lambda page: page.category == 'diagram')     .save_pdf(\"output.pdf\", original=True) ) In\u00a0[5]: Copied! <pre>pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='text')\n\nfor page in pdf.pages:\n    print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\")\n</pre> pdf.classify_pages(['diagram', 'text', 'invoice', 'blank'], using='text')  for page in pdf.pages:     print(f\"Page {page.number} is {page.category} - {page.category_confidence:0.3}\") <pre>Device set to use mps:0\n</pre> <pre>Page 1 is text - 0.629\nPage 2 is invoice - 0.468\nPage 3 is invoice - 0.418\nPage 4 is diagram - 0.832\nPage 5 is diagram - 0.669\nPage 6 is text - 0.6\nPage 7 is diagram - 0.463\nPage 8 is text - 0.61\nPage 9 is invoice - 0.647\nPage 10 is invoice - 0.462\nPage 11 is text - 0.462\nPage 12 is text - 0.546\nPage 13 is text - 0.451\nPage 14 is text - 0.388\nPage 15 is diagram - 0.938\nPage 16 is text - 0.603\nPage 17 is text - 0.712\n</pre> <p>How does it compare to our vision option?</p> In\u00a0[6]: Copied! <pre>pdf.pages.filter(lambda page: page.category == 'diagram').show(show_category=True)\n</pre> pdf.pages.filter(lambda page: page.category == 'diagram').show(show_category=True) Out[6]: <p>Yes, you can notice that it's wrong, but more importantly look at the confidence scores. Low scores are your best clue that something might not be perfect (beyond manually checking things, of course).</p> <p>If you're processing documents that are text-heavy you'll have much better luck with a text model as compared to a vision one.</p> In\u00a0[7]: Copied! <pre>import natural_pdf\n\npdf_paths = [\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",\n    \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\"\n]\n\n# Import your PDFs\npdfs = natural_pdf.PDFCollection(pdf_paths)\n\n# Run your classification\npdfs.classify_all(['school', 'business'], using='text')\n</pre> import natural_pdf  pdf_paths = [     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\",     \"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\" ]  # Import your PDFs pdfs = natural_pdf.PDFCollection(pdf_paths)  # Run your classification pdfs.classify_all(['school', 'business'], using='text') Out[7]: <pre>&lt;PDFCollection(count=2)&gt;</pre>"},{"location":"tutorials/14-categorizing-documents/#categorizing-documents","title":"Categorizing documents\u00b6","text":"<p>When working with a collection of PDFs, you might need to automatically categorize pages of PDFs or entire collections of PDFs.</p>"},{"location":"tutorials/14-categorizing-documents/#vision-classification","title":"Vision classification\u00b6","text":"<p>These pages are easily differentiable based on how they look, so we can most likely use a vision model to tell them apart.</p>"},{"location":"tutorials/14-categorizing-documents/#text-classification-default","title":"Text classification (default)\u00b6","text":"<p>By default the search is done using text. It takes the text on the page and feeds it to the classifier along with the categories. Note that you might need to OCR your content first!</p>"},{"location":"tutorials/14-categorizing-documents/#pdf-classification","title":"PDF classification\u00b6","text":"<p>If you want to classify entire PDFs, the process is similar. The only gotcha is you can't use <code>using=\"vision\"</code> with multi-page PDFs (yet?).</p>"},{"location":"visual-debugging/","title":"Visual Debugging","text":"In\u00a0[1]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# View a few elements that contain the word \"Summary\"\npage.find_all('text:contains(\"Summary\")').show()\n\n# Crop-only display of the region below the heading\nheading = page.find('text:bold[size&gt;=12]')\nregion_below = heading.below(height=250)\nregion_below.show(crop=True)\n\n# Colour by element type\npage.find_all('text, rect, line').show(group_by='type')\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # View a few elements that contain the word \"Summary\" page.find_all('text:contains(\"Summary\")').show()  # Crop-only display of the region below the heading heading = page.find('text:bold[size&gt;=12]') region_below = heading.below(height=250) region_below.show(crop=True)  # Colour by element type page.find_all('text, rect, line').show(group_by='type') Out[1]: In\u00a0[2]: Copied! <pre># After applying OCR, visualize confidence scores with gradient colors\npage.apply_ocr()\npage.find_all('text[source=ocr]').show(group_by='confidence')\n\n# Visualize font sizes with gradient colors\npage.find_all('text').show(group_by='size')\n\n# Use different colormaps for better visualization\npage.find_all('text').show(group_by='size', color='plasma')    # Purple to yellow\npage.find_all('text').show(group_by='size', color='viridis')   # Blue to yellow\npage.find_all('text').show(group_by='size', color='coolwarm')  # Blue to red\n\n# Focus on specific ranges\npage.find_all('text').show(group_by='size', bins=[8, 14])      # Only sizes 8-14\npage.find_all('text').show(group_by='size', bins=[8, 12, 16])  # Small/medium/large\n</pre> # After applying OCR, visualize confidence scores with gradient colors page.apply_ocr() page.find_all('text[source=ocr]').show(group_by='confidence')  # Visualize font sizes with gradient colors page.find_all('text').show(group_by='size')  # Use different colormaps for better visualization page.find_all('text').show(group_by='size', color='plasma')    # Purple to yellow page.find_all('text').show(group_by='size', color='viridis')   # Blue to yellow page.find_all('text').show(group_by='size', color='coolwarm')  # Blue to red  # Focus on specific ranges page.find_all('text').show(group_by='size', bins=[8, 14])      # Only sizes 8-14 page.find_all('text').show(group_by='size', bins=[8, 12, 16])  # Small/medium/large <pre>Using CPU. Note: This module is much faster with a GPU.\n</pre> <pre>/Users/soma/Development/natural-pdf/.nox/docs/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n  warnings.warn(warn_msg)\n</pre> Out[2]: <p>This makes it easy to spot patterns in your data - like identifying all the large headings or low-confidence OCR results.</p> <p>Note: When using quantitative data, you'll automatically get a color scale/bar instead of a discrete legend, showing the continuous mapping from values to colors.</p> <p>Tip \u2013 <code>crop=True</code></p> <p>Pass <code>crop=True</code> to <code>.show()</code> (or <code>.render()</code>) when you want the smallest image that still contains all of the selected elements/region. This works for both <code>Region</code> objects and regular <code>ElementCollection</code>s \u2013 perfect for quickly zooming into the exact area you're debugging.</p> In\u00a0[3]: Copied! <pre># Zoom into just the bold text you found\npage.find_all('text:bold').show(crop=True)\n</pre> # Zoom into just the bold text you found page.find_all('text:bold').show(crop=True) Out[3]: In\u00a0[4]: Copied! <pre>from natural_pdf import PDF\n\npdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\")\npage = pdf.pages[0]\n\n# Find different types of elements\nsummary_elements = page.find_all('text:contains(\"Summary\")')\ndate_elements = page.find_all('text:contains(\"Date\")')\nline_elements = page.find_all('line')\n\nwith page.highlights() as h:\n    h.add(summary_elements, label='Summary')\n    h.add(date_elements, label='Date')\n    h.add(line_elements, label='Lines')\n    h.show()\n</pre> from natural_pdf import PDF  pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/01-practice.pdf\") page = pdf.pages[0]  # Find different types of elements summary_elements = page.find_all('text:contains(\"Summary\")') date_elements = page.find_all('text:contains(\"Date\")') line_elements = page.find_all('line')  with page.highlights() as h:     h.add(summary_elements, label='Summary')     h.add(date_elements, label='Date')     h.add(line_elements, label='Lines')     h.show() In\u00a0[5]: Copied! <pre>title = page.find('text:bold[size&gt;=12]')\ntext = page.find('text:contains(\"Critical\")')\nrect = page.find('rect')\n\nwith page.highlights() as h:\n    h.add(title, color=\"red\", label=\"Title\")           # Color name\n    # h.add(title, color=\"#FF0000\", label=\"Title\")        # Hex color\n\n    # Add a label to the highlight (appears in legend)\n    h.add(text, label=\"Critical\")\n\n    # Combine color and label\n    h.add(rect, color=(0, 0, 1), label=\"Box\")\n\n    h.show()\n</pre> title = page.find('text:bold[size&gt;=12]') text = page.find('text:contains(\"Critical\")') rect = page.find('rect')  with page.highlights() as h:     h.add(title, color=\"red\", label=\"Title\")           # Color name     # h.add(title, color=\"#FF0000\", label=\"Title\")        # Hex color      # Add a label to the highlight (appears in legend)     h.add(text, label=\"Critical\")      # Combine color and label     h.add(rect, color=(0, 0, 1), label=\"Box\")      h.show() In\u00a0[6]: Copied! <pre># Find different types of elements\nheadings = page.find_all('text[size&gt;=14]:bold')\ntables = page.find_all('region[type=table]')\n\nwith page.highlights() as h:\n    h.add(headings, color=(0, 0.5, 0, 0.3), label=\"Headings\")\n    h.add(tables, color=(0, 0, 1, 0.2), label=\"Tables\")\n    h.show()\n</pre> # Find different types of elements headings = page.find_all('text[size&gt;=14]:bold') tables = page.find_all('region[type=table]')  with page.highlights() as h:     h.add(headings, color=(0, 0.5, 0, 0.3), label=\"Headings\")     h.add(tables, color=(0, 0, 1, 0.2), label=\"Tables\")     h.show() In\u00a0[7]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Highlight the region\ncontent.show()\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Highlight the region content.show() Out[7]: <p>Or look at just the region by itself</p> In\u00a0[8]: Copied! <pre># Find a title and create a region below it\ntitle = page.find('text:contains(\"Violations\")')\ncontent = title.below(height=200)\n\n# Crop to the region\ncontent.show(crop=True)\n</pre> # Find a title and create a region below it title = page.find('text:contains(\"Violations\")') content = title.below(height=200)  # Crop to the region content.show(crop=True) Out[8]: In\u00a0[9]: Copied! <pre># Analyze and visualize text styles\npage.clear_highlights()\n\npage.analyze_text_styles()\npage.find_all('text').show(group_by='style_label')\n</pre> # Analyze and visualize text styles page.clear_highlights()  page.analyze_text_styles() page.find_all('text').show(group_by='style_label') Out[9]: In\u00a0[10]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\")\npage = pdf.pages[0]\n\nlines = page.find_all('line')\nlines.show(annotate=['width', 'color'], width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/Atlanta_Public_Schools_GA_sample.pdf\") page = pdf.pages[0]  lines = page.find_all('line') lines.show(annotate=['width', 'color'], width=700) Out[10]: <p>Does it get busy? YES.</p> In\u00a0[11]: Copied! <pre>pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\")\npage = pdf.pages[0]\npage.show(width=700)\n</pre> pdf = PDF(\"https://github.com/jsoma/natural-pdf/raw/refs/heads/main/pdfs/0500000US42007.pdf\") page = pdf.pages[0] page.show(width=700) Out[11]: In\u00a0[12]: Copied! <pre>response = page.ask(\"How many votes did Kamala Harris get on Election Day?\")\nresponse\n</pre> response = page.ask(\"How many votes did Kamala Harris get on Election Day?\") response <pre>Device set to use mps\n</pre> Out[12]: <pre>{'question': 'How many votes did Kamala Harris get on Election Day?',\n 'answer': '60',\n 'confidence': 0.31857365369796753,\n 'start': 31,\n 'end': 31,\n 'found': True,\n 'page_num': 0,\n 'source_elements': &lt;ElementCollection[TextElement](count=1)&gt;}</pre> In\u00a0[13]: Copied! <pre>response['source_elements'].show()\n</pre> response['source_elements'].show() Out[13]:"},{"location":"visual-debugging/#visual-debugging","title":"Visual Debugging\u00b6","text":"<p>Sometimes it's hard to understand what's happening when working with PDFs. Natural PDF provides powerful visual debugging tools to help you see what you're extracting.</p>"},{"location":"visual-debugging/#quick-visualization-with-show","title":"Quick Visualization with <code>.show()</code>\u00b6","text":"<p>The fastest way to see what you have selected is to call <code>.show()</code> on the element (or collection/region) that you receive. This simply returns a <code>PIL.Image</code> with highlights that you can display right away.</p>"},{"location":"visual-debugging/#visualizing-quantitative-data","title":"Visualizing Quantitative Data\u00b6","text":"<p>Natural PDF automatically detects when you're working with quantitative data (like confidence scores, sizes, or coordinates) and uses gradient colors instead of categorical colors:</p>"},{"location":"visual-debugging/#adding-multiple-highlights","title":"Adding Multiple Highlights\u00b6","text":"<p>Use the context manager pattern with <code>page.highlights()</code> to show multiple groups of elements together in a single image. This is perfect when you want to visualize different types of elements with different colors and labels.</p>"},{"location":"visual-debugging/#customizing-multiple-highlights","title":"Customizing Multiple Highlights\u00b6","text":"<p>Customize the appearance of multiple highlights using the context manager:</p>"},{"location":"visual-debugging/#highlighting-multiple-collections","title":"Highlighting Multiple Collections\u00b6","text":"<p>Adding an <code>ElementCollection</code> to the context manager applies the highlight to all elements within it. This is great for comparing different types of elements on the same page.</p>"},{"location":"visual-debugging/#viewing-regions","title":"Viewing Regions\u00b6","text":"<p>You can visualise regions to see the exact area you're working with:</p>"},{"location":"visual-debugging/#working-with-text-styles","title":"Working with Text Styles\u00b6","text":"<p>Visualize text styles to understand the document structure:</p>"},{"location":"visual-debugging/#displaying-attributes","title":"Displaying Attributes\u00b6","text":"<p>You can display element attributes directly on the highlights:</p>"},{"location":"visual-debugging/#document-qa-visualization","title":"Document QA Visualization\u00b6","text":"<p>Visualize document QA results:</p>"},{"location":"visual-debugging/#next-steps","title":"Next Steps\u00b6","text":"<p>Now that you know how to visualize PDF content, you might want to explore:</p> <ul> <li>OCR capabilities for working with scanned documents</li> <li>Layout analysis for automatic structure detection</li> <li>Document QA for asking questions directly to your documents</li> </ul>"}]}